Evaluating Search Engines by Clickthrough Data

Jing He and Xiaoming Li

Computer Network and Distributed System Laboratory, Peking University, China

{hejing,lxm}@pku.edu.cn

Abstract. It is no doubt that search is critical to the web. And it will be
of similar importance to the semantic web. Once searching from billions
of objects, it will be impossible to always give a single right result, no
matter how intelligent the search engine is. Instead, a set of possible
results will be provided for the user to choose from. Moreover, if we
consider the trade-off between the system costs of generating a single
right result and a set of possible results, we may choose the latter. This
will naturally lead to the question of how to decide on and present the
set to the user and how to evaluate the outcome.

In this paper, we introduce some new methodology in evaluation of
web search technologies and systems. Historically, the dominant method
for evaluating search engines is the Cranfield paradigm, which employs a
test collection to qualify the systems performance. However, the modern
search engines are much different from the IR systems when the Cranfield
paradigm was proposed: 1) Most modern search engines have much more
features, such as snippets and query suggestions, and the quality of such
features can affect the users utility; 2) The document collections used in
search engines are much larger than ever, so the complete test collection
that contains all query-document judgments is not available. As response
to the above differences and difficulties, the evaluation based on implicit
feedback is a promising alternative employed in IR evaluation. With this
approach, no extra human effort is required to judge the query-document
relevance. Instead, such judgment information can be automatically predicted from real users implicit feedback data. There are three key issues
in this methodology: 1) How to estimate the query-document relevance
and other useful features that useful to qualify the search engine per-
formance; 2) If the complete judgments are not available, how can
we efficiently collect the most critical information from which the system performance can be derived; 3) Because query-document relevance
is not only feature that can affect the performance, how can we integrate
others to be a good metric to predict the system performance. We will
show a set of technologies dealing with these issues.

1 Introduction

Search engine evaluation is critical for improving search techniques. So far, the
dominant method for IR evaluation has been the Cranfield evaluation method.
However, it also has some disadvantages. First, it is extremely labor intensive
to creating relevance judgments. As a result, we often have a limited number of

P.F. Patel-Schneider et al.(Eds.): ISWC 2010, Part II, LNCS 6497, pp. 339354, 2010.
c Springer-Verlag Berlin Heidelberg 2010

J. He and X. Li

queries to experiment with. Second, the Cranfield paradigm was proposed for
evaluating traditional information retrieval (IR) systems, but it cannot reflect
some new features in modern search engines. The modern search engines usually
provide more than a ranked document list, such as snippet and related query
suggestion. The quality of such new features can affect search users utility.

As a very promising alternative, automatic evaluation of retrieval systems
based on the implicit feedback of users has recently been proposed and studied.
One important direction is to leverage the large amount of clickthrough data
from users to evaluate retrieval systems. Since clickthroughs are naturally available when users use a search engine system, we can potentially use this strategy
to evaluate ranking methods without extra human effort. On the other hand, the
clickthrough data can not only reflect the quality of the retrieved documents,
but also some other features of search engines. Both factors make it an attractive
alternative to the traditional Cranfield evaluation method.

In this paper, we introduce two categories of methods of evaluating search
engines based on clickthrough data. The methods of first category are for comparing two search engines. The basic idea of this method is to interleave the
retrieved results from two search engines, and the search engine which gets more
click on its results wins. The methods of second category infer the document
relevance first and utilize the relevance information to evaluate the search en-
gines. The document relevance is usually estimated from a probabilistic click
model, and we can reorder the retrieved documents to more efficiently collect
the relevance information for evaluation. Finally, we propose a new metric that
is able to embed snippet generation quality in the evaluation.

2 Rule Based Methods: Interleaving and Comparing

The basic idea of methods in this category is to interleave the search results
returned by different systems for the same query in a somewhat random manner
and present a merged list of results to the user. The users would then interact
with these results in exactly the same way as they would with normal search
engine results, i.e., they would browse through the list and click on some promising documents to view. The clickthroughs of users would then be recorded and
a system that returned more clicked documents would be judged as a better
systems.

We can easily see that any method in this category consists of two func-
tions: (1) an interleaving function which determines how to combine the ranked
lists of results returned from two systems, and (2) a comparison function which
decides which ranked list is preferred by the user based on the collected user
clickthroughs. In general, the interleaving function and the comparison function
are synchronized to work together to support relative comparison of retrieval
systems. There have been two major methods proposed to instantiate these two
functions: the balanced interleaving method [1] and the team-draft interleaving
method [2].
?

?

?
2.1 Methods

The balanced interleaving method was proposed in [1]. It merges two lists by
taking a document from each list alternatively so as to ensure that the numbers
of documents taken from both lists differ by at most one [1]. Specifically, its
interleaving function works as follows. It starts with randomly selecting a list
(either A or B) as the current list L. It then iteratively pops the top document d
from the current list L and appends d to the merged list M if d is not already in
M. After each iteration, it would update the current list so that it would point
to a different list. This process is repeated until the two lists are empty. To tell
which system (list) performs better, the balanced interleaving method looks at
which documents are clicked by the user and assumes that list A performs better
than B if A has contributed more clicked documents in the stage of constructing
the merged list than B.

Table 1. Example for Interleaving Strategies

(a, b, c, d)
(b, c, a, d)
(a, b, c, d)
(b, a, c, d)

interleaving A
lists

A-B
balanced
B-A
A-B-A-B (a(A), b(B), c(A), d(B))
A-B-B-A (a(A), b(B), c(B), d(A))
B-A-B-A (b(B), a(A), c(B), d(A))
B-A-A-B (b(B), a(A), c(A), d(B))

team
draft

The team-draft interleaving method was proposed in [2] to prevent some bias
in the balanced interleaving method. In each round, it start with randomly selecting a list L(either A or B), and appends M with Ls most preferred document
that has not been in M. And then it turns to the other list and does the same
thing. The rounds continue until all the documents in A and B are in M. To
predict which system(list) performs better, the team-draft method counts the
number of clicked documents selected from list A and B respectively. It assumes
that A performs better than B if there are more clicked documents selected from
(dc  TA), where dc is a
A. Formally, it scores system A by score(A) =
dc
clicked document and  is a binary indicator function.
?

?

?
In Table 1, we show examples of merging lists A and B using balanced and

team-draft function.

The common drawback of both methods is that they are not sensitive to the positions of the clicked documents in the ranked lists. We propose an improvement to
the balanced method to overcome this limitation. Specifically, in this new method
(called preference-based balanced interleaving), we would interleave the ranked
lists in the same way as the balanced method, but make prediction about which
system is better based on a new preference-based comparison function (ppref).

A preference relation between two documents indicates that one document is
more relevant than the other (with respect to a query), which we denote by di >p

J. He and X. Li

dj. It has been shown that some preference relations extracted from clickthrough
data are reliable [3]. We would first try to convert the collected clickthroughs
into a set of preference relations based one two rules: (1) a clicked document
are preferred to the skipped documents above it; (2) a clicked document is more
relevant than the next unclicked one. Both rules have been justified in some
previous work [3]. Now, our key idea is to design a preference-based measure to
score each ranked list by treating these inferred incomplete preference relations
between documents as our golden standard. In this study we use precision of
preference(ppref)[4].

2.2 Evaluation of Interleaving Strategies
In this section, we propose a simulation-based approach for evaluating and comparing different interleaving methods systematically. Our basic idea is to first
systematically generate many test cases, each being a pair of ranked lists of doc-
uments, and then apply each interleaving method to these test cases and evaluate its performance. Thus our overall approach consists of two components: 1)
metrics to be used for measuring the performance of an interleaving method or
comparing two methods, and 2) simulation strategies to be used to generate the
test cases.

Metrics. Intuitively, we would expect an ideal method to merge the two lists
in such a way that we can differentiate the ranking accuracy of the two systems
accurately based on the collected clickthroughs. Thus our first criterion is the
effectiveness of differentiation (EoD) of a method. Moreover, it is also important
that the utility of the merged list from a users perspective is high. Thus a second
criterion that we consider is the utility to users(UtU) of a method.

To quantify the utility for users of an interleaving method, in general, we may
apply any existing IR relevance evaluation measures (MAP is this paper) to
the merged result list generated by the interleaving method. The effectiveness of
differentiation of a method can be measured based on the accuracy of the method
in predicting which system is better. Again, since we use simulation to generate
test cases, we will have available the utilities of the two candidate lists (by MAP
is this paper) and decide which is better. By comparing the prediction result of
an interleaving method with this ground truth, we may measure the accuracy
of the prediction. Since there are three types of results when comparing system
A and B, i.e., (1) System A wins; (2) System A loses; or (3) the two systems
tie. In general, we can associate different costs with different errors. However,
it is not immediately clear how exactly we should set these costs. Leaving this
important question for future research, in this paper, we simply assume that
all correct predictions have zero cost, all tie errors have a cost of 1, and all
opposite predictions have a cost of 2. With these costs, given two candidate
ranked lists and the prediction of an interleaving method, we will be able to
measure the accuracy of the method with the cost of the prediction; a lower cost
would indicate a higher accuracy. Since the cost value is presumably not sensitive
to specific queries, the absolute value of cost is meaningful, but we mainly use
it to compare different interleaving methods in this paper.
?

?

?
Generation of Test Cases. A test case for our purpose consists of the following
elements: (1) two ranked lists of documents of length n: A and B, (2) a set of nr
documents assumed to be relevant, R, and (3) the number of documents a user
is assumed to view, K (K  n). We assume that once a merged list of results
are presented to the user, the user would view sequentially (starting from the
top) K results and click on any relevant document encountered. Thus, given a
merged list of results, the clickthroughs can be uniquely determined based on
R and K. Thus in general, we can use a straightforward strategy to generate a
large random sample to compare different interleaving methods. However, such a
blind test provides limited information on how an interleaving method performs
in different scenarios.

It would be much more informative and useful to compare different interleaving methods in various scenarios such as known item search vs. high-recall search
(modeled by relevant document number nr), comparing two systems that have
similar retrieval results vs. very different retrieval results (modeled by kendalls
, comparing two similar-performance or different-performance retrieval systems
(modeled by relative average precision RAP ) or obtaining clickthroughs from a
patient user vs. an impatient user (modeled by viewing number K). Thus each
such possible scenario should be simulated separately to understand relative
strengths and weaknesses of two methods in these different scenarios. It is possible that we may find out that some method tends to perform better in some
scenarios, while others perform better for other scenarios. We can stop the sampling process when we have sufficient test cases to obtain a relatively reliable
estimate of the UtU and EoD of the interleaving methods being evaluated.

Results. In our experiments, we control the first parameter n by setting it to
10 and vary all the other parameters to simulate different evaluation scenarios.
The ground truth about which system is better is decided based on the average
precision of the top 10 documents for each system. Variations of other parameters
and the scenarios simulated are summarized in Table 2.

We show the results of these three methods in all the different scenarios in
Table 3. Because the UtU results for these three methods are very similar, it is

Table 2. Summary of Evaluation Scenarios

Scenario Variation

Parameter Setting

Known-Item Search nr = 1
Easy Topic
Difficult Topic
Result
High
Similarity Low
Precision High
Similarity Low
User
High
Patience Medium

Prec@10Doc = 0.6
Prec@10Doc = 0.3
0.5 <  < 1.0
1 <  < 0.5
0 < RAP < 0.2
0.3 < RAP
k = 8
k = 5
k = 3

Topic

Low

J. He and X. Li

Table 3. MAP and Cost for Specific Scenarios

Topic Result Prec. K

Cost of Prediction

Preference

Balanced Team

Sim. Sim.
3 0.61(0.24) 0.63(0.25) 0.61(0.24), (0.00, 0.03)
Low Low 5 0.38(0.23) 0.41(0.27) 0.38(0.23), (0.00, 0.07)
8 0.15(0.13) 0.18(0.19) 0.15(0.13), (0.00, 0.17)
3 1.00(0.00) 1.00(0.00) 1.00(0.00), (0.00, 0.00)
Known- Low High 5 0.93(0.06) 0.95(0.08) 0.93(0.06), (0.00, 0.02)
8 0.60(0.24) 0.70(0.32) 0.60(0.24), (0.00, 0.14)
item
3 1.00(0.00) 1.00(0.00) 1.00(0.00), (0.00, 0.00)
High High 5 0.80(0.23) 0.87(0.29) 0.80(0.23), (0.00, 0.08)
8 0.26(0.19) 0.48(0.47) 0.26(0.19), (0.00, 0.46)
3 0.31(0.23) 0.31(0.26)
0.31(0.21), (0.00, 0.00)
Low Low 5 0.26(0.22) 0.27(0.22) 0.11(0.11), (0.58, 0.59)
8 0.35(0.33) 0.27(0.25) 0.13(0.20), (0, 63, 0.52)
3 0.85(0.27) 0.86(0.34) 0.81(0.40), (0.05, 0.06)
Low High 5 0.77(0.36) 0.79(0.42) 0.68(0.55), (0.12, 0.14)
8 0.73(0.44) 0.72(0.47) 0.57(0.64), (0.22, 0.21)
3 0.78(0.24) 0.84(0.31) 0.75(0.47), (0.04, 0.11)
High High 5 0.68(0.37) 0.73(0.56) 0.56(0.35), (0.18, 0.23)
8 0.69(0.48) 0.66(0.57) 0.39(0.36), (0.43, 0.41)
3 0.29(0.22) 0.32(0.24) 0.06(0.08), (0.79, 0.81)
Low Low 5 0.17(0.15) 0.19(0.15) 0.01(0.02), (0.94, 0.95)
8 0.15(0.16) 0.07(0.07) 0.00(0.00), (1.00, 1.00)
3 0.71(0.48) 0.72(0.55) 0.54(0.63), (0.24, 0.25)
Low High 5 0.66(0.46) 0.66(0.53) 0.41(0.50), (0.28, 0.28)
8 0.65(0.42) 0.56(0.43) 0.34(0.48), (0.48, 0.39)
3 0.69(0.31) 0.74(0.56) 0.59(0.36), (0.14, 0.20)
High High 5 0.77(0.35) 0.67(0.60) 0.45(0.35), (0.41, 0.33)
8 0.85(0.38) 0.58(0.57) 0.37(0.35), (0.56, 0.36)

Hard

Easy

omitted here due to the space constraint. In general, the preference method performs better than the other two methods and the balanced method is preferred
to the team-draft method. For known-item search, its always better to use the
balanced or preference method. For searches with more relevant documents, if
users are expected to view very few top documents, the balanced and preference
method is also preferred, but when the user is patient and willing to view more
documents, team-draft and preference method may be more appropriate.

3 Evaluation Based on Click Models

The interleaving method can compare the performance of two ranked list for a
specific query. However, it has two problems. First, we cannot get the confidence
level about the comparison result. Second, it is a little expensive to compare the
search engine pairly to get the relative performance for a large number of search
engines.
?

?

?
To address the these problems, various unsupervised graphical click models
have been recently proposed [5,6,7,8]. Click models connect the document relevance and users behaviors with probability graphical models. They provide a
principled approach to infer relevance of the retrieved documents. In general,
an examined document is inferred to be more relevant if it is clicked. The click
models can not only give the document relevance estimation, but also give how
reliable it is.

3.1 Click Models

Click models usually model two types of search user behaviors: examination and
click. Once a user submitted a query, the search engine returns a ranked list of
snippets, each of which corresponds to a retrieved document (the document list
is denoted as D = {d1, . . . , dM}). Then, the user usually examine these snippets
and click on those which seem to be relevant to his information need. Usually,
the examination and click events on a document di are modeled as two binary
variable (denoted as Ei and Ci respectively). Then the documents relevance are
connected with these events by some hypotheses.

One category of hypotheses define the click behavior. The most commonly
used hypothesis in this category is examination hypothesis [9]. For a document
di, it defines that the click behavior (Ci) depends on both examination (Ei)
and document relevance (rdi). When a document is not examined, it cannot be
clicked. And once it is examined, the probability of click is proportional to its
relevance.

Another category of hypotheses defines the examination behavior. Most existing work has a common assumption called cascade hypothesis, stating that
users examine the document in a top-down manner, i.e., if one document at
rank i is not examined, all documents below it would not be examined. Besides
the cascade hypothesis, click models define different functions determining the
examination probability. The cascade model [10] assumes that the users would
continue examine until the first click, and they stop examining after first click.
The dependent click model [8] assumes that users continue examining until a
click, and the probability of keeping examining after a click depends on the po-
sition. The click chain model [5] assumes that users may stop even when they
does not click, and the probability of keeping examination after click depends on
clicked documents relevance. While the user browsing model [6] does not use the
cascade hypothesis and assumes that the examination probability is determined
by its absolute rank and its distance to the previous clicked document.

All examination and click events can be modeled as nodes in the graphical
click model, and the hypotheses on relationship between these events are modeled
as edges in the graph. In this graphical click model, only the click variables are
observed. The task is to estimate the parameters such as document relevance,
and it usually can be done in EM algorithm (or other more efficient method
for some specific model). With these models, we can estimate the how these
document relevances distribute, so we can not only get the expected relevance
value, but also know how reliable the estimation is. In general, the relevance

J. He and X. Li

estimation for a document is more reliable if it is examined more frequently. It
has shown that click chain model (denoted as CCM) can predict user behavior
accurately with the document relevance it estimates. In the later experiments,
we use CCM to predict the document relevance.

3.2 Efficiently Collecting Relevance Information

With the relevance information collected by a click model, we can use them to
evaluate a search engine with some graded relevance based IR metrics (such
as DCG or RBP ). One challenge of using this method is that the evaluation
results may be questionable for some tail queries, due to the limited number of
query submissions. Intuitively, the retrieved documents contribute differently for
evaluating IR systems. The main idea here is to measure the benefit of collecting
a documents relevance, thus it can guide us collect the relevance of documents
which can bring more benefit.

The first intuition is that the benefit of examining a document is affected by
its relevance uncertainty reduction. For example, if we have been very confident
about the relevance of a document, it provides little information by further examining this document. Therefore, it is a good choice to move up the document
with larger relevance uncertainty reduction. Naturally, the relevance uncertainty
of a document can be measured by the variance of its inferred relevance, i.e.,
the larger variance is, we are more uncertain about the relevance. We can formulate the inferred relevance variance reduction from examining a document di
as follows:

V (ri) = P (ci = 1| ri)1V (ri)
+ P (ci = 0| ri)0V (ri)

(1)

Where ri and ri are the inferred and actual relevance of di respectively; P (ci =
1| ri) and P (ci = 0| ri) are clicking and skipping probability given the actual relevance level ri respectively. According to examination hypothesis, P (ci = 1| ri =
r) = r. Unfortunately, we dont know the exact value of ri in reality, so we approximate by replacing it with inferred relevance ri. 1V (ri) and 0V (ri) are
variance reduction for clicking and skipping cases respectively.

The second intuition is that we should encourage the users to examine deeper,
because it helps to collect more documents relevance information. The users
would generally stop examining when their information need has been satisfied.
To encourage the users to examine deeper, we can delay their satisfaction by
moving the relevant documents down. Though this strategy obviously sacrifices
the user utility, the effect may not be very serious because only a very limited
percent of queries are employed for evaluation purpose in real search engine.
Assuming the ranked list is (d1, . . . , dn), the list benefit function b(d1, . . . , dn)
can be defined as:

b(d1, . . . , dn) =

P (ei = 1)b1(di)

(2)
?

?

?
i
?

?

?
Generally, the users examine the top document, but the probability of examining
deeper documents depends on the documents ranked above. The above document
relevance factor can be plugged in the list benefit function:

b(d1, . . . , dn) = b1(d1) + P (e2 = 1| r1)b(d2, . . . , dn)

Where P (e2| r1) is the probability of continuing examining document d2 given
actual relevance r1.

Obviously, an optimal presented document list is to maximize the benefit
function. Unfortunately, this problem is intractable. We approximate it by a
greedy algorithm: at each step, we select the document that leads to maximal
benefit and append it to the end of the presented list. We approximate the
benefit of examining below by the maximal document benefit of the unselected
documents. Thus, the weight of a document di in an unselected document set D
can be formulated as:

w(di) = b1(di) + P (enext|ri) max
djD
i=j

b1(dj)

(3)

The third intuition is that the highly ranked documents contribute more to the
overall performance score. Most IR metrics model this in their formulas. For
log2(k+1); Thus it
example, in DCG, the document at rank k can weighted as
generally requires to infer relevance of highly ranked document more accurately.
Click models can infer a distribution of document relevance, so the mean and
variance of DCG value distribution can be expressed as:

E(DCG(A)) =

V (DCG(A)) =
?

?

?
diA
diA

E(ri)

log2(Ai + 1)

V (ri)
2(Ai + 1)

log2

In evaluating one search engine, the purpose is to reduce the uncertainty of the
metrics score, so the contribution of each document does not only depend on its
variance reduction but also its original rank. The weight of document at rank k
is 1/ log2

2(k + 1), so the benefit of examining document di is:

b2(di) = V (ri)

log2

2(Ai + 1)

(4)

For list benefit function(Equation 2) and document weight function(Equation 3),
we can replace the document benefit function b1(di) by b2(di).

Finally, in the context of comparing two systems, retrieved documents have
different effect on distinguishing two systems. Therefore, in addressing the comparison problem, it benefits from moving up the documents that are ranked

J. He and X. Li

E(DCGA,B) =

differently. The mean and variance of DCG score difference from two ranked
lists A and B can be expressed as:
?

?

?
diAB

diAB
log2(Si+1) if di  S

otherwise

(WA(di)  WB(di))V (ri)
(WA(di)  WB(di))2V (ri)

V (DCGA,B) =

WS(i) =

We expect to reduce the uncertainty of DCG value, so one documents contribution to the overall performance difference can be formulated as:

b3(di) = (WA(di)  WB(di))2V (ri)

(5)

The corresponding list benefit and document weight can be expressed by replacing b1(di) to b3(di) in Equation 2 and 3 respectively.

3.3 Experiments and Results

The first experiment is a user study, which is designed to resemble the common
search engine usage. We recruited 50 college students to answer ten questions
with the help of our system. These questions contain both open and close questions and vary in difficulty and topic. For each question, we designed a query
and collected 10 results from search engines A and B respectively. A user was
presented with a question following ten ranked snippet results in a page. The
user can answer the question by examining and click the results.

We implemented five reordering functions in our system: three of them presented the results A from one search engine, and two of them interleaved results
A and B from two search engines. For one-system results reordering, the baseline function (base1) presents the results A unchanged. Two other reordering
functions (f un1 and f un2) use benefit function in Equation 1 and Equation
4 respectively. For two-system results reordering, the baseline function(base2)
presents the results A and B alternatively, i.e., balanced interleaving method
used in [1]. The another reordering function(f un3) determines the presented list
using benefit function in Equation 5.

As a golden standard, we asked three assessors to provide the relevance judgments for the results. The relevance level is then normalized into a value between
0 and 1. The engines performance on a query can be measured by DCG based
DCG). As mentioned, DCG can
on the actual relevance judgments(denoted as
also be calculated based on the inferred relevance(denoted as DCG).

For one-system evaluation task, it is measured by the relative difference
DCG and DCG. For two-system comparison task, it

(denoted as rerr) between
is measured by the ratio that predicted comparison result is incorrect.

The results from the user study is presented in Table 4. In the one-system
evaluation results we can find that the both f un1 and f un2 perform much
?

?

?
Table 4. User Study Results

Evaluation Comparison
base1 fun1 fun2 base2 fun3
0.77
1.33

0.27 0.26 C
?

?

?
1.47 1.32
3 1.64 1.97 2.07 C
1.32 0.45 0.69 C
?

?

?
0.43 0.09 0.32
?

?

?
1.61 0.48 0.58 C
7 0.43 0.47 1.24 C

0.24 0.02 C
9 0.01 0.02 0.17 C

0.44 0.26
10 0.47
All 0.87 0.59 0.69
0.30

0.70
?

?

?
0.00

better than base1 on most questions. In the two-system comparison results, C
denotes correct and E denotes error. We find that f un3 can distinguish two
search engines accurately on all questions but base3 can do only 7 out of the 10
questions.

We further conduct a simulation study. The simulation experiment is deployed
in the similar manner as that of Section 2.2. But the user behavior is synthesized
from a click model instead of clicking all relevant documents. For one-system
evaluation problem, we conduct the experiment for base1, f un1 and f un2. For
two-system evaluation problem, besides base2 and f un3, we also test f un1 and
base3.

Fig. 1. One-system Evaluation (Left) and Two-system Comparison (Right)

In base3, it uses the balanced strategy[1] to merge the documents from the
two ranked lists into the presented list. We present the one-system evaluation
and two-system comparison results in Figure 1.

The experimental results suggest that at the first few query submissions, base1
and base3 are good choices for one-system evaluation and two-system compar-
ison. When the query is submitted a little more, f un2 and f un3 are generally
good choices for one-system evaluation and two-system comparison problems
respectively.

J. He and X. Li

4 Beyond Document Relevance

All methods use document relevance to evaluate the quality of information retrieval component. There are some research to investigate the correlation between the Cranfield paradigm experiment and user study. Some recent evaluation
work [11,12] reported the positive correlation between the Cranfield evaluation
based on document relevance and real user study. However, in the use of a real
search engine, the effectiveness and the satisfaction of a user is also affected by
some factors other than retrieval system performance. Our problem is: how to
embed the quality of other components in search engine for the evaluation.

The document snippet is a very important feature for modern search engine.
Good snippets can guide the users to find out the relative documents from the
retrieved results, or even contain relative information itself. On the contrary,
users may miss relevant documents or waste time clicking and examining irrelevant documents due to bad snippets. Turpin et al. [13] investigated this problem
and showed that it makes difference by including snippet quality in search engine
evaluation. In this paper, we interpret traditional IR metric precision as effective
time ratio of the real user, i.e., the ratio between the time used in reading relevant information and the total search time, and extend it in the scenario when
the search engines provide document snippets.

4.1 Effective Time Ratio

Intuitively, for a search engine with snippets, the users satisfaction is affected
by both retrieval system and snippet generation qualities. Precision is one of
most important metrics in IR. In order to derive the metric that can be used
to evaluate search engines with document snippet, we first interpret precision
as effective time ratio (denoted as ET R) when using retrieval system without
snippet presentation.

Definition 1. (Effective Time Ratio): Effective Time Ratio is the ratio between
effective time used in get relevant information and the total search time.

We assume an IR system presents to the user a ranked list of documents for a
query. We further assume the time spent on examining each document is identical
the effective time, which is used to examine relevant documents, is T
(denoted as T ). Thus a user needs T  N time to examine top N documents, but
i=1 R(di),
where di is the i-th ranked document in the result list, and R is a binary function,
indicating relevance of a document. With very simple derivation, we can find that
ET R@N is identical to P @N, so we can interpret precision as effective time ratio
when using the retrieval system without providing document snippets.

However, the modern search engines usually present the users a list of snippets
instead of documents. In this scenario, a user examines i-th snippet, which is
generated from i-th ranked document (i is initially assigned 1). If he finds the
snippet is relevant, he would click and examine the corresponding document;
otherwise, he would examine the next snippet. After examining a document, the
