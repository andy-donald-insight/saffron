Enterprise Data Classification Using Semantic

Web Technologies

David Ben-David1, Tamar Domany2, and Abigail Tarem2

1 Technion  Israel Institute of Technology, Haifa 32000, Israel

2 IBM Research  Haifa, University Campus, Haifa 31905, Israel

davidbd@cs.technion.ac.il

{tamar,abigailt}@il.ibm.com

Abstract. Organizations today collect and store large amounts of data
in various formats and locations. However they are sometimes required
to locate all instances of a certain type of data. Good data classification
allows marking enterprise data in a way that enables quick and efficient
retrieval of information when needed. We introduce a generic, automatic
classification method that exploits Semantic Web technologies to assist
in several phases in the classification process; defining the classification
requirements, performing the classification and representing the results.
Using Semantic Web technologies enables flexible and extensible con-
figuration, centralized management and uniform results. This approach
creates general and maintainable classifications, and enables applying semantic queries, rule languages and inference on the results.

Keywords: Semantic Techniques, RDF, Classification, modeling.

1 Introduction

Organizations today collect and store large amounts of data in various formats
and locations. The data is then consumed in many places, sometimes copied or
cached several times, causing valuable and sensitive business information to be
scattered across many enterprise data stores. When an organization is required
to meet certain legal or regulatory requirements, for instance to comply with
regulations or perform discovery during civil litigation, it becomes necessary to
find all the places where the required data is located. For example, if in order to
comply with a privacy regulation an organization is required to mask all Social
Security Numbers (SSN) when delivering personal information to unauthorized
entities, all the occurrences of SSN must be found. That is when classification
becomes essential.

Data discovery and classification is about finding and marking enterprise data
in a way that enables quick and efficient retrieval of the relevant information when
needed. Since classifying and tagging everything in the organizations data sources
is time-consuming, inefficient and rarely done well, usually organizations need to
choose what to classify, determining the relevant and important pieces of information that need tracking. The classical approach today is to start by examining the

P.F. Patel-Schneider et al.(Eds.): ISWC 2010, Part II, LNCS 6497, pp. 6681, 2010.
c Springer-Verlag Berlin Heidelberg 2010
?

?

?
purpose of the classification and how the classified data is going to be used. The
process consists in manually examining the relevant laws or regulations, identifying which types of information should be considered sensitive and what are the
different sensitivity levels, and then building the classes and classification policy
accordingly. The main drawback of such an approach is that each time the requirements change (e.g., due to changes in existing laws or internal policies or the
addition of new laws) it requires re-classification of the data.

Moreover, enterprise data can be stored in many varied formats: unstructured,
semi-structured and structured data; and is typically stored in various data stores
such as relational databases, file systems, mail servers, etc., sometimes even at
different physical locations (sites) in the organization. Most existing solutions for
automatic data classification apply to a single data type, mainly unstructured
data, and can generally only identify predefined sets of known fields, depending
on the domain. As a result, organizations wishing to classify their data typically
need to deploy and configure several different products and store and manage
their results separately. A major drawback of these traditional solutions is the
lack of common interfaces.

The need to build a generic classification system with manageable results introduces several challenges. First, a common language that can be used as input to
all classifiers must be devised. This language must enable an accurate classification process but also provide flexibility and extensibility to new types of classifi-
cations, classes and data types. In addition, a uniform format for the classification
results must be created so as to maximize their usability and homogeneity.

In this paper we introduce a generic, automatic classification method that
enables classifying data elements from all types, formats and sources based on
a common classification scheme, and making it possible to use the results in
a uniform manner. We suggest a method based on Semantic Web technologies
for classifying any type of data from any domain, by modeling the information
in question as an ontology. According to our approach, a model (or ontology)
provides the means for describing the entities to discover, the classes to map
them to, and information on how to discover them. Such an ontology can serve
as input to several different classifiers, each for a different source of data. The
ontology is then used again as a schema for representing the discovery results,
thus unifying the results across the different sources.

The ontology used as input for the classification process can describe any type
of content. It can be generic, containing information that may apply to many
different organizations and/or requirements, such as knowledge on what identifies a person (PII); it can be specific to a certain domain, such as medications
and treatments for the Healthcare domain; and it can be specific to a certain
organization or even a particular application.

We use the Resource Description Framework (RDF) and its extensions
(RDFS and OWL) to represent the classification models and results. By using
RDF for these purposes we maximize modularity and extensibility and facilitate
easy navigation between the results, the data models and the actual data in the
data sources, as will be described in the following sections.

D. Ben-David, T. Domany, and A. Tarem

Using models, in combination with RDF and additional Semantic Web tools
gives us several advantages. First, it enables decoupling the classification process
from the intended usage, allowing more general and maintainable classifications
with less dependency on changes. Using the model as a common input to all classifiers enables centralized management, auditing and change control. A uniform
format for all classification results enables fast and easy location of the data
when needed, thus increasing its usability. It is then possible to apply semantic
queries on the results, feed them to inference engines to deduce additional rela-
tions, and even apply rule languages such as AIR [13] and TAMI [27] to enforce
and verify compliance with existing policies.

Using an ontological model to describe the input and output of the different
classification algorithms, each working on a different type of data source, enables
decoupling the data descriptions from the actual implemented algorithms. This
design pattern can prove beneficial for many different enterprise data management tasks, data classification being just one example.

The paper is organized as follows: Section 2 describes related work, both
in the area of classification in general, and in the use of RDF in information
management. Section 3 describes the general idea of classification models, as
well as the PII model as a specific example. In Section 4 we discuss general
requirements from classifiers and describe in detail a relational data classifier.
In Section 5 we give a short description of our implementation and some results,
and we conclude in Section 6.

2 Related Work

2.1 Classification

There are many existing algorithms for automatic classification of unstructured
documents, using various techniqes such as pattern maching, keywords, document similarity and different machine learning techniques (Bayesian, Decision
Trees, Resource Limited Immune Classifier System, k-Nearest Neighbor and
more), as well as technologies such as Autonomys Meaning-Based Computing
Technology [2]. Any of the above can be used as part of the classification process.
In many existing works, the classes and the entities to discover are predefined,
deriving from the specific use case and domain of the solution. There are several
works that suggest a way to find and describe the classes. Miler [18] suggests a
method for automatically enriching the classes based on the discovery. Ben-Dor
et al. [3] suggest a way to find the meaningful classes from the content in the
computational biology domain. Taghva at al. [24] suggest to use ontologies to
describe the classes when classifying emails.

Our approach is to enable modelling any type of data to be classified, from
any domain, and to use the same model to describe the classification schema for
all input types, thus simplifying the creation and management of classification
policies across the enterprise.

Most work related to classifying relational data deals with classifying or clustering records based on common patterns or associations. Much less has been
?

?

?
done in the area of classifying based on metadata (tables, columns, types), according to specific categories. The same applies to semi-structured data such as
XML files. One scenario in which we found some similar work is electronic discovery (eDiscovery)1. Butler et al. [7] developed a top level ontology to represent
enterprise data for eDiscovery, and created a semantic mapping that manually
maps the data models of heterogeneous sources into the ontology.

In addition, many existing classification techniques classify only whole documents or assets, and not single data elements. We suggest a method to automaticaly classify data from different sources according to a model, enabling accurate
classification at the level of a single data element.

2.2 Use of Ontologies and RDF in Information Management
The use of Semantic Web methodologies in the world of information manage-
ment, has lately gained much momentum.

The use of ontologies in information and knowledge management has long been
recognized as a natural connection. According to Staab at al. [23], who present
an approach for ontology-based Knowledge Management, ontologies open the
way to move from a document-oriented view of Knowledge Management to a
content-oriented view, where knowledge items are interlinked, combined, and
used. Maedche et al. [17] also present an integrated enterprise-knowledge management architecture, supporting multiple ontologies and ontology evolution.
Ontology-driven information and knowledge management in specific domains
has also been investigated [9].

RDF specifically can also be used to unify data access across multiple sources.
Langegger et al. in their work [16] presented a system that provides access to distributed data sources using Semantic Web technology designed for data sharing
and scientific collaboration. Warren and Davies [26] studied a semantic approach
to information management for integrating heterogeneous information as well as
coping with unstructured information.

RDF has recently begun to be used in the context of data classification. Jenkins et al. [11], use RDF to express the results of classifying HTML documents
using the Dewey Decimal Classification. In addition, Xiaoyue and Rujiang [28]
showed how using RDF ontologies to perform concept-based classification of text
documents significantly improved text classification performance.

We use RDF to express both the input and the output of the classification
process, thus enabling high connectivity and easy navigation between the models,
the results and the original data sources and providing a unified representation
of all classified data sources.

3 Classification Models

The main goal of our solution is to enable organizations to annotate their knowledge bases with semantic meaning. Our approach offers to automate the classification process using classification models, which are ontologies that describe
1 Discovery in civil litigation that deals with information in electronic format.

D. Ben-David, T. Domany, and A. Tarem

entities in the organizations knowledge base and their relationships. These models can be generic, domain-specific or even organization-specific.

The classification process is performed at the single data element level and
results in bi-directional references between the individual data parcels and terms
in the model. This method enables the classify once, enforce many approach
for future application of policies and other semantic applications.

The Data Discovery and Classification process involves four steps:

1. Defining and identifying the data to discover (the valuable pieces of infor-

mation to locate).

2. Building a classification scheme. One example of a common classification

scheme is top secret, secret, confidential, etc.

3. Discovering where the valuable data is located.
4. Documenting the findings in a useful manner.

We offer to use the same model for several purposes throughout the discovery
and classification process: the description of what entities to search for (step 1),
the classification scheme to which the findings are associated (step 2), directives
on how to search for each entity (input to step 3) and the schema for representing
the discovery results (step 4).

Selecting RDF as the language in which to represent the classification model
enables us to exploit existing and evolving tools for annotating, reasoning, query-
ing, etc. the classified data. We can take advantage of common vocabularies
(RDFS, OWL) to express stronger relations and properties, best suited for on-
tologies, and benefit from the powerful features of RDF that makes it easy to
expand, merge and combine existing classification models, and generate new
models for different purposes.

3.1 Models as Input

The classification models are used as input to an automatic classification tool
(henceforth: Classifier), which is the software component that performs the actual mapping. Our goal is to have a general-purpose classifier, capable of handling any domain-specific knowledge, and so the input for the classifier must be
designed to answer not only what to search for, but also how to search. This
means that in addition to functioning as an ontology, the classification model
must also contain additional characteristics of the entities such as type, format
and possible synonyms. This allows full automation of the classification process
by use of natural language processing and information retrieval techniques.

To implement such a general-purpose classifier, a need emerged for a metamodel to describe the expected structure and contents of a valid classification
model. An input model complying with this meta-model is regarded as an instance of the meta-model. The same meta-model serves for describing models
for all data types; the specifics of searching in each data format are encapsulated
in the classifiers implementation.
?

?

?
3.2 The Meta-model

We designed a preliminary meta-model for classification models used as input to
our system implementation, which is depicted in Figure 1. The top-most element
in the classification meta-model is the subject element that is the root node of
any domain-specific ontology. A subject can possibly contain several categories,
and each category can hold several fields, which are the basic terms linked by
the classifier to data units in the organizations data sources. Each field can be
associated with a data type and format (e.g., denoted by a regular expression),
and one or more synonym elements, which represent keywords or phrases that
can be used to describe the field. Each keyword (or token) in a synonym can
be stated as optional, required or exact. These synonyms are processed by the
classifier to create different search strings used to search for potential matching
elements in the data stores. We elaborate on this matter in section 4.2.

The meta-model also supports mapping between a field in the classification
model and one or more actual data elements. This mapping, obtained as the
result of the classification process, is also represented in the model.

Fig. 1. The classification meta-model

3.3 Models as Output

The classification results, which are basically a mapping between fields in the
model and specific data elements, are also saved in RDF format, as an extension
of the models used to perform the search (i.e. using the same vocabulary defined
in the meta-model). They are composed of triples, each triple representing a
link between a classification field and a data element. The mapping uses the
data elements URI, which differs according to the type of data and the manner
in which it is represented in RDF. For example, in relational databases the
mappings are to table columns, and the URIs used are those of the database
server, schema, table and column. In XML files, the mappings are to specific
XPaths, and the URIs are those representing the XML file location (e.g. in a file

D. Ben-David, T. Domany, and A. Tarem

system or Content Management system) and the specific elements XPath. In
unstructured data, the mapping is to a word or group of words in a document,
and the URIs represent the location of the document and the location of the
word/s in the document (e.g. chapter/page, paragraph, sentence, etc.).

The advantages in using RDF to represent the classification results are many:
1. Different results from different runs can be linked together to form one unified classification source containing all discovered entities (a concept demonstrated by Langegger at al. in [16]). If a model has been extended or a new
model introduced, a partial search (only on the new parts) can be peerformed
and the results combined with the existing ones. It is also possible to express
information both on the location of the classified data (such as a database
column or XPath), as well as the data itself (an actual value).

2. Thanks to the use of URIs to describe the different resources, it is very easy
to navigate between the discovery results, the classification models and the
RDF representations of the searched data sources. In this way, while accessing the results, additional information about the model or data source can
be accessed to verify that the results are correct, add new links or add new
entities to the models to improve future discovery. It is also possible to obtain additional information about certain resources from external knowledge-
bases.

3. The use of RDF allows performing semantic queries on the results. Many existing query languages for the RDF language (such as SPARQL [19], RDQL
[20] and many more) can be utilized to extract useful information from
the classification. This information may be used to verify the correctness
of the results or even learn new insights that can be used to further enrich
the models and/or the search algorithms to improve the discovery process.
4. Inference (or reasoning) can be applied to the existing RDF triples to derive
additional RDF assertions that are not explicitly stated. These new assertions are inferred from the base RDF together with any optional ontology
information, axioms or rules. There have also been a few attempts to bridge
the gap between RDF and logic programming and languages [6]. N3logic [4]
provides a logical framework to express logic in RDF, enabling the use of
rules to make inferences, choose courses of action, and answer questions.

5. Several policy and rule expression languages for RDF (such as AIR [13],
KAoS [25], and Rei policy language [12], etc.) as well as tools for checking
compliance with policies (such as TAMI [27]) exist, enabling the enforcement
of privacy rules or any other business policies on the discovered data.

3.4 Example: PII Model
As a sample use case, we may consider the field of information security. In modern
times, organizations retain an explosively growing volume of sensitive data to
which access is available to an expanding user base. The growing public concern
over the security and privacy of personal data has placed these organizations in
the spotlight, and countries around the world are developing laws and regulations
designed to support confidentiality.
?

?

?
Fig. 2. Partial PII RDF Model

Personally Identifiable Information (PII) refers to information that can be
used  whether alone or in combination with other personal or identifying information  to uniquely identify a single individual. For example, national identification numbers, vehicle registration plates and fingerprints are often used to
distinguish individual identities and are clearly PII. Even less identifying characteristics such as city of residence, gender and age are considered PII for the
potential that when joined together they might be linked with other public available information to identify an individual.

By successfully identifying and classifying PII in an organizations data, it is
possible to apply virtually any privacy policy. Figure 2 displays a partial view
of the PII model we have developed. The root element is Person, and it contains
categories such as PersonName and Address. Each category contains fields such
as firstName, middleName and lastName.

3.5 Advantages of Classification Using Models

There are several advantages for using ontologies in general, and RDF in partic-
ular, in the discovery and classification process:

 Extensibility and Flexibility: The model can be extended both by adding
more entities to search for and by adding additional tips and hints to aid
the discovery of existing entities. Since the information on how to search for
the entities is included in the model that is used as input to the discovery
algorithm, the model can be extended, edited or replaced without the need
to change the discovery algorithm itself.

 Linking to external knowledge bases: The models used to perform the
classification can themselves be derived from existing business models and/or
domain- or industry-specific models and ontologies. Furthermore, additional
knowledge from external ontologies or taxonomies can be linked into the
model, enabling automatic extension of the model, addition of new syn-
onyms, related terms, etc.

 Resistance to policy changes: By classifying data sources according to
specific classes of information (first name, phone number. . . ) as opposed

D. Ben-David, T. Domany, and A. Tarem

to general, abstract categories (confidential, top-secret. . . ) we can use the
same classification to enforce several different policies and no reclassification
is needed in case of policy changes.

 Homogeneity: By using a model containing the sensitive entities and classification schema as input, the same model can be used as input to many
different classifiers that classify different types of data sources (unstructured
documents, forms, emails, XML files, relational data, etc.). By using the
same output format, we unify the classification results and improve their
usability. For example, privacy policies can be enforced in a unified manner
across all data types and formats in the organization.

 Centralization: Everything is linked through the use of unique IDs. This
allows the model to serve as a centralized point to manage all valuable information in the organization and enables easy location of all related pieces of
data in one click. On the other hand, given the unique ID of data location,
we can easily find its classification.

4 Classifier

Once the models describing the data the organization wishes to classify are
designed, these models are used as input to a software component that performs
the classification of the actual data stores. This component is referred to as the
Classifier. In this section we will describe the approach in general and a relational
data classifier in more detail.

4.1 Classifier Inputs
The classifier receives as input RDF files of two types: the first type is the RDF
files representing the models described in the previous section; the second type
is RDF files representing the data to be searched, for example, the schema of a
database. The classifier performs a search for relevant items in the data schemas,
refines these results according to several filters, and creates a new RDF file
containing the search results.

Using one or more models to perform the search. New models can be
created as needed for domain or organization-specific classifications and existing
models can be extended. Any combination of models can be used as input when
performing the search on a data source, and searches may be performed on
different combinations of models to yield results for different purposes (such
as privacy enforcement, retrieving specific information within a set timeframe
in order to meet regulations, de-duplication and increasing data quality, etc.).
Each model used as input to the classifier must be an instance of the classification
meta-model described in Section 3.2.

RDF representation of data source schemas. Organizations today have
many different data sources, which may include structured data (e.g., relational
databases), semi-structured data (e.g., XML files), and unstructured data (e.g.,
?

?

?
e-mail archives and text documents). RDF provides a means to represent different types of schemas in a unified manner in order to perform semantic queries on
them. This is what allows us to perform the classification in a uniform manner
on all data sources, without the need for any prior knowledge about the specific
underlying schemas.

Much work has been done in the domain of representing relational database
metadata in RDF format. RelationalOWL [15] is an OWL-based representation
format for relational data and schema components. The D2RQ API [5] provides
access to relational database content from within the Jena and Sesame RDF
frameworks.

Some work has been done in the direction of transforming XML schemas and
instances to RDF format, however a completely generic technique for fully representing any XML file in RDF has still not been found at the time of writing this
article. Existing work has been focused on either transforming RDF-enabled
XML files to RDF or representating any XML document in RDF based on a
simplified RDF syntax. These techniques, described in [22] and demonstrated
in the XML2OWL2, XSD2OWL and XML2RDF3 tools, use XSLT to perform
the transformations. Many additional implementations also exist. We believe
that using a combination of such existing tools, possibly also extending them if
needed, can be sufficient to provide adequate representations of almost all XMLs
in an organizations data stores and enable meaningful classification of these.

Regarding unstructured data, the search techniques must of course differ,
since there is no underlying data schema. A great deal of work has been done
on classifying unstructured data: several tools, such as SystemT [14], InfoSphere
Classification Module (ICM)4, RSA Data Loss Prevention Suite5 and Symantec
Data Loss Prevention product6 analyze and extract information from unstructured text such as documents and e-mails. Many new methods for efficiently
classifying unstructured text have been examined [21], and some domain-specific
techniques have been developed.

We believe that classifying unstructured data based on classification models
can greatly enhance the existing techniques for the classification of this type of
data, since it enables a clear separation between the data classification phase and
the policy enforcement phase. Moreover, representing the classification results in
RDF format enables high connectivity between the search results on the different
data types, and increases the variety of possible uses for these results, a concept
that has also been used in [1].

4.2 Performing the Search
The model used as input to the discovery process describes the entities for which
to search and how to search for them. For each such entity there is a list of

http://www.avt.rwth-aachen.de/AVT/index.php?id=524
http://rhizomik.net/html/redefer/
http://www-01.ibm.com/software/data/content-management/classification
http://www.rsa.com/node.aspx?id=3426
http://www.symantec.com/business/data-loss-prevention

D. Ben-David, T. Domany, and A. Tarem

synonyms that describe the concept, and for each synonym we can indicate
which words (or tokens) in the phrase are required, which are optional and
which need to appear in the same exact form. These synonyms are used to create
several search strings to compare to the data stores metadata. In the example of
relational databases, they are compared to the database table and column names.
For example, for the term family name, the additional synonyms second
name and surname can be stated, and for the synonym second name we
can indicate that the word second is required but name is optional. In
addition an entity can have one or more types associated with it and additional
information on the expected format of the data. These are used at runtime to
verify that a column suspected of containing a certain data element from the
model indeed matches the expected type and format for that element.

During the discovery process, for each data element in the input models, a
list of search strings is created from the synonyms defined for that element.
The aim is to cover all common abbreviations and hyphenations that may be
used to describe that data element in the data sources. The algorithm uses
syllabifying techniques to divide each token (which is not required to appear
exactly) into syllables, creates different possible substrings representing each
syllable according to specific rules, and then uses all possible combinations of
the substrings and tokens to search the metadata representation for matching
data. In this phase, in addition to syllabification, additional linguistic techniques
can be employed, such as stemming and affix stripping.

5 Implementation

We developed a reference implementation for model based discovery and classification in relational databases. We used the PII model described above as our
test case, but any model that complies with the meta-model can be used just
as easily. We chose the RelationalOWL ontology (with some slight changes) and
implementation to transform the database metadata to RDF format. The current implementation uses the syllabifying techniques described in Section 4.2,
as well as type checking. Future extensions are planned to include the use of
additional linguistic tools (such as stemming) and the use of sample content to
verify the datas format.

5.1 NeOn-Based Implementation

As a basis for this implementation we chose to use the NeOn toolkit [10], which is
an open-source ontology engineering environment based on the Eclipse platform.
The toolkit provides an extensive set of plug-ins covering different aspects of ontology engineering. In addition we used the Eclipse Data Tools Platform (DTP)7
for defining connections with local and remote databases, RelationalOWL [15]
for creating an RDF representation of the database metadata, and the Jena

http://www.eclipse.org/datatools
