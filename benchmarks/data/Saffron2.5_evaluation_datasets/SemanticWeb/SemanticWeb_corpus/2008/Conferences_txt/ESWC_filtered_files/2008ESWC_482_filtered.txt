The Creation and Evaluation of

iSPARQL Strategies for Matchmaking

Christoph Kiefer and Abraham Bernstein

Department of Informatics, University of Zurich, Switzerland

{kiefer,bernstein}@ifi.uzh.ch

Abstract. This research explores a new method for Semantic Web service matchmaking based on iSPARQL strategies, which enables to query
the Semantic Web with techniques from traditional information retrieval.
The strategies for matchmaking that we developed and evaluated can
make use of a plethora of similarity measures and combination functions from SimPackour library of similarity measures. We show how
our combination of structured and imprecise querying can be used to
perform hybrid Semantic Web service matchmaking. We analyze our approach thoroughly on a large OWL-S service test collection and show
how our initial strategies can be improved by applying machine learning
algorithms to result in very effective strategies for matchmaking.

1 Introduction

Imagine the following situation: Kate, a young and successful Semantic Web
researcher, is trying to find a set of services to invoke for her latest project.
Unfortunately, her department does not offer any useful semantically annotated
services. She decides to look for some suitable services in a large database of
crawled OWL ontologies that her department has gathered. Two issues arise:
(1) as the task requires searching, she probably wants to use SPARQL; and (2),
because there does not seem to be an ultimate, widely-accepted ontology for the
domain, she will also have to consider approximate matches to her queriesa
task for which statistics-based techniques from traditional information retrieval
(IR) are well-suited.

In this paper, we address the task of matching a given user request with
available information in a particular knowledge base (KB) assuming the lack of a
perfect domain modelone of the basic underlying assumptions in Semantic Web
research. More precisely, for a given input service request (i.e., Kates preference
criteria), we aim to find all the relevant services that are closest to the query.

While many matchmaking algorithms have been proposed [11,14,17,18,24],
we suggest not to build a specialized matchmaker (algorithm) but to build a
matchmaker out of off-the-shelf components that embeds both structured (i.e.,
logical) as well as statistical elements. Our method is based on iSPARQL queries
(i.e., iSPARQL strategies) [13], which enable the user to query the Semantic
Web with a SPARQL extension that incorporates the notion of similarity
an approach often used in traditional IR. The strategies for matchmaking that

S. Bechhofer et al.(Eds.): ESWC 2008, LNCS 5021, pp. 463477, 2008.
c Springer-Verlag Berlin Heidelberg 2008

C. Kiefer and A. Bernstein

we developed and evaluate in this paper make use of a plethora of similarity
measures and combination functions from SimPackour library of similarity
measures for the use in ontologies.1

We show how simple it is to find well-performing matchmaking strategies in an
iterative and playful procedure. Furthermore, we show how the performance of
the initial matchmaking strategies can be greatly improved by applying standard
machine learning algorithms such as regression, decision trees, or support vector
machines (SVMs) to result in very effective strategies for matchmaking.

To evaluate our approach, we took a large collection of OWL-S services and
compared the effectiveness of a number of simply constructed and learned/in-
duced iSPARQL strategies in the search of the most effective strategies. We found
that some simple-to-construct strategies performed surprisingly well, whilst a
simple extension of those strategies based on machine learning outperformed
even one of the most sophisticated matchmakers currently available.

The paper is structured as follows: next, we introduce the most important related work, before we give a brief overview of our iMatcher approach for matchmaking in Section 3. Section 4 discusses the performance measures and data sets
used in the evaluation. The results of our experiments are presented in Section 5.
We close the paper with a discussion of the results, our conclusions, and some
insights into future work.

2 Related Work

In 1999, Sycara et al. [24] proposed LARKSthe Language for Advertisement
and Request for Knowledge Sharing. The language defines a specification for
the input and output parameters of services (among others) as well as for the
constraints on these values. Their matchmaking process contains a similarity
filter that computes the distances between service descriptions using TF-IDF [1]
and other ontology-based similarity measures.

Three years later, Paolucci et al. [18] continued the work of [24] focusing
on DAML-S.2 One of the major differences between the LARKS and DAML-S
approach is that the latter solely relies on logic and ontological reasoning.

Also related is the work of Di Noia et al. [17] who proposed a service matchmaking approach based on CLASSIC [4]. They discuss a purely logic-based implementation that matches service demands and supplies based on their explicit
normal form (i.e., demands and supplies are terminologically unfolded into their
names, number restrictions, and universal role quantifications). The matchmaking algorithm then distinguishes between potential and partial matches of demands and supplies (i.e., matches with no conflicts and matches with conflicting
properties of demands and supplies).

In 2005, Jaeger et al. [11] presented the OWLSM approach for matching service inputs, service outputs, a service category, and user-defined service matchmaking criteria. The four individual matching scores are aggregated resulting in
1 http://www.ifi.uzh.ch/ddis/simpack.html
2 http://www.daml.org/
?

?

?
an overall matchmaking score. The result to the user is a ranked list of relevant
services.

Recently, Klusch et al. [14] introduced the OWLS-MX matchmaker. As its
name already implies, OWLS-MX focuses on the matching of services described
in OWL-S, the successor of LARKS and DAML-S. The main focus of the matcher
lies on a service profiles input and output (I/O) parameters. In comparison to
the approaches proposed in [17,18], OWLS-MX uses both logic-based as well as
IR-based matching criteria to identify services which match with a given query
service. Specifically, OWLS-MX successively applies five different matchmaking filters: exact, plug in, subsumes, subsumed-by, and nearest-neighbor. Among
those, the first three are purely logic-based, whereas subsumed-by and nearestneighbor are hybrid as they incorporate a similarity measure to compute the
syntactic similarity between query and service I/O concept terms. As such, the
hybrid filters let some syntactically similar, but logically disjoint services be included in the answer set of a query, and thus, help improve query precision and
recall. To compute the similarities, OWLS-MX particularly relies on similarity
measures which have been shown to perform well in information retrieval [6].

A similar approach to [14] is described by Bianchini et al. [3], in which
they presented the FC-MATCH hybrid matchmaking algorithm that is based
on WordNet and the Dice coefficient [8].3

Finally, we would like to mention two studies that are similar to iSPARQLthe
approach our matchmaker is based on. First, Corby et al. [7] introduced the conceptual graph-based Corese search engine that defines an RDF query language
enabling both ontological and structural approximations.4 Ontological approximation deals with distances between nodes in ontologies, whereas structural approximation is about structural divergences between the query and the queried
RDF data set. Especially the latter allows the user to search for resources related
by an arbitrary relations path between them. Second, Zhang et al. [27] presented
the SPARQL-based Semplore system that combines structured querying with keyword searches using existing IR indexing structures and engines.

3 The iMatcher Approach

In this section we explain how our proposed matchmaking approach called
iMatcher works. To that end, we first introduce the underlying iSPARQL technology and then explain the functionality of iMatcher itself.

3.1 Foundations: iSPARQL Matchmaking Strategies

The matchmaking approach we present in this paper is based on our previous
work on iSPARQL [13]. We, therefore, succinctly review the most fundamental concepts. In iSPARQL we make use of ARQ property functions to apply
similarity operators to SPARQL query elements.5 The concept behind property
3 WordNet lexical database of English, http://wordnet.princeton.edu/
4 Conceptual graphs working draft, http://www.jfsowa.com/cg/cgstand.htm
5 http://jena.sourceforge.net/ARQ/extension.html#propertyFunctions

C. Kiefer and A. Bernstein

{ # Basic graph p a t t e r n ( BGP ) m a t c h i n g part ( lines 4 -8)

<P1 > profile : name ? name1 ;
profile : desc ? desc1 .
profile : name ? name2 ;
profile : desc ? desc2 .

? p2

1     	  isparql : < java : isparql . >
2     	  profile : < http :// www . daml . org / services / owl - s /1.1/ Profile . owl # >

4       ? p2 ? sim1 ? sim2 ? sim
5     
?

?

?
{ ? sim1 isparql : n a m e S i m i l a r i t y ( ? name1 ? name2 ) .
? sim2 isparql : t e x t S i m i l a r i t y ( ? desc1 ? desc2 ) .
? sim

# V i r t u a l t r i p l e p a t t e r n m a t c h i n g part ( lines 12 -14)
	      	  

isparql : a g g r e g a t e

( 0.3 ? sim1 0.7 ? sim2 ) .

}

}           (? sim )

Listing 1.1. Example iSPARQL matchmaking strategy

functions is simple: whenever the predicate of a triple pattern is prefixed with
a special name (e.g., isparql), a call to an external similarity function is made
and arguments are passed to the function (in our case by the object of the
triple pattern). The similarity between the arguments is computed and bound
to the subject variable of the triple pattern. As an example, consider the simple iSPARQL strategy in Listing 1.1 that matches the service profile P 1 to the
following RDF data set (note that P 1, P 2, and P 3 stand for particular URLs,
e.g., http://example.org/grocerystore food service.owls#GROCERYSTORE
FOOD PROFILE):

D = { (P 1,
(P 1,
(P 2,
(P 2,
(P 3,
(P 3,

profile:name,
profile:desc,
profile:name,
profile:desc,
profile:name,
profile:desc,

FoodService),

Returns food of grocery store),

StoreService),

This service returns store food,
GroceryFoodService),
A grocery service selling food) }

We briefly explain the evaluation procedure of this strategy. Refer to [13] for
a more elaborate description of this procedure. In a nutshell, the evaluation includes the following two steps: first, the basic graph pattern (BGP) matching
part (lines 710) returns the Cartesian product of the sets of bindings from the
defined variables; and second, the evaluation of the virtual triple patterns (lines
1416) computes the similarities between the passed arguments and merges them
with the solutions from BGP matching. Applying simple name and text similarity measures from SimPacka library of similarity measures which iSPARQL
usesthis could end up in the similarities scp1,p2 = 0.30.571+0.70.667 = 0.8095
for P 1 and P 2, and scp1,p3 = 0.3  0.8 + 0.7  0.189 = 0.372 for P 1 and P 3, being
P 2 the most accurate service profile for P 1 (note that scp1,p1 = 1.0).

It is important to note that the discovery phase of services is potentially very
costly with our implementation as we do not optimize our iSPARQL strategies.
Our queries likely result in the evaluation of expensive cross joins of triples as
well as the (possibly) repeated computation of similarities in the evaluation of
the virtual triple patterns. The first issue can be addressed with iSPARQL query
?

?

?
optimization, which we investigated in [2,22]. The second issue, the optimization
of virtual graph patterns inside an IMPRECISE clause, can be addressed with
similarity indexes to cache repeated similarity computationsan issue which we
have not addressed so far.

3.2 The iMatcher Procedure

Our proposed matchmaking system is called iMatcher.6 The i stands for imprecise emphasizing its ability for approximate matching using techniques from
information retrieval (IR).
iMatcher performs hybrid Semantic Web service
matchmaking. That is, it uses both logic- and IR-based techniques to search
for suitable services, which makes it similar to other hybrid systems such as
OWLS-MX [14] and FC-MATCH [3]. On the one hand, iMatcher uses logical
inferencing in ontologies to reason about the services under consideration, and
on the other hand, it computes statistics about the data as well as makes use of
indexes and weighting schemes to determine the degree of match between queries
and services.

Specifically, iMatcher computes similarities between the query and the services
in the KB. The more similar a service to a query, the more likely it is to be a
correct result. As parameters, iMatcher is given a set of similarity measures SM,
a data set of queries and correct answers T , and a learning algorithm R. To train
iMatcher, it first computes a similarity-based training set T sim that contains the
similarities between all the queries and services in T for all measures in SM and
an indication if the combination of the service and the query is correct (i.e., a
true positive) or not. It then applies the algorithm R to T sim to learn/induce
an induction model M. To use iMatcher, it computes the similarities (by the
measures SM) of a given query q to all services in a given KB, then uses M
to predict the combined similarity (or likelihood) of a match, and returns the
answers in decreasing order of similarity.

As iMatcher is based on iSPARQL, any similarity measure defined within
SimPack can be use for SM. For the learning algorithms R, any induction algorithm implementing the Weka interface can be employed.7 As a training set
T , iMatcher expects a training data set consisting of a knowledge base with ser-
vices, a list of queries, and a file specifying which services are the correct answers
for any given query.

For the evaluation we ran the results through a simple statistics handler that
knows of the set of relevant services (true positives) for a given input query ser-
vice. This information is used to compute the precision vs. recall figures that we
show throughout our evaluation (see Section 5). Furthermore, iMatcher implements the SME2Plugin interface to initialize the matchmaker with a particular
service knowledge base and to run queries.8

6 http://www.ifi.uzh.ch/ddis/imatcher.html
7 http://www.cs.waikato.ac.nz/ml/weka/
8 Used at the 1st International Semantic Service Selection Contest (S3) at ISWC 2007
(http://www-ags.dfki.uni-sb.de/klusch/s3/index.html).

C. Kiefer and A. Bernstein

4 Performance Evaluation

In this section we first briefly review the performance measures we use in our
evaluation and then describe our detailed experimental procedure.9 Note that
both the performance measures and the procedure are slight extensions of the
ones introduced and, hence, validated by Klusch [14].

4.1 Performance Measures for Matchmaking

|Raq|
|Aq| , Req =

Precision and Recall. Probably the most often used performance measures
for matchmaking are precision and recall [1]. Given a query q, precision P r is the
fraction of the answer set Aq (retrieved by the matchmaker) that is relevant to
the query, whereas recall Re is the fraction of relevant documents Rq which have
|Raq|
been retrieved, i.e., P rq =
|Rq| , where Raq denotes the subset of
relevant documents in Aq. As the evaluation of a single query is oftentimes not
sufficient to make a statistically significant statement, many queries are involved
and the averages of precision and recall computed.
Macro-Average. We are interested in macro-averaging precision and recall
[20,21] over all queries, as it gives equal weight to each user query [16]. We
introduce the set L = {0, 0.05, 0.1, . . . , 1.0} of 21 standardized recall levels [1]
as our goal is to compute average precision vs. recall figures (see Section 5) for
each investigated matchmaking strategy. Furthermore note that an interpolation
procedure (aka ceiling) is necessary to compute precision and recall at each
standardized recall level as each query likely has a different number of relevant
services (i.e., different Rq values). For each query qi, i  {1, . . . , n}, macroaveraging computes precision and recall separately at each level j, where j  L,
and then computes the mean of the resulting n values, i.e.,
 n

 n

ReM

P rM

j =

n

|Raqi
|
|
|Aqi

j

j

i=1

j =

n

|Raqi
|
|
|Rqi

j

i=1

R-Precision. R-Precision RP r [1] is used in this paper as single value summary
of the performance of a particular matchmaking algorithm. It is the fraction of
relevant services which have been retrieved when considering only the first |Rq|
|Ra

services of the answer set, denoted Ra
|Rq| . We use R-Precision
q
to compare two matchmaking algorithms A and B on a per-query basis, i.e.,

q, i.e., RP rq =

|

RP rA/B

q

= RP rA
q

 RP rB

q

A positive result for RP rA/B

highlights the fact that algorithm A is more
effective than B for query q, and vice versa. A zero result denotes equal performance of both algorithms.

q

9 For an elaborate treatment of these measures, refer to the textbook of Baeza-Yates

and Ribeiro-Neto [1] as well as to the [16,21].
?

?

?
4.2 iMatcher Evaluation Procedure and Benchmarking Data Set

The evaluation procedure is the following: based on our experiences, we (1) create
a set of iSPARQL strategies (see Section 3) that we assume will perform well;
for each query, we (2) run our matchmaker to obtain a list of services ranked by
their similarity to the query; given these ranked lists of services, we (3) compute
macro-averages by considering the queries relevance sets (their true answers) as
explained in Section 4.1; finally, we (4) plot the results to visually examine the
effectiveness of the created matchmaking strategies. By iteratively identifying
and replacing weak parts of strategies with parts that improve the matchmaking
performance, we are able to find well-performing strategies for the given task
and data set.

For all our experiments we use OWLS-TC v2 an OWL-S Semantic Web service retrieval test collection as benchmarking data set.10 The collection consists
of 576 services from seven different domains. It specifies 28 queries with their relevance sets (true answers) allowing us to compute the aforementioned statistics
for our matchmaker.

5 Experimental Results

The ultimate goal of our experiments was to demonstrate the ease of use of
iMatcher to perform Semantic Web service matchmaking based on iSPARQL
strategies. To that end, we evaluated three sets of strategies: (1) primitive strate-
gies: evaluation of a set of simple, off-the-shelf strategies; (2) induced strate-
gies: assessment of the quality of machine learning techniques for matchmaking;
and (3) customized strategies: estimation of the improvements of iteratively im-
proved, self-engineered strategies. We close our experiments with a short comparison of strategies from other systems.

5.1 Primitive Strategies
String-based Strategies. We start our evaluation with the comparison of simple string-based similarity strategies for matchmaking (see Figure 1(a)). In ad-
dition, the results for OWLS-MX M4 (measure no. 6) are shown on this and
every subsequent precision vs. recall figure presented in this work. OWLS-MX
M4 is reported to be the best-performing matchmaker variant of the OWLS-MX
matchmaker [14]. It uses the extended Jaccard similarity coefficient to compare
two services based on their sets of unfolded input/output concepts.11 Further-
more, Figure 1(a) illustrates the results of the TF-IDF measure (no. 5) that
compares services based on their service descriptions [1].

As the results in Figure 1(a) show, TF-IDF clearly outperforms all other
measures in terms of precision and recall. It also outperforms OWLS-MX M4
10 http://projects.semwebcentral.org/projects/owls-tc/
11 In all our experiments, we used nearest-neighbor as minimum degree of match and a
value of 0.7 as syntactic similarity threshold for OWLS-MX M4. These values were
suggested by the authors of OWLS-MX to obtain good results for OWLS-TC v2.

C. Kiefer and A. Bernstein

i

i

n
o
s
c
e
r

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

Levenshtein Strategy (1)
Bi-gram Strategy (2)
Jaro-Winkler Strategy (3)
Monge-Elkan Strategy (4)
TF-IDF Strategy (5)
OWLS-MX M4 minDeg=NN minSim=0.7 (6)

 0.1

 0.2

 0.3

 0.4

 0.5
Recall

 0.6

 0.7

 0.8

 0.9

i

i

n
o
s
c
e
r

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

Euclidean In/Out Inferred Strategy (1)
Euclidean In/Out Non-Inferred Strategy (2)
Jaccard In/Out Inferred Strategy (3)
Overlap In/Out Inferred Strategy (4)
OWLS-MX M4 minDeg=NN minSim=0.7 (5)

 0.1

 0.2

 0.3

 0.4

 0.5
Recall

 0.6

 0.7

 0.8

 0.9

(a) String-based strategies.

(b) Vector-based strategies.

Fig. 1. Performance comparison of simple string- and vector-based strategies

until about half of the relevant services have been retrieved. Both strategies find
all relevant services in the end (recall of 1.0), but OWLS-MX M4 with much
higher, almost constant precision.

A clear performance trend is recognizable among the set of measures that
syntactically compare the names of the services (measures no. 14). Among them,
the Bi-gram string similarity measure (no. 2) performs slightly better than other
prominent measures from this domain, such as the Levenshtein string similarity
(no. 1) [15] and the Jaro measure (even with Winklers reweighting scheme) [25].
Figure 2(a) additionally underscores the superiority of TF-IDF over the other
strategies showing exemplarily a comparison with the Bi-gram measure on a
per-query basis. For 19 out of 28 queries, comparing their textual descriptions
with descriptions of services of the service KB turned out to be more efficient for matchmaking than comparing their service names. We speculate that
these performance figures could be improved using even richer textual service
descriptions.

We learned from this evaluation that the simple TF-IDF full-text similarity
measure is very well suited for matchmaking in OWLS-TC v2 to achieve good
results. We, therefore, will reuse it in subsequent strategies. Furthermore, to
compare service names, we will use the Bi-gram measure.
Vector-based Strategies. Next, we compare a set of simple vector-based
strategies for matchmaking. The results are depicted in Figure 1(b). Basically,
these strategies take the sets of service I/O concepts, logically unfold them (us-
ing the Pellet reasoner), transform them to vectors, and measure the similarity
between those vectors using one of the vector similarity measures from SimPack.
More formally, let n, m be the number of input concepts of two particular
web services a and b. The set of all input concepts Ia, Ib can be represented
as binary vectors xa, xb of size |Ia| + |Ib|  |Ia  Ib|, where |Ia| = n, |Ib| = m.
?

?

?
 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

-0.1

-0.2

-0.3

-0.4

Performance

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

 0.5

 0.4

 0.3

 0.2

 0.1

-0.1

-0.2

-0.3

Performance

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Query Number

Query Number

(a) TF-IDF vs. Bi-gram.

(b) J48 Decision Tree vs. Euclidean I/O
inferred strategy.

Fig. 2. R-Precision comparison of selected matchmaking strategies

Consider, for example, the two services surfing-beach-service and sports-town-
service from OWL-S TC v2 with their sets of unfolded input concepts Ia =
{Surf ing, Sports, Activity, T hing} and Ib = {Sports, Activity, T hing}. The
vector representation of these two sets is xa = [1, 1, 1, 1]T and xb = [0, 1, 1, 1]T ,
|Ia|+|Ib||IaIb| = 0.75.
that have, for instance, an extended Jaccard similarity of
Likewise, the similarity of the vectors representing the services outputs is com-
puted, which results in a value of 0.4. Averaging the two similarity scores, we
obtain an overall similarity of 0.575 for the compared services a and b.

|IaIb|

Figure 1(b) illustrates that simple measures such as the metric Euclidean vector distance are sufficient to achieve good results for I/O-based service match-
making.12 The extended Jaccard measure is only slightly outperformed by the
simple Euclidean distance. The figure also shows that the Overlap measure fulfills the matchmaking task less precisely than the Euclidean and the extended
Jaccard measure for about 65% of retrieved services. After that, its performance
is comparable to the other measures.

It is interesting to observe the remarkable influence of ontological reasoning over the I/O concepts on the matchmaking task. Comparing the Euclidean
measure with reasoning support turned on (measure no. 1 in Figure 1(b)) vs.
the same measure without reasoning (no. 2) evidently shows that enabling reasoning boosts performance about 20% for the given matchmaking task. As a
consequence, we will use reasoning in all subsequent experiments.

5.2 Machine-Learned Strategies

As we showed in our previous work [12], techniques from machine learning are
well-suited for Semantic Web service discovery. Hence, we intend to also test
their applicability for Semantic Web service matchmaking.

12 The Euclidean distance d is converted into a similarity score by 1

1+d .

C. Kiefer and A. Bernstein

Table 1. T sim result table to learn a matchmaking strategy

Query Service Bi-gram TF-IDF EuclidIn EuclidOut TP(q,sv)

q1,sv1

q1,sv1

q1,sv1

q1

q1

qn

qn

sv1

svm

sv1

svm

scxs

scxs

scxs

scxs

q1,svm

qn,sv1

q1,svm

sctf idf

q1,sv1
sctf idf

sctf idf

qn,sv1
sctf idf

qn,svm

scei

scei

scei

scei

q1,svm

q1,svm

qn,sv1

qn,sv1

sceo

sceo

sceo

sceo







qn,svm

qn,svm

qn,svm

To that end, we induced four different machine learning models using algorithms from Weka13 and LibSVM14 as explained in 3.2. From Weka, we chose
a linear regression, a logistic regression, and a J48 decision tree learner for the
prediction if a service is a true answer to a query. From LibSVM, we chose
the support vector regression model (	-SVR) with an RBF kernel and cross-
validation/grid-search done as recommended in [10] to search for the best parameter setting for the RBF kernel. For a more detailed discussion of these
algorithms, refer to the textbook of Witten and Frank [26] for Weka and to
Chang and Lin [5] for LibSVM.

For the four similarity measures Bi-gram, TF-IDF, EuclidIn, and EuclidOut,
this resulted in T sim shown in Table 1. From this table, we induced the models
M, whilst ignoring the columns Query and Service, using the aforementioned
algorithms from Weka and LibSVM.

The resulting iSPARQL strategy from Wekas linear regression model (its
IMPRECISE clause) is shown in Listing 1.2. The model defines a weighted, linear
combination of the input features (the similarity scores) for the prediction of the
membership of a service to a querys relevance set. The weights learned for this
particular model are given on the last two lines in Listing 1.2. The final similarity
score sc is computed by aggregating/combining the individual scores, i.e., sc =
?

?

?
wi  simi, where SM = {Bi-gram, T F -IDF, EuclidIn, EuclidOut}.

iSM
The results in Figure 3(a) show that the 	-SVR strategy outperforms all other
strategies in terms of precision until about 65% of the relevant services have been
retrieved on average. Then, the decision tree takes the lead until about 90%
retrieved relevant services, before it gets again slightly outperformed by 	-SVR.
Linear and logistic regression perform worse than the non-linear models.

The accuracy of the prediction for the J48 learner is 98.95% and 98.45% for
the logistic regression learner. Note that the sole use of accuracies is, however,
misleading, as they are heavily dependent on the prior of the data set. Therefore,
Figure 3(b) graphs the Receiver Operating Characteristics (ROC) and the area
under the ROC-curve (AUC; in legend) that both provide a prior-independent
approach for comparing the quality of a predictor [19] for two of the chosen

13 http://www.cs.waikato.ac.nz/ml/weka/
14 http://www.csie.ntu.edu.tw/cjlin/libsvm/
?

?

?
	      	  

{ ? sim1 isparql : bigram

(? s e r v i c e N a m e ? q u e r y N a m e) .
(? s e r v i c e D e s c r i p t i o n ? q u e r y D e s c r i p t i o n) .
? sim2 isparql : tfidf
? sim3 isparql : euclidIn
(? s e r v i c e P r o f i l e ? q u e r y P r o f i l e) .
? sim4 isparql : e u c l i d O u t (? s e r v i c e P r o f i l e ? q u e r y P r o f i l e) .
# l e a r n e d c o m b i n a t i o n of four d i f f e r e n t s t r a t e g i e s
? sim

isparql : a g g r e g a t e (0.0443 ? sim1 0.785 ? sim2 0.481 ? sim3

}

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

0.146 ? sim4 -0.158 1.0)

Listing 1.2. Machine-learned strategy using a linear regression model

 0.9

 0.8

 0.7

 0.6

 0.5

 0.9

 0.8

 0.4

 0.7

 0.1

 0.2

e

t

a
r
 

e
v
i
t
i
s
o
p
e
u
r

-SVR Strategy (1)
Linear Regression Strategy (2)
Logistic Regression Strategy (3)
J48 Decision Tree Strategy (4)
OWLS-MX M4 minDeg=NN minSim=0.7 (5)

 0.1

 0.2

 0.3

 0.4

 0.5
Recall

 0.6

 0.7

 0.8

 0.9

 0.3

 0.2

 0.1

J48 Decision Tree: ACC=98.95, AUC=0.951
Logistic Regression: ACC=98.45, AUC=0.99
random

 0.1

 0.2

 0.3

 0.5

 0.4
 0.6
False positive rate

 0.7

 0.8

 0.9

i

i

n
o
s
c
e
r

(a) Learned/Induced strategies.

(b) ROC-curves.

Fig. 3. Performance comparison of learned matchmaking strategies

methods. The x-axis shows the false positive rate and the y-axis the true positive
rate. AUC is, typically, used as a summary number for the curve. A random class
assignment (either YES or NO) is also shown as a line form the origin (0,0) to
(1,1) and the ideal ROC-curve would be going from the origin straight up to
(0,1) and then to (1,1).

Note that with this very simple approach we are able to clearly outperform
OWLS-MX M4 in terms of precision and recall for almost 90% of relevant ser-
vices. Also, the performance on a per-query basis illustrates the superiority of
the machine learned strategies as plotted exemplarily in Figure 2(b) for the J48
decision tree strategy and the simpler vector-based Euclidean I/O strategy. We,
therefore, conclude that the combination of logical deduction and statistical induction produces superior performance over logical inference only, which can be
easily achieved with our approach.

5.3 Customized Strategies

As claimed in the introduction, iMatcher enables the creation of efficient matchmaking strategies in an iterative and playful procedure. To point this out,

C. Kiefer and A. Bernstein

i

i

n
o
s
c
e
r

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

Bi-gram Strategy (1)
TF-IDF Strategy (2)
Bi-gram + TF-IDF Strategy (3)
(3) + Euclidean In/Out Inferred Strategy (4)
J48 Decision Tree Strategy (5)
OWLS-MX M4 minDeg=NN minSim=0.7 (6)

 0.1

 0.2

 0.3

 0.4

 0.5
Recall

 0.6

 0.7

 0.8

 0.9

i

i

n
o
s
c
e
r

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

OLA: Hamming Strategy (1)
OLA: Tri-gram Strategy (2)
OLA: Sub-string Strategy (3)
SMOA Strategy (4)
OWLS-MX M0 (5)
OWLS MX M4 minDeg=NN minSim=0.7 (6)

 0.1

 0.2

 0.3

 0.4

 0.5
Recall

 0.6

 0.7

 0.8

 0.9

(a) Hand-crafted/Customized strategies.

(b) Other systems strategies.

Fig. 4. Performance comparison of customized matchmaking strategies and strategies
from other systems

Figure 4(a) summarizes the results for four iteratively improved strategies. Starting with the very simple name-comparing strategy Bi-gram (measure no. 1), we
successively can create better strategies by first applying TF-IDF (no. 2), then
combining 1 and 2 (= no. 3), followed by no. 4 that additionally takes service
inputs and outputs into account, and last, by using machine learning to result
in the best-performing strategy in this experiment.

5.4 Strategies from Other Systems

To close our experimental section, we succinctly compare the results of three
other systems: (1) the ontology alignment tool OLA (OWL-Lite Alignment) [9];
(2) OWLS-MX [14]; and (3), the string metric for ontology alignment (SMOA)
[23]. Note that two of these tools were initially created for ontology alignment
rather than for matchmaking. However, as they also involve similarity measures
to find correspondences in different ontologies, they can also be used by iMatcher.
The results in Figure 4(b) show that the Hamming strategy from OLA to
compare the names of services is by far the most inaccurate approach for match-
making. All of the other measures have much higher performance. The SMOA
strategy behaves very similar to OLAs Tri-gram strategy. Finally, a comparison
of OWL-S MX M0 that performs purely logic-based matchmaking with OWLSMX M4 again illustrates the usefulness of taking into account simple methods
from IR to improve precision and recall for Semantic Web service matchmaking.

6 Discussion and Limitations

Clearly, our empirical results are limited to an artificial data set, namely OWL-
S TC v2. Therefore, the results must be taken with a pinch of salt. Given,
?

?

?
however, the very low number of comparable test collections publicly available,
this limitation is rather natural and must be accepted. Similarly, we note that
focusing only on OWL-S as service description language is a limitation of our
experimental setup and not of our approach itself.

Particularly interesting is the high influence of the IR techniques on matchmaking performance. The strategies that exploit textual information of the services turned out to be very effective, which underlines again the importance of
the statistics-based approaches for matchmaking. On the other hand, as can be
seen from Figure 1(b), strategies that involve reasoning over the input data can
boost matchmaking performance by a factor of about 20%. It is interesting to
observe that the combination of both approaches (see measure no. 4 in Figure
4(a)) is superior to each of the individual approaches. Whilst this is not a new
finding, it emphasizes, nevertheless, the importance of combining statistical inference with logical deduction, which can also be concluded from our machine
learning experiments in Section 5.2.

Of course, creating customized indexes for keyword search and document
weighting (e.g., for TF-IDF) involves a pre-processing step of the data, which
might be costly depending on the size and dynamics of the data set.

As mentioned earlier, we did not yet consider iSPARQL optimization tech-
niques. Besides the work achieved for SPARQL basic graph pattern optimization
through selectivity estimation [2,22], we did, however, experiment with various
similarity index structures, whose elaborate treatment we postpone to future
work. This is especially important if our approach should be scalable and applicable to data sets which are much larger than the one used in this paper.

Another issue is the approximate matching procedure of iMatcher as opposed
to a formal approach. Obviously, both approaches have their important role. Formal matching procedures ensure a correct match, which is especially important
when, for example, automatically finding a service that shall be invoked without any adaptation. The formal approaches, however, might be too restrictive
in open domains, where an exact match cannot be assumed [14]. Approximate
matching is more suitable when an exact match cannot be found, when the formal approach provides too many answers, or when some adaptation procedure
would adapt the calling code to the found service before invoking it. Also,
approximate matching raises the issue of a threshold under which an answer
should not be considered. In our evaluation we assumed that the caller would
like an answer in any case and would like to examine the most suitable result.
In other scenarios such a threshold would, however, be appropriate.

7 Conclusions and Future Work

We presented a new approach to perform Semantic Web service matchmaking
based on iSPARQL strategies. We showed how our combination of structured
and imprecise querying can be used to perform hybrid Semantic Web service
matchmaking.

C. Kiefer and A. Bernstein

We evaluated our approach by analyzing a multitude of different matchmaking
strategies that make use of the logical foundations of the Semantic Web, apply
techniques from traditional information retrieval, involve machine learning, or
employ a combination of the three to find very effective strategies for matchmak-
ing. Our empirical results suggest that simple strategies are oftentimes sufficient
to obtain good results. However, using weighted and learned combinations of
simple measures boosts matchmaking performance even further.

It is left to future work to analyze iMatchers behavior for different Semantic
Web data sets, such as OWLS-S TC v2.2, and service description formats, such
as WSMO15 or SAWSDL.16

Finally, coming back to the introductory example, the question is, of course,
what Kate would say to these results? Given our experimental results, iMatcher
seems to be a practical and easy to use tool to help Kate solving the requirements
of her latest project, ensuring she will receive good grades for her work.

Acknowledgment

We would like to thank the anonymous reviewers for their valuable comments,
and Matthias Klusch for some interesting discussions regarding OWLS-MX and
iMatcher.
