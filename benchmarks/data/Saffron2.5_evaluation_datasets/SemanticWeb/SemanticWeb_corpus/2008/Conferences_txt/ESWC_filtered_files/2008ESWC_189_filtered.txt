Mapping Validation by Probabilistic Reasoning 

Silvana Castano1, Alfio Ferrara1, Davide Lorusso1,

Tobias Henrik N ath2, and Ralf M oller2

1 Universit`a degli Studi di Milano,

DICo, 10235 Milano, Italy

{castano,ferrara,lorusso}@dico.unimi.it

2 Hamburg University of Technology,

Institute of Software Systems,

21079 Hamburg, Germany

{r.f.moeller,tobias.naeth}@tu-harburg.de

Abstract. In the semantic web environment, where several independent
ontologies are used in order to describe knowledge and data, ontologies
have to be aligned by defining mappings among the elements of one ontology and the elements of another ontology. Very often, mappings are not
derived from the semantics of the ontologies that are compared. Rather,
mappings are computed by evaluating the similarity of the ontologies terminology and/or of their syntactic structure. In this paper, we propose
a new mapping validation approach. The approach is based on the notion of probabilistic mappings and on the use of probabilistic reasoning
techniques to enforce a semantic interpretation of similarity mappings as
probabilistic and hypothetical relations among ontology elements.

1 Introduction

In the semantic web environment, as well as in any information system where
two or more independent ontologies are used in order to describe knowledge and
data, some kind of ontology interoperability is needed. It can be realized either
by integrating the different ontologies into a new one or by aligning them with a
mechanism for translating a query over an ontology A into a query over another
ontology B. In both cases, two (or more) ontologies have to be aligned by defining correspondences among the elements (e.g., concepts, properties, instances)
of one ontology and the elements of another ontology. Such correspondences are
called mappings. Usually a mapping is featured by a relation holding between
the two elements mapped and a measure in the range [0,1], which denotes the
strength of the relation between the two elements. In literature, many ontology
matching approaches and systems have been proposed for automatically discovering ontology mappings [1]. In most of the proposed approaches, mappings are
not derived from the semantics of the ontologies that are compared, but, rather,
from an evaluation of the similarity of the terminology used in the two ontologies

 This paper has been partially funded by the BOEMIE Project, FP6-027538, 6th EU

Framework Programme.

S. Bechhofer et al.(Eds.): ESWC 2008, LNCS 5021, pp. 170184, 2008.
c Springer-Verlag Berlin Heidelberg 2008
?

?

?
or of their syntactic structure. The reason is that mapping discovery is not really
a deductive problem, if the two ontologies are really independent. In fact, the
information about the correspondence of an element of the first ontology with an
element of the second one is not implicitly contained in any of the two. However,
mappings derived from linguistic/syntactic approaches cannot be interpreted as
semantic relations among the ontology elements as it should be in order to use
them for query processing. On the other hand, some approaches for detecting
mappings with the support of logical reasoning have been proposed [2]. In this
case, a set of semantic relations is derived from mappings, but all the relations
are interpreted as ontology axioms. However, a certain level of uncertainty is
intrinsically due to the mechanism used for mapping discovery. In fact, ontology
mapping tools derive such mappings from uncertain sources or techniques (e.g.,
string matching, graph matching) or they rely on external lexical sources (e.g.,
WordNet) that are not specifically tailored for the domain at hand. This uncertainty leads to mapping inaccuracy. A way to overcome inaccuracy is to manually
validate each mapping not only per se (i.e., to state that the mapping is correct
with respect to the meaning of the involved elements), but also with respect to
all the other mappings in the set. Of course, this activity is time consuming and
it would however be affected by a subjectivity component, as it is performed by
human experts. In all these cases, a post-processing activity on ontology mappings is required in order to validate mappings with respect to the semantics of
the ontologies involved and, at the same time, by maintaining the uncertain nature of mappings. In this paper, we propose a new mapping validation approach
for interpreting similarity-based mappings as semantic relations, by coping also
with inaccuracy situations. The idea is to see two independent ontologies as a
unique distributed knowledge base and to assume a semantic interpretation of
ontology mappings as probabilistic and hypothetical relations among ontology
elements. We present and use a probabilistic reasoning tool in order to validate
mappings and to possibly infer new relations among the ontologies.

The paper is organized as follows: in Section 2, we present our approach
and we show an example of mapping acquisition. In Section 3, we present the
P-SHIQ(D) formalism and we show how mappings can be represented as probabilistic knowledge. In Section 4, we discuss probabilistic reasoning and its use
for mapping validation. In Section 5, we discuss some related work concerning
our approach. Finally, in Section 6, we give our concluding remarks.

2 Ontology Mappings

Our approach to mapping validation is articulated in four phases.

1. Ontology mapping acquisition. In this phase, we acquire mappings produced
using an ontology matching system; the matching system can rely on syn-
tactic, linguistic, structural, or even semantic matching techniques.

2. Probabilistic interpretation of mappings. In this phase, acquired mappings
are interpreted as probabilistic constraints holding among the elements of the

S. Castano et al.

two ontologies; in other words, the mapping is associated with a probability
that the relation expressed by the mapping is true.

3. Probabilistic reasoning over mappings. In this phase, the merged ontology
obtained by combining the two initial ontologies using the semantic relations
derived from probabilistic mappings is validated by means of a probabilistic
reasoning system.

4. Mapping validation and revision. In this phase, mappings are revised according to the reasoning results; mappings causing incoherences within the
merged ontology are discarded while new mappings are inferred from the
valid mappings.

In this paper, the main contribution regards the representation of mappings as
probabilistic knowledge and the reasoning-driven mapping validation (phases 2,
3, and 4), which are addressed in more detail in Sections 3, and 4, respectively.
In the remainder of this section, we first introduce the notion of probabilistic
mapping and then we describe an example of acquisition of linguistic mappings
generated by our ontology matching system HMatch 2.0 [3].

mi,j,r = r(i, j), v

O1 is a collection of mappings

2.1 Probabilistic Mappings
Given two ontologies O1 and O2, a mapping set MO2
mi,j,r of the form:
where i  O1 and j  O2 are two elements of O1 and O2, respectively, r is a
binary relation holding between i and j, and 0  v  1 is a value denoting the
strength of the relation r. Such kind of mappings can be automatically defined
by a wide range of available techniques and by using one of the ontology matchmaking tools proposed in literature. Goal of the mappings is to define which is
the kind of semantic relation which holds between an element of O1 and an element of O2. The relation r associated with a mapping depends on the matching
technique used. In most cases, it denotes a generic correspondence between two
elements (e.g., similarity), while, in other cases, it represents a terminological relationship between the names of i and j (e.g., synonymy, hypenymy, hyponymy).
The value v usually denotes the strength of the belief in the proposition r(i, j).
In order to take into account the intrinsic inaccurate nature of a mapping, we
define a probabilistic mapping as follows.
Definition 1. Probabilistic Mapping. Given two ontology elements i and j, a
probabilistic mapping pmi,j,r between i and j is a mapping mi,j,r whose value v
is the probability that the mapping relation r(i, j) is true, that is:

pmi,j,r = r(i, j), v, such that P r(r(i, j)) = v

In defining probabilistic mappings out of generic mappings, we introduce the hypothesis that the strength of the relation holding between two mapped elements
can be expressed in terms of probabilities. In particular, we want to measure the
probability of the assertion r(i, j) to be true.
?

?

?
2.2 Mapping Acquisition

In mapping acquisition, we work under the assumption that the mechanism
used for ontology matching is compatible with a probabilistic interpretation of
the resulting mappings. In this paper, we show how this can be achieved using
a linguistic matching technique based on the WordNet lexical system. In par-
ticular, we rely on the linguistic matching technique of HMatch 2.0, which determines the linguistic mapping between two ontology elements (i.e., concepts,
properties, individuals) on the basis of their names and of the terminological
relationships holding between in WordNet. The mapping degree v  [0, 1] is
calculated through a Linguistic Affinity function (LA), which returns a value
proportional to the probability that a terminological relationship holds between
two terms. Given two elements i and j of two ontologies O1 and O2, the interaction between HMatch 2.0 and the lexical system WordNet produces a set of
linguistic mappings of the form:

mi,j,r = r(Ti, Tj), v

where:
 Ti and Tj are terms used as names for i and j, respectively.
 r  {SYN, BT, NT} is a terminological relationship defined in WordNet for Ti
and Tj, where SYN denotes synonymy, BT denotes hypernymy, and NT denotes
hyponymy.

 v is the mapping value in the range [0,1] associated with r.

The LA function works on the synsets of WordNet. In WordNet, each possible
meaning of a term is represented by a set of synonyms, called synset. Given two
terms Ti and Tj, we search in WordNet the set R of terminological relationships
holding between at least one synset of Ti and at least one synset of Tj. When
linguistic mappings are interpreted as probabilistic mappings, many terms can
have more than one meaning and we do not know which specific meaning has
been used in the ontologies to be compared. If we find a terminological relationship ri(sl, sk)  R between two synsets sl of Ti and sk of Tj, the probability
P r(ri(sl, sk)) is equal to P r(P r(sl)  P r(sk)), where P r(sl) is the probability
that the intended meaning of Ti in its ontology is expressed by the synset sl,
while P r(sk) is the probability that the intended meaning of Tj in its ontology is
expressed by the synset sk, respectively. In general, given a term T , the probability P r(st) that st is the synset which denotes the intended meaning of T depends
on the total number NT of synsets for T in WordNet, such that P r(st) = 1
.

Thus, the probability P r(ri(sl, sk)) of the terminological relationship ri to be
true is defined as:

P r(ri(sl, sk)) = P r(P r(sl)  P r(sk)) =

NTi

 1
NTj

Example. Let us consider the two simple ontologies of Figure 1. In order to
define mappings between concepts described as classes, we consider their names.

S. Castano et al.

Ontology 1

hotel  building  hosts.tourist

Ontology 2
accommodation  hosting.traveler
motel  accommodation
hostel  accommodation
camping  accommodation

Fig. 1. Example of two ontologies in the touristic domain

Consider the terms hotel and hostel. In WordNet, hotel is associated only
with a synset s(hotel)1, defining the hotel as a building where travelers can pay
for lodging and meals and other services; hostel is associated with two synsets
s(hostel)1 and s(hostel)2, stating that a hostel is a hotel providing overnight
lodging for travelers and a inexpensive supervised lodging, respectively. By
means of the linguistic component of HMatch 2.0, we retrieve the relationship
BT (s(hotel)1, s(hostel)1). Our intuition is that such a relation is correct if the
intended meaning of the term hotel is expressed by s(hotel)1 and if the intended
meaning of the term hostel is expressed by s(hostel)1. In the first case, since
hotel has only one meaning, the probability is equal to one. In the second case,
since hostel has two possible meanings, the probability of s(hostel)1 to be the
correct synset is equal to 1/2 = 0.5. Then, the comprehensive probability of the
relation between hotel and hostel is calculated as follows:

P r(BT (s(hotel)1, s(hostel)1) =
?

?

?
 1

= 0.5

In order to define linguistic mappings between concepts expressed by a property restriction of the form ( |  | n | n |= n)R.C, the quantifier must
be equal and a linguistic mapping between the two properties R and the two
concepts C must hold. In particular, given a terminological relationship r1 between the properties of two restrictions and a terminological relationship r2
between the concepts of two restrictions, we establish a terminological relationship r3 between the two restrictions by choosing r3 as the most restrictive terminological relationship between r1 and r2. The probability associated
with r3 is calculated as P r(r3) = P r(r1)  P r(r2). For example, take into account the concepts hosts.tourist and hosting.traveler. Given the mappings
SY N(hosts, hosting) with probability 1.0 and N T (tourist, traveler) with probability 1.0, we obtain the mapping:

P r(N T (hosts.tourist,hosting.traveler)) = 1.0

When the two ontologies of Figure 1 are aligned, we collect all the mappings
available among the concepts of the two ontologies, that are shown in Figure 2
together with their probabilities.
3 Formalizing Probabilistic Mappings Using P-SHIQ(D)
In this section, we present P-SHIQ(D), the probabilistic description logics we
use in order to formalize probabilistic mappings as semantic relations.
?

?

?
Mapping relation
BT(hotel, motel)
NT(hosts.tourist, hosting.traveler)
NT(tourist, traveler)
BT(hotel, hostel)
BT(building, motel)
BT(building, hostel)

Probability

1.0
1.0
1.0
0.5
0.25
0.125

Fig. 2. Example of probabilistic mappings between the ontologies of Figure 1

3.1 A Probabilistic Extension of SHIQ(D)
In order to extend a description logic for dealing with probabilistic knowledge
an additional syntactical and semantical construct is needed. The additional
construct is called a conditional constraint. This extension of description logics
has been first formalized in [4] for SHOQ(D). Later the extension was adapted
to SHIF(D) and SHOIN (D) in [5], due to the fact that probabilistic reasoning problems are reduced to solving linear programs and standard satisfiability
tests regarding the underling DL . We use SHIQ(D) as underling DL, hence the
name P-SHIQ(D). For Syntax and Semantic of SHIQ(D) the reader is referred
to [6]. A conditional constraint consists of a statement of conditional probability for two concepts C, D as well as a lower bound l and an upper bound u on
that probability. It is written as follows: (D|C)[l, u] Where C can be called the
evidence and D the hypothesis. To gain the ability to store such statements in
a knowledge base it has to be extended to a probabilistic knowledge base PKB.
Additionally to the TBox T of a description logic knowledge base we introduce
the PTBox PT , which consists of T and a set of conditional constraints Dg, and
a PABox PA holding sets of conditional constraints Do for each probabilistic individual o. We also define the set of probabilistic individuals Ip, which contains
all individuals o for which some probabilistic knowledge is available and therefore
a set Do. In [4] there is no ABox declared, knowledge about so called classical individuals is also stored inside the TBox using nominals. Dg therefore represents
statistical knowledge about concepts and Do represents degrees of belief about
the individual o. To be able to define the semantics for a description logic with
probabilistic extension the interpretation I = (I ,) has to be extended by a
probability function  on the domain of interpretation I. The extended interpretation is called the probabilistic interpretation Pr = (I, ). Each individual
o in the domain I is mapped by the probability function  to a value in the
interval [0,1] and the values of all (o) have to sum up to 1 for any probabilistic
interpretation Pr. With the probabilistic interpretation Pr at hand the probability of a concept C, represented by Pr(C), is defined as sum of all (o) where
o  CI. The probabilistic interpretation of a conditional probability Pr(D|C)
Pr (C) where Pr(C) > 0. A conditional constraint (D|C)[l, u] is
Pr (CD)
is given as
satisfied by Pr or Pr models (D|C)[l, u] if and only if Pr(D|C)  [l, u]. We
will write this as Pr |= (D|C)[l, u]. A probabilistic interpretation Pr is said to
satisfy or model a terminology axiom T , written Pr |= T , if and only if I |= T .

S. Castano et al.

A set F consisting of terminological axioms and conditional constraints, where
F denotes the elements of F, is satisfied or modeled by Pr if and only if Pr |= F
for all F  F. The verification of a conditional constraint (D|C)[l, u] is defined
as Pr(C) = 1 and Pr has to be a model of (D|C)[l, u]. We also may say Pr
verifies the conditional constraint (D|C)[l, u]. On the contrary the falsification
of a conditional constraint (D|C)[l, u] is given if and only if Pr(C) = 1 and Pr
does not satisfy (D|C)[l, u]. It is also said that Pr falsifies (D|C)[l, u]. Further
a conditional constraint F is said to be tolerated under a Terminology T and
a set of conditional constraints D if and only if a probabilistic interpretation
Pr can be found that verifies F and Pr |= T  D. With all these definitions at
hand we are now prepared to define the z-partition of a set of generic conditional
constraints Dg. The z-partition is build as ordered partition (D0, . . . ,Dk) of Dg,
where each part Di with i  {0, . . . , k} is the set of all conditional constraints
F  Dg \ (D0    Di1), that are tolerated under the generic terminology
Tg and Dg \ (D0    Di1). If the z-partition can be build from a PABox
PT = (T ,Dg), it is said to be generically consistent or g-consistent. A probabilistic knowledge base PKB = (PT , (Po)oIp) is consistent if and only if PT is
g-consistent and Pr |= T  Do for all o  Ip.
We use the z-partition for the definition of the lexicographic order on the
probabilistic interpretations Pr as follows: A probabilistic interpretation Pr is
called lexicographical preferred to a probabilistic interpretation Pr if and only
if some i  {0, . . . , k} can be found, that |{F  Di | Pr |= F}| > |{F  Di |
Pr |= F}| and |{F  Dj | Pr |= F}| = |{F  Dj | Pr |= F}| for all i < j  k.
We say a probabilistic interpretation Pr of a set F of terminological axioms
and conditional constraints is a lexicographically minimal model of F if and only
if no probabilistic interpretation Pr is lexicographical preferred to Pr. By now
the meaning of lexicographic entailment for conditional constraints from a set
F of terminological axioms and conditional constraints under a PTBox PT is
given as: A conditional constraint (D|C)[l, u] is a lexicographic consequence of
a set F of terminological axioms and conditional constraints under a PTBox
PT , written as F  (D|C)[l, u] under PT , if and only if Pr(D)  [l, u] for
every lexicographically minimal model Pr of F  {(C|)[1, 1]}. Tight lexicographic consequence of F under PT is defined as F tight (D|C)[l, u] if and
only if l is the infimum and u is the supremum of Pr(D). We define l = 1 and
u = 0 if no such probabilistic interpretation Pr exists. Finally we define lexicographic entailment using a probabilistic knowledge base PKB for generic and
assertional conditional constraints F . If F is a generic conditional constraint,
then it is said to be a lexicographic consequence of PKB, written PKB  F if
and only if   F under PT and a tight lexicographic consequence of PKB,
written PKB tight F if and only if  tight F under PT . If F is an assertional conditional constraint for o  IP , then it is said to be a lexicographic
consequence of PKB, written PKB  F , if and only if Do  F under PT and
a tight lexicographic consequence of PKB, written PKB tight F if and only if
Do tight F under PT .
?

?

?
3.2 Interpreting Mappings as Probabilistic Knowledge

The basic idea behind the formalization of mappings as probabilistic knowledge
is to transform probabilistic mappings into conditional constraints. In order to
do that, we assume as hypothesis that a mapping relation r can be seen as
a corresponding constraint over the ontology concepts. As an example, we say
that, given a terminological relation BT (A, B) with probability p, it can be interpreted as the probability that an instance of B is also an instance of A. In
general, given the set R of relations produced by a matching system, we introduce a set R of rules for the translation of each kind of relation in R into a
corresponding conditional constraint. Taking into account the example of Section 2, we start from the set of relations R = {SY N, BT, N T} and we define the
following translation rules: i) P r(SY N(A, B)) = p  (A|B)[p, p]  (B|A)[p, p];
ii) P r(BT (A, B)) = p  (A|B)[p, p]; iii) P r(N T (A, B)) = p  (B|A)[p, p]. The
goal of the translation process is to assume a hypothetical representation of probabilistic mappings as semantic relations in P-SHIQ(D). Given two SHIQ(D)-
compatible ontologies O1 and O2, we build a new P-SHIQ(D) ontology O1,2
by merging the axioms of O1 and O2 and by augmenting the resulting ontology
with conditional constraints derived from mappings. Recalling the example of
Section 2, the resulting ontology is shown in Figure 3.

TBox

PTBox

hotelbuilding  hosts.tourist
accommodationhosting.traveler (hosting.traveler|hosts.tourist)[1.0, 1.0]

(hotel|motel)[1.0, 1.0]

motelaccommodation
hostelaccommodation
campingaccommodation

(traveler|tourist)[1.0, 1.0]
(hotel|hostel)[0.5, 0.5]
(building|motel)[0.25, 0.25]
(building|hostel)[0.125, 0.125]
Fig. 3. P-SHIQ(D) representation of the ontologies of Figure 1

4 Probabilistic Reasoning Over Mappings
Now we will recap the techniques required to reason in P-SHIQ(D). The motivation of this discussion is to present algorithmic issues on the one hand, and point
to implementation issues on the other. Subsequently, we describe our approach
to mapping validation by using the proposed ContraBovemRufum reasoning
System (CBR).
4.1 Reasoning with P-SHIQ(D)

As previously mentioned the proposed approach for solving probabilistic reasoning problems relies on use of standard reasoning services provided by a description logic reasoner in order to build linear programs which may be handed
over to a solver in order to decide the solvability of the programs. Although this
approach seems attractive usually severe performance problems can be expected

S. Castano et al.
?

?

?
rRT (F),r|=DC
?

?

?
lyr +

rRT (F),r|=DC

uyr +

rRT (F),r|=DC
?

?

?
rRT (F),r|=DC
?

?

?
(1  l)yr  0 (for all (D|C)[l, u]F)(1a)

(u  1)yr  0 (for all (D|C)[l, u]F)(1b)
?

?

?
rRT (F)

yr = 1
yr  0 (for all r  RT (F))

(1c)

(1d)

Fig. 4. Constraints of the linear program

for expressive logics. The complexity of the used algorithms lies within N P or
worse depending on the underlying DL as it has been analyzed in [5]. Despite
these results we think it is still worth investigating the approach to develop
optimization techniques to bring down average case complexity.
In order to deciding satisfiability the first objective is to build a set RT (F). It
contains the elements r, which map conditional constraints Fi = (Di|Ci)[li, ui],
elements of a set of conditional constraints F, onto one of the following terms
Di  Ci, Di  Ci or Ci under the condition, that the intersection of our r is
not equal to the bottom concept given the terminology T , written T |= r(F1) 
  r(Fn)  . In the following we will write r instead of r(F1)    r(Fn)
as an abbreviation. With the set RT (F) at hand we are able to set up linear
programs to decide the satisfiability of the terminology T and a finite set of
conditional constraints F. The constraints of the linear program are displayed
in Figure 4. We say that T  F is satisfiable if and only if the linear program
with the constraints 1a-d is solvable for variables yr, where r  RT (F). This
means, that in the objective function all coefficients preceding the variables yr
are set to 1. We further need to introduce the meaning of r |= C which is used
as index of the summation in 1a and 1b. It is an abbreviation for  |= r  C.
So the coefficient preceding the variables yr is set in linear constraints 1a and
1b if either r |= D  C or r |= D  C may be proven.
Why is the creation of linear programs reasonable? Consider the following: By
definition a conditional constraint is satisfied if u  Pr(D|C)  l  uPr(C) 
Pr(DC)  lPr(C). This may lead us to linear constraints 1a and 1b. Lets focus
on the upper bound, whose derivation is displayed in Figure 5. The derivation
for the lower bound 1a follows analogously. The linear constraints 1c and 1d
reflect that all (o) have to sum up to 1 and all (o)  [0, 1] Lets have a look
at the number of subsumbtion tests which need to be performed to set up the
system of linear constraints. At a first glance, finding a |= under each sum, one
might say four tests per variable and conditional constraint. Looking closer we
discover that only two are required because they are identical for lower and
upper bound. But even this may be optimised further. Considering that the r
represents a map of all conditional constraints on Di  Ci, Di  Ci or Ci
and they are tested on subsumbtion against D  C and D  C we observe
?

?

?
u
rRT (F),r|=C
?

?

?
u
?

?

?
rRT (F),r|=(DC)(DC)
?

?

?
yr 

yr 

yr 
?

?

?
rRT (F),r|=DC
?

?

?
rRT (F),r|=DC
?

?

?
rRT (F),r|=DC

yr  (2a)

yr  (2b)

yr  (2c)

(2d)

u

rRT (F),r|=DC
?

?

?
yr + u

rRT (F),r|=DC
?

?

?
rRT (F),r|=DC

uyr +

rRT (F),r|=DC

(u  1)yr  0

Fig. 5. Upper bound derivation

that only if the first subsumbtion test of r  D  C failed the second one
is necessary. Therefore significantly reducing the number of required tests per
variable and conditional constraint. With the tool at hand to decide satisfiability,
we may also decide if a conditional constraint may be tolerated by a set of
conditional constraints F. To verify a constraint we add a conditional constraint
(C|)[1, 1]. With the extended set the linear program is generated and solved.
If an unfeasible solution is computed the conditional constraint is conflicting.
If an optimal solution is found, the conditional constraint is tolerated. Now the
z-partition of a set of conditional constraints is computable. How to compute
tightest probability bounds for given evidence C and conclusion D in respect to
a set of conditional constraints F under a terminology T ? The task is named
tight logical entailment and denoted T  F |=tight (D|C)[l, u]. Given that T  F
is satisfiable, a linear program is set up for F  (C|)[1, 1]  (D|)[0, 1]. The
yr. So the coefficient in front of the variables
objective function is set to
yr are set 1 if r |= D. The tight logical entailed lower bound l is computed by
minimising, respectively the upper bound u by maximising the linear program. In
order to compute tight probabilistic lexicographic entailment for given evidence
C and conclusion D under a PKB the following steps have to be taken:
1. Compute the z-partition of Dg in order to be able to generate a lexicographic
2. Compute lexicographic minimal sets D of conditional constraints of Dg as
3. Compute the tight logical entailment T  F  D |=tight (D|C)[l, u] for all

rR,r|=D
?

?

?
ordering
elements of D.
D  D.

4. Select the minimum of all computed lower bounds and the maximum of all

upper bounds.

The 2. step needs some explanation since a new task compute lexicographic
minimal sets is introduced. In order to define a lexicographic minimal set D,
a preparatory definition is required. A set D  Dg lexicographic preferable to

S. Castano et al.

D  Dg if and only if some i  {0, . . . , k} exists such that |D Di| > |D Di|
and |D  Di| > |D  Di| for all i < j  k. With the lexicographic order
introduced onto the sets D the definition of lexicographic minimal is given as:
D is lexicographic minimal in S  {S|S  Dg} if and only if D  S and no
D  S is lexicographic preferable to D.

We implemented the algorithms presented by [4,5] in the Java programming
language into the ContraBovemRufum System. For the reasoning tasks RacerPro
with its JRacer interface is used. As solvers a native Java solver by Opsresearch
and the Mosek linear program solver have been integrated. For application programmers two different sets of interfaces are provided. The first set contains the
ProbabilisticKBInterface, which provides all operations related to setting up and
modifying PTBox and PABox, and the ProbabilisticEntailmentInterface, which
offers the probabilistic inference operations to decide consistency for PT and
PKB as well as probabilistic subsumption and probabilistic instance checking.
The second set of interfaces handles the configuration of the services. Using the
SolverConfiguration interface the selection of the solver may be changed at run-
time. The ReasonerConfiguration interface makes the settings for the reasoner.
With the EntailmentConfiguration interface the algorithm used to compute the
entailment may be chosen at runtime. Currently tight logical entailment and the
two available algorithms for tight lexicographic entailment are supported.

4.2 Mapping Validation and Revision

After mapping acquisition and representation as probabilistic constraints, we
want to use tight lexicographic entailment in order to check if one (or more)
constraints used to represent mappings causes incoherence within the PTBox of
the ontology obtained by merging the two ontologies. A mapping can produce
contradictions in two ways: i) the constraint itself is contradictory with respect
to some of the axioms already present in the two original ontologies; ii) the constraint is correct, but the probability associated with it is not compatible with
the probability constraints already present in the ontologies. Moreover, incoherence can be caused by two different situations: i) the mapping is not compatible
with one or more axioms already present in the merged ontology; ii) the mapping is not compatible with other mappings introduced in the merged ontology.
In the first case, incompatible mappings are discarded in order to preserve the
original knowledge provided by the ontologies at hand. In the second case, we
need a heuristic for choosing the incompatible mapping(s) to be discarded. In
our approach, we propose to keep the mappings featured by higher probabilities
to be true under the assumption that relations which are more probable are
also more reliable. In order to implement this approach, we adopt the mapping
validation procedure depicted in Figure 6.

The procedure takes as input a mapping set together with a merged ontology.
As a first step, the mapping set M is ordered by descending ordering from
the most probable to the less probable mapping. In such a way, we first insert
the more probable mappings and, in case of incoherence, we discard always the
latter (i.e., less probable) mapping(s). Then, for each mapping, we translate the
?

?

?
mapping validation(M ,O):

input: a mapping set M , a merged ontology O
output: a validated mapping set M
begin
?

?

?
associated with each mapping mi  M ;

M := sort M w.r.t. the probability
for mi  M:
ci := translate mi into a conditional constraint;
add ci to O;
check coherency of O with CBR;
if O is coherent then:

continue;

else:

remove ci from O;

end if

end

Fig. 6. Mapping validation and revision procedure

mapping into a conditional constraint as shown in Section 3 and we insert such
a constraint into the P-SHIQ(D) ontology. We use the CBR reasoning system
in order to check the coherence of the ontology augmented with the conditional
constraint. In the coherence check fails, we discard the mapping and we delete
the conditional constraint from the ontology. The final result is that the ontology
is augmented only with mappings which do not cause incoherence problems into
the PTBox.

Example. As an example, we consider mappings of Figure 2 and the merged ontology of Figure 3. The first four mappings do not cause any incoherence within
the PTBox, then the corresponding conditional constraints are added to the
ontology. The mapping BT (building, motel) with probability 0.25 is not compatible with other mappings. In particular, it happens that an instance of motel
is also an instance of hotel with probability 1.0, due to the first mapping. More-
over, the concept hotel is subsumed by the concept building. Then, an instance
of motel must be an instance of building with probability p  1.0. According
to the mapping validation procedure, this mapping is discarded. We note that

Table 1. Mappings after validation and revision

Discarded constraints
(building|motel)[0.25, 0.25]
(building|hostel)[0.125, 0.125]

Revised constraints
(hosts.tourist|motel)[1.0, 1.0]
(hosts.tourist|hostel)[0.5, 1.0]
(building|motel)[1.0, 1.0]
(building|hostel)[0.5, 1.0]
(accommodation|hotel)[1.0, 1.0]
(accommodation|hosts.tourist)[1.0, 1.0]
(hosts.tourist|accommodation)[1.0, 1.0]

S. Castano et al.

such a mapping is also useless, since the relation between motel and building
is already implicitly expressed by the axioms of the first ontology and by the
first mapping. Analogously, we discard also the mapping BT (building, hostel)
with probability 0.125. Finally, we can check which conditional constraints can
be inferred by the CBR system by considering the validated mappings. The final
result of our example is summarized in Table 1.

5 Related Work

Related work concerns both ontology mapping validation and probabilistic reasoning techniques. The problem of automatically discovering ontology/schema
mappings has been studied both in the field of ontologies [1] and in the field of
databases [7]. In the literature, some approaches have been proposed for validating mappings with respect to the semantics of the involved ontologies, but
in most of the cases these approaches do not deal with uncertain mappings. An
example of work in this direction is the S-Match system [2]. The key idea behind S-Match is to compute an initial set of relations between concepts of two
taxonomies by means of external sources (i.e. WordNet); then, the problem of
matching is translated into a validity check problem on the concept taxonomy.
However, the validation approach of S-Match does not take into account the
uncertain nature of mappings which are instead translated into crisp relations.
Another important step towards mapping validation is represented by the theoretical study proposed in [8], which differs from previous works in that logical
reasoning about mappings is performed. Mappings are translated into logical
constraints and represented by means of Distributed Description Logics [9], an
extension of classical DLs for distributed knowledge bases. An implementation
of this approach has been discussed in [10]. However, also in this case, mapping uncertainty is not supported. Other relevant work has been done in using
fuzzy reasoning for handling uncertainty in ontologies [11]. A fuzzy approach is
different from our proposal in that a different understanding of the semantics
of mappings is applied. However, especially for instance matching, the use of
fuzzy reasoning can be integrated in our approach. In [12] the authors propose
a mapping validation approach based on the idea of representing mappings as
probabilistic rules. With respect to this work, we present a linguistic matching
procedure which is compatible with the probabilistic interpretation of mappings
instead of applying probabilities to every kind of results produced by matching
tools. In recent years, the problem of modeling uncertainty has become one focus
of interest also in description logic research. First probabilistic extensions started
by modelling a degree of overlap between concepts via probabilities at the TBox
level [13] as well as believes about certain individuals at the ABox level [14]. In
P-CLASSIC [15] an approach was presented which integrates Bayesian networks
as underlying reasoning formalism. Further work on integration of description
logic and Bayesian networks has been presented in [16]. Efforts to integrate several approaches of uncertainty representation into one coherent framework have
been made in [17]. The presented extension differs from the aforementioned ones,
due to its ties to default logic, a non-monotonic reasoning method developed by
?

?

?
Reiter [18]. Here lexicographic entailment for defaults [19] was adapted to probabilistic description logics which provides non-monotonic inheritance properties
along subclass relationships.

6 Concluding Remarks

In this paper, we have proposed a mapping validation approach with the following original contributions: i) separation of concern: the mapping validation
process is independent from the mapping discovery process, in order to enable interoperability with existing matching systems; uncertainty-compliance: a notion
of probabilistic mapping is introduced to capture the uncertain nature of the
correspondences stated by mappings; well-founded approach: the probabilistic
extension P-SHIQ(D) of SHIQ(D) is used to represent probabilistic mappings
and a sound reasoning system for P-SHIQ(D) is presented and used for working
with probabilistic mappings and formally validate them.

Our approach to mapping validation is adopted in the European BOEMIE
project, where ontology mappings are used as a support for ontology evolution.
In the context of BOEMIE, an evaluation of this validation approach is being performed on linguistic mappings between the athletics domain ontology of
BOEMIE and a set of external knowledge sources. Goal of the evaluation is to
compare the quality of mappings in terms of precision and accuracy before and
after the validation process. Our ongoing and future work is mainly devoted to:
i) use of validated mappings for ontology evolution; ii) investigation on optimization techniques for the probabilistic reasoning algorithms implemented in
the CBR System, in order to improve the systems average case performance; iii)
investigation on the problem of automatically detecting the minimal set of mappings causing incoherence during validation, in order to substitute the current
heuristic-based approach to mapping revision with a reasoning-based approach.
