RoundTrip Ontology Authoring

Brian Davis1, Ahmad Ali Iqbal1,3, Adam Funk2, Valentin Tablan2,
Kalina Bontcheva2, Hamish Cunningham2, and Siegfried Handschuh1

1 Digital Enterprise Research Institute, Galway, Ireland

2 University of Sheffield, UK

3 University of New South Wales, Australia

Abstract. Controlled Language (CL) for Ontology Editing tools offer
an attractive alternative for naive users wishing to create ontologies, but
they are still required to spend time learning the correct syntactic structures and vocabulary in order to use the Controlled Language properly.
This paper extends previous work (CLOnE) which uses standard NLP
tools to process the language and manipulate an ontology. Here we also
generate text in the CL from an existing ontology using template-based
(or shallow) Natural Language Generation (NLG). The text generator
and the CLOnE authoring process combine to form a RoundTrip Ontology Authoring environment: one can start with an existing imported
ontology or one originally produced using CLOnE, (re)produce the Controlled Language, modify or edit the text as required and then turn
the text back into the ontology in the CLOnE environment. Building
on previous methodology we undertook an evaluation, comparing the
RoundTrip Ontology Authoring process with a well-known ontology ed-
itor; where previous work required a CL reference manual with several
examples in order to use the controlled language, the use of NLG reduces
this learning curve for users and improves on existing results for basic
ontology editing tasks.

1 Introduction

Formal data representation can be a significant deterrent for non-expert users
or small organisations seeking to create ontologies and subsequently benefit
from adopting semantic technologies. Existing ontology authoring tools such
as Prot eg e1 attempt to resolve this, but they often require specialist skills in
ontology engineering on the part of the user. This is even more exasperating
for domain specialists, such as clinicians, business analysts, legal experts, etc.
Such professionals cannot be expected to train themselves to comprehend Semantic Web formalisms and the process of knowledge gathering; involving both
a domain expert and an ontology engineer can be time-consuming and costly.
Controlled languages for knowledge creation and management offer an attractive
alternative for naive users wishing to develop small to medium sized ontologies
or a first draft ontology which can subsequently post-edited by the Ontology

1 http://protege.stanford.edu

A. Sheth et al. (Eds.): ISWC 2008, LNCS 5318, pp. 5065, 2008.
c Springer-Verlag Berlin Heidelberg 2008
?

?

?
Engineer. In previous work[1], we presented CLOnE - Controlled Language for
Ontology Editing which allows naive users to design, create, and manage information spaces without knowledge of complicated standards (such as XML,
RDF and OWL) or ontology engineering tools. CLOnEs components are based
on GATEs existing tools for IE (Information Extraction) and NLP (Natural
Language Processing) [2].

The CLOnE system was evaluated using a repeated-measures, task-based
methodology in comparison with a standard ontology editor  Prot eg e. CLOnE
performed favourably with test users in comparison to Prot eg e. Despite the benefits of applying Controlled Language Technology to Ontology Engineering, a
frequent criticism against its adoption, is the learning curve associated with following the correct syntactic structures and/or terminology in order to use the
Controlled Language properly. Adhering to a controlled language can be, for
some naive users, time consuming and annoying. These difficulties are related to
the habitability problem, whereby users do not really know what commands they
can or cannot specify to the NLI (Natural Language Interface) [3]. Where the
CLOnE system uses natural language analysis to unambiguously parse CLOnE
in order to create and populate an ontology, the reverse of this process, NLG
(Natural Language Generation), involves the generation of the CLOnE language
from an existing ontology. The text generator and CLOnE authoring processes
combine to form a RoundTrip Ontology Authoring(ROA) environment: a user
can start with an existing imported ontology or one originally produced using
CLOnE, (re)produce the Controlled Language using the text generator, modify or edit the text as required and subsequently parse the text back into the
ontology using the CLOnE environment. The process can be repeated as necessary until the required result is obtained. Building on previous methodology
[1], we undertook a repeated-measures, task-based evaluation, comparing the
RoundTrip Ontology Authoring process with Prot eg e. Where previous work required a reference guide in order to use the controlled language, the substitution
of NLG can reduce the learning curve for users, while simultaneously improving
upon existing results for basic Ontology editing tasks. The remainder of this
paper is organized as follows: Section 2 discusses the design and implementation
of the ROA pipeline focusing on the NLG component - the ROA text generator,
Section 3 presents our evaluation and discusses our quantitative findings. Section 4 discusses related work. Finally, Section 5 and Section 6 offer conclusions
and future work.

2 Design and Implementation

In this section, we describe the overall architecture of the Round Trip Ontology
Authoring (ROA) pipeline which is implemented in GATE [2]. We discuss briefly
extensions to existing CLOnE components of ROA, but focus the attention of
this section towards describing the CLOnE text generator, the algorithm used
and the XML configuration file containing templates needed to configure the
controlled language output of the generator.

B. Davis et al.

2.1 RoundTrip Ontology Authoring (ROA) and CLOnE

ROA builds on and extends the existing advantages of the CLOnE software and
input language, which are described below:

1. ROA requires only one interpreter or runtime environment, the Java 1.6

JRE.

2. ROA like CLOnE uses a sub-language of English.
3. As far as possible, CLOnE is grammatically lax; in particular it does not matter whether the input is singular or plural (or even in grammatical agreement).
4. ROA can be compact; the user can create any number of classes or instances

in one sentence.

5. ROA is more flexible and easier to learn by using simple examples of how
to edit the controlled language generated by the text generator in order to
modify the Ontology. It reduces the need to learn the Controlled Language
by following examples, style guides or CLOnE syntactic rules. Instead, a
user can create or modify various classes and instances in one (generated)
sentence or (using simple copy and paste) create new properties between
new or existing classes and instances.

6. The CLOnE grammar within ROA has been extended to handle simple verbs

and phrasal verbs.

7. Like CLOnE any valid sentence of ROA can be unambiguously parsed.
8. The advantage of the GATE Ontology API allows users to import existing
Ontologies for generation, subsequent editing in ROA and export the result
to different Ontology formats.

9. SimpleNLG2 has been added into the ROA text generator to lexicalize unseen

properties.

Procedurally, CLOnEs analysis consists of the ROA pipeline of processing
resources (PRs) shown in Figure 1 (left dotted box). This pipeline starts with
a series of fairly standard GATE NLP tools which add linguistic annotations
and annotation features to the document. These are followed by three PRs developed particularly for CLOnE: the gazetteer of keywords and phrases fixed in
the controlled language and two JAPE3 transducers which identify quoted and
unquoted chunks. Names enclosed in pairs of single or double quotation marks
can include reserved words, punctuation, prepositions and determiners, which
are excluded from unquoted chunks in order to keep the syntax unambiguous.
The last stage of analysis, the CLOnE JAPE transducer, refers to the existing
ontology in several ways in order to interpret the input sentences. Table 1 below
provides an excerpt of the grammar rules of the CLOnE language. We refer the
reader to [1,4] for additional rules and examples.

2 http://www.csd.abdn.ac.uk/ereiter/simplenlg/
3 GATE provides the JAPE (Java Annotation Pattern Engine) language for matching regular expressions over annotations, adding additional annotations to matched
spans, and manipulating the match patterns with Java code.
?

?

?
Fig. 1. The ROA RoundTrip Ontology Authoring pipeline

Table 1. Excerpt of CLOnE grammar with examples

Sentence Pattern
Forget everything.

Example
Forget everything.

(Forget that) There is/are
<classes>.

<in-
a/are

is/are

that)
is

(Forget
stances>
<class>.
that) <sub-
(Forget
classes>
a
type/types of <super-
class>.

There are researchers,
universities and
conferences.
Ahmad Ali Iqbal and
Brian Davis are Ph.D.
Scholar.
Ph.D. Scholar is a
type of Student.

(Forget that) <classes/
instances>
<verb
property> <classes/
instances>.

Professor supervises
student.

Usage
Clear the whole ontology
corpus to start with the
new ontology.
Create or delete (new)
classes.

Create
stances of the class.

(or delete)

in-

Make subclass(es) of an
existing super-class. For-
get that only unlinks the
the
subclass-superclass
relationship.
Create the property of the
form Domain verb Range
either between two classes
or instances.

2.2 Text Generation of CLOnE

The text generation component in Figure 1 (right dotted box) displayed in the
ROA pipeline is essentially an Ontology Verbalizer. Unlike some NLG systems,
the communicative goal of the text generator is not to construct tailored reports
for specific content within the knowledge base or to respond to user specific
queries. Hence no specific content selection subtask or choice is performed
since our goal is to describe and present the Ontology in textual form as unambiguous subset of English - the CLOnE language for reading, editing and
amendment. We select the following content from the Ontology: top level classes,
subclasses, instances, class properties, their respective domain and ranges and
instance properties. The text generator is configured using an XML file, whereby
text templates are instantiated and filled by the values from the Ontology. This

B. Davis et al.

Fig. 2. Example of a generation template

file is decoupled from the text generator PR. Examples of two templates used to
generate top level classes and class properties are displayed in Figure 2. The text
generator (See Generator in Figure 1) is realised as a GATE PR and consists
of three stages:

Stage 1 within the text generator converts the input ontology into an internal GATE ontological resource and flattens it into RDF style triples. This
is executed in a breadth-first mannerso lists are created where super-classes
always precede their corresponding subclassesin the following order: top-level
classes, subclasses, instances, class properties, and instance properties.

Stage 2 matches generation templates from the configuration file (See Figure 2) with the triples list derived from the Ontology in Stage 1. A generation
template has three components: (1) an in element containing a list of triple
specifications, (2) an out element containing phrases that are generated when a
successful match has occurred and (3) an optional ignoreiIf element for additional triple specifications that cause a match specified in the in element to be
ignored if the conditions are satisfied. The triple specifications contained within
the in portion of the template can have subject, property and object XML el-
ements. The triple specifications act as restrictions or conditions, such that an
input triple generated from the Ontology must match this template. If more than
one triple is included in the in element they are considered as a conjunction of
restrictions, hence the template will only match if one or more actual triples for
all triple specifications within the in element are found. One triple can reference another, i.e., a specification can constrain a second triple to have the same
object as the subject of the first triple. Only backward referencing is permitted
?

?

?
since the triples are matched in a top down fashion according to their textual
ordering. An example of referencing can be seen in line 188 of the out element
of the template shown in Figure 2 for generating class properties.

In Stage 3 the out section of the template describes how text is generated
from a successful match. It contains phrase templates that have text elements
and references to values matched within the in elements. Phrases are divided into
singular and plural forms. Plural variants are executed when several triples are
grouped together to generate a single sentence (Sentence Aggregation) based
on a list of Ontology objects (i.e., There are Conferences, Students and
Universities). Text elements within a template are simply copied into the
output while reference values are replaced with actual values based on matching
triple specifications. We also added a small degree of lexicalization into the Text
Generator PR, whereby, for example, an unseen property, which is treated as a
verb is inflected correctly for surface realisation i.e. study and studies. This
involves a small amount of dictionary look-up using the SimpleNLG Library
to obtain the third person singular inflection studies from study to produce
Brian Davis studies at NUIG. The out elements of the generation template
also provide several phrase templates for the singular and plural sections. These
are applied in rotation to prevent tedious and repetitious output.

Stage 2 also groups matches together into sets that can be expressed together
in a plural form. For this to proceed, the required condition is that the difference between matches, occurs in only one of the references used in the phrase
templates, i.e., if singular variants would only differ by one value. A specialized
generation template with no in restrictions is also included in the configuration
file. This allows for the production of text where there are no specific input triple
dependencies.

3 Evaluation

3.1 Methodology

Our methodology is deliberately based on the criteria previously used to evaluate
CLOnE [1,4], so that we can fairly compare the earlier results using the CLOnE
software with the newer RoundTrip Ontology Authoring(ROA) process. The
methodology involves a repeated-measures, task-based evaluation: each subject
carries out a similar list of tasks on both tools being compared. Unlike our
previous experiment, the CLOnE reference guide list and examples are withheld
from the test users, so that we can measure the benefits of substituting the
text generator for the reference guide and determine its impact on the learning
process and usability of CLOnE. Furthermore, we used a larger sample size and
more controls for bias. All evaluation material and data are available online
for inspection, including the CLOnE evaluation results for comparison4. The
evaluation contained the following:

4 http://smile.deri.ie/evaluation/2008/ROA

B. Davis et al.

 A pre-test questionnaire asking each subject to test their degree of knowledge with respect to ontologies, the Semantic Web, Prot eg e and Controlled
Languages. It was scored by assigning each answer a value from 0 to 2 and
scaling the total to obtain a score of 0100.

 A short document introducing Ontologies, the same quick start Prot eg e
instructions as used in [4] (partly inspired by Prot eg es Ontology 101 documentation [5]), and an example of editing CLOnE text derived from the
text generator. The CLOnE reference guide and detailed grammar examples
used in for the previous experiment [4] were withheld. Subjects were allowed
to refer to an example of how to edit generated Controlled Language but did
not have access to CLOnE reference guide.

 A post-test questionnaire for each tool, based on the System Usability Scale
(SUS), which also produces a score of 0100 to compare with previous
results [6].

 A comparative questionnaire similar to the one used in [4] was applied to
measure each users preference for one of the two tools. It is scored similarly
to SUS so that 0 would indicate a total preference for Prot eg e, 100 would
indicate a total preference for ROA, and 50 would result from marking all
the questions neutral. Subjects were also given the opportunity to make
comments and suggestions.

 Two equivalent lists of ontology-editing tasks, each consisting of the following

subtasks:
 creating two subclasses of existing classes,
 creating two instances of different classes, and
 either (A) creating a property between two classes and defining a property between two instances, or (B) extending properties between two
pairs of instances.

For both task lists, an initial ontology was created using CLOnE. The same
ontology was loaded into Prot eg e for both tasks and the text generator was
executed to provide a textual representation of the ontology for editing pur-
poses(see Figure 3), again for both tasks.

For example, Task List A is as follows.

 Create a subclass Institute of University.
 Create a subclass Workshop of Conference.
 Create an instance International Semantic Web Conference of class Confer-

ence.

 Create an instance DERI of class Institute.
 Create a property that Senior Researchers supervise Student.
 Define a property that Siegfried Handschuh supervises Brian Davis.

3.2 Sample Quality

We recruited 20 volunteers from the Digital Enterprise Research Institute, Gal-
way5. The sample size (n = 20) satisfies the requirements for reliable SUS

5 http://www.deri.ie
?

?

?
Fig. 3. Text Generated by ROA

evaluations [7]. We recruited subjects with an industrial background (I) and
participants with a research background (R). See (in Table 5) for details. In
addition we attempted to control bias by selecting volunteers who were either:
 Research Assistants/Programmers/Post-Doctoral Researchers with an industrial background either returning (or new) to Academic Research respec-
tively(I),

 Postgraduate Students who were new to the Semantic Web and unfamiliar

 Researchers from the E-learning and Sensor Networks lab but not from the

 Researchers with no background in Natural Language Processing or Ontol-

with Ontology Engineering(R),

Semantic Web Cluster(R),

ogy Engineering(R) or

 Industrial Collaborators (I).

In all cases, we tried to ensure that participants had limited or no knowledge
of GATE or Prot eg e. First, subjects were asked to complete the pre-test ques-
tionnaire, then they were permitted time to read the Prot eg e manual and Text
Generator examples, and lastly they were asked to carry out each of the two task
lists with one of the two tools. (Half the users carried out task list A with ROA
and then task list B with Prot eg e; the others carried out A with Prot eg e and
then B with ROA.) Each users time for each task list was recorded. After each
task list the user completed the SUS questionnaire for the specific tool used,
and finally the comparative questionnaire. Comments and feedback were also
recorded on the questionnaire forms.

3.3 Quantitative Findings
Table 2 summarizes the main measures obtained from our evaluation. We used
SPSS6 to generate all our statistical results. In particular the mean ROA SUS

6 SPSS 2.0, http://www.spss.com

B. Davis et al.

Table 2. Summary of the questionnaire scores

Measure
Pre-test scores

ROA SUS rating

Prot eg e SUS rating 10
R/P Preference

min mean median max
?

?

?
70 100
?

?

?
Table 3. Confidence intervals (95%) for the SUS scores

Tool

Confidence intervals

Task list A Task list B Combined

Prot eg e

2855
6377

2951
6984

3249
6879

Table 4. Correlation coefficients

Measure
ROA time
Prot eg e time
ROA SUS
Prot eg e SUS
Prot eg e time

Measure
Pre-test
Pre-test
Pre-test
Pre-test
ROA time
ROA time ROA SUS
Prot eg e time Prot eg e SUS
ROA time
Prot eg e SUS
Prot eg e time ROA SUS
Prot eg e SUS
ROA SUS
ROA SUS
R/P Preference
Prot eg e SUS R/P Preference

Pearsons Spearmans Correlation
weak 
none
none
weak 
+

+
none
none
none
+
none

-0.41
-0.28
-0.02
-0.32
0.53
-0.65
0.53
-0.14
-0.02
0.04
0.58
-0.01

-0.21
-0.35
-0.00
-0.29
0.58
-0.52
0.56
-0.10
-0.09
-0.01
0.56
0.10

score is above the baseline of 6570% while the mean SUS score for Prot eg e is well
below the baseline [8]. In the ROA/Prot eg e Preference (R/P Preference) scores,
based on the comparative questionnaires, we note that the scores also favour on
average ROA over Prot eg e. Confidence intervals are displayed in Table 3.7

We also generated Pearsons and Spearmans correlations coefficients [9,10].

Table 4 displays the coefficients. In particular, we note the following results.

 The pre-test score has a weak negative correlations the with ROA task time.
 There are no correlations with pre-test score and the ROA SUS score.
 The pre-test score has a weak negative correlation with the Prot eg e SUS

score.

 There are no correlations with pre-test score and the Prot eg e time.

7 A data samples 95% confidence interval is a range 95% likely to contain the mean

of the whole population that the sample represents [9].
?

?

?
 In previous results in comparing CLOnE and Prot eg e, the task times for
both tools were more positively correlated with each other while in the case
of ROA and Prot eg e, there correlation has being weakened by a significant
32% of its original value (of 78%) reported for CLOnE [1], indicating that
the users tended not spend the equivalent time completing both ROA and
Prot eg e tasks.

 There is a moderate correlation with Prot eg e task time and Prot eg e SUS

scores.

 There is a strong negative correlation of -0.65 between the ROA task time
and the ROA SUS scores. Our previous work reported no correlation between the CLOnE task time and CLOnE SUS time. A strong negative or
inverse correlation implies that users who spent less time completing a task
using ROA tended to produce high usability scores - favouring ROA. More
importantly, we noted that the associated probability reported by SPSS, was
less then the typical 5% cut-off point used in social sciences. This implies
there is a 5% chance that the true population coefficient is very unlikely to
be 0 (no relationship). Conversely, one can infer statistically that for 19 out
of 20 (95%)users, with little or no experience in either NLP or Prot eg e who
favour RoundTrip Ontology Authoring over Prot eg e also tend to spend less
time completing Ontology editing tasks.

 The R/P Preference score correlates moderately with the ROA SUS score,
similar to previous results, but no longer retains a significant inverse correlation with the Prot eg e SUS score. The reader should note the R/P Preference
scores favour ROA over Prot eg e.

We also varied the tool order evenly among our sample. As noted previously
in [1], once again the SUS scores have differed slightly according to tool order
(as indicated in Table 3). Previous SUS scores for Prot eg e tended to be slightly
lower for B than for A, which we believe may have resulted from the subjects
decrease in interest as the evaluation progressed. While in previous results there
was a decrease in SUS scores for CLOnE (yet still well above the SUS baseline),
in the case of ROA however, the SUS scores increased for task B (see Table 3),
implying that if waning interest was a factor in the decrease in SUS scores
for CLOnE, it does not appear to be the case for ROA. What is of additional
interest is that group I, subjects with industrial background scored on average
10% higher for both ROA SUS and ROA/Prot eg e, which implies that Industrial
collaborators or professionals with an Industrial background favoured a natural
language interface over a standard Ontology Editor even more than Researchers.

3.4 User Feedback

The test users also provided several suggestions/comments about ROA.

 RoundTrip Ontology Authoring becomes much easier, once the rules are
learnt. (This is very interesting considering that no syntax rules, extended
examples or restricted vocabulary list were provided).

B. Davis et al.

 Use of inverted commas should be used only once and afterwards, if same
the class /instance is reused, the system should automatically recognise it
as the previous word.

 Many users suggested displaying the ontology pane on the right hand side
of the text pane, where test users edit the text instead of moving between
two separate panes.

 Some users suggested dynamic ontology generation, once a user finishes typing a sentence, the changes should be displayed automatically in the ontology
pane.

 Similar suggestions to the previous evaluation were provided for user auto-
completion, syntax highlighting, options about available classes, instances
or property names and keywords should be displayed, a similar concept to
modern Word Processor or programming IDEs such as eclipse.

 Some test users with an industrial background demonstrated concern regarding scalability and ROA using with a larger business related ontology and
suggest capabilities for verbalizing a portion of the ontology tree within the
Ontology viewer, using text generation for subsequent editing.

 Some test users appreciated the singular/plural forms and sentence handling

of ROA (e.g., study, studies).

Table 5. Groups of subjects by source and tool order

Source

R Researcher
I Industry
Total

Tool order Total
?

?

?
Table 6. Comparison of the two sources of subjects

ROA SUS

Measure
Pre-test

Group min mean median max
?

?

?
80 100
?

?

?
R/P Preference R
?

?

?
Prot eg e SUS

4 Related Work

Controlled Natural Languages (CL)s are subsets of natural language whose
grammars and dictionaries have been restricted in order to reduce or eliminate
both ambiguity and complexity[11]. CLs were later developed specifically for
?

?

?
computational treatment and have subsequently evolved into many variations
and flavours such as Smarts Plain English Program (PEP), Whites International Language for Serving and Maintenance (ILSAM) [12] and Simplified Eng-
lish.8 They have also found favour in large multi-national corporations, usually
within the context of machine translation and machine-aided translation of user
documentation [11,12].

The application of CLs for ontology authoring and instance population is an
active research area. Attempto Controlled English9 (ACE) [13], is a popular CL
for ontology authoring. It is a subset of standard English designed for knowledge
representation and technical specifications, and is constrained to be unambiguously machine-readable into DRS - Discourse Representation Structure. ACE
OWL, a sublanguage of ACE, proposes a means of writing formal, simultaneously
human- and machine-readable summaries of scientific papers [14,15]. Similar to
RoundTrip Ontology Authoring, ACE OWL also aims to provide reversibility
(translating OWL DL into ACE). The application NLG, for the purposes editing
existing ACE text, is mentioned in [16]. The paper discusses the implementation of the shallow NLG system - an OWL Verbalizer, focusing primarily on
the OWL to ACE rewrite rules, however no evaluation or quantitative data are
provided in attempt to measure the impact of NLG in the authoring process.
Furthermore OWLs allValuesFrom must be translated into a construction which
can be rather difficult for humans to read. A partial implementation is however
available for public testing10.

Another well-known implementation which employs the use of NLG to aid the
knowledge creation process is WYSIWYM (What you see is what you meant).
It involves direct knowledge editing with natural language directed feedback. A
domain expert can edit a knowledge based reliably by interacting with natural
language menu choices and the subsequently generated feedback, which can then
be extended or re-edited using the menu options. The work is conceptually similar to RoundTrip Ontology Authoring, however the natural language generation
occurs as a feedback to guide the user during the editing process as opposed
to providing an initial summary in Controlled Language for editing. A usability
evaluation is provided in [17], in the context of knowledge creation, partly based
on IBM heuristic evaluations11, but no specific quantitative data that we are
aware of, is presented. However, evaluation results are available for the MILE
(Maritime Information and Legal Explanation) application, which used WYSI-
WYM, but in the context of query formulation for the CLIME12 project, of
which the outcome was favourable [17].

Similar to WYSIWYM is GINO (Guided Input Natural Language Ontology Editor) provides a guided, controlled NLI (natural language interface) for

8 http://www.simplifiedenglish-aecma.org/Simplified English.htm
9 http://www.ifi.unizh.ch/attempto/
10 http://attempto.ifi.uzh.ch/site/tools/
11 http://www-03.ibm.com/able/resources/uebeforeyoubegin.html
12 CLIME, Cooperative Legal Information Management and Explanation, Esprit

Project EP25414.

B. Davis et al.

domain-independent ontology editing for the Semantic Web. GINO incrementally parses the input not only to warn the user as soon as possible about errors
but also to offer the user (through the GUI) suggested completions of words
and sentencessimilarly to thecode assist feature of Eclipse13 and other development environments. GINO translates the completed sentence into triples
(for altering the ontology) or SPARQL14 queries and passes them to the Jena
Semantic Web framework. Although the guided interface facilitates input, the
sentences are quite verbose and do not allow for aggregation. A full textual
description of the Ontology is not realized as is the case of the CLOnE text generator [18]. Furthermore, similar, to our evaluation, a small usability evaluation
was conducted using SUS [6], however the sample set of six was too small to infer
any statistically significant results [7]. In addition, GINO was not compared to
any existing Ontology editor during the evaluation. Finally, [19] presents an Ontology based Controlled Natural Language Editor, similar to GINO, which uses
a CFG (Context-free grammar) with lexical dependencies - CFG-DL to generate RDF triples. To our knowledge the system ports only to RDF and does not
cater for other Ontology languages. Furthermore no quantitative user evaluation
is provided.

Other related work involves the application of Controlled Languages for Ontology or knowledge base querying, which represent a different task than that of
knowledge creation and editing but are worth mentioning for completeness sake.
Most notably AquaLog 15 is an ontology-driven, portable Question-Answering
(QA) system designed to provide a natural language query interface to semantic mark-up stored in a knowledge base. PowerAqua [20] extends Aqua-
Log, allowing for an open domain question-answering for the semantic web.
The system dynamically locates and combines information from multiple
domains.

5 Conclusion and Discussion

The main research goal of this paper is to assess the effect of introducing Natural
Language Generation (NLG) into the CLOnE Ontology authoring process to facilitate RoundTrip Ontology Authoring. The underlying basis of our research
problem is the habitability problem (See Section 1): How can we reduce the
learning curve associated with Controlled Languages? And how can we ensure
their uptake as a Natural Language Interface (NLI)? Our contribution is empirical evidence to support the advantages of combining of NLG with ontology
authoring, a process known as RoundTrip Ontology Authoring (ROA).

The reader should note, that we compared Prot eg e with ROA, because
Prot eg e is the standard tool for ontology authoring. Previous work [1] compared CLOnE with Prot eg e. Hence, in order to compare ROA with CLOnE, it

13 http://www.eclipse.org/
14 http://www.w3.org/TR/rdf-sparql-query/
15 http://kmi.open.ac.uk/technologies/aqualog/
?

?

?
was necessary to repeat the experiment and use Prot eg e as the baseline. We
make no claims that Prot eg e should be replaced with ROA, the point is that
ROA can allow for the creation of a quick easy first draft of a complex Ontology by domain experts or the creation of small to medium sized Ontologies by
novice users. Domain experts are not Ontology Engineers. Furthermore, a large
percentage of an initial Ontology would naturally consists of taxonomic relations
and simple properties/relations.

Our user evaluation consistently indicated that our subjects found ROA (and
continue to find CLOnE ) significantly more usable and preferable than Prot eg e
for simple Ontology editing tasks. In addition our evaluation differs, in that we
implemented more tighter restrictions during our selection process, to ensure
that users had no background in NLP or Ontology engineering. Furthermore,
40% of our subjects with an industrial background, tended to score ROA 10%
higher then Researchers indicating that a NLI to a Ontology Editor might be a
preferred option for Ontology development within industry.

In detail, this evaluation differs from previous work [1] by two important
factors: (1) we excluded the CLOnE reference manual from the training material
provided in the previous evaluation; and (2) we introduced a Text Generator,
verbalizing CLOnE text from a given populated Ontology and asked users to edit
the Ontology, using the generated CLOnE text based on an example provided.
We observed two new significant improvements in our results: (1) the previous
evaluation indicated a strong correlation between CLOnE task times and Prot eg e
task times, this correlation has significantly weaken by 32% between ROA and
Prot eg e task times. Hence, where users previously required the equivalent time
to implement tasks both in CLOnE and Prot eg e, this is no longer the case with
ROA (the difference being the text generator); and (2) our previous evaluation
indicated no correlation between either CLOnE/Prot eg e task times and their
respective SUS scores. However, with ROA, we can now infer that 95% of the
total population of naive users, who favour RoundTrip Ontology Authoring over
Prot eg e, would also tend to spend less time completing Ontology editing tasks.
We suspect that this is due to the reduced learning curve caused by the text
generator. Furthermore, ROA tended to retain user interest, which CLOnE did
not. We suspect that the absence of the need to refer to the CL reference guide
was a factor in this. While Prot eg e is intended for more sophisticated knowledge
engineering work, this is not the case for ROA. Scalability, both in performance
and usage, was also an issue raised by our test subjects. From a performance
perspective, when loading large Ontologies, we do not forsee any major issues
as ROA is currently being ported to the newest release of GATE which contains
a completely new Ontology API that utilises the power of OWLIM - OWL
in Memory, a high performance semantic repository developed at Ontotext16.
Finally, from a user perspective, authoring memory frequently used in translation
memory systems or text generation of selective portions of the Ontology (using
a Visual Resource) could significantly aid the navigation and authoring of large
Ontologies.

16 http://www.ontotext.com/owlim/

B. Davis et al.

6 Continuing and Future Work

Several interesting and useful suggestions for improvements to ROA were made,
many of which were already under development within the Nepomuk17 (The
Social Semantic Desktop) project. ROA has been ported to a Nepomuk-KDE
18 application, Semn19 for Semantic Notetaking and will be also be targeted towards the task of semi-automatic semantic annotation. Furthermore, the ROA
text generator was recently used in KnowledgeWeb20 for the verbalization of suggestions for semi-automatic ontology integration. Finally, ROA is being applied
within the EPSRC-funded Easy project to create a controlled natural language
interface for editing IT authorization policies (access to network resources such
as directories and printers) stored as Ontologies.

Acknowledgements

This research has been partially supported by the following grants: KnowledgeWeb (EU Network of Excellence IST-2004-507482), TAO (EU FP6 project
IST-2004-026460), SEKT (EU FP6 project IST-IP-2003-506826, L on (Science
Foundation Ireland project SFI/02/CE1/1131) and NEPOMUK (EU project
FP6-027705).
