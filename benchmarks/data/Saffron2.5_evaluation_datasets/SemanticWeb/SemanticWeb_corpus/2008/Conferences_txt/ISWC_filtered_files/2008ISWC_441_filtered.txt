A Kernel Revision Operator for Terminologies 

Algorithms and Evaluation

Guilin Qi1, Peter Haase1, Zhisheng Huang2, Qiu Ji1, Jeff Z. Pan3,

and Johanna V olker1

1 Institute AIFB, University of Karlsruhe, Germany

{gqi,pha,qiji,jvo}@aifb.uni-karlsruhe.de

2 Department of Mathematics and Computer Science, Vrije University Amsterdam

huang@cs.vu.nl

3 Department of Computing Science, The University of Aberdeen

jpan@csd.abdn.ac.uk

Abstract. Revision of a description logic-based ontology deals with the problem
of incorporating newly received information consistently. In this paper, we propose a general operator for revising terminologies in description logic-based on-
tologies. Our revision operator relies on a reformulation of the kernel contraction
operator in belief revision. We first define our revision operator for terminologies
and show that it satisfies some desirable logical properties. Second, two algorithms are developed to instantiate the revision operator. Since in general, these
two algorithms are computationally too hard, we propose a third algorithm as a
more efficient alternative. We implemented the algorithms and provide evaluation
results on their efficiency, effectiveness and meaningfulness in the context of two
application scenarios: Incremental ontology learning and mapping revision.

1 Introduction

Ontologies are typically not static entities, but they evolve over time and need to revised.
Changes to an ontology may be caused, e.g., by modifications in the application domain,
the reorganization of existing information, or the incorporation of additional knowledge
according to changes in the users needs.

An important problem in revising ontologies is maintaining the consistency of the
ontology, i.e. the accommodation of new knowledge in an ontology without introducing
logical contradictions. Due to the variety of sources and consequences of changes, such
a revision is not a trivial process and thus cannot be left as manual work to the ontology
engineer. Especially in the context of semi-automated ontology engineering, in which
the ontology engineer is supported by agents (e.g. in the form of ontology learning
tools) that suggest ontology changes, an automated revision is desired.

 Guilin Qi, Peter Haase, Qiu Ji and Johanna V olker are partially supported by the EU in the
IST project NeOn (http://www.neon-project.org/). Jeff Z. Pan is partially supported by the
EU MOST project (http://www.most-project.eu/). Zhisheng Huang is partially supported by
EU-funded Projects OpenKnowledge and LarKC. We thank the reviewers for very helpful
comments to improve the quality of our work.

A. Sheth et al. (Eds.): ISWC 2008, LNCS 5318, pp. 419434, 2008.
c Springer-Verlag Berlin Heidelberg 2008

G. Qi et al.

Generally, we can distinguish two kinds of logical contradictions: inconsistency and
incoherence. An ontology is inconsistent iff it has no model, i.e., it is inconsistent in the
first-order sense. An ontology is incoherent iff there exists some unsatisfiable concept
(i.e, an unsatisfiable concept stands for the empty set). There is a close relationship
between inconsistency and incoherence [5], i.e., inconsistency is often caused by adding
instances of concepts and relations to an incoherent ontology. However, an ontology can
be incoherent but consistent. Incoherence is a problem that occurs in terminologies of
ontologies. Resolving incoherence in a single terminology has been widely discussed
(for example, see [19, 20]). However, there is very little work on resolving incoherence
between terminologies of different ontologies.

There exists a number of prior work on revision in DLs, such as those reported
in [5, 6, 7, 16]. Most of them focus on postulates for revision operators. For example,
an important principle is that one should delete information in the original ontology
as little as possible to accommodate the new knowledge consistently. Theoretically, it
is important to know how to characterize a revision operator in terms of postulates.
However, for practical applications, we require concrete revision operators that can be
used. There are concrete revision operators defined to deal with inconsistency [7, 16].
But to the best of our knowledge, there is no revision operator dealing specifically with
incoherence (as opposed to inconsistency) in the context of revision.

In this paper, we propose a kernel revision operator in Description Logic-based ontologies based on MIPS (minimal incoherence-preserving sub-terminologies) and an
incision function. The notion of MIPS is originally developed for non-standard reasoning service in debugging incoherent terminologies [19, 20]. It is similar to the notion of
a kernel set in belief base change defined in [10]. In order to resolve the logical contra-
diction, the incision function is used to select from each MIPS the axioms to be removed
from the original ontology. Our revision operator focuses on revising terminologies, i.e.
the TBox-part of ontologies. Two algorithms are developed to define specific kernel
revision operators. The first algorithm is based on the reformulation of Reiters Hitting
Set Tree (HST) algorithm given in [20] and a scoring function. In this algorithm, we
first compute all the MIPSs of the original ontology w.r.t. the new ontology. Then we
calculate for each axiom in the MIPS a score corresponding to the number of MIPS
which contain this axiom. Finally, we take subsets of those MIPSs that contain axioms
with maximal scores and apply the reformulated HST algorithm to get a set of axioms
to be deleted. The second algorithm is applied to ontologies where each axiom is attached a confidence value which indicates the reliability of the axiom. Such confidence
values, as well as other kinds of provenance information, are typically generated by
automated agents such as ontology learning or matching tools. The motivation for exploiting confidence information in this algorithm is to delete only axioms that are least
reliable from each MIPS of the original ontology w.r.t. the new ontology. In this algo-
rithm, we need to compute all the MIPSs, which is computationally hard in general.
Therefore, we propose a third, alternative algorithm, which utilizes confidence values
attached to axioms in the ontology to resolve unsatisfiable concepts without computing
all the MIPSs. Compared to the second algorithm, this algorithm is computationally
easier, but it does not necessarily remove more axioms from the original ontology after
?

?

?
revision. Although it does not produce a kernel revision operator, it can be viewed as a
good variant of the second algorithm.

We implemented the three algorithms and provide evaluation results on their efficiency and effectiveness in the context of two application scenarios: Incremental ontology learning and mapping revision. To evaluate the scalability of our algorithms, we
iteratively add a set of new terminology axioms to an ontology. We also evaluate the
effectiveness of the algorithms by counting the number of axioms deleted from the old
ontology by our algorithms in each iteration. Finally, we evaluate the meaningfulness
of the revision results by means of a user study, where the meaningfulness is measured
by the ratio of correct removals.

The rest of this paper is organized as follows: Section 2 provides a preliminary introduction to Description Logics and various notions in ontology debugging. Section 3
overviews related work of revision in DLs. Section 4 presents our revision operator for
terminologies. Section 5 proposes some algorithms to instantiate the revision operator.
In Section 6, we report on evaluation results with real life data. We conclude in Section 7.

2 Preliminaries

This section introduces some basic notions of Description Logics (DLs) as well as the
essential notions of debugging terminologies. Since our revision operator is independent of a specific DL language, and thus can be applied to any DL, we only give a
general overview of description logics.
In our work, we focus on DL-based terminological ontologies: A terminology (TBox)
T consists of concept axioms and role axioms. A subset of a TBox is called a sub-TBox.
Concept axioms have the form C  D where C and D are (possibly complex) concept
descriptions1, and role axioms are expressions of the form RS, where R and S are
role descriptions. We will refer to both concept axioms and role axioms as terminology
axioms.

,I) consists of a non-empty domain set #I

The semantics of DLs is defined via a model-theoretic semantics, which explicates
the relationship between the language syntax and the model of a domain: An interpretation I = (#I
and an interpretation
function I
, which maps from concepts and roles to elements of the domain, subsets of
the domain and binary relations on the domain, respectively.
Given an interpretation I, we say that I satisfies a concept axiom C  D (re-
spectively, a role inclusion axiom R  S) if C
, respectively). An
interpretation I is called a model of a TBox T , iff it satisfies each axiom in T . We use
M od(T ) to denote all the models of a TBox T . A named concept C in a terminology T
I = . A terminology T is incoherent iff
is unsatisfiable iff, for each model I of T , C
there exists an unsatisfiable named concept in T . Two TBoxes T and T 
are equivalent,
denoted by T  T 
, iff M od(T ) = M od(T ).

I  S

ID

(R

We now introduce the notions of MIPS and MUPS which will be used to define our
revision operator. Both of these terms have originally been defined in [19] and are used
to pinpoint errors in an ontology.

1 A complex concept is a concept that is formed by some atomic concepts and constructors such
as conjunction  and disjunction .
?

?

?
.

, and A is satisfiable in every sub-TBox T   T 

Definition 1. Let A be a named concept which is unsatisfiable in a TBox T . A set
T T is a minimal unsatisfiability-preserving sub-TBox (MUPS) of T w.r.t. A if A is
unsatisfiable in T 
A MUPS of T w.r.t. A is a minimal sub-TBox of T in which A is unsatisfiable.
Example 1. Let T = {AB, AB, CA, C  D, C  D}. There are two unsatisfiable concepts in T : A and C. It is easy to check that there are two MUPSs of T w.r.t.
C: {AB, AB, CA} and {C  D, C  D}, and there is one MUPS of T w.r.t.
A: {AB, AB}.
MUPSs are useful for relating sets of axioms to the unsatisfiability of specific concepts,
but they can also be used to calculate minimal incoherence preserving sub-TBoxes,
which relate sets of axioms to the incoherence of a TBox in general and are defined as
follows.
Definition 2. Let T be an incoherent TBox. A TBox T T is a minimal incoherencepreserving sub-TBox (MIPS) of T if T 
is
coherent.
A MIPS of T is the minimal sub-TBox of T which is incoherent. For T in Example 1,
we get the following MIPSs: {AB, AB} and {C  D, C  D}.

is incoherent, and every sub-TBox T T 

3 Related Work and Motivation

This work is related to belief revision which has been widely discussed in the literature.
The theory of belief revision in propositional and first-order logic deals with logical
inconsistency resulting from revising a knowledge base by newly received information.
Alchourr on, Gardenfors and Markinson (AGM for short) [1] propose a set of postulates
to characterize a revision operator. In AGMs work, beliefs of an agent are represented
by a set of formulas closed under logical consequence, called a belief set. A revision
operator is an operation that maps a belief set and a formula to a belief set. This representation is afflicted by a number problems. For example, there is potentially an infinite
number of formulas in a belief set. Therefore, several researchers have proposed to use
a belief base which is a set of formulas that is not closed under logical consequence to
represent the beliefs of an agent [11, 13]. In the scenario of ontology change, this later
representation seems to be more natural because we do not require that an ontology
should be closed under logical consequence.

The problem of revision in DLs has been extensively studied in the literature. In [6],
Flouris, Plexousakis and Antoniou generalize the AGM framework in order to apply the
rationales behind the AGM framework to a wider class of logics, i.e. a larger class of
logics which are AGM-compliant. In [5], a framework for the distinction between incoherence and inconsistency of an ontology is proposed. A set of rational postulates for
a revision operator in DLs is proposed based on the distinction between coherent negation and consistent negation. However, in [5] no concrete revision operator is proposed.
In [16], reformulated AGM postulates for revision are adapted to DLs. Two revision
operators that satisfy the adapted postulates are given, but no algorithm to implement
any of the operators is introduced.
?

?

?
Similar to our revision operator, the revision operator defined in [18] also utilizes an
incision function to select axioms to be removed from the original ontology. Our work
differs from theirs in several aspects. First, our revision operator deals with incoherence
instead of inconsistency. Second, we provide algorithms for computation of specific revision operators and discuss evaluation results on their implementation. This work is
also related to the work presented in [8], in which an algorithm is given to determine
consistent sub-ontologies by adding an axiom to an ontology. The algorithm is based
on a selection function by assuming that all axioms in the ontology are connected. Re-
cently, a revision operator has been defined to repair erroneous mappings derived by
automated ontology alignment systems [14]. Their revision operator, however, calculates neither MUPS nor MIPS and may remove too much information.

According to the discussion of related work, although there is no revision operator
dealing with incoherence, it is possible to define such a revision operator based on the
result of debugging and diagnosis. However, there are several problems to be solved.
First, the notions of MUPS and MIPS are defined on a single ontology, whilst we need to
consider two ontologies such that one of them is more important than the other. There-
fore, we need to generalize the notions of MUPS and MIPS. Second, it has been shown
in [20] that finding all the MUPS and MIPS in ALC is time-consuming and efficiency
is a problem that prevents us from calculating all the MUPS and MIPS. This problem is
even more serious for more expressive DLs (thus computationally harder). Third, even
if we can find an efficient algorithm for calculating all the MIPS, we must find an efficient way to remove as few axioms as possible to restore coherence. We tackle these
problems by first defining a generalized MIPS and a general revision operator based on
it. We then give three algorithms to instantiate the general revision operator.

4 Kernel Revision Operator for Terminologies

In this section, we define our revision operator based on the notion of MIPS. Originally,
the notion of a MIPS is defined on a single TBox, whereas a revision operator deals
with conflicts between two TBoxes. We therefore generalize MIPS by considering two
TBoxes: the TBox T to be revised, and the newly received TBox T0. In the following,
we further assume that both T and T0 are coherent.
Definition 3. Let T and T0 be two TBoxes. A minimal incoherence-preserving subof T w.r.t. T0 is a sub-TBox of T which satisfies (1) T T0 is incoTBox (MIPS) T 
herent; (2) T T 
, T T0 is coherent. We denote the set of all MIPSs of T w.r.t T0
by M IP ST0(T ).
A MIPS of TBox T w.r.t. TBox T0 is a minimal sub-TBox of T that is incoherent with
T0. This definition of MIPS is similar to the notion of a minimal axiom set given in [2]
where an ontology is split into a static part and a rebuttal part. It can be considered as
the kernel defined by Hansson in [10]. Similar to Definition 3, we can define a MUPS
of T w.r.t. T0 and an unsatisfiable concept of T  T0. When T0 is an empty set, then
Definition 3 is reduced to Definition 2. In classical logic, given a knowledge base A
which is a set of classical formulas and a formula , a -kernel of A is the minimal
subbase of A that implies . To define a contraction function for removing knowledge

G. Qi et al.

from a knowledge base, called kernel contraction, Hansson defines an incision function
which selects formulas to be discarded in each -kernel of A. We adapt the incision
function to define our revision operator.
Definition 4. Let T be a TBox. An incision function for T , denoted as , is a function
( : 22T  2T ) such that for each TBox T0
Ti;

(i) (M IP ST0(T ))
(ii) if T M IP ST0(T ), then T (M IP ST0(T )) = .
An incision function for a TBox T is a function such that for each TBox T0, it selects
formulas from every MIPS of T w.r.t. T0 if this MIPS is not empty. Condition (i) says
the axioms selected by an incision function must belong to some MIPSs of T w.r.t. T0.
Condition (ii) says each MIPS of T w.r.t. T0 must have at least one axiom selected. The
incision function plays a similar role as concept pinpointing in [19]. However, the latter
is only applied to a single ontology.

TiMIP ST0 (T )

An important incision function is the one which is called minimal incision function [4]. The idea of this incision function is to select a minimal subset of elements
from the set of kernel sets. We adapt this incision function as follows.
Definition 5. Let T be a TBox. An incision function  for T is minimal if there is
(M IP ST0(T ))
for T such that there is a TBox T0, 
no other incision function 
(M IP ST0(T )).
A minimal incision function selects a minimal subset of T w.r.t. the set inclusion.
However, among all the minimal incision functions, some of them select more axioms
than others. To make the number of selected axioms minimal, we define a cardinalityminimal incision function.
Definition 6. Let T be a TBox. An incision function  for T is cardinality-minimal if
(M IP ST0(T ))|<
there is no other incision function 
|(M IP ST0(T ))|.
It is clear that a cardinality-minimal incision function is always a minimal incision
function.
Proposition 1. Let T be a TBox. Suppose  is a cardinality-minimal incision function
for T , then it is a minimal incision function.
From each incision function, we can define an operator for revising a TBox T by a
newly received TBox T0. The idea is that we first calculate the MIPS of TBox T w.r.t
TBox T0, then delete axioms in T selected by the incision function. After that, we take
the union of the modified TBox and T0 as the result of the revision.
Definition 7. Let T be a TBox, and  be an incision function for T . The kernel revision
operator  for T is defined as follows: for each TBox T0,

such that there is a TBox T0, |
?

?

?
T  T0 = (T \ (M IP ST0(T )))  T0.

The result of a revision by the kernel revision operator only contains a single TBox.
According to the definition of an incision function, the resulting TBox of the kernel
revision operator is always a unique coherent TBox.

0 , then T  T0  T  T 
0 .

A Kernel Revision Operator for Terminologies  Algorithms and Evaluation

Proposition 2. Let T be a TBox, and  be an incision function for T . The operator 
satisfies the following properties: for any TBoxes T0, T 
(R1) T0  T  T0.
(R2) If T  T0 is coherent, then T  T0 = T  T0.
(R3) If T0 is coherent, then T  T0 is coherent.
(R4) If T0  T 
(R5) If   T and   T  T0, then there is a subset S of T and a subset S0 of T0
such that S  S0 is coherent, but S  S0  {} is not.
Proof. (sketch) It is clear that (R1)-(R3) hold. We show that (R4) holds. Suppose T0 
0 . According to Definition 3, we must have M IP ST0(T ) = M IP ST 
0 (T ). Therefore,
T 
we have (M IP ST0(T )) = (M IP ST 
0 . (R5)
holds because the incision function that is used to define a kernel revision operator only
selects axioms from MIPSs of T w.r.t. T0. Therefore, these axioms must be in a subset
of T that is in conflict with some axioms in T0.
Properties (R1)-(R4) are adapted from postulates (O+1), (O+2*), (O+3*) and (O+4) in
[5]. (R1) says that every axiom in the new TBox should be accepted after revision. (R2)
says, if two TBoxes have no contradiction, then we do not need to change anything.
(R3) means that if the new TBox is coherent, then the result of revision should also
be coherent. (R4) is a weakened form of syntax-independence. That is, the revision
operator is independent of the syntactic form of axioms in the new TBox. (R5) is a new
property which is adapted from the core-retainment postulate in [10]. It states that if an
axiom is deleted after revision, then it must be responsible for the conflict.

0 (T )). It follows that T  T0  T  T 

5 Algorithms

The kernel revision operator is defined by an incision function. However, we have not
given any incision function up to now. In the following, inspired by the work reported
in [21], we propose some algorithms for computing an incision function based on Re-
iters Hitting Set Tree (HST) algorithm [17] which is reformulated in [20]. We briefly
introduce Reiters theory. Given a universal set U , and a set S = {s1, ..., sn} of subsets
of U which are conflict sets, i.e. subsets of the system components responsible for the
error. A hitting set T for S is a subset of U such that siT =  for all 1  i  n. A min-
  T is a hitting set for S. Reiters
imal hitting set T for S is a hitting set such that no T
algorithm is used to calculate minimal hitting sets for a collection S = {s1, ..., sn} of
sets by constructing a labeled tree, called a Hitting Set Tree (HST). We select one arbitrary minimal hitting set of M IP ST0(T ) given by HST algorithm in [20]. We denote
the revised HST algorithm as HST ree. We do not apply the revised HST algorithm to
M IP ST0(T ) because there may have a large number of hitting sets if we use all the
MIPSs and the algorithm will be very slow. Instead, we apply the revised HST algorithm
to the set of subsets of the MIPSs in M IP ST0(T ).

The first algorithm is based on the scoring function on axioms2 which is defined as

follows.
2 A scoring function has been used in [15] to measuring inconsistency in a single ontology and

is defined by MIPS, whilst ours is not defined by MIPS.

G. Qi et al.

Algorithm 1. Algorithm for Repair based on scoring function
Data: Two TBoxes T and T0, where T is the TBox to be revised
Result: A repaired coherent TBox T  T0
beginC = 

calculate M IP ST0 (T )
for ax  S

Ti do
TiM IP ST0 (T )
wax := SM IP ST0 (T )({ax})
for TiM IP ST0 (T ) do
Ai := {ax  Ti : ax
C := C  {Ai}

(M IP ST0 (T )) := HST ree(C)
T  T0 := T \ (M IP ST0 (T ))  T0
return T  T0

  Ti, wax > wax}

end

Definition 8. Let T be a TBox and M be a set of sub-TBoxes of T . The scoring function
for T w.r.t. M, is a function SM : P(T )  N such that for all T P(T )

SM(T 

) = |{TiM : TiT =}|.

The scoring function SM for T returns for each subset T 
of M that have an overlap with T 
{axi}, where axi is an axiom in T , we can attach a degree to each axiom in T .

of T the number of elements
. If we apply the scoring function to each singleton
In Algorithm 1, we first calculate all the MIPSs of T w.r.t. T0 (MIPSs for short). The
approach for calculating all the MIPSs is based on a black-box algorithm for finding all
justifications proposed in [12]. We then compute the score of each axiom in the union
of the MIPSs (see the first for loop) by applying the scoring function SMIP ST0 . For
each MIPS, we select a subset of it containing those axioms whose scores are maximal
among all the axioms in the MIPS, and we apply the modified HST algorithm to these
axioms (see the second for loop and the line after it). The result of the modified HST
algorithm is the set of axioms to be deleted, i.e., (M IP ST0(T )). After removing
them, we restore coherence of the TBox T w.r.t. T0. In our algorithm, we use subsets of
MIPSs consisting of those axioms with highest scores as an input to the HST algorithm,
instead of using all the MIPSs. So, the number of removed axioms may not be minimal.

Example 2. Suppose that we have two TBoxes:
T = {E  B, D  B, F  B, F  C}, and T0 = {D  E, G  D, F 
D, H  A}. The MIPSs of T w.r.t. T0 are T  = {E  B, D  B} and T  = {D 
B, F  B}.
The score of the disjointness axiom D  B is 2, because it belongs to both MIPSs.
The scores of the other axioms are 1. Therefore, C = {{D  B},{D  B}} and
(M IP ST0(T )) = {D  B}. So we delete D  B and the result of revision is
T  T0 = {E  B, F  B, F  C, D  E, G  D, F  D, H  A}.
In some cases, there are confidence values attached to axioms in an ontology. These
confidence values can be generated during an ontology learning process (see [9]) or
?

?

?
Algorithm 2. Algorithm for Repair based on confidence values
Data: Two TBoxes T and T0, where T is the TBox to be revised, each axiom ax in T is
Result: A repaired coherent TBox T c T0
beginC = 

attached a confidence value wax

calculate M IP ST0 (T )
for TiM IP ST0 (T ) do
Ai := {ax  Ti : ax
C := C  {Ai}

(M IP ST0 (T )) := HST ree(C)
T c T0 := T \ (M IP ST0 (T ))  T0
return T c T0

  Ti, wax < wax}

end

given by human experts. When confidence values are attached to axioms in the TBox,
we can choose an axiom with least confidence from a MIPS and delete it. Note that we
do not need to know the exact values attached to the axioms. What matters is the total
ordering between axioms. A natural idea is to replace the score wax of each axiom ax
in Algorithm 1 by its confidence value if applicable. This leads us to Algorithm 2.

In Algorithms 1 and 2, we extract a subset of each MIPS which consists of either
those axioms with the maximal score or those with least confidence values. We then
apply the modified HST algorithm to these subsets to find a hitting set. Our algorithms
clearly compute an incision function, which is not the minimal incision function. How-
ever, according to our experiment on real life ontologies, our algorithms delete only a
small number of axioms in order to restore consistency and have acceptable evaluation
of meaningfulness.
Example 3. (Example 2 Continued) Suppose axioms in the TBox T are attached with
confidence values as follows:
wEB = 0.4, wDB = 0.5, wFB = 0.6, wFC = 0.9.
The axioms in T0 are assigned weight 1, i.e., they are firmly believed.
It is clear that A = {E  B} and A = {D  B}. Therefore, C = {{E 
B},{D  B}} and (M IP ST0(T )) = {E  B, D  B}. Therefore, T c T0 =
{F  B, G  D, D  E, H  A, F  C, F  D}.
In Algorithm 3, when resolving incoherence of a TBox, we do not compute all the
MIPSs. Instead, we resolve incoherence by iteratively dealing with unsatisfiable con-
cepts. That is, we remove axioms in the MUPSs of an unsatisfiable concept and make
it satisfiable before dealing with another unsatisfiable concept, and so on. The function
which computes all the MUPSs of T w.r.t. T0 and C is similar to the algorithm to compute MUPS in [12], and it is denoted by GET M U P ST0(C,T ). The only difference is
that after computing a single MUPS of T  T0 w.r.t. C, we only take the intersection of
the MUPS and T as the node in the Hitting Set Tree. For each unsatisfiable concept, we
take the subset of every MUPS which contains axioms with minimal confidence values
and then apply the HST algorithm to select the axioms to be deleted. In this sense, this
algorithm still achieves some kind of minimal change when resolving unsatisfiability of

G. Qi et al.

Algorithm 3. Adapted algorithm for Repair based on confidence values
Data: Two TBoxes T and T0, where T is the TBox to be revised, axioms in T are attached
Result: A repaired coherent TBox T w T0
beginC := 

with confidence values

for CGET ALLCON CEP T S(T  T0) do
while T  T0 |= C   do
MC,T ,T0 := GET M U P ST0 (C,T )
for Ti  MC,T ,T0 do

  Ti, wax < wax}

Ti := {axTi :  ax
C := C  {Ti}
TC := HST ree(C)
T := T \ TC
C := 
return T  T0

end

a concept, even if the revision operator implemented by this algorithm is not a kernel
revision operator. As we we do not need to calculate all the MIPSs, the algorithm is
much more efficient than Algorithm 2 as long as all the MIPS in Algorithms 1 and 2
are calculated from all the MUPS as suggested by Schlobach and Cornet in [19].
Example 4. (Example 3 Continued) There are three unsatisfiable concepts in T  T0:
G, D and F . Suppose our algorithm chooses F first. The MUPS of F in T w.r.t. T0 is
T  = {D  B, F  B}. So MF,T ,T0 = {T }. Since wDB < wFB, we have
C = {{D  B}}. So TC = {D  B}.
We replace T by T \ {D  B}. It is easy to check that T  T0 is coherent now.
So, the algorithm terminates and the result of the revision is T w T0 = {F  B, E 
B, G  D, D  E, H  A, F  C, F  D}.

6 Experimental Evaluation

Our algorithms have been implemented in Java as part of the RaDON plugin3 for the
NeOn Toolkit.4 In this section, we provide an evaluation and comparison of the algorithms with respect to efficiency, effectiveness and meaningfulness. The experiments
have been performed on a Linux server running Suns Java 1.5.0 with a maximum heap
space 2048 MB. For each revision operation, the maximal time limit is 1 hour.

6.1 Application Scenarios and Data Sets

We performed the evaluation in an ontology learning scenario and an ontology mapping scenario. All data sets can be downloaded from RaDON website5. In the ontology

3 http://radon.ontoware.org/
4 http://www.neon-toolkit.org/
5 http://radon.ontoware.org/downloads/data-revision-iswc08.zip
?

?

?
learning scenario, an ontology is automatically and incrementally generated using ontology learning algorithms. Dealing with incoherence is especially important in ontology learning: Due to the nature of ontology learning algorithms, the acquired ontologies
inherently represent uncertain and possibly contradicting knowledge. In the ontology
mapping scenario, we start with two heterogeneous source ontologies, which are then
extended and revised by adding mappings relating elements of the two ontologies. The
mappings are created by an ontology matching system. As in the case of ontology learn-
ing, also the matching systems produce uncertain and potentially erroneous mappings.
As a result, the integrated ontologies become incoherent in many cases. Resolving the
incoherence caused by the mappings is a critical task to improve the quality of ontology
mapping results.

Ontology learning scenario: We applied the ontology learning framework Text2Onto6
on a text corpus consisting of abstracts from the knowledge management information
space of the BT Digital Library. We extracted concepts, taxonomic and non-taxonomic
relationships, as well as disjointness axioms from the documents in the information
space. The generated axioms are annotated with confidence values based, e.g., on lexical
context similarity or the frequency of lexico-syntactic patterns matched in the text. The
generated ontology bt km comprises 4, 000 terminological axioms in total.
Starting with an initially empty ontology, in every revision step we incrementally7
add an ontology T0 of 100 randomly generated axioms to the ontology T . For each
iteration, if T w.r.t. T0 turns incoherent, we apply our algorithms to obtain a coherent
revised ontology. Otherwise, we simply add T0 to T . Then the revised ontology (i.e. the
modified T ) serves as input for the next iteration.

Ontology mapping scenario: Here we address the scenario of integrating two heterogeneous source ontologies via mappings. While the individual source ontologies are
locally coherent, relating them with mapping axioms may turn the integrated ontologies globally incoherent. In this scenario, we assume the two source ontologies to be
fixed and the generated mappings to be revised in the case of logical contradictions.
Therefore, we apply our revision algorithm to remove only mapping axioms and treat
the source ontology axioms as stable.

For this scenario, we use the ontology mapping data sets provided by the University
of Mannheim.8 The data sets include some source ontologies and mappings used in the
ontology alignment evaluation initiative9, which provides a platform to evaluate ontology matching systems. For our test, we use as source ontologies different ontologies
about the domain of scientific conferences: CONFTOOL (a SIF(D) ontology), CMT
(a ALCIF(D) ontology), EKAW (a SHIN ontology), CRS (a DL-Lite ontology) and
SIGKDD (a ALI(D) ontology) with 197, 246, 248, 69 and 122 axioms respectively.
The pairwise mappings were generated automatically by the HMatch system [3]. They

6 http://ontoware.org/projects/text2onto/
7 By adding set of axioms incrementally, we are actually doing iterated revision. The purpose
of doing this is to evaluate the scalability of our algorithms. Discussions on iterated revision
using our revision operators are out of the scope of this paper and will be left as future work.

8 http://webrum.uni-mannheim.de/math/lski/ontdebug/index.html
9 http://om2006.ontologymatching.org/

G. Qi et al.

include CONFTOOL-CMT with 14 mapping axioms, EKAW-CMT with 46 mapping
axioms and CRS-SIGKDD with 22 mapping axioms. We selected these ontologies and
mappings for our experiments because they exhibit inconsistencies when integrated,
and are thus interesting for revision experiments.

6.2 Evaluation Measures

In the following, we evaluate our algorithms with respect to aspects of efficiency, effectiveness and meaningfulness.

For measuring its efficiency, we provide the revision time t including the time to
check whether the ontology is incoherent as well as the time to debug and resolve
the incoherence. We further measure the effectiveness of our algorithms in terms of
the number R of axioms which need to be removed from T to restore the coherence.
The fewer axioms are removed, the better the algorithm complies with the principle of
minimal change.

In order to measure the meaningfulness of our algorithms, four users are asked to
assess whether the removal of an axiom in a particular revision was correct from their
point of view. Specifically, we provide several axioms which are selected for removal
by our algorithms as well as the MIPSs and MUPSs containing them (and scores of the
axioms or confidence degrees attached to the axioms if applicable). For each removed
axiom, we ask the users to decide whether the removal: (1) was correct, (2) was incor-
rect, or (3) whether they are unsure. For the evaluated results returned by each user, the
meaningfulness is then measured by the ratio of correct removals:

Correctness = #Correct Removals
#T otal Removals

Similarly we can define an Error Rate based on the incorrect removals and an
Unknown Rate based on the removals where the users were unsure. We combine the
obtained Correctness (respectively Error Rate and Unknown Rate) values from different users by averaging them.

6.3 Evaluation Results

Analysis of Efficiency and Effectiveness

Results for the ontology learning scenario: The runtime performance of our algorithms
over ontology bt km is depicted by Figure 1. Additional details for the entire ontology
(4, 000 axioms) are shown in Table 1. It can be seen from the figure that the accumulative revision time does not linearly increase with the number of TBox axioms. This is
because the revision time is related not only to the size of the input TBox, but also to
the number of MUPSs. Take the iteration when the size of the current TBox T reaches
about 2, 900 as an example. In this iteration, Algorithm 1 computes 124 unsatisfiable
concepts and 154 MUPSs based on its previous revision results. The MUPSs found in
this iteration are much more than those obtained in previous iterations, and thus the accumulative time for Algorithm 1 increases sharply in such case. This explanation can be
also applied to Algorithm 2, while for Algorithm 3 in this iteration, only 3 unsatisfiable
?

?

?
)
s
(

e
m

i
t

i

n
o
s
v
e
r

i

l

e
v
i
t
a
u
m
u
c
c

700 1000 1300 1600 1900 2200 2500 2800 3100 3400 3700 4000

Number of TBox axioms

Algorithm 1

Algorithm 2

Algorithm 3

Fig. 1. The runtime performance of ontology revision for ontology bt km

Table 1. Accumulative results for the entire ontology bt km

Algorithm # of Unsatisf. Concepts # of MUPS # of Removals Revision time t (Seconds)
Algorithm 1
Algorithm 2
Algorithm 3

6, 856
4, 486
?

?

?
concepts and 3 MUPSs are computed, similarly to the previous iterations. Thus in this
iteration, the accumulative time for this algorithm increases smoothly.

Let us now compare Algorithm 1 with Algorithm 2, since they share the same procedure to debug incoherence10, but apply different strategies to resolve incoherence:
Table 1 shows that Algorithm 1 removes fewer axioms than Algorithm 2, while taking
more time to revise. On the one hand, the strategy using a scoring function better follows the principle of minimal change. On the other hand, as Algorithm 2 removes more
axioms, more potential incoherence is resolved which may exist when new information
is added. Thus, less MUPSs are computed in the end.

Second, we compare Algorithm 2 with Algorithm 3, because they use the same strategy to restore coherence while relying on different debug procedures. Algorithm 3 is
considerably faster than Algorithm 2, since less MUPSs need to be computed. From
the MUPSs obtained by the two algorithms, we observe that most of the unsatisfiable
concepts can be derived from others. In such case, if we resolve the unsatisfiability of
some concepts, others will be resolved automatically. Therefore, Algorithm 3 is much
more efficient than Algorithm 2.

Results for the ontology mapping scenario: Table 2 presents the evaluation results of
our algorithms based on the mapping data set described above. According to table 2,
Algorithm 3 outperforms the other two w.r.t. efficiency. The reason is that Algorithm 3

10 When we say debug incoherence, we mean finding all the MUPSs of an unsatisfiable concept

or finding all MIPS.

G. Qi et al.

Table 2. Evaluation results to revise mappings

Mappings

Strategy

# of Unsatisf. # of MUPS # of MUPS MUPS Size # of Removed Time

Concepts

Algorithm 1
CONFTOOL-CMT Algorithm 2
Algorithm 3
Algorithm 1
Algorithm 2
Algorithm 3
Algorithm 1
CRS-SIGKDD Algorithm 2
Algorithm 3

EKAW-CMT
?

?

?
All
?

?

?
Avg
?

?

?
Avg

Axioms

seconds
?

?

?
does not need to handle all the unsatisfiable concepts, for example, 4 by Algorithm 3
versus 26 by other algorithms for CONFTOOL-CMT. Algorithm 1 has similar efficiency
as Algorithm 2. This shows the efficiency to resolve incoherence using confidence values is similar to that using scoring function, since both algorithms share the same procedure to debug incoherence, but apply different strategies to resolve it.

Regarding to the effectiveness, Algorithm 2 removes more axioms than Algorithm 1
to restore the coherence in most cases. The reason is that for each found MIPS, there
is always one axiom with the lowest confidence value. In such case, we have no other
choice but removing this axiom when using confidence values to resolve incoherence.
But for Algorithm 1, usually we have several choices for each MIPS. Therefore, by
applying the Hitting Set Tree algorithm, Algorithm 1 can find a hitting set which is
cardinality-smaller than that of Algorithm 2. For example, Algorithm 2 removes 8
axioms when repairing mappings in CONFTOOL-CMT, while Algorithm 1 removes
only 4 axioms. But for EKAW-CMT, Algorithm 1 removes a few more axioms than
Algorithm 2, because in most cases there are at least two axioms with the lowest confidence values for each MIPS. For all the test ontologies, Algorithm 3 removes less
axioms than Algorithm 2. The reason is that Algorithm 3 may remove an axiom in a
MUPS which belongs to several MIPS and Algorithm 2 always removes one axiom
with the lowest confidence value in each MIPS.

To sum up, Algorithm 1 removes the least number of axioms in most cases, best
complying with the requirement of minimal change. Algorithm 3 has excellent runtime
performance compared with other two algorithms. At the same time, it sometimes removes fewer axioms than Algorithm 2. Thus it is the preferable option to deal with
incoherence for large data sets when we have information about confidence values (or
other ranking information) for axioms in the ontology.

Analysis of Meaningfulness. Table 3 shows the results for the meaningfulness of the
repair based on the expert users assessment whether the removal was correct. That is,
if the definition of a removed axiom does not make sense according to the expert users
experience, we consider the removal as correct.

From Table 3 we can see that for all data sets and algorithms the rate of correct
removals is considerable higher than that of the erroneous removals. This shows that
?

?

?
Table 3. Evaluation results for meaningfulness

Data set

bt km

Algorithm # of Removed Axioms Correctness Error Rate Unknown Rate
Algorithm 1
Algorithm 2
Algorithm 3
Algorithm 1
CONFTOOL-CMT Algorithm 2
Algorithm 3
Algorithm 1
Algorithm 2
Algorithm 3
Algorithm 1
Algorithm 2
Algorithm 3

0.28
0.19
0.13
0.31
0.03
0.03
0.11
0.05
0.07
0.40
0.25
0.07

0.41
0.53
0.65
0.56
0.97
0.97
0.68
0.64
0.84
0.60
0.50
0.79
?

?

?
0.21
0.31
0.09

0.25
0.14

0.31
0.28
0.22
0.13
?

?

?
EKAW-CMT

CRS-SIGKDD

generally that the ranking of axioms in our approach works well for resolving inco-
herence. The exact ratios largely depend on the data set. Especially the Unknown Rate
varies considerably for the different data sets; this is due to the nature of the data sets:
For bt km, there are many cases in which the users do not know whether the removal
make sense or not, as the concepts in this data set are quite abstract like model,
knowledge and order, it is hard to decide the relationships among those concepts.
Comparing the meaningfulness results obtained by different algorithms, Algorithm 1
using scoring function is designed to comply with the principle of minimal change, and
thus it typically removes fewer axioms. Yet, as it does not take any information about
the confidence into account, Algorithm 2 and 3 using confidence values to resolve incoherence outperform the Algorithm 1 in terms of meaningfulness in most cases. For data
set CRS-SIGKDD, the correctness for Algorithm 2 is higher than that for Algorithm 1,
but the Error Rate is much lower. Algorithm 3 consistently yields the most meaningful
results. This shows that relying on confidence values, as provided by ontology learning
tools applied to bt km, or generated by ontology matching systems, leads to considerably more meaningful results when applying them for resolving incoherence.

7 Conclusions

In this paper, we have proposed a kernel revision operator for terminologies using an
incision function. We have shown that our operator satisfies desirable logical properties.
Further, we have provided two algorithms to instantiate our revision operator, one based
on a scoring function and another one based on confidence values. Since these two algorithms need to compute all the MIPSs of the original ontology w.r.t. the new ontology,
they are computationally very hard. Therefore, we have proposed an alternative algorithm which repairs the ontology by calculating MUPSs of the original ontology w.r.t.
the new ontology and an unsatisfiable concept. According to our experimental results
with real life ontologies, this last algorithm shows good scalability, although it may potentially remove slightly more axioms than the first one. An interesting future work is
to explore efficient algorithms for generating minimal (or cardinality minimal) incision
functions.

G. Qi et al.
