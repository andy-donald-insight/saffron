On the Semantics of Trust and Caching in the

Semantic Web

Simon Schenk

ISWeb, University of Koblenz-Landau, Germany

sschenk@uni-koblenz.de

http://isweb.uni-koblenz.de/

Abstract. The Semantic Web is a distributed environment for knowledge representation and reasoning. The distributed nature brings with it
failing data sources and inconsistencies between autonomous knowledge
bases. To reduce problems resulting from unavailable sources and to improve performance, caching can be used. Caches, however, raise new problems of imprecise or outdated information. We propose to distinguish between certain and cached information when reasoning on the semantic
web, by extending the well known FOUR bilattice of truth and knowledge orders to FOUR  C, taking into account cached information. We
discuss how users can be offered additional information about the reliability of inferred information, based on the availability of the corresponding
information sources. We then extend the framework towards FOUR  T ,
allowing for multiple levels of trust on data sources. In this extended set-
ting, knowledge about trust in information sources can be used to com-
pute, how well an inferred statement can be trusted and to resolve inconsistencies arising from connecting multiple data sources. We redefine the
stable model and well founded semantics on the basis of FOUR  T , and
reformalize the Web Ontology Language OWL2 based on logical bilattices,
to augment OWL knowledge bases with trust based reasoning.

The Semantic Web is envisioned to be a Web of Data [2]. As such, it integrates
information from various sources, may it be through rules, data replication or
similar mechanisms. Obviously, in a distributed scenario, information sources
may become unavailable. In order to still be able to answer queries in such
cases, mechanisms like caching can be used to reduce the negative implications of
failure. Alternatively, some default truth value could be assumed for unavailable
information. However, cached values may be inaccurate or outdated, default
assumptions can be wrong. Moreover, also available information sources may be
trusted to different extents. We propose a framework for reasoning with such
trust levels, which allows to give additional information on the reliability of
results to the user. In particular, we are able to tell whether a statements truth
value is inferred based on really accessible information, or whether it might
change in the future, when cached or default values are updated. Consequently,
we extend the framework towards multiple levels of trust, taking into account
a trust order over information sources, which can possibly be partial. While
assigning absolute trust values has little semantics, users are usually good at
comparing the trustworthiness of two information sources.

A. Sheth et al. (Eds.): ISWC 2008, LNCS 5318, pp. 533549, 2008.
c Springer-Verlag Berlin Heidelberg 2008

S. Schenk

This problem is highly relevant, because fault tolerance and reliable data
integration is a main prerequisite for a distributed system like the semantic web.
The level of reliability of a piece of information can strongly influence further
usability of derived information. For this reason, our approach can be seen as a
bridge between the rules, proof and trust layers of the semantic web layercake.
The availability of multiple integration mechanisms for distributed resources
makes formulating a generic framework a non-trivial task. Moreover, classical
two-valued logic fails to capture the unknown truth value of unavailable in-
formation. In fact, many applications today rely on simple replication of all
necessary data, instead of more flexible mechanisms.

Our approach extends a very flexible basis of most logical frameworks, namely
bilattices, which allow to formalize many logics in a coherent way [9]. Hence,
it is applicable to a broad range of logical languages. We propose extensions
FOUR  C and FOUR  T to the the FOUR bilattice. These extensions add
trust orders to truth values. As we allow possibly partial orders and also use
multiple  and  values, FOUR  T is strictly more expressive than for example
fuzzy logics.

We investigate support for connected and interlinked autonomous and distributed semantic repositories (for RDF or OWL)  a basic idea behind the semantic
web effort. These repositories exchange RDF and OWL data statically (e.g. by
copying whole RDF graphs, as in the caching scenario) or dynamically using
views or rules. We model our example scenario as follows: Information sources
consist of facts (`a la RDF) or axioms (`a la OWL). Information sources are connected through views, which are clauses of normal programs. This for example
subsumes SPARQL queries and SPARQL based views [13], [16]. One possible
syntactic and semantic realization of this scenario are Networked Graphs, which
we describe in [16].
We show how FOUR  C and FOUR  T can be used for an extension of the
stable model and well founded semantics, which are very popular for rule based
mechanisms and also underlie our Networked Graphs mechanism. We extend the
web ontology language OWL2 [8] to be based on logical bilattices and use trust
levels for inconsistency resolution in OWL2. Finally, we review related work in
section 8 before concluding the paper.

1 Use Case

Oscar is a project officer at a large funding agency and supervises several research
projects. One of his regular tasks is to check the timely publication of project
deliverables. Fortunately, the Semantic Web has made his life much easier. All
his projects are advised to publish their deliverables on their websites using the
FOAF vocabulary1.
In the following, we annotate predicates p(x) with the information source,
where the corresponding data is expected to come from as follows: p(x)|source.
Rules and DL axioms A are written in the usual syntaxes. Analogously, we

1 http://xmlns.com/foaf/spec
?

?

?
annotate them with the repositories they are stored in as follows: A||source. We
use functional style syntax for all facts.

Oscar sets up a view listing all deliverables of all projects and their due dates.
The view makes use of his own data and the projects websites to determine all
timely deliverables.

(1) deliverable(report1).||oscar
(2) due(report1, 20081005).||oscar
(3) hasDeliverable(project1, report1).||oscar
(4) T imelyDeliverable(X)  deliverable(X)|oscar, due(X, Y )|oscar,

published(X, Z)|project1  project2, Z  Y |oscar.||oscar

Rule (4) uses information from Oscars knowledge base to find all deliverables
and due dates, as well as information about publishing dates from the project
websites, to infer whether deliverables are published on time. When it is time for
Oscars next check, he opens his knowledge base and lists all delayed deliverables.
Unfortunately on this day, Project1s website is not working due to technical
problems. However, Oscars webserver still has partial results cached from last
month. Now Oscar would like to be able to infer, which deliverables have been
delivered, and which information about deliveries might be outdated. He does not
want to send a formal reminder to Project1 by mistake. The next day, Project1s
website works again, listing

(5) published(report1, 20081001).||project1
Oscar wants to easily produce reports, so he adds the following DL axioms to

distinguish between good and bad projects, and some additional facts:

(5) GoodP roject = P roject  hasDeliverable.T imelyDeliverable.||oscar
(6) BadP roject = P roject  GoodP roject.||oscar
(7) hasDeliverable(project1, report2).||oscar
(8)  2 project1.hasDeliverable.||oscar
(9) report1  report2.||oscar
(10) timelyDeliverable(report2).||oscar
(11) P roject(project1).||oscar
When Oscar goes on vacation, he asks his secretary Susan to monitor deliv-
erables, while he is away. Susan does her own bookkeeping, also using OWL.
When Oscar returns, he imports Susans data into his knowledge base, causing
an inconsistency, as Susan has added an axiom

(12) timelyDeliverable(report2).||susan
As Oscar assumes his own data to be more reliable, he would like to automatically resolve such inconsistencies in the future by discarding lowly trusted
information. Additionally, he wants to make sure, that projects can not cheat.
He trusts projects less than his secretary. As he does not prefer any project over
others, however, there is no trust order among projects. So if we have
Successf ulP roject(project1).||project2,

(13) Successf ulP roject(project1).||project1 and

S. Schenk
?

?

?
 
?

?

?
Fig. 1. Repositories in Use Case (left)
and Oscars Trust Order (right)

Fig. 2. The knowledge and truth order

he wants to discard both axioms, while in the case of
(14) Successf ulP roject(project1).||project1 and

Successf ulP roject(project1).||susan,

only Successf ulP roject(project1).||project1 would be discarded.
All repositories involved in this scenario are shown in the left part of fig 1. In
the lower part we see that also additional repositories might be involved, which
are not directly relevant and known to Oscar. In the right part, we see Oscars
trust order. Obviously, there is a need for multiple levels of trust in information
sources in our scenario. However, not all sources need to be comparable. In this
paper, we propose a very flexible mechanism for reasoning with such partial trust
orders.

2 FOUR

Most logic programming paradigms, including classical logic programming, stable model and well founded semantics, and fuzzy logics can be formalized based
on bilattices of truth values and fixpoints of a direct consequence operator on
such a bilattice. Therefore, if we build our extension into this foundational layer,
it will directly be available in many different formalisms.
A logical bilattice [5] is a set of truth values, on which two partial orders are
defined, which we call the truth order t and the knowledge order k. Both t
and k are complete lattices, i.e. they have a maximal and a minimal element
and every two elements have exactly one supremum and infimum.
In logical bilattices, the operators  and  are defined as supremum and
infimum wrt. t. Analogously join () and meet () are defined as supremum
and infimum wrt. k. As a result, we have multiple distributive and commutative
laws, which all hold. Negation () simply is an inversion of the truth order.
Hence, we can also define material implication (a  b = a  b) as usual.
?

?

?
The smallest non trivial logical bilattice is FOUR, shown in figure 2. In addition to the truth values t and f, FOUR includes  and .  means unknown,
i.e. a fact is neither true or false.  means overspecified or inconsistent, i.e.
a fact is both true and false.

In traditional, two valued logic programming without negation, only t and f
would be allowed as truth values. In contrast, e.g. the stable model semantics,
allows to use  and . In this case, multiple stable models are possible. For
example, we might have a program with three clauses:

man(bob)  person(bob),woman(bob).
woman(bob)  person(bob),man(bob).
person(bob).
Using default f, we might infer both man(bob)  woman(bob) and
woman(bob)  man(bob). While in two valued logics we would not be able find
a model, in four values, we could assign truth values t  f =  and t  f = .
In fact, both would be allowed under the stable model semantics, resulting in
multiple models for a single program.
The well founded semantics distinguishes one of these models  the minimal
one, which is guaranteed to always exist and only uses t, f, and . In a similar
way, other formalisms can be expressed in this framework as well. Particularly,
we can also formalize open world based reasoning, using  instead of f as default
value. In order to keep this paper short, we refer the reader to the very good
overview in [3].
3 FOUR  C

To apply our work to a variety of different logical formalisms, we directly extend
FOUR as the theoretical basis. We will give two examples of how the extensions
can be used by applying them to the well founded semantics and to OWL in the
remainder of this paper.

To distinguish between certain information, which is local or currently available online, and cached information (or information derived from cached infor-
mation), we extend the set of possible truth values: For information, of which
we know the actual truth value we use the truth values {tk, fk,k,k}. For
cached information, we use a different set: {tc, fc,c,c}. The basic idea of the
extension is that cached information is always potentially outdated. For example a cached false value might actually be true. Therefore, we assume cached
information to be always a bit less false or true than certain information  as
the truth value might have changed.

In our scenario, let us assume Project1s website is currently inaccessible.
In a normal closed world setting, we would assume published(report1, ) to
be f, hence also timelyDeliverable(report1), by rule (4). Changing our default to   unknown  would also not help us determine, whether Project1s
website is just updated slowly, or whether the available information might be
inaccurate. In FOUR  C, we assign fc (or c in an open world setting) to
published(report1, ). We can then conclude from (1-4) and the unavailability

S. Schenk
?

?

?
of (5) that timelyDeliverable(report1) is tk  fc = fc, and hence possibly out-
dated. Therefore, Oscar will simply update his report later, when all relevant
data sources are available again, instead of sending a reminder by mistake. Anal-
ogously, if we run into an inconsistency, we want to be aware, if this inconsistency
could potentially be resolved by updating the cache. Summarizing, our operators
should act as in FOUR, if we only compare truth values on the same trust level.
If we compare values from multiple trust levels, we would like to come up with
analogous truth values as in the four valued case, but on the trust level, which
is the lowest of the compared values.
Ginsberg [5] describes how we can obtain a logical bilattice: Given two distributive lattices L1 and L2, create a bilattice L, where the nodes have values
from L1  L2, such that the following orders hold:
 a, b k x, y iff a L1 x  b L2 y and
 a, b t x, y iff a L1 x  y L2 b
If L1 and L2 are infinitely distributive  that means distributive and commutative laws hold for infinite combinations of the lattice based operators from
section 2  then L will be as well.
We use L1 = L2 = tk > tc > fc > fk
as input lattices, resulting from our basic
idea that cached values are a bit less true
and false. As L1 and L2 are totally ordered
sets, they are complete lattices and hence
infinitely
resulting
FOUR  C bilattice shown in fig. 3. In fig.
3 we label nodes of the form fx, ty with
xy, tx, fy with xy, fx, fy with fxy and
tx, ty with txy.
The artificial truth values fkc, fck, tkc, tck,
kc,ck,kc and ck are only used for reasoning purposes. Users will only be interested in trust levels, which are equivalence
classes of truth values: Given an order of
k > c, the trust level of a truth value is the
minimal element in its subscript. For example the trust level of tc,kc and ck
is c. In fig. 3, these equivalence classes are separated by dotted lines. As we only
have two trust levels here, there are exactly two equivalence classes - one for currently accessible (truth values tkk, fkk,kk,kk) and one for cached information
(truth values tcc, fcc,cc,cc) and information derived from cached information
(truth values tkc, fkc,kc,kc, tck, fck,ck,ck).
Obviously, FOUR  C meets our requirements from the beginning of this
section: We have two sub-bilattices isomorphic to FOUR, one on each trustlevel.
Additionally, we always come up with truth values on the right trust level, e.g.
fkk  tcc = kc, which is on trust level c and kk cc = ck, which is on trust
level c and correctly reflects the fact that the result may be inaccurate in case
cc needs to be corrected to some fxx.

Fig. 3. FOUR  C

distributive.
?

?

?
The
?

?

?
In the caching scenario, we can assume a default truth value of  or f (de-
pending on whether we do open or closed world reasoning) to all statements,
where the actual truth value can not be determined at the moment. However,
some more information may be available for example due to caching, statistics
or similar. Using FOUR  C, we can still do inferencing in the presence of such
unreliable sources. Moreover, a user or application can determine, whether a
piece of information is completely reliable, or if more accurate information may
become available.

4 Extension Towards Trust Levels

In the previous chapter we have focused on two levels of reliability of information.
More generally, we would like to be able to infer multiple levels of trust in a
distributed setting, as we have seen in the use case in Oscars trust order.

Definition 1 (Trust Order)
A trust order T is a partial order over a finite set of information sources with a
maximal element, called .
 is the information source with the highest trust level, assigned to local
data. For any two information sources a and b comparable wrt. T , we have a
FOUR  C lattice as described above, with the less trusted information source
corresponding to the inner part of the lattice. Extended to multiple information
sources, this results in a situation as depicted in fig. 4, where the outermost
bilattice corresponds to local, fully trusted information.

If a and b are not comparable we introduce virtual information sources infab

and supab, such that

 infab < a < supab and infab < b < supab;
 c<a,c<b : c < infab and
 d>a,d>b : d > supab
To understand the importance of this last step, assume that c > a > d and
c > b > d and a, b are incomparable. Then the truth value of a  b would have a
trust level of c, as c is the supremum in the trust order. Obviously this escaping
to a higher trust level is not desirable. Instead, the virtual information sources
represent that we need to trust both, a and b, if we believe in the computed
truth value. We illustrate this situation in fig. 5 (We abbreviate infab by < and
supab by >).
In the general case ( 3 incomparable sources) such a trust order results in
a non-distributive lattice. This can be fixed, however, by introducing additional
virtual nodes. The basic idea here is to create a virtual node for each element
in the powerset of the incomparable sources, with set inclusion as the order. We
will call this modified trust order completed. We can again derive a complete
lattice from the completed trust order. As it can become quite large, we only
show for  how a fragment of the logical bilattice is derived from the trust order
for the case of two incomparable information sources in fig. 5.

S. Schenk
?

?

?
Fig. 4. FOUR  T

Fig. 5. Virtual Sources

Using the same method as in the previous chapter, we can construct the
corresponding bilattice from a given completed truth order as follows: Given a
trust order T , generate a lattice L1, such that
 fa <L1 fb, iff a >T b;
 tb <L1 ta, iff a >T b and
 a : fa <L1 ta.
The result is a lattice with t and f as maximal and minimal elements. Now
create the logical bilattice L from L1  L1 as described in the previous section.
In our scenario, (10) and (12) (as well as (13) and (14)) are inconsistent.
We could obtain a consistent knowledge base by removing any of the axioms,
however we need to choose which one. Making use of the trust order, we can
choose to discard the axiom obtained from Susan. In chapter 6 we define how to
do such choices for arbitrarily complex ontologies in an extension of SROIQ.
Please note that in spite of the rather simple example, we can have complex
derived facts instead and also multiple ways to derive them, as shown in the
dependency graph in the left part of in fig. 1.

5 Extended Stable and Well Founded Semantics

The stable model semantics is popular for assigning models to normal logic
programs (which are used for modeling our scenario) without limitations on the
use of negation. For every normal program there is a distinguished stable model,
called the well founded model. We now define a well-founded semantics [19],
which takes into account the extensions defined in the previous chapter. We
follow the definitions in [3] and [14], which are based on FOUR and {f,, t}
respectively. As we use complete, distributive bilattices, the results extend to
FOUR  T [3], [5]. In the following we refer to normal logic programs when
using the term program. In fact, it would even be possible to use  and  as
operators in programs.
?

?

?
Definition 2 (Valuations)
Let P be a program. ground(P ) is the set of ground instantiated clauses of P . A
valuation of P wrt. a bilattice L is a mapping from ground(P ) to truth values
of L. For any ground atom A  ground(P ) and valuation v, v(A) denotes the
truth value assigned to A by v.
Let v and w be valuations wrt. L. We define (vw) as a mapping from ground
literals to truth values of L as follows: (vw)(A) = v(A) and (vw)(A) =
w(A). (vw) extends to more complex terms in the natural way: (vw)(BC) =
(vw)(B)  (vw)(C), (vw)(B  C) = (vw)(B)  (vw)(C), analogous for
 and .

As usual, we use a fixpoint operator to define the semantics of a program:

Definition 3 (Single Step Immediate Consequence Operator)
P (v, w) = {A, u|A  B  ground(P )  u = (vw)(B)}
P uses v to assign values to positive literals in rules and w to assign values to
negative literals. Hence, we treat negation symmetrically. In our caching scenario
we would use the local, cached and default information to initialize the valuations
v and w. If no cached information is available, we start from the known true facts
only. We have the following properties of P :

Proposition 1 (Properties of P [3])
 P (v, w) is monotone wrt. k in both v and w;
 P (v, w) is monotone wrt. t in v and
 P (v, w) is anti-monotone wrt. t in w.
An operator O(x) is anti-monotone, if x1  x2  O(x2)  O(x1). Obviously,
O2(x) must be monotone. Since P (v, w) is monotone in v, we can define

Definition 4 (Immediate Consequence Operator)
P (w) is the least fixpoint of (v)P (v, w) wrt. t and L.



P :
Similarly to P , we have the following properties of 
?

?

?
P [3])
Proposition 2 (Properties of 
P (w) is monotone wrt. k and

P (w) is anti-monotone wrt. t.
?

?

?
 
 
L is a complete lattice, hence the first item guarantees, that fixpoints of 

exist and that there is a minimal one wrt. k. Obviously, as we still have w as

parameter, 
?

?

?
P (w) can have multiple fixpoints.

Definition 5 (Stable Model)
P wrt. L is called
Let P be a program and L a complete bilattice. A fixpoint of 

a stable model of P . The minimal stable model is called the well founded model
of P .

S. Schenk

In contrast to the usual three or four valued definitions of the well founded and
stable models, we can have true and false values with trust levels, but also 
and  values with trust levels. While this may seem a bit odd at first, it is very
useful in our caching and trust setting: A  value with a trust level represents the
maximally trusted information source, which is responsible for an inconsistency.
We will use this idea in section 7 to resolve inconsistencies by dropping lowly
trusted information.  values with trust levels lead to a propagation of lower
trust levels through the reasoning process, also in the presence of  defaults or
cache entries. As a result, we may come up with lowly trusted t, f or  values
later. Intuitively, this reflects the assumption that the inferred value may be
wrong, if the initial  was already wrong in the cache or untrusted information
source.

Theorem 1 (Complexity)
The data complexity of the well founded semantics wrt. FOUR  T is polyno-
mial, the combined complexity is EXPTIME.

Proof (sketch) We start from the known complexities of the well founded semantics in FOUR. Our semantics is defined analogously to that in FOUR. The only
difference is that we no longer have a fixed logical bilattice. Therefore, we need
to consider possible additional complexity for finding the supremum or infimum
of two truth values. As our bilattices are built based on orders over a finite set
of information sources, they must be finite. A finite bilattice L has finitely many
edges n  L2. Assume we use a very basic algorithm to find a supremum (infi-
mum): starting from both values to be compared, we follow all possible -edges
in the relevant order, until we find the other value. To do so, we need to follow
less than 2n edges. Hence, the additional effort is polynomial.
Based on existing work for the well founded semantics on FOUR we can also
define an operational semantics: The alternating fixpoint procedure proposed
by Gelder [19] computes the two t-extremal stable models mt and Mt of a
program P , using the anti-monotonicity of P wrt. t. The well founded model
of P is then obtained as mt  Mt [3]. The alternating fixpoint procedure in turn
gives rise to an implementation.

6 Extension Towards OWL
In this section we extend SROIQ, the description logic underlying the proposed
OWL2 [8], to SROIQ  T evaluated on a logical bilattice. The extension towards logical bilattices works analogously to the extension of SHOIN towards
  are the
a fuzzy logic as proposed in [18]. Operators marked with a dot, e.g.
lattice operators described above, all other operators are the usual (two valued)
boolean operators. For two valued operators and a logical bilattice L we map t
to maxt(L) and f to mint(L) to model that these truth values are absolutely
trusted2. Please note that while we limit ourselves to SROIQ here, analogous
2 In FOUR  T these would be f and t, but we start with the general case.
?

?

?
extensions are possible for SROIQ(D) to support datatypes. Please also note
that we do not include language constructs, which can be expressed by a combi-
  R and
nation of other constructs defined below. In particular, Sym(R) = R
T ra(R) = R  R  R.
Definition 6 (Vocabulary)

A vocabulary V = (NC, NP , NI) is a triple where
 NC is a set of OWL classes,
 NP is a set of properties and
 NI is a set of individuals.
NC , NP , NI need not be disjoint.

A first generalization is that interpretations assign truth values from any given
bilattice. In contrast, SROIQ is defined via set membership of (tuples of) individuals in classes (properties) and uses two truth values only.

,L,IC ,IP ,Ii) is a 5-tuple

Definition 7 (Interpretation)
Given a vocabulary V an interpretation I = (

where
I is a nonempty set called the object domain;
 
 L is a logical bilattice and  is the set of truth values in L
 IC is the class interpretation function, which assigns to each OWL class
A  NC a function: A
 IP is the property interpretation function, which assigns to each property
R  NP a function R
 Ii is the individual interpretation function, which assigns to each individual
a  NI an element a
I is called a complete interpretation, if the domain of every class is 
I and

I  ;
IC : 
I  
IP : 
I.
Ii from 
I  
I.

I  ;

the domain of every property is 
We extend the property interpretation function IP to property expressions:



)

(R

= {(x, y, u)|(y, x, u)  R

I}

The second generalization over SROIQ is the replacement of all quantifiers
over set memberships with conjunctions and disjunctions over . We extend the
class interpretation function IC to descriptions as shown in table 1.
Satisfaction of axioms in an interpretation I is defined in table 2. With  we
denote the composition of binary relations. For any function f, dom(f) returns
the domain of f. The generalization is analogous to that of IC . Note that for
equality of individuals, we only need two valued equality.
Satisfiability in SROIQ  T is a bit unusual, because when using a logical
bilattice we can always come up with interpretations satisfying all axioms by
assigning  and . Therefore, we define satisfiability wrt. a truth value:
Definition 8 (Satisfiability)
We say an axiom E is u-satisfiable in an ontology O wrt. a bilattice L, if there exists a complete interpretation I of O wrt. L, which assigns a truth value val(E,I)
to E, such that val(E,I) k u.

S. Schenk

Table 1. Extended Class Interpretation Function
(x) = yy, where y is the information source, definingI
(x) = yy, where y is the information source, definingI
(x) = C

(x)

(x)

I
I
(C1  C2)

(C1  C2)

(C)
?

?

?

(S
)
(R.C)
(R.C)
(R.Self)

( nS)
( nS)
{a1, ..., an}I

(x)

(y, x)

(x) =

2 (x)

2 (x)

1 (x)  C

1 (x)  C

(x) = C
(x) =  C
?

?

?
(x, y) = S

 yI R
 yI R

 
 {y1,...,ym}I ,mn
(x) =    {y1,...,yn+1}I
 

(x, y)  C
(x, y)  C

(x) = R

(x) =

(x) =

(x, x)

(y)

(y)

n

i=1

(x, yi)

(x, yi)

n+1

i=1

(x) =

 

n

i=1

i = x

a

Table 2. Satisfaction of Axioms

 {x2,...,xn}

 

n

i=1

i (xi, xi+1)

(y, x))

(x, y)

(x, y)

(x, y)

(R  S)

(R = S)
(R1  ...  Rn  S)

=

=

=

=

=

(x, y)  R

(x, y)  S
 x,yI R
(x, y)  S
 x,yI R
 x1,xn+1dom(SI)
 x,yI  (R
 xI R
 xI  R
 x,yI R

 xI C
?

?

?
= R
a  b = a

= b
a  b = a
I = b

(x, x)
(x, y)    S
(x)  D

(x, x)

(a

, b
?

?

?
= C

(x)
?

?

?
(a

=

=

=

)

)

.

(Asy(R))

(Ref (R))

(Irr(R))

(Dis(R, S))
(C  D)

(a : C)

((a, b) : R)
?

?

?
We say an ontology O is u-satisfiable, if there exist a complete interpretation
I, which u-satisfies all axioms in O and for each class C we have |{a|a, v 
C  v t u}| > 0, that means no class is empty.
Now we define a special kind of satisfiability, which reflects our trust order:

Definition 9 (Trust Satisfiability)
Let I be a complete interpretation, O an ontology, which is composed from multiple data sources {S1, ..., Sn}, and T a trust order over {S1, ..., Sn}. Let source(E)
denote the T -maximal datasource, which axiom E comes from. O is trust satis-
fiable, if there exists an I, which satisfies O, such that for all axioms in table 2
and all E  O : val(E,I) t tsource(E).
Analogously we define consistency wrt. the knowledge order:

Definition 10 (Consistency)
Let I be a complete interpretation, O an ontology, which is composed from multiple data sources {S1, ..., Sn} and T a trust order over {S1, ..., Sn}. Let source(E)
denote the T -maximal datasource axiom E comes from.
We say O is u-consistent, if there exists an I, which assigns a truth value
val(E,I) to all axioms E in O, such that u k val(E,I). I is called a u-model
of O.
We say O is consistent, if there exist an I, which assigns a truth value
val(E,I) to all axioms E in O, such that x : val(E,I) / {x,x}. We say I
is a model of O.

If I models O and trust satisfies O, I is called a trusted model of O.

Finally, we define entailment:

Definition 11 (Entailment)
O entails a SROIQ  T ontology O
model of O

. O and O

 (O  O

), if every model of O is also a

 are equivalent if O entails O

 and O

 entails O.

The following theorem shows that we have indeed defined a strict extension of
SROIQ:
Theorem 2. If FOUR is used as logical bilattice, SROIQ  T is isomorphic
to SROIQ.
Proof (sketch). From a model of a SROIQ  T ontology O wrt. FOUR, we can
derive a model for the same ontology in SROIQ by doing the following steps:
 For each class C, replace C(a) = t by a  C and C(a) = f by a / C.
 As O is consistent, we do not have  and  truth values in a model.
 The only connectives used in the ontology language are ,,, these are

Analogous for properties.

equivalent to their boolean counterparts in FOUR.

 As we only have one trust level, we can simply ignore it.

S. Schenk

 replace trust-consistent in SROIQ  T with consistent in SROIQ.

Analogous for satisfiability.

For a complete proof we need to show for every rule in tables 1 and 2 that we
can transform it to the SROIQ form (wrt. FOUR) by replacing conjunctions
and disjunctions by quantifiers over set membership or inclusion.

7 Resolving Inconsistencies

Inconsistencies in ontologies often emerge, when ontologies are integrated from
various sources using ontology modules, ontology mappings and similar mechanisms [12]. In this section we propose to use trust based reasoning for inconsistency resolution.

Definition 12 (Maximally Trust Consistent Interpretation)
We say an interpretation I is maximally trust consistent, if it does not assign
any artificial  values, i.e. xy with x = y. An ontology O is said to be maximally
trust consistent, if it has an interpretation I, which is maximally trust consistent.
This means, a trust maximal interpretation can still be inconsistent, but such
inconsistency then arises from information obtained from a single information
source. We now define, how a maximally trust consistent ontology can be derived
from any given ontology.

Various approaches for repairing inconsistent ontologies have been proposed.
In most approaches, axioms are removed from the ontology until the rest is
a consistent ontology (cf.[12]). There usually are multiple possible choices for
axioms to remove. While this might not seem too bad in our example, consider a similar scenario involving a red traffic light and we accidentally remove
RedLightSituation  BetterBreakSituation. Here trust based reasoning comes
into play. We use the trust level as input to a selection function, which determines
axioms to be removed, a point left open in [12].

Definition 13 (Minimal Inconsistent Subontology)
Let O be an inconsistent ontology. A minimal inconsistent subontology O
is an inconsistent ontology, such that every O

 is consistent.

  O

  O

Now trust maximal consistency is reestablished by iteratively removing all ax-
, until the resulting ontology is trust
ioms with the lowest trust level from O
maximal consistent. This captures the idea, that in the case of an inconsistency,
humans tend to ignore lowly trusted information first.

If a trust maximal consistent ontology still is inconsistent, we need a different
selection function. However, in this case already the local knowledge is inconsis-
tent. A similar situation arises, if we have an inconsistency resulting from two
incomparable information source. In the latter case, however, we can choose to
discard information from both. Of cause, depending on the actual application,
we can also use a more sophisticated selection function, choosing among axioms
on the lowest trust level.
?

?

?
8 Related Work

Relevant related work comes from the fields of semantic caching, multi-valued
logics  particularly based on logical bi-lattices  from belief revision and trust.
The following works are closely related:

The term Semantic Caching refers to the caching of semantic data. Examples
are such diverse topics as caching results of semantic webservice discovery [17],
caching of ontologies [1] and caching to improve the performance of query engines
[10]. These approaches have in common, that they discuss how to best do caching
of semantic data. In this paper, we describe which additional information about
the reliability of knowledge we can infer, given a heterogeneous infrastructure
containing semantic caches.

Katz and Golbeck propose to use trust levels obtained from the analysis of online social networks to prioritize default logics [11]. A trust level in a rule then is
a global value. Here, we do not specify, how the trust order is determined, but assume it is supplied by the user. [11] is based on a two valued default logic, that
means trust levels of inferred facts are not computed. In [15], rule based reasoning
over annotated information sources is done to establish a trust relation between
the provider and the requester of a resource. The focus, is on establishing a trust
relation using fine grained negotiation, instead of determining trust in a statement.
Much work has been done about basing logical formalisms on bilattices (cf. [3],
[9]). Most of these works, however propose a certain logic by manually designing
a suitable bilattice, or discuss how a particular logic can be formalized using
a bilattice. In contrast to these works, we do not propose a fixed bilattice or
logic. Instead, we automatically derive logical bilattices for trust based reasoning.
Hence, we propose a whole family of logics, which can automatically be tailored
to the problem at hand.
Deschrijver et al. propose a bilattice based framework of handling graded truth
[4]. They extend fuzzy logics, which has a single, continuous order t towards
bilattices which also have a fuzzy k. While this is obviously closely related
to ours, we propose to use possibly partial orders, instead of strict orders as in
fuzzy logics. Additionally we show, how the logical framework an be used with
rule based and description logics.

Using belief revision, a single, consistent world view is retained in the presence of contradictory information by discarding e.g. lowly trusted information.
In contrast, paraconsistent reasoning, as applied here, limits the influence to inconsistencies to fragments of the knowledge base. An approach similar to ours,
but based on belief revision is proposed in [7].

Other works can be considered orthogonal to our approach: Following Gol-
becks categorization of trust [6], our approach deals with trust in content (vs.
trust in people or services). Further, we provide means for computing trust. In
contrast to existing systems (cf. [6], chap. 2), we allow to infer trust levels on
the very fine level of axioms, instead of the usual level of documents. As our approach is agnostic to the actual trust order, e.g. social trust derived from social
networks or P2P based algorithms for computing trust measures can be used to
provide this order.

S. Schenk

9 Conclusion
We have proposed an extension to the logical bilattice FOUR, called FOUR  T ,
which allows to reason with trust levels. As bilattices are a basis for various logical
formalisms, this allows to extend many languages with trust based reasoning.
We have started by applying the extension to the well founded semantics. We
have re-formalized SROIQ to work on bilattices, so our extension is applicable
in both, open and closed world reasoning. As applications of FOUR  T we
have investigated caching and inconsistency resolution. We are sure that our
mechanism can be a good component of a future Semantic Web trust layer.
As part of our future work we will investigate additional applications. We will
investigate the complexity of trust based reasoning with description logics and
plan an implementation, extending existing reasoning engines.

Acknowledgements. This work has been supported by the European project Lifecycle Support for Networked Ontologies (NeOn, IST-2006-027595).
