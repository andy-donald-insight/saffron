Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Syndication on the Web using a description logic approach
Christian Halaschek-Wiener a,, Vladimir Kolovski b

a Clados Management LLC, 1840 Gateway Dr., Suite 200, San Mateo, CA 94404, USA
b Oracle NEDC, Nashua, NH 03062, USA

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 November 2007
Received in revised form 11 June 2008
Accepted 29 June 2008
Available online 7 September 2008

Keywords:
Syndication systems
Publish/subscribe
Description logic
Incremental reasoning
Query answering

1. Introduction

Syndication systems on the Web have attracted vast amounts of attention in recent years. As technologies
have emerged and matured, there has been a transition to more expressive syndication approaches; that
is, subscribers and publishers are provided with more expressive means of describing their interests and
published content, enabling more accurate information filtering. In this paper, we formalize a syndication
architecture that utilizes the Web Ontology Language (OWL) and description logic reasoning for selective
content dissemination. This provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches, both of which are not achievable in less expressive approaches.
We then address one of the main limitations with such a syndication approach, namely matching newly
published information with subscription requests in an efficient and practical manner. To this end, we
investigate incremental query answering for a large subset of OWL and present an approach to reduce the
portion of the ontology that must be considered for query answering in the event of updates. Lastly, an
evaluation of the query approach is shown, demonstrating its effectiveness for syndication purposes.

 2008 Elsevier B.V. All rights reserved.

Web-based syndication systems have attracted a great amount
of attention in recent years as the amount of streaming content
on the Web has increased at dramatic rates. In typical syndication frameworks, users register their subscription requests with
syndication brokers; similarly, content publishers register their
feeds with syndication brokers, and it is then the brokers task
to match newly published information with registered subscrip-
tions. As technologies have emerged, there has been a transition
to more expressive syndication approaches; that is publishers (and
subscribers) are provided with more expressive means for describing their published content (respectively interests), allowing more
accurate dissemination. This has been enabled by the maturation of technologies for sharing information on the Web and the
standardization of representation languages for Web content. In
particular, through the years there has been a transition from keyword based approaches to attribute-value pairs and more recently
to XML. Given the lack of expressivity of XML (and XML Schema) as
a knowledge modeling language, there has been interest in using

 The results presented in this paper are the outcome of research performed while
the author was a member of the MINDSWAP research group at the University of
Maryland, College Park.
 Corresponding author.
E-mail addresses: christian@clados.com (C. Halaschek-Wiener),

vladimir.kolovski@oracle.com (V. Kolovski).

1570-8268/$  see front matter  2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2008.06.002

the Resource Description Framework (RDF) and its accompanying
schema language, RDF Schema, for syndication purposes. RDF has
even been adopted as the standard representation format of RSS

Todays syndication approaches still provide relatively weak
expressive power from a modeling perspective (i.e., XML and RDF
are comparatively inexpressive languages) and provide very little
automated reasoning support. However, if a more expressive syndication approach with a formal semantics can be provided, many
benefits can be achieved; these include a rich semantics-based
mechanism for expressing subscriptions and published content
allowing increased selectivity and finer control for filtering, and
automated reasoning for discovering subscription matches not
found using traditional syntactic syndication approaches [45].

In this work, we consider using the Web Ontology Language
(OWL) for representing published content. As the semantics of a
large subset of OWL is aligned with description logics (DLs), reasoning techniques for DLs can then be leveraged for matching content
with subscription requests. In such an approach, the previously
mentioned benefits of using a formal representation language can
therefore be achieved. An additional benefit of an OWL-based syndication approach is its native Web embedding and power as a data
integration language. Further, such an approach can be seen as a
natural extension of existing RSS 1.0 syndication systems, as OWL

1 RSS 1.0 Specification: http://web.resource.org/rss/1.0/spec.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Table 1
Illustration of expressivity in OWL-based syndication

can be encoded in RDF.

To demonstrate the advantages of an OWL-based syndication
approach, consider the following example: suppose we are disseminating news information in the financial domain. Also suppose that
a stock trader is interested in articles that could discuss companies whose stocks are likely to become volatile; specifically, let us
assume that the trader is interested in any RiskyCompany which the
trader defines to be a company that has a product which causes an
infection or allergic reaction. Using an XML-based approach syndication brokers can provide an XML schema that contains an element
RiskyCompany and such companies can be declared to be this type
of element. A limitation of such an approach is that publications
(i.e., XML documents) must explicitly declare entities to be a Risky-
Company. This is because XML query languages such as XPath and
XQuery only provide syntactic matching of the XML documents representing publications. If we consider an RDF-based approach, then
the syndication broker can model the financial domain using RDF
Schema. Therefore, additional matches can be obtained as one can
logically infer that a company is a RiskyCompany. For example, if the
domain of a property hasProductWithAdverseEffect is declared to be
of type RiskyCompany and we are given that BauschAndLomb hasProductWithAdverseEffect Renu, then we would have a (inferred) match
for the subscription; such logical inference (although simplistic)
is not possible with an XML-based approach. However, in an RDF
based approach, more complex logical definitions (and therefore
finer-grained control) of RiskyCompany are not expressible.

If we now consider an OWL-based approach, such functionality
is clearly provided. For example, the knowledge broker can define
a R iskyCompany as an OWL class whose necessary and sufficient
conditions for inclusion are that it be a company that has some
product which is an AdverseEffectProduct; similarly, an AdverseEffectProduct can be defined to be any product that causes some
infection or allergic reaction. Using an OWL approach, this can easily
be represented by the OWL descriptions in Table 1.2

Given this domain model, if we assume it is previously known
that BauschAndLomb is a company that has product Renu, which is
known to be a Product, and we receive the publication that Renu
causes some infection, then standard DL reasoning services can be
employed to automatically infer that BauschAndLomb is a RiskyCompany and thus there is a match for the subscription.

While OWL-based syndication approaches provide increased
expressivity over XML and RDF, previous DL-based syndication
approaches suffer from scalability issues due to the inherent complexity of DL reasoning [45,34,23]. This is an issue in domains such
as the syndication of financial news because response times must

be minimal as critical information must be delivered in near real
time (e.g., for stock trading purposes). One of the main limitations in
an OWL-based syndication approach is related to DL reasoning over
changing data; this is primarily due to the static nature of existing
DL reasoning techniques. In particular, the addition of information from newly published documents and data can be viewed as a
change in the underlying knowledge base (KB). In current DL reasoning algorithms, reasoning on the updated KB is performed from
scratch: consistency of the KB must be ensured, queries must be
re-evaluated, etc. In this paper, we formalize an OWL-based syndication framework (originally presented in [24]), which leverages
DL reasoning to determine subscription matches. We then address
the scalability of the DL reasoning services necessary for the syndication framework, by investigating incremental query answering
over OWL KBs; we specifically present a technique for reducing the
portion of the KB that must be considered as candidate query bindings after an update. This effectively allows a smaller subset of the
KB to be considered for possible subscription matches. The techniques we present are applicable to conjunctive retrieval queries
that can be rolled-up into a distinguished variable (discussed in
the next section) and containing only simple roles (i.e., no transitive roles or super-roles of a transitive role). The approach extends
our previous work [24] to support arbitrary KBs expressed in the
description logic SHI (a large subset of OWL). Lastly, an evaluation
of the incremental reasoning techniques is provided, demonstrating their effectiveness for OWL-based syndication.

2. Preliminaries

In this section, we briefly provide an overview of OWL and
description logics, query answering for DL KBs, and tableau algorithms for DL reasoning.

2.1. The Web Ontology Language

The W3C-approved Web Ontology Language (OWL) is the recommended standard for formally representing content on the Web.
One of the main benefits of OWL is the support for formal reason-
ing, as the semantics of a variety of its sub-languages are firmly
founded in description logics (a decidable fragment of First Order
Logic). In particular, the sub-language OWL DL is a syntactic variant of the description logic SHOIN [28], with an OWL DL ontology
corresponding to a SHOINKB. In this work, we address a subset
of SHOIN, namely SHI; therefore, we briefly introduce the syntax and semantics of SHI. Let C, R, I be non-empty and pair-wise
disjoint sets of atomic concepts, atomic roles, and individuals respec-
tively. The set of SHI roles (roles, for short) is the set R  {R
|R R},
 denotes the inverse of the atomic role R. Concepts are
where R
inductively using the following grammar:
C  A|C|C1  C2|C1  C2|R.C.|R.C.
where A C, a I, C(i) a SHI concept, R a role, and S a simple role (i.e.,
no transitive roles or super-roles of a transitive role).3 We write 
and  to abbreviate C  C and C  C, respectively. A role inclusion
axiom is an expression of the form R1  R2, where R1, R2 are roles.
A transitivity axiom is an expression of the form Trans(R), where
R R. An RBox R is a finite set of role inclusion axioms and transitivity axioms. For C, D concepts, a concept inclusion axiom is an
expression of the form C  D. A TBox T is a finite set of concept
inclusion axioms. An ABox A is a finite set of concept assertions of
the form C(a) (where C can be an arbitrary concept expression), role

2 Note that this is expressed using standard Turtle syntax (as opposed to RDF/XML)

and can be easily generated in todays OWL ontology editors.

3 See [28] for a precise definition of simple roles.

, .

I  	

An interpretationI is a pairI = (	

assertions of the form R(a, b) and inequality (equality) assertions of
the form a /= b (respectively a = b). A KB K = (T, R, A) is composed
of TBox T, RBox R and ABox A. Denote the set of individuals in KB K
(ABox assertion  ) as IK (respectively I ).
I is a non-empty
I), where 	
I is the interpreta-
set, called the domain of the interpretation, and .
tion function. The interpretation function assigns to A C a subset
I, to each R R a subset of 	
I and to each a I an element
of 	
I. The interpretation function is extended to complex roles and
of 	
concepts as given in [28]. The satisfaction of a SHI axiom/assertion
  in an interpretation I, denoted I    is defined as follows: (1)

I  (R2)
I  R1  R2 iff (R1)
; (2)I  Trans(R) iff for every a, b, c  	
I,
I; (3) I  C  D iff
I) R
I) R

if (a
, c
, b
I  C
I  D
I; (4) I  C(a) iff a
I; and

(6) I  a /= b iff a
I. The interpretation I is a model of a RBox
R (TBox T) if it satisfies all the axioms in R (respectively T). I is a
model of A, denoted by I  A, if it satisfies all the assertions in A.
Lastly, I is a model of K, denoted by I  K, iff I is a model of T, R,
and A.

I, then (a
I; (5) I  R(a, b) iff (a

I and (b

I /= b

I) R

I) R

, c

, b

2.1.1. Conjunctive queries for description logics

A conjunctive query Q contains a non-empty set of concept
and role atoms, C(x) and R(x, y), respectively, where x can be a
named individual (i.e., taken from I) or variable name see [29]
for a precise presentation. The variable names are assumed to
be typed such that each variable is either d istinguished or non-
distinguished; distinguished variables must be mapped to named
individuals, where as the non-distinguished variables are existentially quantified. Denote the distinguished variables by DVar(Q ).
Query answering is the task determining if Q is a logical consequence of the KB K (denoted K  Q ); that is, checking if for all models
I of K, I  Q . As query retrieval is addressed in this work, we briefly
introduce the following notation: (x1, . . . , xn)  Q indicates that
the variables x1, . . . , xn appearing in Q must be bound to individual names, therefore constituting the answer to the query (i.e., the
distinguished variables). The answer set of a query (x1, . . . , xn)  Q
w.r.t. to K is the set n-ary tuples defined by the following:
{(a1, . . . , an) In

|K  Q [x1/a1, . . . , xn/an]}

where Q [x/a] represents the query, Q, with all occurrences of variable x substituted by the individual name a. If a query can be
partitioned into unconnected components (i.e., components that
do not share variables), then they are considered independently.
Without loss of generality, we assume queries are connected in
the remainder of this work [17]. As retrieval queries are addressed
in this work, the remainder of this discussion will only address
queries of this form. Query answering can be reduced to ABox consistency checking; that is, given the query (x)  C(x) and individual
a, if K  Company(a), then K  Company(a) must be inconsistent
[29,43]. To extend this approach to role atoms one can transform
each role atom in the query into a concept atom, which is referred
to as rolling-up the query [29,43]. This rolling-up process if often
enabled via the use of nominals, however, many DLs do not support
the use of nominals; for example the DL SHIF (i.e., OWL Lite) does
not include such expressivity. There is a well-known workaround,
in which the use of nominal can be simulated by substituting each
nominal in the rolled-up query concept with a new concept name
that does not occur in the knowledge base [29,43]. Additionally,
an assertion is added to ensure that each individual instantiates
its representative concept. The basic approach to support retrieval
queries is to roll up the query and then to iterate over possible
substitutions of named individuals for distinguished variables and
perform the necessary consistency checks.

2.2. Tableau algorithms

DL tableau-based algorithms decide the consistency of an ABox
Awith respect to a TBox T and RBox R by trying to construct (an
abstraction of) a common model for A, T, and R, called a c ompletion graph [28]. Each node in the graph represents an individual
that is labeled with a set of concepts that it satisfies (in the particular model). Formally, a completion graph for an ABox A with
respect to Tis a directed graph G = (V,E,L,  /= ). Each node x V is
labeled with a set of concepts L(x), and each edge e = x, y with a
set L(e) of role names. The binary predicate  /= is used for recording
inequalities between nodes. This graph is constructed by repeatedly applying a set of tableau expansion rules, adding new concept
labels and edges to the graph when necessary. This process continues until the tableau is fully expanded and no additional rules can
be applied or a clash occurs. In SHI, a node, x, contains a clash if
a contradiction exists in its label; that is {C,C}  L(x). Denote a
clash in L(x) by (x,C, C). It is noted that the tableau algorithm can
be saturated such that all possible completion graphs of a KB are
found (corresponding to all models). Denote by Comp(K) the set of
all complete and clash-free completion graphs of K. When referring
to the node in a completion graph that corresponds to named individual a IK, we will denote the node by xa and refer to it as a root
node.
We note that reasoning with a general TBox T and role hierarchy
R can be reduced to only reasoning with R. This is because the
TBox can be internalized into a single concept that is added to all
individuals [28]. This process is briefly introduced,4 as it will be
referred to later. First, all TBox axioms in the KB are transformed
into a single concept as follows:
CT = Ci

 KCi  Di.

Di

Following this, a transitive role U is introduced that does not
occur in the KB, and the role hierarchy for the KB is extended such
that U is a transitive super-role of all roles occurring in the KB;
that is, the role hierarchy is extended as follows: RU = R  {R 
U, Inv(R)  U|R occurs in K}. Given this, it has been shown that the
consistency of the KB can be reduced to checking the consistency
of simply the ABox w.r.t the role hierarchy RU by extending the
ABox with (CT  U.CT )(a) for all named individuals a IK [28].
Given a concept D, let clos(D) denote the smallest set of concepts
containing D that is closed under sub-concepts in negation normal form (NNF).5 Then given KB K, let clos(K) denote the union of
clos(CT ) and all clos(D) for each concept assertion D(a) K; that is
clos(K) = clos(CT ) 

D(a) Kclos(D).

3. Syndication framework

In this section, we formally define the DL-based syndication
framework proposed in this work. In the framework, a publication is a set of ABox assertions (which correspond to a set of OWL
instance assertions). A publication is also associated with a number
of time units in which the publication is valid; after the specified
time units have passed, the publication is discarded from the syndication brokers KB (discussed below). Additionally, a boolean value
is associated with a publication, denoting if the assertions should be
added (or retracted) to (respectively from) the brokers KB. Retractions are supported as in many realistic syndication applications,
revisions to previous publications are sometimes necessary; such
a revision can be viewed as a deletion followed by an addition.

4 See [28] for a more detailed discussion.
5 See [28] for a discussion regarding this form.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Definition 1 (Publication). A publication P is defined as a tuple
(, t, ), where  is a set of DL ABox assertions that expire after t
time units (t > 0), and  is a boolean value that is true in the event
of an addition and false for retractions.

Given a publication P, we denote the set of ABox assertions
as P(), the expiration time as P(t), and the boolean addi-
tion/retraction value as P(). Next, a publisher is composed of a
set of publications and an unique identifier:

Definition 2 (Publisher). A publisher Pub is defined to be a pair
(p, i), where p is a set of publications and i is an unique identifier.

We use Pub(p) and Pub(i) to denote a publishers set of publica-

tions and identifier, respectively.

The main component of a subscription is a conjunctive ABox
query, which represents the subscribers interests. In the frame-
work, subscriptions are represented as retrieval queries; this
assumption is made as it provides various match types (discussed
soon), and in many real world applications investigated in OWL
and DL literature, including publish/subscribe applications [23],
queries typically have some number of distinguished variables. A
subscription is also composed of the number of time units that
the subscription is valid. Intuitively, the subscription query can be
thought of as a continuous conjunctive query (defined below) that
should be evaluated for the specified number of time units. There-
fore, the query is issued once over a changing KB and the answer
set of the query is continuously updated as the ABox changes.

Definition 3 (Continuous conjunctive retrieval query). Given a continuous conjunction ABox retrieval query Qc with n distinguished
variables (i.e., DVar(Qc) = {d1, . . . , dn}), define the answers of K at
time t, denoted Kt, to Qc to be those n-tuples (a1, . . . , an) In
such
that the following holds:
Kt  Qc[d1/a1, . . . , dn/an]

Kt

When referring to a continuous conjunctive retrieval query,
continuous or incremental query will be used. Given a continuous query, denote the set of answer tuples at time t by Qc(t).
A subscription is assumed to be composed of a continuous query,
in addition to a number of time units which the query should be
evaluated:

Definition 4 (Subscription). A subscription S is defined as a pair
(Qc, t), where Qc is a continuous query that is evaluated for t time
units (t > 0).

The continuous query of a subscription is denoted as S(Qc) and
the expiration time is denoted as S(t). Next, a subscriber is introduced and intuitively is composed of a set of subscriptions and an
unique identifier:

Definition 5 (Subscriber). A subscriber Sub is defined to be a pair
(s, i), where s is a set of subscriptions and i is an unique identifier.

Similar to publishers and subscriptions, Sub(s) and Sub(i)
denotes a subscribers set of subscriptions and identifier, respec-
tively. A syndication broker maintains a local DL KB, in which newly
published information is integrated. In the framework, this KB can
initially contain a TBox and ABox providing background domain
information. It is assumed that ABox assertions in publications utilize the terminology defined in the TBox of the broker and thus the
brokers TBox is fixed. Lastly, the syndication broker maintains the
currently registered subscribers, which have associated subscrip-
tions, and publishers. This is formally defined as follows:

Definition 6 (Syndication broker). A syndication broker B is defined
as a triple (S, P, K), where S is a set of subscribers, P is a set of
publishers, and K is the brokers local DL KB.

A syndication brokers subscribers, publishers, and KB are
denoted as B(S), B(P), and B(K), respectively. If it is clear from the
context of the discussion, the brokers KB (resp. KB at time t) will
simply be referred to as K (resp. Kt). Additionally, the notation B(KP)
will be used to denote the set of ABox assertions present in the
brokers KB that are included in a non-expired publication.

After a new publication is received, it is the brokers task to determine the subscribers for which this new information is relevant.
Prior to doing this, the new publications must be integrated in the
brokers KB. Unfortunately, there have been recent negative results
found when trying to apply leading theories for handling updates in
DL KBs. For example, the minimal model change update semantics
[48] is not representable in OWL DL [33] and the postulates imposed
by the AGM belief revision model [2] cannot be satisfied for OWL
DL [16]. Given this, in this framework a syntactic change/update
of ABox assertions is adopted, referred to as syntactic updates. Intu-
itively, syntactic updates can be described as an update in which all
new ABox assertions are directly added (or removed) to the asserted
(base) axioms.

Definition 7 (Syntactic updates). Let A be the ABox of DL KB K. Then
under syntactic updates, updating K with an ABox addition (resp.
deletion) , written as K +  (resp. K  ), results in an updated set

of ABox axioms A

 = A  {} (resp. A

such that A

 = A \ {}).

If the update type is clear from the context of the discussion,
K   will simply be used to denote the syntactic update of K with .
Under this type of updates, the brokers KB can become inconsistent
in the event of an addition; in this case, some action to regain consistency must be taken in order to allow reasoning to be performed.
Therefore, in the remainder of this paper, it is assumed that if a
new publication causes the brokers KB to become inconsistent, that
publication is rejected; later, we discuss more advanced techniques
to regain consistency. Let us now consider subscription matches. As
information is published from multiple publishers and can remain
valid in the brokers local KB for varying time periods, a match
for a subscription can be a composition of the information from
multiple publications. Further, the brokers KB could additionally
contain background knowledge which can attribute to subscription matches as well. To our knowledge, recent approaches have
not investigated such functionality (e.g., [11,40,45,34]); rather, only
information from individually published documents form a match
for a given subscription. Such a capability is beneficial, as information can be considered collectively and form matches not found
otherwise (examples are discussed later). Note that it is assumed
that deletion publications do not remove initial ABox assertions in
the brokers KB that correspond to background information.

A distinction between two types of subscription matches is
now made, namely information and publication matches. Intu-
itively, an information match refers to the individuals bound to the
distinguished variables of a continuous query representing a sub-
scription. This type of match aligns with recent work in XML-based
syndication literature, in which the actual information is filtered
and the query answers are returned to the user (e.g., [32]). In con-
trast, a publication match refers to the collection of publications that
satisfy a subscription; that is, given an information match for a registered subscription, it is all minimal sets of publications that cause
this match to occur. This aligns with the task of selective contentbased filtering of publications (e.g., [11]). The distinction between
these two match types is made as the type of match required is
application dependent; for example, in OWL-based syndication of
news feeds, publication matches are needed. In contrast, in the
financial domain, analysts are generally interested with the actual
information rather than the documents themselves. We now define
an i nformation match as follows:

Definition 8 (Information match). Define a n-tuple of individuals
(a1, . . . , an) In
to be an information match, denoted MI, at broker
B for subscription S at time t, if the following condition holds:
B(Kt)  S(Qc)[x1/a1, . . . , xn/an]

Kt

Due to the fact that publications can persist at the syndication
broker for varying time periods, answer tuples may remain valid
for varying time periods as well. Given this, there are various ways
in which the broker could maintain these answers and notify sub-
scribers. For example, the broker could maintain a list of all current
bindings and only forward new information matches. However, this
will have some ramifications with respect to the space that it takes
to store the answer sets. In contrast, in some applications it may
be better to pass all current bindings to the subscriber; however,
yet again, there are performance impacts due to such an approach
related to the transmission cost of transferring all bindings (includ-
ing those already transferred) to a subscriber. Given the fact that
this is application dependent, in the formalization it is not dictated
how an actual instantiation of this framework should proceed with
respect to this issue; rather it is left to the individuals deploying
such a framework.

Related to this issue is that a previous information match may be
invalidated in the event of a retraction publication (or the expiration
of a publication). Once again, there are various notification strategies that can be adopted. Specifically, a subscriber could be notified
if a previous information match is invalidated due to a retraction
publication; in contrast, such a notification may not be necessary in some scenarios. Therefore, such a decision is not imposed
here. When discussing examples, specific decisions regarding these
issues will be made if it is not clear from the context of the discus-
sion.

Given an information match, additional computation is needed
to derive all the minimal sets of publications responsible for the
entailment. Clearly, in the event of a new information match for
a subscription, the most recent (addition) publication which is
received at the broker contributes to a publication match. However,
we must determine the other publications which contribute to the
match. For this purpose, the notion of minimal justifications for an
entailment in DLs is utilized. This topic has been formally investigated in literature (e.g., [31]) and techniques have been developed
to solve this problem. For purpose of this work, we leverage work
on minimal justifications [31], which are defined below:
Definition 9 (Minimal justification). Let K   , where   is a DL
  K is a minimal justification
axiom and K a DL KB. A fragment K
for   in K if K
. Denote the set of
minimal justifications for K    as Just(K,  ).

    for every K

    and K

Given this, we present the definition of a publication match:

Definition 10 (Publication match). Let a n-tuple of individuals
(a1, . . . , an) In
be an information match, MI, at broker B at time
Kt
t for subscription S, where S(Qc) has n distinguished variables (i.e.,
DVar(S(Qc)) = {d1, . . . , dn}). Additionally, let J be the set of minimal
justifications for MI such that:
J = Just(Kt, S(Qc)[d1/a1, . . . , dn/an])

Define a set of publications P to be a publication match, denoted
MP, at broker B for subscription S at time t if there exists j  J such
that the following holds:
 for all P P, there exists some   j such that   P() and
 for all   j one of the following holds:
 there exists some P P and   P()) or
   / B(KP).

As in the case for information matches, there are various ways
to proceed related to the manner in which subscribers should be
notified about publications matches (e.g., in the event of a retraction publication, subscribers could be notified about invalidated
publication matches). Again, due to the application dependence of
such a decision, a choice is not imposed here.

We conclude this section with a brief example demonstrating
matches and the framework in general. Let us assume that a syndication broker B is aware of two subscribers, S1 and S2, and two
publishers, P1 and P2. Additionally, assume that the broker contains
existing background information in its knowledge base, including
the axioms defined previously in Table 1, in addition to the type
assertions that BauschAndLomb is a Company and FusariumEyeInfection is an Infection. Assume that S1 is interested in information about
risky companies. Given this interest, S1 registers a non-expiring
subscription with the broker that is composed of a continuous
query for all individuals of type RiskyCompany. On the other hand,
assume that S2 is interested in products that have some adverse
effect; these subscriptions are represented as follows, where 
indicates that the subscription does not expire:
((x)  RiskyCompany(x),) S1(s)
((x)  AdverseEffectProduct(x),) S2(s)

Next, suppose that P1 publishes an addition publication that
expires in 24 h which contains the assertions that BauschAndLomb
has a product named Renu and that Renu is a Product. In addition,
assume that P2 publishes an addition publication that also expires
in 24 h, however it contains the assertion that Renu causes the
FusariumEyeInfection. These publications are formally represented
as follows, where 24 h indicates that the publications expire in 24 h:

PP1
PP2

= ({hasProduct(BauschAndLomb, Renu), Product(Renu)}, 24 h,true, P1)
= ({causes(Renu, FusariumEyeInfection)}, 24 h,true, P2)
Assume that PP1 and PP2 arrive at the broker at times 1 and
2, respectively. When PP1 arrives at the broker, PP1 () is inte-

grated into B(K), resulting in a updated broker KB K
. At this time
the individual BauschAndLomb will not satisfy the subscription, as
it cannot be inferred that Renu is an AdverseEffectProduct; there-
fore, there will not be a match for S1 or S2 at time 1. However,
when PP2 is published at time 2 and integrated into K
, there will
be a new information matches for both registered subscriptions,
as the brokers KB now entails that Renu is an AdverseEffectProduct and that BauschAndLomb is a RiskyCompany. This is fairly
straightforward given the domain model and assertions in the
syndication brokers KB. There is additionally a composite publication match {PP1 , PP2
} for both subscriptions. We only consider the
subscription from S1, as the second follows in a similar manner;
first, we must determine the justifications for the entailment that
BauschAndLomb is a RiskyCompany. In this case, there is only one
justification: {Company(BauschAndLomb), Product(Renu), hasProd-
uct(BauschAndLomb, Renu), causes(Renu, FusariumEyeInfection)}.6
Collectively PP1 and PP2
form a publication match, as PP1
contains the assertions hasProduct(BauschAndLomb, Renu) and
Product(Renu), while PP2 contains causes(Renu, FusariumEyeInfec-
tion). Note that Company(BauschAndLomb) was originally contained
in the brokers KB independent of any publications (i.e., it is background information); therefore, {PP1 , PP2
} satisfies the definition of
a publication match. As discussed earlier, the main limitation in the
proposed syndication framework is related to DL reasoning through
incremental changes to the underlying KB. Therefore, the remain-

6 For ease of presentation, we have omitted TBox axioms from the justification.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

der of this paper addresses the two required reasoning services
within the syndication framework; namely consistency checking
and query answering through updates.

4. Incremental consistency checking

After newly published information is integrated in the brokers
KB, consistency must be re-checked. With large ABoxes, checking
consistency introduces substantial overhead. In the case of syndi-
cation, this problem is compounded as the brokers KB will become
substantially large because the KB can contain permanent domain
knowledge, as well as publications that remain valid for substantial
time periods. We have recently investigated incremental consistency checking in OWL KBs. In [25,26] we present an approach for
incrementally updating tableau completion graphs under syntactic ABox updates in the description logics SHIQ and SHOQ [25,26],
which encompass a large portion of OWL DL. To support addition
updates the update algorithm adds new components (edge, nodes,
or labels) introduced by the update to a (cached) completion graph
from the consistency check prior to the update; after this, standard
tableau completion rules are re-fired to ensure that the model is
complete. In order to support deletions, we extend work on axiom
tracing [31,4] so that the dependent structures in a previous completion graph can be rolled back. In both cases, the completion graph
built prior to the update is updated such that if a model exists (i.e.,
the KB is consistent after the update), then a new complete and
clash-free completion graph will be constructed. It was observed
that updates do not have a large effect on the existing completion
graph; therefore, orders of magnitude performance improvements
are achieved [25,26].

5. Incremental query answering

After guaranteeing consistency of an updated KB, the various
subscriptions registered with the broker can be (re)evaluated. One
of the main issues w.r.t. to the syndication framework with current query answering algorithms is that after an update the entire
knowledge base is considered when re-evaluating the query. Given
this, the technique developed in this section aims to reduce the portion of the KB that must be considered as candidate answers after
an update; therefore, the query only needs to be re-evaluated over
a subset of the KB. The technique developed is applicable to the
DL SHI (a large subset of OWL DL). Thus, in the remainder of this
section, it is assumed all KBs are expressed in SHI. Let us assume
that we are given a retrieval query (x)  C(x) for some concept C
and named individual a. Additionally, say that an ABox update  is
integrated into the KB under syntactic updates, such that the resulting KB is consistent. The main insight underlying the approach is
that the dependencies of clashes in completion graphs for the KB
caused by  and C(a) can be exploited to provide an overestimate
of the individuals that instantiate C after the update. Before formally presenting the necessary conditions for the entailment, the
definition of the dependency of a node label in a completion graph
is presented.
Definition 11 (Label dependence). Define a node label l L(x) to be
dependent on a node label C L(y) (or node yV, edge y, zE,
or edge label RL(y, z)) if during the application of expansion
rules to construct completion graph G, l is added to L(x) due to the
existence of C L(y) (respectively yV, y, zE, RL(y, z)).

The notion of clash dependence is a straightforward extension
of this definition; that is, if a clash c = (x,C, C) is observed and
either C or C is dependent on some structure s (node, edge, or
label), then the clash is said to be dependent on s. Finally, we say
that a label l L(x) is dependent on an update  if  causes the

addition of some structure s s.t. l is dependent on s; note that a
clash dependency on an update can be defined in a similar way.
Given this, the main theorem underlying the approach developed
in this section is introduced. For ease of exposition, when referring
to the addition of the structure of a set of ABox assertions  to a
completion graph G and then applying any sequence of the necessary expansion rules (as discussed in Section 47), the terminology
adding  to G, denoted G  , will be used.

Let K be a consistent SHI KB,  an ABox addition (or
Theorem 1.
deletion), and C some SHI concept. If K  C(a) (respectively K  C(a))
for some a IK  I, K +  , and K +   C(a) (respectively K   
C(a)), then one of the following conditions is satisfied:

(1) there exists G  Comp(K) (respectively G  Comp(K  )) s.t. G 
(  {C(a)}) results in a clash c that is dependent on both C(a)
and ,
(2) there exists the same node x with D1  D2 L(x) in {G1, G2} 
Comp(K) (respectively {G1, G2}  Comp(K  )) such that:
 G1  (  {C(a)}) results in a clash that is independent of C(a)
and is dependent on both  and D1  D2 L(x),
 G2  (  {C(a)}) results in a clash that is independent of  and
is dependent on both C(a) and D1  D2 L(x).

Proof. Consider addition updates. This case will be shown by
contradiction. Assume that (1) K  C(a), K +  , K +   C(a),
(2) there does not exist G  Comp(K) s.t. the first condition of the
theorem holds, and (3) there does note exist the same node x
with D1  D2 L(x) in {G1, G2}  Comp(K) s.t. the second condition
of the theorem holds. First note that (a) Comp(K + {C(a)}) /= ,
as K  C(a), (b) Comp(K + ) /= , as it is assumed K +  , (c)
Comp(K + {C(a)} + ) = , as K +   C(a), and (d) the additional
expansion rule applications caused by adding  and C(a) to each
G  Comp(K) must cause a clash; this follows from the completeness
of the incremental consistency checking algorithm (shown in Theorem 2 of [26]) and property c. Next, assumption 2 implies that every
clash observed when updating each G  Comp(K) withC(a) and  is
not dependent on bothC(a) and . This implies that each observed
clash c must be dependent on C(a) or  (but not both), as well as
some non-deterministic choice (i.e., some D1  D2 L(x) for some
x V); otherwise, K  C(a) or K +   must hold. Clearly, every
clash observed cannot be dependent on  (or C(a)), as this would
imply K +   (respectively K  C(a)). Next, it must be the case
that for some set of clashes {c1, . . . , cn} observed, any ci, 1  i  n,
is dependent on the same non-deterministic choice as any cj, i /= j;
this is a consequence of the previous properties and the fact that
each clash observed is dependent on a non-deterministic choice. It
suffices to show that for some set of observed clashes {c1, . . . , cn}
that are dependent on the same non-deterministic choice, there
exists ci, 1  i  n, that is dependent on  and there exists cj, i /= j,
that is dependent on C(a). This follows easily as if this were not
the case then either K +   or K  C(a). Note that the only cause
of non-determinism in the SHI tableau algorithm are disjunctions;
thus ci and cj must be dependent on the same disjunction. This
implies that there exists {G1, G2}  Comp(K) with the same node
x s.t. ci and cj are dependent on D1  D2 L(x), and ci and cj are
dependent on  and C(a), respectively. Thus, we have arrived at a
contradiction. Due to the assumption that K  C(a) and K     C(a),
it is the case that (K   ) +    C(a). Therefore, the case for deletions
is a consequence of the case for additions. 

7 In this discussion, it is assumed that back-jumping is not used, as all completion

graphs are maintained.

It is important to reiterate that in condition 2 of the theorem,
the node x in G1 and G2 corresponds to the same node (which corresponds to either a named or existential individual). Note that the
case in which x corresponds to an existential is not problematic
to maintain, as nodes are not merged and all completion graphs
are maintained; therefore, when a disjunction is encountered on
an existential and various new completion graphs are constructed,
the correspondences between the existential node x in each of the
completion graphs can trivially be determined.

Intuitively, the first condition of Theorem 1 states that for a
named individual a to instantiate a concept after an addition, then
some clash observed when incrementally updating a completion
graph for the original KB with  and C(a) will be dependent
on structures from both  and C(a). On the other hand, the
second condition states that C(a) and  will cause clashes in
different completion graphs that are dependent on the same
non-deterministic choice (i.e., a disjunction label). Analogous statements can be made for deletions. Thus, given a retrieval query
composed of a single DL concept, if all completion graphs for the
KB are maintained through updates and the two conditions are
checked, then the detection of new candidate bindings (respec-
tively invalidated bindings for deletions) for a given query can be
accomplished; then, only this subset of the individuals would have
to actually be checked for the entailment. The main insight is that
if this technique (or an overestimate of it) can be accomplished in a
practical manner, then this set of candidates may be a small subset
of the original KB, thereby decreasing the overhead of re-evaluating
queries given an ABox update. In the remainder of this section, we
briefly discuss a naive approach which exploits Theorem 1 for this
task. Then, we discuss how to make the technique practical.

Let us first consider addition updates; to take into account
the first condition of Theorem 1 for additions, one can update all
completion graphs for the initial KB with  and track the label
dependencies for . Following this, one can update the resulting complete and clash-free completion graphs with C(a) and
determine which clashes are dependent on both  and C(a).
This is sufficient due to the fact that there is not an expansion
rule application ordering imposed for SHI [27] and condition 1
of Theorem 1 states the clash must be dependent on both  and
C(a) when applying any sequence of the necessary expansion
rules.8 An additional observation related to this is made; for a clash
to be dependent on both  and C(a), then after adding  to all
G  Comp(K) resulting in the set of complete and clash-free completion graphs GK+, it must be the case that when adding C(a)
  GK+, a root node that had a node label, edge, or edge
to some G
label added due to  must have a label added due C(a). This is a
consequence of the previous observation and the tree-like model
property of SHI, which intuitively states that the completion graph
will be a forest of trees rooted at nodes corresponding to named
individuals. Therefore, one can easily determine an overestimate of
the individuals that could satisfy condition 1 by first updating all
G  Comp(K) with , while tracking the root nodes N with a node
label, edge, or edge label change. Then, the negated query concept,
C, can be to L(xa) 9 in each updated completion graph, and for the
condition to be satisfied for a, a label must be added to some n N
due to the addition of C to L(xa).

A straightforward observation is made regarding the second
claim for Theorem 1 as well. In particular, when updating each

8 Note, however, there could be a case in which there is an immediate clash as a
result of adding the structures of  and C(a) to some G  Comp(K) prior to applying any expansion rules. Using this approach, such a case would not be detected;
however this case can trivially be covered by inspecting  and C(a).
9 Given a named individuals a, we denote by xa the root node corresponding to a.

G  Comp(K) with , all clashes dependent on  that are independent of C will be observed. Therefore, if the dependencies of labels
on disjunctions are tracked during the tableau algorithm, one can
easily determine the individuals that satisfy the second condition
by adding C to each individual and checking if this causes a clash
that is dependent on some disjunction that contributed to a clash
when updating each G  Comp(K) with . An overestimate of this
approach can be provided in an analogous manners as the previous
case. Such a disjunction dependency tracking function can easily
be maintained in a similar manner to the axiom tracing technique
discussed in Section 4. In particular, when a disjunction is added
to a node in a completion graph, the function will be updated with
the label addition events that are a result of the disjunction. As this
can be accomplished in an analogous manner as axiom tracing, this
function is simply assumed.

Now consider deletion updates; by additionally leveraging the
axiom tracing function discussed in Section 4, a similar technique
to the approach just discussed for addition updates can be used.
Specifically, due to the completeness of the axiom tracing func-
tion, the complete and clash-free completion graphs for K   can
be obtained by rolling-back the change events in each G  Comp(K)
that are dependent on some    and then applying the necessary
expansion rules (see [26] for more details). Thus, the deletions can
be supported in the same manner as additions.

It is clear that adopting such a naive approach to determine
the candidate individuals will impose substantial overhead, and
likely be worse than simply running the query from scratch. This is
because one would have to perform such a check for each named
individual in all completion graphs, which would clearly be ineffec-
tive. Further, maintaining all completion graphs for the KB is itself
impractical due to the potential exponential number of completion
graphs. Given this, in the remainder of this paper, a more practical approach to exploit Theorem 1 is presented. First, the issue
of adding the negated query concept to all named individuals is
addressed in Section 5.1. Following this, we develop a technique to
avoid maintaining all completion graphs for the KB.

In the techniques developed in the remaining sections of this
paper some restrictions and assumptions are imposed on the
queries supported. First, it must be the case that the query can be
rolled-up into a distinguished variable. This implies that the rollingup technique must be applicable to the query and that it must
contain at least one distinguished variable (implying it is a retrieval
query). While this is a restriction of the approach in general, it is
noted that the requirement that the query is a retrieval query does
not directly impact the utility of the technique for the purpose of
the syndication framework, as subscriptions are in fact retrieval
queries. The second restriction is that role atoms in the query must
be simple roles; this is necessary to ensure completeness of the
proposed techniques. It is noted, that in general rolling-up arbitrarily shaped queries in the presence of transitive roles is known
to be problematic [43,18]; therefore, this restriction implies that
the query can in fact be rolled-up (as required in the first restric-
tion). Lastly, an additional syntactic restriction is imposed on the
queries supported, however this will be discussed shortly in the
next section.

5.1. Concept guide

In this section, an approach is presented for avoiding the addition of the negated query concept to all named individuals. The
main goal behind the approach is to build a structure that can be
used to determine the propagation of labels to root nodes in a completion graph due to the addition of the negated query concept.
In the remainder of this section, only retrieval queries involving a
single DL concept are assumed (i.e., concept retrieval queries); how-

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

ever, the technique introduced here is extended to complex query
patterns in Section 5.3.

In order to support general TBoxes, a restriction is imposed on
the queries supported in the approach. Given retrieval query for
concept C (in NNF), it is assumed that if R.D. clos(K)  clos(C),
then it must be the case that P.E / clos(C), where

Intuitively, given a complete and clash-free completion graph G and
some named individual a, this restriction ensures that if a new edge
is added to G as a result of extending G with C(a), then no concepts
will be transferred back up this newly added edge. This effectively
isolates the propagation of labels due to expansion rule applications
from the addition of the negated query concept to G (we discuss
the impact of this restriction in practice later). Given a KB K and
concept C, we will say that C is safe with respect to K if C satisfies
the restriction just introduced.

We now introduce the structure that is leveraged for determine
the effects of adding a concept name to the label of a node in a
completion graph; this is referred to as a concept guide. Intuitively, a
concept guide is a labeled, directed graph, that is built by repeatedly
inspecting the form of the concept names in node labels in the
concept guide.
Definition 12 (Concept guide). Define a concept guide G to be a
labeled directed graph G = (NG, EG, LG). Each node n NG is labeled
with a non-empty set of SHI concepts, and each edge (n, m) EG
is labeled with a non-empty set of role names. Given SHI KB
K, SHI concept C, and G = (,, L), define the concept guide for
C, denoted guide(C), to be initialized with n NG and LG(n)  {C}.
Then define the following rules to be repeatedly applied to the
concept guide node labels until no further modifications can be
made to G:
(1) if R.D. LG(n) and (n, m) / EG s.t. R LG((n, m)) and D LG(m)

then
(a) add a new node m to NG and set LG(m)  {D},
(b) set EG  EG  {(n, m)},
(c) set LG((n, m))  LG((n, m))  {R}.
(2) if R.D. LG(n), there is some S with Trans(S) and
, and {(n, m), (m, m)}  EG s.t. R LG((n, m)), D LG(m), and
S  LG((n, m)) and S  LG((m, m)) for all S s.t. Trans(S) and
then
(a) add a new node m to NG and set LG(m)  {D},
(b) set EG  EG  {(n, m)}  {(m, m)},
(c) set LG((n, m))  LG((n, m))  {R},

(d) set

and

LG((n, m))  LG((n, m))  {S}

LG((m, m)) 

(3) if C1  C2  LG(n) or C1  C2  LG(n) and {C1, C2}  LG(n) then set

LG((m, m))  {S} for all S s.t. Trans(S) and

LG(n)  LG(n)  {C1, C2}.

terminate;

the concept guide will

It is a straightforward consequence of the definition that
the construction of
this
is implied by the conditions checked prior to performing
the operations in the definition and the fact
that clos(C)
is of finite size. To demonstrate the construction of a concept guide,
let us consider the previous query from Section
5, (x)  (hasProduct.(causes.Infection))(x). Additionally, assume
that hasProduct is a transitive role; observe that the negation of the
query concept is hasProduct.(causes.Infection). Let us consider
the concept guide for the negated query concept; when constructing the concept guide, a new node will first be added and labeled
with the negated query concept. Following this, the second condition of Definition 12 will be applicable to this node label. Therefore,
a new node and edge will be created with labels hasProduct and
causes.Infection, respectively. Further, because the hasProduct
role is transitive, a self-looping edge labeled with this role will be
added to the most recently added node. Lastly, due to the newly
addedcauses.Infection label, a new edge and node will be created
and labeled with causes and Infection, respectively. The resulting
concept guide is depicted in Fig. 1.

5.1.1. Approach

In this section, we discuss how the concept guide is used to
determine the propagation of labels due to an update in a completion graph. First, the notion of a concept guide path between two
nodes in a completion graph in defined.
Definition 13 (Concept guide path). Let K be a SHI KB, C some
concept, G = guide(C), G  Comp(K), and n, m NG. Define there
to be an n-m-concept guide path between two nodes x, yV,
denoted path(n, m, x, y,G, G), if there is a sequence of edge traver-
sals, x1, y1, . . . ,xk, ykE and (n1, m1), . . . , (nk, mk) EG, such
that the following holds:

(1) x1 = x and n1 = n (i.e., the path starts at n NG and x V),
(2) for each edge traversal, xi, yiE, and corresponding edge
traversal, (ni, mi) EG, it is the case that for some R LG((ni, mi)),
xi has R-neighbor yi:

Fig. 1. Example Concept Guide.

(a) if some node z V is blocked by w V, then an edge , w
with L(, w)  L(, z), where  is the predecessor of z,
is also considered for traversal,

(3) yk = y and mk = m (i.e., the path ends at m NG and yV).

Condition 2(a) is necessary due to blocking conditions utilized
during the creation of SHI completion graphs, as cyclic models can
occur. Condition 2(a) overcomes the problematic case where there
would be a path in the completion graph that satisfies the constraints imposed by the concept guide, yet due to blocking, the path
does not explicitly exist. Assuming the restrictions imposed on the
form of C, it can be shown that if adding a concept C to root node
xa in a complete and clash-free completion graph causes a label to
be added to a root node xb, then there is a concept guide path that
starts at the concept guide node labeled with C between the two
nodes.
Theorem 2. Let K be a SHI KB, G  Comp(K), C be a SHI concept that
is safe with respect to K, and G = guide(C). If adding C(a), a IK,
to G causes a concept name to be added to L(xb), b IK, then there
is a concept guide path path(n, m, xa, xb,G, G) for some n, m NG s.t.
C LG(n).

The proof of this theorem is omitted, however it can be found in
Appendix A.2.2 of [26]. Theorem 2 implies that the concept guide
can be used to avoid adding C to all individuals in the KB to take
into account the first condition of Theorem 1 for additions. This is
because, one can maintain the root nodes x with a node label, edge,
or edge label change due to , and then search for all nodes that
satisfy the concept guide path relation involving x. Similarly, the
second condition of the theorem can be taken into account; that
is, if the disjunction dependencies of labels are tracked during the
tableau algorithm, then one can easily determine the root nodes
N which have a label that is dependent on one of the disjunctions
that contributed to clash observed due to . Thus, the concept guide
can then be used to determine the root nodes n N reachable due to
C. Given the discussion presented earlier, it is clear that deletion
updates can be handled in a similar manner.
Next, a variety of notation is introduced; given ABox addition
, G  Comp(K) and G
 the result of G   (containing a clash or
) the set of named individuals
clash-free), denote by Dep(, G, G
whose corresponding root nodes have a node label, or incom-

ing/outgoing edge or edge label changed when constructing G
Further, denote by DisjK(D1  D2, x) the set of named individuals
whose corresponding root nodes have a label that is dependent on
disjunction D1  D2 L(x) in some completion graph G  Comp(K).
Given this, we define the set of c oncept candidates, which intuitively is an overestimate of the individuals which instantiate (or no
longer instantiate) a concept after an addition (respectively dele-
tion).
Definition 14 (Concept candidates). Let K be a SHI KB,  an ABox
addition (or deletion), C be a SHI concept that is safe with respect
to K and , and G = guide(C). Then define the concept candidates,
denoted CC(K, C, ), to be the set of all named individuals a IK  I
such that adding  to some G  Comp(K) (respectively G  Comp(K 
 and one of the following conditions is satisfied for
)) results in G
some n, m NG, where C LG(n):
(1) a I,
 clash-free, b Dep(, G, G
) and there is a concept guide path
(2) G
path(n, m, xa, xb,G, G
(3) a clash is observed that is dependent on D1  D2 L(y)
and for some b DisjK(D1  D2, y) (respectively b DisjK(D1 
D2, y)) or b Dep(, G, G
) there is a concept guide path
path(n, m, xa, xb,G, G

  Comp(K) \ G.

),

) in some G

Theorem 3 implies the correctness of the approach using the
concept guide for determining the candidate new (respectively
invalidated) bindings for a retrieval query consisting of a SHI con-
cept.

Let K be a SHI KB,  a n ABox addition (or deleTheorem 3.
tion), C be a SHI concept that is safe with respect to K and ,
and G = guide(C). If for some a IK  I, K  C(a) and K +   C(a)
(respectively K  C(a) and K    C(a)), then a CC(K, C, ).

The proof of this theorem is omitted, however it can be found in
Appendix A.2.3 of [26]. We conclude this section with a brief discussion regarding the approach. First, the construction of the concept
guide assumes the standard SHI tableau expansion rules [27]. In
many DL reasoning systems, a variety of optimizations are uti-
lized, some of which introduce additional tableau expansion rules
(e.g., unfolding rule, domain/range rule [44]). It is noted that the
approach can easily be extended to take into account the unfolding
and domain/range expansion rules via a simple extension to the
definition of the concept guide (see [26] for a more detailed discus-
sion). Lastly, a brief comment is in order regarding the impact of this
restriction in practice. To investigate this, we gathered 460 of the
OWL ontologies10 used during a recent survey of the publicly available OWL ontologies available on the Web [47] and investigated the
actual impact of the restriction. Each concept in the ontology was
selected and tested to see if it violated the restrictions imposed
by the approach.11 Of the 460 ontologies, only 4 had at least one
concept that violated the restriction. This indicates that the restriction should not have a large impact when attempting to use the
technique for many ontologies. Further, in the four violating ontolo-
gies, on average 6.8 concepts actually violated the restriction. This
is promising, as the number of concepts that cannot be used in
queries is very small.

5.2. Summary completion graph

Incrementally maintaining all completion graphs for a given KB
is not practical. Further, in the presence of a reasonable degree of
non-determinism in a KB, constructing all completion graphs is
a very expensive process. To overcome this issue, an approach is
developed in which a completion graph structure is constructed
that represents a summary of the structures present in all completion graphs for the KB; this structure is referred to as a summary
completion graph. Importantly, this structure can be utilized to
locate the candidate individuals, and therefore all completion
graphs for the KB do not have to be maintained.

Definition 15 (Initial summary completion graph construction). Let
SG be the summary completion graph for SHI KB K constructed by
applying the SHI tableau algorithm to K, however with the following modifications:
(1) the -rule is replaced as follows: if C1  C2 L(x), x is not indirectly blocked and {C1, C2}  L(x) then L(x)  L(x)  {C1, C2},
(2) if a clash is encountered, it is ignored and the algorithm contin-

ues.

Termination of the construction of the summary completion
graph follows easily from the fact that the termination for the SHI

10 Note this is a subset of the surveyed ontologies, as a number of the ontologies
were no longer available.
11 The statistics were gathered after absorption and internalization of the general
TBox. It was also assumed that the unfolding and domain/range rules were utilized;
therefore, the syntactic restriction was extended to cover the concept encountered
when unfolding the query concept.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Table 2
Modified tableau expansion rules for the summary completion graph
-rule:

-rule:

-rule:

-rule:

+-rule:

If (1) C1  C2 L(x), x is not indirectly blocked and
(2) either (a) 
((C1  C2, x)) == false or (b) {C1, C2}  L(x)
then set 
((C1  C2, x)) = true and L(x) = L(x)  {C1, C2}
If (1) C1  C2 L(x), x is not indirectly blocked and
(2) either (a) 
((C1  C2, x)) == false or (b) {C1, C2}  L(x)
then set 
((C1  C2, x)) = true and L(x) = L(x)  {C1, C2}
If (1) S.C L(x), x is not blocked and
(2) either (a) 
((S.C, x)) == false or (b) x has no S-neighbor y with C L(y)
then set 
((S.C, x)) = true and create a new node y with L(x, y) = S and L(y) = C
If (1) S.C L(x), x is not indirectly blocked and
(2) either (a) 
((S.C, x, y)) == false and there is an S-neighbor y of x with C L(y) or (b) there is an S-neighbor y of x with C / L(y)
then set 
((S.C, x, y)) = true and L(y) = L(y)  C
If (1) S.C L(x), x is not indirectly blocked and
(2) there is some R with Trans(R) and
(3) either (a) 
((S.C, R, x, y)) == false and there is an R-neighbor y of x with R.C.L(y) (b) there is an R-neighbor y of x with R.C. / L(y)
then set 
((S.C, R, x, y)) = true and L(y) = L(y)  {R.C.}

tableau algorithm is independent of clash detection; therefore it
can be shown in an identical manner as termination for the SHI
tableau algorithm [27]. The construction of the summary completion graph proceeds in an identical manner as the regular tableau
algorithm, however when the -rule is applied, all concept names
of the disjunction are added to the node label (in the same manner
as the -rule). Note that condition 2 of the definition is required, as
adding all concept names from a disjunction can introduce clashes
which would not occur in the different completion graphs.

We now show how the summary completion graph can be
used to avoid maintaining all completion graphs for the purpose
of exploiting Theorem 1. This is accomplished by showing that an
overestimate of the concept candidates introduced in Definition 14
can be determined by simply using the summary completion graph.
Clearly, the first condition of the definition is trivial; therefore, only
conditions 2 and 3 are addressed. In the following discussion, we
only address addition updates, as the approach for deletions follows
in a similar manner and will be addressed later.

For the second condition of Definition 14 to be taken into
account using the summary completion graph, intuitively it must
be shown that we can determine the propagation of labels due to
the addition in all complete and clash-free completion graphs for
a KB by simply using the summary completion graph. The main
idea of the approach is that given a set of assertions , the structures for  can be added to the summary completion graph and
the expansion rules can be applied to the added labels in a similar manner as when incrementally updating a complete graph
(as in Section 4). Then, it can be shown that the propagation of
labels to root nodes in the summary completion graph subsumes
the propagation in all completion graphs. Further, we show that
the concept guide paths must also exist in the summary completion graph; collectively, this implies the completeness of the
approach.
Due to the specialized treatment of the -rule, there may be
labels present in the summary completion graph which prohibit
the application of an expansion rule. For example,  may include a
type assertion of the form R.C.(a) and in the summary completion
graph all R-neighbors of a already have C in their label; however, in
the different complete and clash-free completion graphs for the KB
prior to the addition of , there could exists some R-neighbor that
does not contain C in its label (this is due to the non-determinism
in the tableau algorithm). In this case, this neighbor would in fact
have a label change, causing it to be considered when detecting concept guide paths. This problem is overcome by a modification when
checking if the expansion rules can be applied to a node. Specifi-
cally, if when updating the summary completion graph with , it is
the case that a node label exists which prevents the application of
an expansion rule, then it is applied anyway. In order to ensure that

the algorithm still terminates, rule applications are tracked using
a marking function 
 when they have been applied to a specific
node during the update of the summary completion graph (shown
in Table 2). Therefore, the re-application will only happen once.
Given this, the approach for updating of the summary completion
graph is defined as follows.
Definition 16 (Summary completion graph update). Let K be a SHI
KB, SG be the summary completion graph for K, and  a set of
ABox assertions. Then SG is incrementally updated with  using
the approach discussed in Section 4, however:
 clashes are ignored,
 the modified expansion rules defined in Table 2 are assumed and
are applied to the following nodes:
(1) each node xa corresponding to some individual a I,
(2) any node subsequently reached by the application of an expan-

sion rule due to conditions 13,

(3) any node that was previously blocked, yet the block is invalidated because of the addition of a node label due to conditions
13.

) for some G  Comp(K) and G

Termination of the update process is shown in Appendix A.2.5
of [26]. Denote by update(, SG) the update of summary completion
graph SG with  according to Definition 16. Additionally, denote by
Dep(, SG) the set of named individuals whose corresponding root
nodes have a node label, or incoming/outgoing edge or edge label
that is (re)added during update(, SG). In order to show completeness of the approach, we must demonstrate that SG can be used to
find all a Dep(, G, G
 the result of
adding  to G. First, note that if an edge x, yE, where x, y are root
nodes, is added to G, it must have been added due to a role assertion
in  (the same holds for edge labels for edges between root nodes);
this is due to the fact that the tableau expansion rules do not add
edges or edge labels between named individuals. Therefore, it suffices to show that if a label is added to some root node xa during
the update of some G  Comp(K), then a label will be (re)added to
the corresponding root node in the summary completion graph.
Lemma 1. Let K be a SHI KB, G  Comp(K), SG be the summary completion graph for K, and  a set of ABox assertions. If when adding  to
G, a root node x has a concept name added to L(x), then x will have a
concept name (re)added to LSG (x) when updating SG with .

The proof for this lemma can be found in Appendix A.2.6 of
[26]. Given this, the summary completion graph can be used to
) by tracking the nodes that are reached
determine Dep(, G, G
during the update of the summary completion graph. However,

one must then find the concept guide paths involving these indi-
 (i.e., the result of adding  to G  Comp(K)).
viduals in each G
Once again, it can be shown that it suffices to simply use the
. This
updated summary completion graph, rather than all G
is implied by the following theorem; again, the proof for this
lemma is omitted, however it can be found in Appendix A.2.7 of
[26].
Lemma 2. Let K be a SHI KB, G  Comp(K), SG be the summary completion graph for K, C be a SHI concept that is safe with respect to K,
and G = guide(C). If for some a IK, n, m NG s.t. C LG(n) there is
a concept guide path path(n, m, xa, xb,G, G), then there is a concept
guide path path(n, m, xa, xb,G, SG).

Now let us consider the third condition of Definition 14 for
addition updates. An approach must be developed that uses the
summary completion graph to determine the clashes observed
when adding  to G  Comp(K) that that are dependent on some
D1  D2 L(x); additionally, the root nodes with a label that is also
dependent on D1  D2 L(x) in a completion graph G
  Comp(K) \ G
must also be determined. First, it can be shown that all clashes
observed when updating each G  Comp(K) will be observed on
some node when updating the summary completion graph. While
clashes are ignored during the construction and update of the summary completion graph, they will be present and can be tracked.
This is not formally shown here, however it is a direct consequence
of Lemma 7 in [26].

Next, we must show that the disjunctions that clashes are
dependent on can be located using the summary completion graph.
Further, we must show that we can locate the root nodes with a
node label dependent on these disjunctions. The general idea of the
approach, and the focus of the remainder of this section, is that a
specialized disjunction dependency function can be introduced for
the summary completion graph, such that the dependencies subsume those in all completion graphs. Due to the fact that labels
from different completion graphs are essentially merged in the
summary completion graph, care must be taken when maintaining
the dependence of labels on disjunctions. In particular, we must
ensure that if a label c ould be added due to a disjunction, then this
is reflected in the disjunction dependencies. To account for this,
a disjunction dependency function for the summary completion
graph is defined in Definition 17:

Definition 17 (Summary disjunction dependency). Given summary
completion graph SG and node x with D1  D2 L(x), inductively
define the set S of concept/node pairs (C, x) that are dependent on
D1  D2 L(x) as follows:
 (D1  D2, x) S,
 if (C, y) S, then if C of the form:

(1) C1  C2, then {(C1, y), (C2, y)}  S,
(2) C1  C2, then {(C1, y), (C2, y)}  S,
(3) R.D., then for each z s.t.

(a) z a R-neighbor of y and DL(z), then (B, z) S for all BL(z),
(b) z a R-neighbor of y, z blocked by m and DL(m), then
(c) z a R-neighbor of y, z blocked by m,S.DL(m) s.t.

(B, m) S for all BL(m),
, and DL(m), then (B, y) S for all BL(y),

(a) z a R-neighbor of y and DL(z), then (D, z) S,
(b) z a R-neighbor of y, z blocked by m and DL(m), then

(4) R.D., then for each z s.t.

(D, m) S,
DL(m), then (D, m) S,

(c) y blocks z, m the predecessor of z, m a R-neighbor of z, and

(5) R.D. and there is some P s.t. Trans(P) and

, then for each

z s.t.
(a) z a P-neighbor of y and P.DL(z), then (P.D, z) S,
(b) z a P-neighbor of y, z blocked by m and P.DL(m), then

(P.D, m) S,
P.DL(m), then (P.D, m) S.

(c) y blocks z, m the predecessor of z, m a P-neighbor of z, and

Denote by DisjSG (D1  D2, x) the named individuals whose corresponding root nodes in SG have a node label dependent on D1 
D2 L(x). The disjunction dependency function can easily be constructed after building or updating a summary completion graph.
Further, if a new disjunction is introduced after an update, then its
dependencies can be easily identified after the update. Given this,
it is simply assumed that the disjunction dependency function is
maintained.
Importantly, it can be shown that if a clash is observed when
updating some G  Comp(K) that is dependent on some disjunction D, then the clashes observed when updating the summary
completion graph will be dependent on disjunctions whose dependencies subsume the root nodes that are dependent on D in some
completion graph. This is not formally shown here, as it is a
direct consequence of Lemmas 2, 3, & 6 of [26]. However, this
implies that we can find all individuals a s.t. there was a clash
is observed when updating G that is dependent on D1  D2 L(y)
and a DisjK(D1  D2, y). Given this, to take into account the third
condition of Definition 14, we must then find concept guide paths
) in
involving these individuals (or some individual Dep(, G, G
  Comp(K) \ G; it is a direct consequence of Lemma 2
some G
that this can be performed simply using the summary completion
graph.

ABox deletions. Until this point, we have only discussed how
the summary completion graph can be used to support addition
updates. Given the axiom tracing function discussed in Section 4,
incremental deletions can be supported in a similar manner. This
is because the completeness of the axiom tracing function when
applying it to the summary completion graph directly follows from
Theorem 1 of [26]. Given this, we can revert the change events in the
summary completion graph that are dependent on the deletion, and
then the necessary expansion rules can be applied to the summary
completion graph; importantly, the modified -rule must be used
(as presented in Definition 15) and clashes must be ignored during
the re-application of the expansion rules. Thus, the summary completion graph for K   is easily obtained; for ease of presentation
denote this process by Del(, SG). Therefore, deletion updates can
be supported by then adding  back to the summary completion
corresponding to K   and performing the same approach as in
the case for additions. It is noted that after a deletion has been
processed using this technique, the summary completion graph
must again be updated to reflect the deletion. As in the original
retraction of , the axiom tracing function can be used for this
purpose. Given this, an overestimate of the concept candidates
is introduced.
Definition 18 (Concept candidates overestimate). Let K be a SHI
KB,  an ABox addition (or deletion), SG the summary comple-
= update(, SG) (respectively

tion graph for K, S

G)), C a SHI concept that is safe with respect to

K and , and G = guide(C). Define the overestimate of candidate

individuals, denoted CCSG (K, C, ), to be the set of named individuals a IK  I such that for some n, m NG s.t. C LG(n), one of the
following conditions is satisfied:

= Del(, SG), S

= update(, S

(1) a I,

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

(2) b Dep(, SG) (respectively b Dep(, S

G),

cept guide path path(n, m, xa, xb,G, S

G)) and there is a con-

(3) the expansion rules are applied to a node x during update(, SG)
G)) such that {A,A}  L(x), AL(x)

(respectively update(, S
or AL(x) is dependent on D1  D2 L(y) (determined using
Definition 17), and for some b DisjS
(D1  D2, y) there is a con-

cept guide path path(n, m, xa, xb,G, S

G).

Lastly, it can be shown that the overestimate is complete; again,
the proof for this theorem is omitted, however it can be found in
Appendix A.2.10 of [26].
Theorem 4. Given a SHI KB K, ABox update , SHI concept C that
is safe with respect to K and , and summary completion graph SG for
K, then CC(K, C, )  CCSG (K, C, ).
5.3. Supporting complex query patterns

Thus far, the techniques presented have only addressed queries
that are simply composed of a single DL concept. Given this, the
approach is now extended to support complex query patterns. As
discussed previously, the general approach for supporting complex
query patterns is to transform each role atom in the query into a
concept atom, which is referred to as rolling-up the query. It can
be shown that the techniques developed in the previous sections
can be used to find the candidate bindings for the distinguished
variable that the query is rolled-up into if the resulting query concept is safe w.r.t. the KB and updates. Further, it can be shown that
the query can simply be rolled-up once using a single set of new
concept names and the additional type assertions for the representative concepts can be ignored when determining the candidates.
Thus, if the query can be rolled-up into a distinguished variable x,
the same approach can be used to find the candidates using the
single rolled-up concept.
Given a retrieval query Q with DVar(Q ) = {x1, . . . , xn}, denote
the rolling-up of Q into xi  DVar(Q ) by Rollup(i, Q ), such that it produces a SHI concept C where each xj, i /= j, has been replaced by a
new atomic concept Dj not appearing in the KB, any update, or the
query. It is assumed that the concept obtained by Rollup(i, Q ) is safe
with respect to K and all updates. Given this, for a new (respectively
invalidated) binding {a1, . . . , an} to occur, it must be the case that
the individual bound to the distinguished variable that the query is
rolled-up into is in the set of concept candidates for the rolled-up
query concept.

Let K be a SHI KB, Q a conjunctive retrieval query
Theorem 5.
that can be rolled-up into a distinguished variable xi  DVar(Q ),
C = Rollup(i, Q ), and  an ABox addition (or deletion).
If K 
Q [x1/a1, . . . , xn/an] and K +   Q [x1/a1, . . . , xn/an] (respectively
K  Q [x1/a1, . . . , xn/an] and K    Q [x1/a1, . . . , xn/an]), then
ai  CC(K, C, ).

Again, the proof for this theorem is omitted, however it can
be found in Appendix A.2.11 of [26]. It is a direct consequence of
Lemma 2, Theorems 4 and 5 and the fact that each Dj is a new
atomic concept that the summary completion graph can be used to
determine an overestimate of concept candidates for the rolled-up
query concept.

It is easy to show, however, that one cannot simply consider the
candidates for the variable that the query is rolled-up into as the
only candidates for the other distinguished variables in the query.
This is because the previously described techniques do not allow us
to make any statements regarding the candidates for any variables
in the query expect for the distinguished variable that it is rolled-up
into. Therefore, we develop a technique determine the remaining
candidates; we refer to this as the query impact on the candidates.

It is first pointed out that given Theorem 5 and the monotonicity of
SHI, in the event of deletions, all that must be considered after the
update are the previous answer sets which have some individuals
that is in the set of query concept candidates. Therefore, one simply
needs to re-check these answer sets to ensure that the entailment
still holds. Given this, the remainder of this section only addresses
ABox additions.

It has previously been shown that a conjunctive query can be
answered be syntactically mapping the query into all completion
graphs for the KB [38]. More specifically, [38] defines a syntactic
mapping from a query Q (restricted to only simple roles) into a
completion graph G, denoted Q  G, using a mapping  from the
variables (both distinguished and non-distinguished) and individuals in Q into the nodes of G such that:
 (a) = xa for each individual a Q ,
 for each atom C(x) in Q, C L((x)), and
 for each atom R(x, y) in Q, (y) is an R-neighbor of (x).

If the query can be mapped into all completion graphs, then
the KB satisfies the query [38]. In order for such an approach to
be complete, a special blocking condition, tree-blocking, must be
used during the tableau algorithm, in which blocking is delayed
to take into account the longest path in the query; this ensures
that such a mapping will be possible in the presence of blocking. It
is important to note that if the query contains only distinguished
variables, then tree-blocking is not necessary and simply dynamic
blocking can be used. This is because root nodes corresponding to
named individuals are never blocked during the tableau algorithm.
Additionally, a TBox axiom   C  C must be added to the KB for
each concept atom C in the query; this is necessary as the query
concepts are syntactically mapped into the completion graph.

A straightforward application of this technique can be leveraged for our purpose. First, it is noted that extending the KB with
  C  C for each query concept may be impractical in the syndication framework when dealing with a substantial number of
registered subscriptions. Therefore, a slight modification of the
approach is used, in which the mapping of concept names in the
query is ignored. Specifically, given a conjunctive retrieval query
Q with DVar = {z1, . . . , zn} that has been rolled-up into a distinguished variable zi  DVar(Q ) resulting in concept C and the set
of query concept candidates CC(K, C, ), the following mapping is
checked in some G  Comp(K + ) for each xb s.t. b CC(K, C, ):
 (a) = xa for each individual a Q ,
 (zi) = xb,
 (zj) = xc for 1  j < i, i < j  n and some root node xc,
 for each atom R(x, y) in Q, (y) is an R-neighbor of (x).

If the original candidate individual b cannot be mapped into xb
such that there is a valid mapping for the remaining query nodes,
then this individual does not need to be considered as a candidate.
This follows as a completion graph (i.e., model) has just been found
in which the query cannot be mapped [38]. However, if the query
can be mapped into the completion graph such that the candidate
individual, b, is mapped into xb, then this individual must be considered as a candidate binding. Importantly, any named individual
in the completion graph that can be mapped into the remaining
distinguished variables of the query graph must also be added to
the candidate set. The mapping only needs to be performed for one
completion graph, as for the entailed to occur, it must be satisfied
by all completion graphs. Given a set of individuals A and KB K,
denote by map impact(A, Q, ) the set of all named individuals corresponding to the root nodes that are mapped into a distinguished
variable in a valid mapping.

Table 3
Test-Suite Ontology Overview

Exp.

# Clas.

# Prop.

# Ind. (KB1/KB2)

# Triples (KB1/KB2)

Sum. Build (s) (KB1/KB2)

Sum. Mem. (mb) (KB1/KB2)

16,942/33,884
17,941/35,882
16,283/37,450
25,272/51,762

54,081/107,734
65,560/130,784
130,800/225,095
246,266/423,031

1.7/3.7
2.4/5.3
2.3/9.2
15.2/30.4

57/115
82/165
59/228
183/331

To illustrate the approach, consider

the query (x, y) 
Company(x)  onSellList(x, y)  hasCEO(y, z). Let us assume that
the query is rolled-up into the variable x; then, when determining
the additional candidates, any individual a CC(K, C, ) that does
not have a onSellList-neighbor, which in turn has a hasCEO neighbor cannot be a candidate binding for the variable x. Again, this is
because a model has just be found in which the entailment does not
hold. However, if a has onSellList-neighbors b and c (both of which
are root-nodes) that also have hasCEO-neighbors d and e, respec-
tively, then a, b, c are considered in the candidate set. Lastly, it can
be shown that the query impact approach is complete; the proof
for this theorem can be found in Appendix A.2.12 of [26].

Let K be a SHI KB, Q a conjunctive retrieval query
Theorem 6.
that can be rolled-up into a distinguished variable xi  DVar(Q ),
C = Rollup(i, Q ),  an ABox addition, and A = CC(A, Q, ).
If
K  Q [x1/a1, . . . , xn/an] and K +   Q [x1/a1, . . . , xn/an],
then
{ai, . . . , an}  map impact(A, Q, ).

5.4. Finding concept guide paths

Until now, the task of finding concept guide paths in a completion graph has not been addressed. It is a fairly straightforward
observation that the task of determining if there is a concept guide
path in the summary completion graph can be reduced to evaluating regular path expressions over a labeled directed graph. This
follows as the summary completion graph is a labeled directed
graph, and it is easily seen that a set of regular path expressions
can be constructed from the concept guide; specifically, for each
node x in the concept guide, a regular path expression can be created that starts at the node that corresponds to the negated query
concept and terminates at x. Then, in the most naive approach, for
each pair of nodes in the completion graph, one needs to test if there
is a path in the completion graph that satisfies one of the regular
path expressions.

There exist known results regarding the complexity of evaluating regular path expression over graph databases. Specifically,
it has been shown that deciding if a graph G contains a directed
path from nodes x to y satisfying regular expression R can be performed in polynomial time [35]. Given this, assume there are m
nodes in the concept guide and s nodes in the summary comple-

tion graph. Then, there are

pairs of nodes in the summary

completion graph, which is O(n2). Thus, the concept guide paths
can be found in O(mn2) time. From a practical point of view, it is
important to note that there has been recent work in XML database
literature that addresses the evaluation of regular path expressions
over graph-based XML data (XML documents with IDREFs) [30].
Therefore, there exist known algorithms which can be utilized to
locate concept guide paths.

5.5. Incremental query answering algorithm

and determining the set of individuals that must be considered
in concept guide paths when determining the concept candidates is provided (shown in Algorithm 1). It is assumed that the
summary completion graph SG is created at startup. For ease of
= update(, SG) and a clash c observed during

exposition, given S

update(, SG) that is dependent on a set of disjunctions in node

labels of S
G, denote by Disj(c) the set of named individuals a s.t.
a DisjS
(D1  D2, x) for some disjunction D1  D2 LS
(x) that c is
dependent on.

Algorithm 1. update summary(SG, )

The general algorithm for incremental query answering is presented in Algorithm 2. The algorithm is presented in terms of
a single query. It is assumed the query can be rolled-up into
the distinguished variable xi  DVar(Q ) and that the initial set of
answers for Qc is previously determined. The algorithm first integrates the update into a consistent KB; if the update is an addition,
it is assumed that KB is consistent after the update. Following
this, the set of candidate individuals is found using the summary completion graph and concept guide searches (lines 23);
for simplicity, given a set of individuals AI and concept guide G, the
location of concept guide paths is denoted as guide search(AIG). If
the update is an addition, the remaining candidates are found by
taking into account the query impact. After this, the set of candidate distinguished variable bindings is iterated over and checked
for entailment. Standard techniques for query answering are used.
In contrast, if the update is a deletion, each tuple in the previous
answer set is iterated over and tuples that do not contain some
individual in the set of affected individuals are still entailed, as the
conditions for the invalidation of the entailment were not satis-
fied; otherwise, the tuples are re-checked for entailment. Lastly,
correctness of Algorithm 2 can be shown. The proof for this theorem is omitted, however it can be found in Appendix A.2.13 of
[26].

Prior to presenting the incremental query answering algorithm,
the pseudo code for updating the summary completion graph

Theorem 7. Algorithm 2 is sound, complete, and terminating.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Algorithm 2. update query results(K, SG, Q,G, R, )

6. Empirical results

A prototype of the algorithm developed in this paper has been
implemented as an extension to the OWL DL reasoner Pellet.12 In
order to evaluate the algorithm for the purpose of OWL-based syn-
dication, an empirical evaluation has been performed using various
OWL KBs with large ABoxes; namely VICODI,13 SEMINTEC,14 the
Lehigh University Benchmark (LUBM),15 and an extension to LUBM,
called the University Ontology Benchmark (UOB)16 have been used.
These ontologies have been selected as a test suite because they
are expressed in the DLs that the algorithm supports and provide
a range of expressivity.17 Also, the constructs used in these ontologies align with common usage of OWL constructs [47]. It is noted
that the ABoxes have been manually created18 or are automatically
generated using dataset generators accompanying the ontologies.
This allows the simulation of background information and publications which persist in the brokers KB for a long period of time, and
allows us to investigate the way in which the incremental query
answering algorithm scales. Table 3 presents an overview of the
ontologies, including DL expressivity, number of classes, properties,
individuals, and triples (we will comment on the last two columns
shortly).

For each of the ontologies, a set of queries has been used to simulate subscriptions. For the LUBM and UOB benchmarks, a set of

12 Pellet Project Homepage: http://pellet.owldl.com/.
13 VICODI project homepage: http://www.vicodi.org/.
14 SEMINTEC project homepage: http://www.cs.put.poznan.pl/alawrynowicz/
semintec.htm.
15 LUBM project homepage: http://swat.cse.lehigh.edu/projects/lubm/.
16 Developed by IBM and is available through their Integrated Ontology Development Toolkit: http://www.alphaworks.ibm.com/tech/semanticstk.
17 Because the approach is applicable to SHI, functional role assertions have been
removed from the SEMINTEC and UOB ontologies.
18 We would like to acknowledge Boris Motik for his creation of the larger VICODI
and SEMITEC datasets (described in [36]).

sample queries accompanies the benchmarks and a representative
subset of these queries are used. In contrast, the VICODI and SEMINTEC ontologies do not have such a query suite, however, there
has been recent work in literature [36] in which a set of queries
has been obtained from the authors of these ontologies. There-
fore, these queries have been used, as they will provide insights
into response times for expected queries over these datasets. In
the experiments, varying sized ABox additions and deletions were
randomly selected from each dataset. Update sizes include 1, 5, 15,
25, and 50 assertions. These sizes were selected as they align with
publication sizes expected in realistic syndication systems. In the
experiments, the initial set of answers for the query was first deter-
mined, and then the randomly selected assertions were added (or
removed) to the KB and the query results were updated. The aim
behind the evaluation was to simulate new publications arriving at
the syndication broker.

Two versions of the OWL DL reasoner Pellet have been used in
the evaluation; a regular version of the reasoner and a version that
has been extended with the algorithm to reduce the candidate indi-
viduals. The DL reasoner RacerPro19 was also used in the evaluation.
Similar to Pellet, RacerPro is a highly optimized tableau-based DL
reasoner, however RacerPro is sound and complete for the DLSHIQ.
Lastly, the KAON220 OWL reasoner was also used in the evaluation;
similar to RacerPro, KAON2 supports reasoning for the DL SHIQ.
Interestingly, KAON2 is not a tableau-based reasoner, but rather it
reduces OWL KBs to disjunctive datalog and is highly optimized
for ABox reasoning [36]. Our aim in using KAON2 in evaluation
was to gain insights into tableau-based algorithms for syndication
purposes when compared to other possible approaches.

The experiments were run on a Linux machine with 2 GB of
RAM and a 3.06 GHz Intel Xeon CPU. Pellet v1.5, RacerPro v1.9.0,
and KAON2 release 2007-09-07 were used and all results were averaged over 75 iterations. Note that in all of the figures showing the
query response time results, the X-axis corresponds to the update
size, and the Y-axis is the response time in milliseconds for query
answering (the scale is logarithmic). Additionally, the response
times shown only include the time to determine the query results;
the response times do not include consistency checking times or
query preparation times performed by Pellet (this is not utilized
in the incremental version of Pellet) and RacerPro. The response
times for the incremental query algorithm developed in this paper
include both the time to find the candidates and execute the query
over the candidate set (i.e., Algorithm 2). In the experiments a maximum response time of 100 s was imposed in the tests, as any time
greater than this will clearly not scale for high-demand syndication
purposes.

As discussed earlier, the summary completion graph must be
built so that the technique can be used. The total time to construct the initial summary completion graph is of interest, as well as
the potential memory overhead imposed by the structure. Table 3
shows the total time (in seconds) to construct the initial summary completion for each of the different datasets, as well as
the memory overhead. As expected, the initial construction of the
summary completion graphs introduces overhead. However, the
process must only be performed once at startup. It is also clear
that there is a memory impact of the summary completion graph.
However, in the implementation of the algorithm, the overlap of
between the summary completion graph and the cached completion graph corresponding to a model of the KB is not exploited.
Todays tableau-based reasoners typically cache the completion

19 RacerPro is commercially supported by Racer Systems GmbH & Co. KG:
http://www.racer-systems.com/index.phtml.
20 KAON2 project homepage: http://kaon2.semanticweb.org/.

Fig. 2. VICODI results: (a) query 1-additions; (b) query 1-deletions; (c) query 2-additions; (d) query 2-deletions; (e) query 3-additions; (f) query 3-deletions.

graph constructed during the initial consistency check, as it is
used in optimizations for other reasoning tasks (e.g., classification).
A substantial portion of the structure in the cached completion
graph will overlap with the summary completion graph. Therefore,
with further engineering, this memory overhead can be potentially
reduced.

For the VOCODI ontology, the following queries have been used:

(1) (x)  Individual(x),
(2) (x, y, z)  Military  Person(x)  hasRole(y, x)  related(x, z),
(3) (x, y)  Military  Person(x)  hasRole(y, x).

Queries 1 and 2 have been used in previous literature [36] to
evaluate DL query answering and were suggested by the ontology authors. Query 3 has been included as it provides additional
insights into the approach (discussed later). The response times for
the queries are shown in Fig. 2 (note that Inc-Pellet corresponds to
Algorithm 2). Let us consider query 1; the query answering times
for the regular version of Pellet is between 1.4 and 3.08 s, depending
on the dataset size. The response time remains comparable through
update sizes, as the query is re-performed from scratch and the

update sizes are small relative to the overall KB size. Response time
for RacerPro exhibits similar properties as Pellet. The incremental
query answering approach demonstrates substantial performance
improvements over both the regular version of Pellet and RacerPro.
For both datasets, approximately 1.53 orders of magnitude performance improvements over Pellet are exhibited, and the response
time is in the 10s of milliseconds (in many cases less than 10 ms).
This is clearly due to the reduction in the portion of the KB that
must be considered for the query after the update. Table 4 presents
the actual number of candidates for addition updates using the
approach for the different update sizes and queries (the results for
deletions are very similar and are therefore omitted). This table also
shows the percentage of the KB that this candidate set represents
(shown in parenthesis). For the first query, the technique provides
a dramatic reduction in the portion of the KB that must be considered (always below 1% of the original KB). It can also be seen in
Fig. 2 that as the update size is increased, the performance of the
approach scales well. Deletions take slightly longer than additions
because the deleted assertions must be retracted from the summary
completion graph, whereas this process is not necessary for addi-
tions. KAON2 outperforms the regular tableau-based reasoners (i.e.,

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Table 4
Addition update candidate sizes for different queries and update sizes for each test-suite ontology

KB/query

VIC-1/1
VIC-2/1
VIC-1/2
VIC-2/2
VIC-1/3
VIC-2/3
SEM-1/1
SEM-2/1
SEM-1/2
SEM-2/2
LUBM-1/1
LUBM-2/1
LUBM-1/2
LUBM-2/2
LUBM-1/3
LUBM-2/3
UOB-1/1
UOB-2/1
UOB-1/2
UOB-2/2
UOB-1/3
UOB-2/3

1 (%)

1.6 (0.009)
1.7 (0.01)
7128 (42)
8375 (24)
1.9 (0.01)
1.9 (0.005)
1.5 (0.008)
1.55 (0.004)
6.2 (0.03)
22 (0.06)
0 (0)
9.97 (0.02)
1.4 (0.008)
2.8 (0.007)
14.6 (0.09)
2 (0.005)
1.9 (0.007)
1.5 (0.002)
3.2 (0.01)
3.2 (0.006)
0.9 (0.003)
0.04 (0.0005)

5 (%)

7.02 (0.04)
7.4 (0.02)
11,440 (67)
19,358 (57)
7.16 (0.04)
7.2 (0.02)
7.2 (0.04)
7.5 (0.02)
27.2 (0.15)
27.8 (0.07)
0 (0)
17 (0.04)
7.3 (0.04)
28.5 (0.07)
50.2 (0.3)
7.2 (0.01)
29 (0.11)
21.7 (0.04)
17.6 (0.06)
12.3 (0.02)
1.1 (0.004)
0.3 (0.0006)

15 (%)

20.7 (0.12)
21 (0.06)
11,553 (68)
22,958 (67)
21.6 (0.12)
21.5 (0.06)
21.8 (0.12)
22.1 (0.06)
95.4 (0.53)
86.6 (0.24)
0 (0)
17 (0.04)
22.3 (0.13)
80 (0.2)
138.8 (0.8)
19.5 (0.05)
66.8 (0.26)
58.8 (0.1)
52.3 (0.2)
55.1 (0.1)
35.1 (0.13)
10.5 (0.02)

25 (%)

34.4 (0.2)
35.4 (0.1)
11,560 (68)
23,062 (68)
34.9 (0.2)
35.3 (0.1)
36.8 (0.2)
37 (0.1)
147.6 (0.82)
157 (0.43)
0 (0)
17 (0.04)
36.7 (0.2)
133.8 (0.3)
216 (1.3)
32.7 (0.08)
111.4 (0.4)
98.3 (0.1)
85.7 (0.33)
87.1 (0.16)
25 (0.09)
20.8 (0.04)

50 (%)

65.6 (0.38)
67.6 (0.19)
11,536 (68)
23,126 (68)
72.8 (0.43)
71.6 (0.2)
71.8 (0.4)
73.4 (0.2)
206.6 (1.1)
243 (0.67)
0 (0)
17 (0.04)
74.1 (0.45)
263.2 (0.7)
404 (2.4)
67.6 (0.18)
238.3 (0.9)
201.6 (0.3)
174.3 (0.6)
157.1 (0.3)
80 (0.3)
46.4 (0.08)

Each column shows the total number of candidates for the specific update size (indicated in the top column) and the percentage of the total KB that this constitutes (shown
in parenthesis).

Pellet and RacerPro) and exhibits comparable results through the
update sizes. However, the incremental version of Pellet performs
better than KAON2 in this experiment.

Fig. 2 also presents the results for queries 2 and 3. In the second query, the query answering times for the regular version of
Pellet and RacerPro exhibit similar characteristic as for the first
query. Further, KAON2 again exhibits comparable response times
through updates sizes and in general performs better than in query
1. However, the incremental approach does not provide as dramatic
performance improvements as it did in the first query. This can be
explained by inspecting Table 4; in particular, it can be seen that a
large portion of the knowledge base is considered after each update,
and therefore, it is like re-running the query from scratch.

If we inspect the results of query 3, additional insights into the
results for the second query are provided. Query 3 is actually a subset of the second query, in which the last role atom from query 2
is excluded. Once again, the incremental algorithm exhibits dramatic performance improvements, as the candidates considered
are a small subset of the original KB. This sheds light on the previous query, as there are individuals in the KB that are related to
an extremely large portion of the KB by the related role. Therefore,
when taking into account the query impact, almost all of the knowledge base is included as a candidate. As we will see in the remainder
of the evaluation, this was the only query for any of the ontologies
that demonstrated this behavior, indicating the approach should be
effective in general. Lastly, it is noted that the average incremental
consistency checking times for this ontology can be found in Chapter 5 of [26]. A detailed presentation of these results is omitted
here, however on average it was always below 10 ms for addition
updates and just over 10 ms for deletions. Clearly, this indicates the
practicality of the approach for syndication purposes.

The queries used for the SEMINTEC datasets are provided below.
Similar to the VICODI queries, the SEMINTEC queries have been
used previously in literature to evaluate DL query answering and
were suggested by the ontology authors:

(1) (x)  Man(x),
(2) (x, y, z)  Man(x)  Gold(y, x)  Region(z) 

isCreditCardOf (y, x)  livesIn(x, z).

Fig. 3 presents the response times for both queries. Pellet, Rac-
erPro, and KAON2 exhibit similar characteristics as for the VICODI
ontologies. Further, the algorithm developed in this paper again
demonstrates dramatic performance improvements for all update
sizes. In many cases, the results are below 10 ms. Table 4 presents
the actual number of candidates considered on average for addition updates. Again, there is a dramatic reduction in the portion of
the KB that must be considered for the query after the updates. The
average incremental consistency checking times for this ontology
can be found in Chapter 5 of [26]; similar to VICODI, they were on
average below 10 ms for addition and deletion updates.

As discussed, the LUBM benchmark includes a set of 14 queries
for performance analysis of DL systems. The results of the following
three queries21 are presented.
(1) (x, y, z)  GraduateStudent(x)  University(y) 

Department(z)  memberOf (x, z)  subOrganization(z, y) 
undergraduateDegreeFrom(x, y),

(2) (x)  Student(x),
(3) (x, y, z)  Student(x)  Faculty(y)  Course(z)  advisor(x, z) 

takesCourse(x, z)  teacherOf (y, z).

This subset was selected because the results for the remaining
queries are similar. Fig. 4 presents the results for the queries. The
incremental algorithm again demonstrates dramatic performance
improvements, as the portion of the KB that must be considered
after the update is very small (shown in Table 4). In the queries, the
approach typically exhibits response times in the 10s of milliseconds and in all cases shows orders of magnitude improvements over
the regular version of Pellet and RacerPro. Further, the approach
outperforms KAON2 in many cases. An interesting observation can
be made from the number of candidates under additions presented
in Table 4. In particular, for query 1 for the smaller of the LUBM
datasets, there are 0 candidates. In this case, there are no answers
for the query in the entire KB, and when performing the syntactic
mapping to take into account the query impact, the initial candi-

21 Note that these correspond to LUBM queries 2, 6, and 9 respectively.

Fig. 3. SEMINTEC results: (a) query 1-additions; (b) query 1-deletions; (c) query 2-additions; (d) query 2-deletions.

dates can never be mapped into the cached completion graph. In
the case of the larger dataset, there are only a few mappings in the
entire KB, which are located during the approach. Another interesting observation can be made regarding query 3; specifically, the
number of candidates is actually smaller on average in the experiments involving the larger of the two datasets. This can be explained
by the fact that in the larger dataset there is a larger number of individuals which do not participate in a concept guide path (for the
negated, rolled-up query concept) with some other individual, and
there is a larger number of individuals that are not mappable into
the query (under query impact). Thus, when the random updates
are selected, there is a greater chance that these individuals will be
selected. The average incremental consistency checking times for
addition updates in this ontology were below 10 ms on average and
in the 10s of milliseconds for deletions (again, the interested reader
is referred to Chapter 5 of [26]).

As with the case for LUBM, the UOB benchmark provides a suite

of queries, and the following queries22 have been used:
(1) (x)  UndergraduateStudent(x)  takesCourse(x, Course0)
(2) (x)  Employee(x)
(3) (x, y)  Publication(x)  Faculty(y) 

isMemberOf (y, University0)  publicationAuthor(x, y)

These queries were selected as the results for the other queries
are similar. Fig. 5 presents the results for the queries. The response
times of all reasoners were comparable in query 1. The technique
presented in this paper resulted in a dramatic reduction in the candidates considered (shown for addition updates in Table 4), and the
incremental version of Pellet outperformed re-running the query
from scratch. In queries 2 and 3, the developed approach substantially outperforms the other reasoners, demonstrating orders

22 Note that these correspond to UOB queries 1, 2, and 4, respectively.

of magnitude performance improvement and generally response
times in the 10s of milliseconds. Lastly, the average incremental
consistency checking times for this ontology were below 20 ms
(see Chapter 5 of [26]) for addition updates and below 60 ms for
deletion updates.

7. Related work

Early syndication systems primarily relied on subject-based
keywords in order to match user interests with published docu-
ments/data [37,49]. Following this, approaches allowed attributevalue pairs to be associated with published content (e.g., [1,15]).
Recently, there has been interest in utilizing XML-based approaches
(e.g., see [3,7,21,11]), in which published documents/data are represented in XML and subscription requests are specified using an XML
query/path language (e.g., XPath [7]). There has also been interest in
using formal knowledge representation languages for representing
published contents (e.g., [9,40,46]); typically such approaches have
used RDF as the formalism for encoding publications (e.g., [40,46]).
In such an approach, RDF graph-based query languages (typically
triple patterns) are used to represent subscription requests and
matching publications with subscriptions reduces to triple pattern
matching. Additionally, in many approaches RDFS is also utilized
to describe domain ontologies which the published RDF content
adheres to, allowing simple semantic inferences to be made. There
has also been work on addressing the scalability of all of the previously mentioned approaches by leveraging distributed syndication
architectures [5,49,11,50,8].

Recently, there has been interest in leveraging DL reasoning in
applications which are similar to syndication systems. In [45,34],
a DL-based approach for Web service matching is presented, in
which DL concepts are used to represent both service subscription
requests as well as published documents/data. Matching reduces
to determining if published concepts and subscriptions are logically equivalent, subsume one another, or are not compatible.

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Fig. 4. LUBM: (a) query 1-additions; (b) query 1-deletions; (c) query 2-additions; (d) query 2-deletions; (e) query 3-additions; (f) query 3-deletions.

While empirical results demonstrate accepted performance times
(20 ms) for matching new subscription requests with a fixed set
of published documents, processing a large amount of incoming
published content is still problematic (10s of seconds) [45,34].
Refs. [22,23] present an agent-based document retrieval system
(which is essentially a publish/subscribe application) in which
published contents are represented as ABox assertions and subscription requests as a DL concept (viewed as an instance retrieval
query). Therefore, matching is reduced to instance retrieval of
the subscription concept. Given the performance issues of using
such an approach (i.e., response times in the 10s of seconds), the
authors introduce two optimizations for more effective incremental
instance retrieval (discussed later).

In general our work is based a more expressive mechanism for
representing published contents. This allows the use of automated
reasoning procedures to infer matches not found using syntactic approaches (keyword, attribute-value pair, and/or XML) and
simpler semantic approaches (e.g., RDF/S). Further, the related DLbased techniques either take a different approach for representing
published contents and subscriptions requests (e.g., [45,34]) or

assume a simpler subscription format with only atomic concepts
(e.g., [22,23]).

While there has been substantial work on optimizing reasoning
services for description logics, the topic of reasoning through evolving DL knowledge bases remains relatively unaddressed. There
are a few notable exceptions; [23] presents two approaches for
optimizing query answering, namely inducing a partial ordering
upon all queries (assumed to be concept retrieval queries) and
disregarding previous individuals that satisfied queries. This is
directly related to the techniques presented in this paper, how-
ever, we present novel techniques to prune the individuals in
the KB that must be considered for queries after updates are
developed. Further, our approach supports conjunctive queries.
There has also been recent work on optimizing classification of
DL KBs in the presence of arbitrary TBox changes [20,39]. While
related, the work presented here proposes different techniques
and addresses different reasoning services, as they are required
for the syndication framework. We also point out that there has
been substantial work on incremental query and view maintenance
in databases (e.g., [6,41,42]) and rule-based systems (e.g., Data-

Fig. 5. OUB: (a) query 1-additions; (b) query 1-deletions; (c) query 2-additions; (d) query 2-deletions; (e) query 3-additions; (f) query 3-deletions.

log [12,13]). While related, our work addresses a more expressive
formalism.

There has additionally been extensive work in Truth Maintenance Systems (TMSs) for logical theories (e.g., [10,14]). When
comparing the contributions of this paper with TMSs, it can be
seen that there are some similarities. However, the approaches
presented in this paper address optimizing the detection of nonentailments in the presence of updates, which is not usually
addressed in TMSs. Further, TMSs traditionally only support propositional logic, however, in this paper a more expressive formalism
is supported.

8. Conclusion

In this paper, we have formalized an OWL-based syndication
framework in which DL reasoning is the means for matching
newly published information with subscription requests. We then
addressed one of the main limitations with such a syndication
framework, namely efficiently matching new information with
registered subscriptions. To this end, we presented a novel algorithm for incremental query answering. Our preliminary results

demonstrate orders of magnitude performance improvements over
state-of-the-art DL reasoners. Further, the technique provides a
matching mechanism that can scale to hundreds and in some
cases thousands of subscriptions under high-demand publish frequencies similar to that encountered in the financial domain (e.g.,
the Dow Jones Newswires has up to 12,000 publications per day
8.3/min23). Future work includes investigating extensions to the
framework such as supporting TBox updates in publications, as
well as developing additional optimizations for the reasoning techniques required for the framework. The later includes extending
the techniques to larger portions of OWL (and beyond) and lifting
the restrictions currently imposed in the approach. Additionally,
we plan to investigate more advanced mechanisms to recover from
inconsistencies that can occur from new publications; currently,
we are working on developing trust-based revision techniques for
OWL DL KBs [19].

23 Source:
DJNewswires.htm.

http://www.dj.com/Products Services/ElectronicPublishing/

C. Halaschek-Wiener, V. Kolovski / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 171190

Acknowledgments

We would like to thank Jennifer Golbeck, Yarden Katz, Bijan Par-
sia, Evren Sirin, and Taowei Wang for all of their contributions to
this work. This work was supported by grants from Fujitsu, Lockheed Martin, NTT Corp., Kevric Corp., SAIC, the National Science
Foundation, the National Geospatial-Intelligence Agency, DARPA,
US Army Research Laboratory, and NIST.
