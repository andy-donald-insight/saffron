Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Managing uncertainty and vagueness in description logics
for the Semantic Web
Thomas Lukasiewicz a,1, Umberto Straccia b,

a Computing Laboratory, University of Oxford, Wolfson Building, Parks Road, Oxford OX1 3QD, UK
b ISTI-CNR, Via G. Moruzzi 1, I-56124 Pisa, Italy

a r t i c l e

i n f o

a b s t r a c t

Ontologies play a crucial role in the development of the Semantic Web as a means for defining shared
terms in web resources. They are formulated in web ontology languages, which are based on expressive description logics. Significant research efforts in the semantic web community are recently directed
towards representing and reasoning with uncertainty and vagueness in ontologies for the Semantic Web.
In this paper, we give an overview of approaches in this context to managing probabilistic uncertainty,
possibilistic uncertainty, and vagueness in expressive description logics for the Semantic Web.

 2008 Elsevier B.V. All rights reserved.

Article history:
Received 25 October 2006
Received in revised form
22 November 2007
Accepted 20 April 2008
Available online 9 June 2008

Keywords:
Description logics
Web ontology languages
Ontologies
Probabilistic uncertainty
Possibilistic uncertainty
Vagueness
Imprecision
Semantic Web

1. Introduction

The Semantic Web [6,7,33,56] has recently attracted much atten-
tion, both from academia and industry, and is widely regarded as
the next step in the evolution of the World Wide Web. It aims at an
extension of the current Web by standards and technologies that
help machines to understand the information on the Web so that
they can support richer discovery, data integration, navigation,
and automation of tasks. The main ideas behind it are to add a
machine-understandable meaning to web pages, to use ontologies for a precise definition of shared terms in web resources, to
use KR technology for automated reasoning from web resources,
and to apply cooperative agent technology for processing the
information of the Web.

The Semantic Web consists of several hierarchical layers, where
the Ontology layer, in form of the OWL Web Ontology Language
[56,150] (recommended by the W3C), is currently the highest layer

 Corresponding author.
E-mail addresses: thomas.lukasiewicz@comlab.ox.ac.uk,

lukasiewicz@kr.tuwien.ac.at (T. Lukasiewicz), straccia@isti.cnr.it (U. Straccia).
1 Alternative address: Institut f  ur Informationssysteme, Technische Universit at
Wien, Favoritenstrae 9-11, A-1040 Wien, Austria.

1570-8268/$  see front matter  2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2008.04.001

of sufficient maturity. OWL consists of three increasingly expressive sublanguages, namely, OWL Lite, OWL DL, and OWL Full. Hence,
ontologies [37] (see especially [116] for an introduction to ontologies including a detailed historical account) play a key role in the
Semantic Web, and a major effort has been put by the Semantic
Web community into this issue. Informally, an ontology consists of
a hierarchical description of important and precisely defined concepts in a particular domain, along with the description of the
properties (of the instances) of each concept. Web content is then
annotated by relying on the concepts defined in a specific domain
ontology.

OWL Lite and OWL DL are essentially very expressive description logics [3] with an RDF syntax [56]. More specifically, ontology
entailment in OWL Lite and OWL DL reduces to knowledge base
(un)satisfiability in the expressive description logics SHIF(D) and
SHOIN(D) [55,57], respectively. Hence, these expressive description logics play an important role in the Semantic Web, since
they are essentially the theoretical counterparts of OWL Lite and
OWL DL, respectively. More generally, description logics are a
logical reconstruction of frame-based knowledge representation
languages, with the aim of providing a decidable first-order formalism with a simple well-established declarative semantics to
capture the meaning of the most popular features of structured
representation of knowledge.

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

However, classical ontology languages and description logics
are less suitable in all those domains where the information to
be represented comes along with (quantitative) uncertainty and/or
vagueness (or imprecision). For example, uncertain information may
be of the form John is a teacher with the degree of certainty 0.3 and
a student with the degree of certainty 0.7 (roughly, John is either a
teacher or a student, but more likely a student), while vague information may be of the form John is tall with the degree of truth 0.9
(roughly, John is quite tall); see Section 2. Formalisms for dealing
with uncertainty and vagueness have started to play an important
role in research related to the Web and the Semantic Web. For exam-
ple, the order in which Google returns the answers to a web search
query is computed by using probabilistic techniques. Furthermore,
formalisms for dealing with uncertainty and vagueness in ontologies have been successfully applied in ontology matching, data
integration, and information retrieval. Vagueness and imprecision
also abound in multimedia information processing and retrieval. In
addition, handling vagueness is an important aspect of natural language interfaces to the Web. There exists a W3C Incubator Group
on Uncertainty Reasoning for the World Wide Web, and an important
recent forum for approaches to uncertainty in the Semantic Web is
the annual Workshop on Uncertainty Reasoning for the Semantic Web
(URSW) at the International Semantic Web Conference (ISWC).

The rising popularity of description logics and their use, and
the need to deal with uncertainty and vagueness, both especially
in the Semantic Web, is increasingly attracting the attention of
many researchers and practitioners towards description logics able
to cope with uncertainty and vagueness. The goal of this paper is to
provide an overview of the current state of the art about the management of uncertainty and vagueness in description logics for the
Semantic Web, which should help the reader to get insights on main
features of the formalisms proposed in the literature.

The rest of this paper is organized as follows. In Section 2, we give
a brief introduction to uncertainty and vagueness at the propositional level. In Section 3, we describe the classical description logic
SHOIN(D), which is the reference language in this paper. Sections
4 and 5 show how to extend classical description logics by probabilistic and possibilistic uncertainty, respectively, while Section 6
describes how to extend classical description logics for the management of vague/imprecise knowledge. In Section 7, we give a
summary and an outlook on open research.

we are uncertain about which world to consider as the right one,
and thus we speak about, e.g. a probability distribution or a possibility distribution over the worlds. For example, we cannot exactly
establish whether it will rain tomorrow or not, due to our incomplete knowledge about our world, but we can estimate to which
degree this is probable, possible, and necessary.

As for the main differences between probability and possibility
theory, the probability of an event is the sum of the probabilities
of all worlds that satisfy this event, whereas the possibility of an
event is the maximum of the possibilities of all worlds that satisfy
the event. Intuitively, the probability of an event aggregates the
probabilities of all worlds that satisfy this event, whereas the possibility of an event is simply the possibility of the most optimistic
world that satisfies the event. Hence, although both probability and
possibility theory allow for quantifying degrees of uncertainty, they
are conceptually quite different from each other. That is, probability
and possibility theory represent different facets of uncertainty.

On the other hand, under vagueness/fuzziness theory fall all those
approaches in which statements (for example, the tomato is ripe)
are true to some degree, which is taken from a truth space. That is,
an interpretation maps a statement to a truth degree, since we are
unable to establish whether a statement is completely true or false
due to the involvement of vague concepts, such as ripe, which
only have an imprecise definition. For example, we cannot exactly
say whether a tomato is ripe or not, but rather can only say that
the tomato is ripe to some degree. Usually, such statements involve
so-called vague/fuzzy predicates [155].

Note that all vague/fuzzy statements are truth-functional, that
is, the degree of truth of every statement can be calculated from
the degrees of truth of its constituents, while uncertain statements
cannot be a function of the uncertainties of their constituents
[25]. More concretely, in probability theory, only the negation is
truth-functional (see Eq. (1)), while in possibility theory, only the
disjunction respectively conjunction is truth-functional in possibilities respectively necessities of events (see Eq. (2)). Furthermore,
fuzzy logics are based on truly many-valued logical operators, while
uncertainty logics are defined on top of standard binary logical
operators.

In the following, we illustrate a typical formalization of uncertain statements and vague statements. In the former case, we
consider a basic probabilistic/possibilistic logic, while in the latter,
we consider a basic many-valued logic.

2. Uncertainty and vagueness

2.1. Probabilistic logic

There has been a long-lasting misunderstanding in the literature of artificial intelligence and uncertainty modeling, regarding
the role of probability/possibility theory and vague/fuzzy theory.
A clarifying paper is [26]. We recall here salient notes, which may
clarify the role of these theories for the inexpert reader.

A standard example that points out the difference between
degrees of uncertainty and degrees of truth is that of a bottle [26]. In
terms of binary truth values, a bottle is viewed as full or empty. But
if one accounts for the quantity of liquid in the bottle, one may, e.g.
say that the bottle is half-full. Under this way of speaking, full
becomes a fuzzy predicate [155] and the degree of truth of the bottle is full reflects the amount of liquid in the bottle. The situation is
quite different when expressing our ignorance about whether the
bottle is either full or empty (given that we know that only one of
the two situations is the true one). Saying that the probability that
the bottle is full is 0.5 does not mean that the bottle is half full.

We recall that under uncertainty theory fall all those approaches
in which statements rather than being either true or false, are true
or false to some probability or possibility (for example, it will rain
tomorrow). That is, a statement is true or false in any world, but

Probabilistic logic has its origin in philosophy and logic. Its roots
can be traced back to Boole in 1854 [11]. There is a wide spectrum
of formal languages that have been explored in probabilistic logic,
ranging from constraints for unconditional and conditional events
to rich languages that specify linear inequalities over events (see
especially the work by Nilsson [100], Fagin et al. [32], Dubois and
Prade et al. [23,28,2,27], Frisch and Haddawy [34], and Lukasiewicz
[83,84,86]; see also the survey on sentential probability logic by
Hailperin [41]). Recently, nonmonotonic generalizations of probabilistic logic have been developed and explored; see especially [88]
for an overview. In this section, for illustrative purposes, we recall
only the simple probabilistic logic described in [100].
We first define probabilistic formulas and probabilistic knowledge bases. We assume a set of basic events 	 = {p1, . . . , pn} with
n  1. We use  and  to denote false and true, respectively. We
define events by induction as follows. Every element of 	  {,}
is an event. If  and   are events, then also , (   ), (   ),
and (   ) are events. We adopt the usual conventions to eliminate parentheses. A probabilistic formula is an expression of the
form   l, where  is an event, and l is a real number from the

unit interval [0, 1]. Informally,   l says that  is true with a probability of at least l. For example, rain tomorrow  0.7 may express
that it will rain tomorrow with a probability of at least 0.7. Notice
also that   1  u encodes that  is true with a probability of at
most u. A probabilistic knowledge base K is a finite set of probabilistic
formulas.

Next, we define worlds and probabilistic interpretations. A world
I associates with every basic event in 	 a binary truth value. We
extend I by induction to all events as usual. We denote by I	 the
(finite) set of all worlds for 	. A world I satisfies an event , or I
is a model of , denoted I  , iff I() = true. A probabilistic interpretation Pr is a probability function on I	 (that is, a mapping
Pr : I	  [0, 1] such that all Pr(I) with I I	 sum up to 1). Intu-
itively, Pr(I) is the degree to which the world I I	 is probable,
that is, the probability function Pr encodes our uncertainty about
which world is the right one. The probability of an event  in Pr,
denoted Pr(), is the sum of all Pr(I) such that I I	 and I  .
The following theorem is an immediate consequence of the above
definitions.

For all probabilistic interpretations Pr and events 

Theorem 2.1.
and  , the following relationships hold:
Pr(   ) = Pr() + Pr( )  Pr(   );
Pr(   )  min(Pr(), Pr( ));
Pr(   )  max(0, Pr() + Pr( )  1);
Pr(   ) = Pr() + Pr( )  Pr(   );
Pr(   )  min(1, Pr() + Pr( )) ;
Pr(   )  max(Pr(), Pr( )) ;
Pr() = 1  Pr();
Pr() = 0;
Pr() = 1.

(1)

A probabilistic interpretation Pr satisfies a probabilistic formula
  l, or Pr is a model of   l, denoted Pr    l, iff Pr()  l. We
say Pr satisfies a probabilistic knowledge base K, or Pr is a model
of K, iff Pr satisfies all F K. We say K is satisfiable iff a model of K
exists. A probabilistic formula F is a logical consequence ofK, denoted
K  F, iff every model of K satisfies F. We say   l is a tight logical
consequence of K iff l is the infimum of Pr() subject to all models
Pr of K. Notice that the latter is equivalent to l = sup{r |K    r}.
The main decision and optimization problems in probabilistic
logic are deciding the satisfiability of probabilistic knowledge bases
and logical consequences from probabilistic knowledge bases, as
well as computing tight logical consequences from probabilistic
knowledge bases, which can be done by deciding the solvability of
a system of linear inequalities and by solving a linear optimization
problem, respectively. In particular, column generation techniques
from operations research have been successfully used to solve large
problem instances in probabilistic logic; see especially the work by
Jaumard et al. [63] and Hansen et al. [46].

2.2. Possibilistic logic

We next recall possibilistic logic; see especially [21]. The main
syntactic and semantic differences to probabilistic logic can be
summarized as follows. Syntactically, rather than using probabilistic formulas to constrain the probabilities of propositional events,
we now use possibilistic formulas to constrain the necessities and
possibilities of propositional events. Semantically, rather than having probability distributions on worlds, each of which associates
with every event a unique probability, we now have possibility distributions on worlds, each of which associates with every event
a unique possibility and a unique necessity. Differently from the
probability of an event, which is the sum of the probabilities of all
worlds that satisfy that event, the possibility of an event is the max-

imum of the possibilities of all worlds that satisfy the event. As a
consequence, probabilities and possibilities of events behave quite
differently from each other (see Eqs. (1) and (2)). These fundamental semantic differences between probabilities and possibilities can
also be used as the main criteria for using either probabilistic logic
or possibilistic logic in a given application involving uncertainty.
In addition, possibilistic logic may especially be used for encoding
user preferences, since possibility measures can actually be viewed
as rankings (on worlds or also objects) along an ordinal scale.

The semantic differences between probabilities and possibilities
are also reflected in the computational properties of possibilistic
and probabilistic logic, since reasoning in probabilistic logic generally requires to solve linear optimization problems, while reasoning
in possibilistic logic does not, and thus can generally be done with
less computational effort. Note that although possibility measures
can be viewed as sets of upper probability measures [24], and possibility and probability measures can be translated into each other
[20], no translations are known between possibilistic and probabilistic knowledge bases as described here.
We first define possibilistic formulas and knowledge bases. Possibilistic formulas have the form P   l or N   l, where  is an
event, and l is a real number from [0, 1]. Informally, such formulas encode to what extent  is possibly respectively necessarily true.
For example, P rain tomorrow  0.7 encodes that it will rain tomorrow is possible to degree 0.7, while N father  man  1 says that
a father is necessarily a man. A possibilistic knowledge base K is a
finite set of possibilistic formulas.
A possibilistic interpretation is a mapping  : I	  [0, 1]. Intu-
itively, (I) is the degree to which the world I is possible. In
particular, every world I such that (I) = 0 is impossible, while every
world I such that (I) = 1 is totally possible. We say  is normalized iff
(I) = 1 for some I I	. Intuitively, this guarantees that there exists
at least one world, which could be considered as the real one. The
possibility of an event  in a possibilistic interpretation , denoted
Poss(), is then defined by Poss() = max{(I)| I I	, I  } (where
max  = 0). Intuitively, the possibility of  is evaluated in the most
possible world where  is true. The dual notion to the possibility of
an event  is the necessity of , denoted Nec(), which is defined by
Nec() = 1  Poss(). It reflects the lack of possibility of , that
is, Nec() evaluates to what extent  is certainly true. The following
theorem follows immediately from the above definitions.

Theorem 2.2. For all possibilistic interpretations  and events  and
 , the following relationships hold:

Poss(   )  min(Poss(), Poss( ));
Poss(   ) = max(Poss(), Poss( ));
Poss() = 1  Nec();
Poss() = 0;
Poss() = 1 (in the normalized case);
Nec(   ) = min(Nec(), Nec( ));
Nec(   )  max(Nec(), Nec( ));
Nec() = 1  Poss();
Nec() = 0 (in the normalized case);
Nec() = 1.

(2)

A possibilistic interpretation  satisfies a possibilistic formula
P   l (respectively, N   l), or  is a model of P   l (respec-
tively, N   l), denoted   P   l (respectively,   N   l), iff
Poss()  l (respectively, Nec()  l). The notions of satisfiability,
logical consequence, and tight logical consequence for possibilistic
knowledge bases are then defined as usual (in the same way as in
the probabilistic case). We refer the reader to [21,53] for algorithms
for possibilistic logic.

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

Table 1
Properties for t-norms and s-norms

Table 3
Combination functions of various fuzzy logics

Axiom name

Tautology/contradiction
Identity
Commutativity
Associativity
Monotonicity

T-norm
a  0 = 0
a  1 = a
a  b = b  a
(a  b)  c = a  (b  c)
if b  c, then a  b  a  c

S-norm
a  1 = 1
a  0 = a
a  b = b  a
(a  b)  c = a  (b  c)
if b  c, then a  b  a  c

ukasiewicz logic

G  odel logic
a  b max(a + b  1, 0) min(a, b)
a  b min(a + b, 1)
max(a, b)
a  b min(1  a + b, 1)

1  a

1 if a  b
b otherwise
1 if a = 0
0 otherwise

Product logic
a  b
a + b  a  b

min(1, b/a)
1 if a = 0
0 otherwise

Zadeh logic

min(a, b)
max(a, b)
max(1  a, b)
1  a

Table 2
Properties for implication and negation functions

Table 4
Some additional properties of combination functions of various fuzzy logics

Implication function

Axiom name
Tautology/contradiction 0  b = 1, a  1 = 1, 1  0 = 0  0 = 1,1 = 0
Antitonicity
Monotonicity

if a  b, then a  c  b  c
if b  c, then a  b  a  c

Negation function

if a  b, then a   b

2.3. Many-valued logics

In the setting of many-valued logics, the convention prescribing that a proposition is either true or false is changed. A more
refined range is used for the function that represents the meaning
of a proposition. This is usual in natural language when words are
modeled by fuzzy sets. For example, the compatibility of tall in
the phrase a tall man with some individual of a given height is
often graded: the man can be judged not quite tall, somewhat tall,
rather tall, very tall, etc. Changing the usual true/false convention
leads to a new concept of proposition, whose compatibility with a
given state of facts is a matter of degree and can be measured on
an ordered scale S that is no longer {0, 1}, but, e.g. the unit interval
[0, 1]. This leads to identifying a fuzzy proposition  with a fuzzy
set of possible states of affairs; the degree of membership of a state
of affairs to this fuzzy set evaluates the degree of fit between the
proposition and the state of facts it refers to. This degree of fit is
called degree of truth of the proposition  in the interpretation I
(state of affairs). Many-valued logics provide compositional calculi
of degrees of truth, including degrees between true and false. A
sentence is now not true or false only, but may have a truth degree
taken from a truth space S, usually [0, 1] or { 0
} for an
n , 1
integer n  1. In the sequel, we assume S = [0, 1].
In the many-valued logic that we consider here, many-valued
formulas have the form   l or   u, where l, u [0, 1] [40,42],
which encode that the degree of truth of  is at least l respectively
at most u. For example, ripe tomato  0.9 says that we have a rather
ripe tomato (the degree of truth of ripe tomato is at least 0.9).
Semantically, a many-valued interpretation I maps each basic
proposition pi into [0, 1] and is then extended inductively to all
propositions as follows:
I(   ) = I()  I( );
I(   ) = I()  I( );
I(   ) = I()  I( );
I() = I(),
where , , , and  are so-called combination functions, namely,
triangular norms (or t-norms), triangular co-norms (or s-norms),
implication functions, and negation functions, respectively, which
extend the classical Boolean conjunction, disjunction, implication,
and negation, respectively, to the many-valued case.

n , . . . , n

(3)

Several t-norms, s-norms, implication functions, and negation functions have been given in the literature. An important
aspect of such functions is that they satisfy some properties that
one expects to hold for the connectives; see Tables 1 and 2.
Note that in Table 1, the two properties Tautology and Contradiction follow from Identity, Commutativity, and Monotonicity.

ukasiewicz logic G  odel logic Product logic Zadeh logic

Property
x   x = 0

x   x = 1

x  x = x

x  x = x

  x = x

x  y =  x  y

 (x  y) = x   y

 (x  y) =  x   y +
 (x  y) =  x   y +

Usually, the implication function  is defined as r-implication, that
is, a  b = sup{c|a  c  b}.

Some t-norms, s-norms, implication functions, and negation
functions of various fuzzy logics are shown in Table 3[42]. In
fuzzy logic, one usually distinguishes three different logics, namely,
ukasiewicz, G  odel, and Product logic; the popular Zadeh logic
is a sublogic of ukasiewicz logic. min(x, y) = x  (x  y) and
max(x, y) = (x  y)  y. Some salient properties of these logics are
shown in Table 4. For more properties, see especially [42,102].
The implication x  y = max(1  x, y) is called Kleene-Dienes
implication in the fuzzy logic literature. Note that we have the
following inferences: let a  n and a  b  m. Then, under KleeneDienes implication, we infer that if n > 1  m then b  m. Under
r-implication relative to a t-norm , we infer that b  n  m.

Note that implication functions and t-norms are also used to
define the degree of subsumption between fuzzy sets and the
composition of two (binary) fuzzy relations. A fuzzy set R over a
countable crisp set X is a function R : X  [0, 1]. The degree of subsumption between two fuzzy sets A and B, denoted A  B, is defined
as inf x  X A(x)  B(x), where  is an implication function. Note that
if A(x)  B(x), for all x  [0, 1], then A  B evaluates to 1. Of course,
A  B may evaluate to a value v  (0, 1) as well. A (binary) fuzzy
relation R over two countable crisp sets X and Y is a function R :
X  Y  [0, 1]. The inverse of R is the function R
1 : Y  X  [0, 1]
1(y, x) = R(x, y), for every x  X and
with membership function R
y Y. The composition of two fuzzy relations R1 : X  Y  [0, 1] and
R2 : Y  Z  [0, 1] is defined as (R1  R2)(x, z) = supy Y R1(x, y) 
R2(y, z). A fuzzy relation R is transitive iff R(x, z)  (R  R)(x, z).
A many-valued interpretation I satisfies a many-valued formula
  l (respectively,   u) or I is a model of   l (respectively,  
u), denoted I    l (respectively, I    u), iff I()  l (respec-
tively, I()  u). The notions of satisfiability, logical consequence,
and tight logical consequence for many-valued knowledge bases
are then defined in the standard way (in the same way as in the
probabilistic case). We refer the reader to [39,40,42] for algorithms
for many-valued logics.

3. Classical description logics

In this section, we recall the expressive description logic
SHOIN(D) [57], which stands behind the web ontology languages
OWL DL [55,56]. The purpose of this section is to make the

An ABox A is a finite set of concept membership axioms a : C,
role membership axioms (a, b) : R (respectively, (a, v) : T), equality
axioms a = b, and inequality axioms a /= b, where C is a concept,
R RA, T  RD, a, b I, and v is a data value. A knowledge base K =
(T,R,A) consists of a TBox T, an RBox R, and an ABox A.

3.2. Semantics

An interpretation I = (

,I) relative to a datatype theory D =
(D,D) consists of a nonempty abstract domain 
I, disjoint from
D, and an interpretation function I that assigns to each a I an
I, to each R RA a subset
element in 
I  D, and to every data
of 
value, datatype, and datatype predicate the same value as D. The
mapping I is extended to all roles and concepts as usual (where
I(x) = {y| (x, y) R
I}, and #X denotes the cardinality of the set X):

I, to each C  A a subset of 
I, to each T  RD a subset of 

I = {(y, x)| (x, y) R

I};

paper self-contained. It also helps in understanding the differences
between classical, probabilistic, possibilistic, and fuzzy SHOIN(D).
The reader confident with theSHOIN(D) terminology may skip this
section.

3.1. Syntax

The expressive description logic SHOIN(D) is a generalization
of SHOIN by datatypes, such as strings and integers, using concrete
domains [4,93,92]

A role is either an abstract role R RA, the inverse R

The elementary ingredients are as follows. We assume a set of
data values, a set of elementary datatypes, and a set of datatype pred-
icates, where each datatype predicate has a predefined arity n  1.
A datatype is an elementary datatype or a finite set of data values. A
datatype theory D = (D,D) consists of a datatype domain D and a
mappingD that assigns to each data value an element of D, to each
elementary datatype a subset of D, and to each datatype predicate
of arity n a relation over D of arity n. We extend D to all datatypes
by {v1, . . .}D = {vD
1 , . . .}. For example, over the integers, 20 may be
a unary predicate denoting the set of integers greater or equal to
20, and thus Person  age.20 may denote a person whose age is
at least 20. Let A, RA, RD, and I be pairwise disjoint sets of atomic
concepts, abstract roles, datatype roles, and individuals, respectively.
 of an abstract
role R RA, or a datatype role T  RD (note that datatype roles do not

have inverses). We use R
A to denote the set of all inverses of abstract
roles in RA.
An RBox R consists of a finite set of transitivity axioms
Trans(R), where R RA, and role inclusion axioms R  S, where either

R, S  RA  R
A or R, S  RD.
We next define the notion of a simple abstract role. For abstract
roles R RA, we define Inv(R) = R
) = R. Let R denote
 and Inv(R
the reflexive and transitive closure of  on
{{R  S,Inv(R) 

}. An abstract role S is simple rela-
Inv(S)}|R  S R, R, S  RA  R
tive to R iff for each abstract role R such that RR S, it holds that

(i) Trans(R) / R and (ii) Trans(Inv(R)) / R. Informally, an abstract
role S is simple iff it is neither transitive nor has transitive subroles.
Concepts are defined by induction as follows. Each A A is a con-
cept,  and  are concepts, and if a1, . . . , an  I, then {a1, . . . , an} is

a concept (called oneOf). If C, C1, C2 are concepts and R RA  R
A ,
then (C1  C2), (C1  C2), and C are concepts (called conjunction,
disjunction, and negation, respectively), as well as R.C, R.C,  n R,
and  n R (called exists, value, atleast, and atmost restriction, respec-
tively) for an integer n  0. If D is an n-ary datatype predicate
and T, T1, . . . , Tn  RD, then T1, . . . , Tn.D, T1, . . . , Tn.D,  n T, and
 n T are concepts (called datatype exists, value, atleast, and atmost
restriction, respectively) for an integer n  0. For example, we may
write the concept
Flower  hasPetalWidth.20 mm  hasPetalWidth.40 mm 
hasColor.Red

to denote the set of flowers having petals dimension within 20 mm
and 40 mm (where we assume that every flower has exactly one
associated petal width) whose color is red. Here, 20 mm and 40 mm
are datatype predicates. We eliminate (and add) parentheses as
usual, and we often use = 1R to abbreviate ( 1R)  ( 1R).
A TBox T is a finite set of concept inclusion axioms C  D, where
C and D are concepts. We often use C = D to abbreviate C  D and
D  C. An abstract role R is functional if the interpretation of the role
R (see below) is always functional. A functional role R can always be
obtained from an abstract role by means of the axiom   ( 1R).
Therefore, whenever we say that a role is functional, we implicitly
assume that   ( 1R) is in the TBox.

};

(R
I = 
I;
I = ;
{a1, . . . , an}I = {a

1, . . . , a
I = C

(C1  C2)

2;
I = C

(C1  C2)

2;
I = 

I \ C
(C)
I;
(R.C)
I = {x  
I | R
I(x)  C
I};
(R.C)
I = {x  
I(x)  C
I | R
I /= };
I = {x  
I | #R
I(x)  n};
( nR)
I = {x  
I | #R
I(x)  n};
( nR)
(T1, . . . , Tn.d)
I = {x  
1(x)  . . .  T

(T1, . . . , Tn.d)
I = {x  
1(x)  . . .  T

,I),
The satisfaction of an axiom E in an interpretation I = (
denoted I  E, is defined as follows: (1) I  Trans(R) iff R
I is tran-
sitive, (2) I  R  S iff R
I  D
I, (4) I 
I, (6) I  (a, v) : T iff
a : C iff a
I /= b
, vD) T

I. We say
(a
I satisfies E, or I is a model of E, iff I  E. We say I satisfies a set of
axioms E, or I is a model of E, denoted I  E, iff I  E for all E E. An
interpretation I satisfies a knowledge base K = (T,R,A), or I is a
model of K, denoted I  K, iff I satisfies each component T, R, and
A. A knowledge base K is satisfiable iff it has a model I. An axiom E is
a logical consequence of K, denoted K  E, iff every model of K satisfies E. A concept C is satisfiable relative to K iff K has a model I such
that C

I, (3) I  C  D iff C
I, (8) I  a /= b iff a
I = b

I  S
I  C
I, (7) I  a = b iff a

I, (5) I  (a, b) : R iff (a

I | T
I | T

n(x)  d

n(x)  d

I};
I /= }.

I /= .

I) R

, b

Example 3.1 (Car Example). Consider the following excerpt of a
simple ontology about cars. Let R =  and let the TBox T contain
the following axioms (where maker and topType are abstract roles,
while passenger capacity and max speed are datatype roles with
the natural numbers N respectively kilometers per hour km/h as
datatypes; the datatype predicate 245 km/h is true if the value is at
least 245 km/h):
( 1 maker)  Car;
( 1 passenger capacity)  Car;
( 1max speed)  Car;
Car  (= 1 maker)
(= 1 passenger capacity)
(= 1max speed);
Roadster  Cabriolet
passenger capacity.=2;
Cabriolet  Car  topType.SoftTop;
SportsCar = Car
max speed.245 km/h.

  maker.Maker;
  passenger capacity.N;
  max speed.km/h;

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

Informally, the roles maker, passenger capacity, and max speed
relate cars to a car maker, a natural number for its passenger capac-
ity, and a value in kilometers per hour for its maximum speed,
respectively. Furthermore, roadsters are cabriolets with the passenger capacity two, cabriolets are cars with a soft top, and sports
cars are exactly cars with a maximum speed of at least 245 km/h.
The ABoxA contains the following concept membership axioms:

mgb : Roadster  maker.{mg}  max speed.170 km/h;
enzo : Car  maker.{ferrari}  max speed.>350 km/h;
tt : Car  maker.{audi}  max speed.=243 km/h.
It is then not difficult to verify that some logical consequences of
the above knowledge base K = (T,R,A) are given as follows:
K  Roadster  Car; K  mg : Maker;
K  enzo : SportsCar; K  tt : SportsCar.

3.3. Main reasoning problems

The main reasoning problems in SHOIN(D) are deciding the
logical consequence of concept inclusion axioms (CSub), concept
membership axioms (CMem), and role membership axioms from
knowledge bases (RMem), deciding the satisfiability of concepts
relative to knowledge bases (CSat), and deciding the satisfiability of knowledge bases (KBSat). Note that (i) CSat and KBSat can
be reduced to each other, (ii) CMem and RMem are special cases of
CSub (in SHOIN(D)), and (iii) CSat and CSub can be reduced to each
other. The above problems are all decidable inSHOIN(D) if all number restrictions in K = (T,R,A) are restricted to simple abstract
roles w.r.t. R [58]. Decision procedures are given in [143,57], and
reasoning tools for SHOIN(D) are, e.g., FaCT++ [145,54] and Pellet
[105].

4. Probabilistic uncertainty and description logics

In this section, we recall an important probabilistic generalization of SHOIN(D) towards sophisticated formalisms for reasoning
under probabilistic uncertainty in the Semantic Web, called P-
SHOIN(D), which has been introduced in [90].

Note that any other classical description logic can be similarly
extended by probabilistic uncertainty. In particular, closely related
probabilistic generalizations of DL-Lite and the description logics
SHIF(D) and SHOQ(D) (which stand behind the web ontology
languages OWL Lite and DAML+OIL, respectively) have been introduced in [90,36]. The syntax and semantics of such an extension
can be defined in the same way as for P- SHOIN(D). Further-
more, if the chosen classical description logic allows for decidable
knowledge base satisfiability, then also the main reasoning tasks
in the probabilistic extension are all decidable. Note that to allow
for probabilistic role membership axioms (encoding that R(a, b)
(respectively, U(a, v)) holds with a probability between l and u), the
extended classical description logic should have the oneOf (respec-
tively, datatype oneOf) construct.
The syntax of the probabilistic description logic P- SHOIN(D)
uses the notion of a conditional constraint from [84] to express
probabilistic knowledge in addition to the axioms of SHOIN(D).
Its semantics is based on the notion of lexicographic entailment in
probabilistic default reasoning [85,87], which is a probabilistic generalization of the sophisticated notion of lexicographic entailment
by Lehmann [72] in default reasoning from conditional knowledge
bases. Due to this semantics, P- SHOIN(D) allows for expressing
both terminological probabilistic knowledge about concepts and
roles, and also assertional probabilistic knowledge about instances

of concepts and roles. It naturally interprets terminological and
assertional probabilistic knowledge as statistical knowledge about
concepts and roles and as degrees of belief about instances of concepts and roles, respectively, and allows for deriving both statistical
knowledge and degrees of belief. As an important additional fea-
ture, it also allows for expressing default knowledge about concepts
(as a special case of terminological probabilistic knowledge), which
is semantically interpreted as in Lehmanns lexicographic default
entailment [72].

The notion of probabilistic lexicographic entailment [85,87] is
an entailment relation for reasoning from statistical knowledge
and degrees of belief, which has very nice features [87]. In par-
ticular, it shows a similar behavior as reference-class reasoning in
a number of uncontroversial examples.1 But it also avoids many
drawbacks of reference-class reasoning (which are pointed out
in [5,87]): differently from reference-class reasoning, probabilistic
lexicographic entailment can handle complex scenarios and even
purely probabilistic subjective knowledge as input, and probabilistic lexicographic entailment draws conclusions in a global way from
all the available knowledge as a whole. Furthermore, probabilistic
lexicographic entailment also has very nice nonmonotonic proper-
ties, which are essentially inherited from Lehmanns lexicographic
entailment [72]. In particular, it realizes an inheritance of properties along subclass relationships, where more specific properties
override less specific properties, without showing the problem of
inheritance blocking (where properties are not inherited to subclasses that are exceptional relative to some other properties). For
example, under probabilistic lexicographic entailment, the default
knowledge (1) generally, cars do not have a red color and (2) gen-
erally, sports cars have a red color, and the probabilistic knowledge
(3) cars have four wheels with a probability of at least 0.9 imply
that sports cars have four wheels with a probability of at least
0.9. That is, the property of having four wheels with a probability of at least 0.9 is inherited from cars down to sports cars, even
though sports cars are exceptional cars relative to the property of
having a red color. As for general nonmonotonic properties, probabilistic lexicographic entailment satisfies (probabilistic versions
of) the rationality postulates by Kraus et al. [69], the property
of rational monotonicity, and some irrelevance, conditioning, and
inclusion properties. For example, as for the property of irrelevance,
under probabilistic lexicographic entailment, the above sentence
(3) implies that also red cars have four wheels with a probability of
at least 0.9. That is, the property of having a red color is irrelevant
to the property of having four wheels with a probability of at least
0.9. All these quite appealing features carry over to the probabilistic description logic P- SHOIN(D). See especially [87] for further
details and background on probabilistic lexicographic entailment.

4.1. Syntax

We now introduce the notion of a probabilistic knowledge base.
It is based on the language of conditional constraints [84], which
encode interval restrictions for conditional probabilities over con-
cepts. Every probabilistic knowledge base consists of (i) a PTBox,
which is a classical (description logic) knowledge base along with

1 Reference-class reasoning [110,70,71,106] is one of the most influential entailment relations for reasoning from statistical knowledge and degrees of belief. The
main idea behind it is to equate the degrees of belief about a particular individual
with the statistics of a reference class, which is informally defined as a set of individuals that contains the particular individual and about which we have some statistics.
If there are several reference classes with conflicting statistics, then the narrowest
one and its statistics are preferred. Even though reference-class reasoning has also
been criticized in the literature, there are several uncontroversial examples, where
it describes exactly the expected inference results.

probabilistic terminological knowledge, and (ii) a collection of
PABoxes, which encode probabilistic assertional knowledge about a
certain set of individuals. To this end, we partition the set of individuals I into the set of classical individuals IC and the set of probabilistic
individuals IP, and we associate with every probabilistic individual a
PABox. That is, probabilistic individuals are those individuals in I for
which we explicitly store some probabilistic assertional knowledge
in a PABox.
We first define conditional constraints as follows. We assume
a finite nonempty set C of basic classification concepts (or basic c-
concepts for short), which are (not necessarily atomic) concepts
in SHOIN(D) that are free of individuals from IP. Informally, they
are the relevant description logic concepts for defining probabilistic relationships. The set of classification concepts (or c-concepts)
is inductively defined as follows. Every basic c-concept  C is a
c-concept. If  and   are c-concepts, then  and (   ) are also c-
concepts. We often write (   ) to abbreviate(   ), as usual.
A conditional constraint is an expression of the form ( |)[l, u],
where  and   are c-concepts, and l and u are reals from [0, 1].
Informally, ( |)[l, u] encodes that the probability of   given  lies
between l and u.

We next define PTBoxes, PABoxes, and probabilistic knowledge

bases as follows:
 A PTBox PT = (T, P) consists of a classical (description logic)
knowledge base T and a finite set of conditional constraints P;
 A PABox P is a finite set of conditional constraints;
 A probabilistic knowledge base K = (T, P, (Po)o IP
) relative to IP
consists of a PTBox PT = (T, P) and one PABox Po for every probabilistic individual o IP.
Note that the meaning of a conditional constraint ( |)[l, u]
depends on whether it belongs to P or to Po for some probabilistic
individual o IP:
 Each ( |)[l, u] in P informally encodes that generally, if an
object belongs to , then it belongs to   with a probability in
[l, u]. For example, (R.{o}|)[l, u] in P, where o IC and R RA,
encodes that generally, if an object belongs to , then it is related
to o by R with a probability in [l, u].
 Each ( |)[l, u] in Po, where o IP, informally encodes that if o
belongs to , then o belongs to   with a probability in [l, u].
For example, (R.{o
  IC, and R RA,
 by R with
expresses that if o belongs to , then o is related to o
a probability in [l, u].
So, a probabilistic knowledge base K = (T, P, (Po)o IP

) extends a
classical knowledge base T by probabilistic terminological knowledge P and probabilistic assertional knowledge Po about every o IP.
That is, P represents our statistical knowledge about concepts, while
every Po represents our degrees of belief about o.
Observe that the axioms in T and the conditional constraints in
every Po with o IP are strict (that is, they must always hold), while
the conditional constraints in P are defeasible (that is, they may
have exceptions and thus do not always have to hold), since T  P
may not always be satisfiable as a whole in combination with our
degrees of belief (and then we ignore some elements of P).
Consequently, a conditional constraint ( |)[1, 1] in P encodes
generally, if an object belongs to , then it also belongs to  , while
( |)[1, 1] in Po encodes if o belongs to , then o also belongs to
 . The latter is equivalent to the implication o :   o :  , while
the former is in general not equivalent to    .
Example 4.1 (Car Example continued). We now extend the classical description logic knowledge base T given in Example 3.1

}|)[l, u] in Po, where o IP, o

by terminological default, terminological probabilistic, and assertional probabilistic knowledge to a probabilistic knowledge base
K = (T, P, (Po)o IP
). We assume an additional atomic concept HasFourWheels and an additional datatype role HasColor between cars
and the elementary datatype colors, which has a finite set of color
names as data values.

The terminological default knowledge (1) generally, cars do not
have a red color and (2) generally, sports cars have a red color,
and the terminological probabilistic knowledge (3) cars have four
wheels with a probability of at least 0.9, can be expressed by the
following conditional constraints in P:
(1) ( HasColor.{red}|Car)[1, 1],
(2) ( HasColor.{red}|SportsCar)[1, 1],
(3) (HasFourWheels|Car)[0.9, 1].
Suppose we want to encode some probabilistic information about
Johns car (which we have not seen so far). Then, the set of probabilistic individuals IP contains the individual Johns car, and the
assertional probabilistic knowledge (4) Johns car is a sports car
with a probability of at least 0.8 (we know that John likes sports
cars) can be expressed by the following conditional constraint in
PJohns car:
(4) (SportsCar|)[0.8, 1].

4.2. Semantics

In this section, we define the semantics of P- SHOIN(D). After
some preliminaries, we introduce the notions of consistency and
lexicographic entailment for probabilistic knowledge bases, which
are based on the notions of consistency and lexicographic entail-
ment, respectively, in probabilistic default reasoning [85,87].

4.2.1. Preliminaries

We now define (possible) objects and probabilistic interpre-
tations, which are certain sets of basic c-concepts respectively
probability functions on the set of all (possible) objects. We also
define the satisfaction of classical knowledge bases and conditional
constraints in probabilistic interpretations.
A (possible) object o is a set of basic c-concepts  C such that
{i :  |   o}  {i :  |  C \ o} is satisfiable, where i is a new indi-
vidual. Informally, every object o represents an individual i that
is fully specified on C in the sense that o belongs (respectively,
does not belong) to every c-concept   o (respectively,  C \ o).
We denote by OC the set of all objects relative to C. An object o
satisfies a classical knowledge base T, or o is a model of T, denoted
o  T, iff T  {i :  |   o}  {i :  |  C \ o} is satisfiable, where i is
a new individual. An object o satisfies a basic c-concept  C, or o is
a model of , denoted o  , iff   o. The satisfaction of c-concepts
by objects is inductively extended to all c-concepts, as usual, by
(i) o   iff o   does not hold, and (ii) o      iff o   and
o   . It is not difficult to verify that a classical knowledge base T is
satisfiable iff an object oOC exists that satisfies T.
A probabilistic interpretation Pr is a probability function on OC
(that is, a mapping Pr : OC  [0, 1] such that all Pr(o) with oOC
sum up to 1). We say Pr satisfies a classical knowledge base T, or Pr
is a model of T, denoted Pr  T, iff o  T for every oOC such that
Pr(o) > 0. We define the probability of a c-concept and the satisfaction of conditional constraints in probabilistic interpretations as
follows. The probability of a c-concept  in a probabilistic interpretation Pr denoted Pr(), is the sum of all Pr(o) such that o  . For
c-concepts  and   such that Pr() > 0, we write Pr( |) to abbreviate Pr(   ) / Pr(). We say Pr satisfies a conditional constraint
(| )[l, u], or Pr is a model of ( |)[l, u], denoted Pr  ( |)[l, u],

iff Pr() = 0 or Pr( |) [l, u]. We say Pr satisfies a set of conditional constraints F, or Pr is a model of F, denoted Pr  F, iff Pr  F
for all F F. It is not difficult to verify that a classical knowledge
base T is satisfiable iff there exists a probabilistic interpretation that
satisfies T.

4.2.2. Consistency

The notion of consistency for PTBoxes and probabilistic knowledge bases is based on the notion of consistency in probabilistic
default reasoning [85,87].
We first give some preparative definitions. A probabilistic interpretation Pr verifies a conditional constraint ( |)[l, u] iff Pr() = 1
and Pr( ) [l, u], that is, iff Pr() = 1 and Pr  ( |)[l, u]. We say
Pr falsifies ( |)[l, u] iff Pr() = 1 and Pr  ( |)[l, u]. A set of conditional constraints F tolerates a conditional constraint F under a
classical knowledge base T iff T  F has a model that verifies F.
A PTBox PT = (T, P) is consistent iff (i) T is satisfiable and (ii)
there exists an ordered partition (P0, . . . , Pk) of P such that each
Pi with i{0, . . . , k} is the set of all F  P \ (P0    Pi1) that are
tolerated under T by P \ (P0    Pi1). Informally, condition (ii)
means that P has a natural ordered partition into collections of
conditional constraints of increasing specificities such that every
collection is locally consistent. That is, any inconsistencies can be
naturally resolved by preferring more specific pieces of knowledge to less specific ones. For example, the inconsistency between
( HasColor.{red}|Car)[1, 1] and ( HasColor.{red}|SportsCar)[1, 1]
when reasoning about sports cars is naturally resolved by preferring the latter to the former. We call the above (unique) ordered
partition (P0, . . . , Pk) of P the z-partition of PT. A probabilistic
) is consistent iff (i) PT = (T, P)
knowledge base K = (T, P, (Po)o IP
is consistent and (ii) T  Po is satisfiable for every probabilistic
individual o IP. Informally, (ii) says that the strict knowledge
in T must be compatible with the strict degrees of belief in Po,
for every probabilistic individual o. Observe that (i) involves T
and P, while (ii) involves T and Po, for every probabilistic individual o. This separate treatment of P and the Pos is due to
the fact that P represents probabilistic terminological knowl-
edge, while each Po represents probabilistic assertional knowledge
(about o).

Example 4.2 (Car Example continued). The probabilistic knowledge base K = (T, P, (Po)o IP
) of Example 4.1 is consistent, since
PT = (T, P) is consistent, and T  Po is satisfiable for every probabilistic individual o IP = {John
s car}. Observe that the z-partition

of (T, P) is given by (P0, P1), where P0 = {(  | )[l, u] P |  = Car}
and P1 = {( |)[l, u] P |  = SportsCar}.

4.2.3. Lexicographic entailment

The notion of lexicographic entailment for probabilistic knowledge bases is based on lexicographic entailment in probabilistic
default reasoning [85,87]. In the sequel, let K = (T, P, (Po)o IP

be a consistent probabilistic knowledge base. We first define
a lexicographic preference relation on probabilistic interpreta-
tions, which is then used to define the notion of lexicographic
entailment for sets of conditional constraints under PTBoxes.
We finally define the notion of lexicographic entailment for
deriving statistical knowledge and degrees of belief about probabilistic objects from PTBoxes and probabilistic knowledge bases,
respectively.

We use the (unique) z-partition (P0, . . . , Pk) of (T, P) (see Section
4.2.2) to define a lexicographic preference relation on proba-
: we say Pr is lexicographically
bilistic interpretations Pr and Pr
iff some i{0, . . . , k} exists

preferable (or lex-preferable) to Pr
such that |{F  Pi | Pr  F}| > |{F  Pi | Pr
  F}| and |{F  Pj | Pr  F}| =
|{F  Pj | Pr
  F}| for all i < j  k. Roughly speaking, this preference

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

relation implements the idea of preferring more specific pieces of
knowledge to less specific ones in the case of local inconsistencies.
It can thus be used for ignoring the latter when drawing conclusions in the case of local inconsistencies. A model Pr of a classical
knowledge base T and a set of conditional constraints F is a lexicographically minimal (or lex-minimal) model of T  F iff no model of
T  F is lex-preferable to Pr.

We define the notion of lexicographic entailment of conditional
constraints from sets of conditional constraints under PTBoxes as
follows. A conditional constraint ( |)[l, u] is a lexicographic consequence (or lex-consequence) of a set of conditional constraints
F under a PTBox PT, denoted F lex ( |)[l, u] under PT, iff
Pr( ) [l, u] for every lex-minimal model Pr of T  F  {(|)[1, 1]}.
We say ( |)[l, u] is a tight lexicographic consequence (or tight lex-
consequence) of F under PT, denoted F lex
tight ( |)[l, u] under PT,
iff l (respectively, u) is the infimum (respectively, supremum) of
Pr( ) subject to all lex-minimal models Pr of T  F  {(|)[1, 1]}.
Note that [l, u] = [1, 0] (where [1, 0] represents the empty inter-
val) when no such model Pr exists. Furthermore, for inconsistent
PTBoxes PT, we define F lex ( |)[l, u] and F lex
tight ( |)[1, 0]
under PT for all sets of conditional constraints F and all conditional
constraints ( |)[l, u].

We now define which statistical knowledge and degrees of
belief follow under lexicographic entailment from PTBoxes PT
and probabilistic knowledge bases K = (T, P, (Po)o IP
), respectively.
A conditional constraint F is a lex-consequence of PT, denoted
PT lex F, iff  lex F under PT. We say F is a tight lex-consequence
of PT, denoted PT lex
tight F under PT. A conditional constraint F for a probabilistic individual o IP is a lex-consequence of
K, denoted K lex F, iff Po lex F under PT = (T, P). We say F is a
tight lex-consequence of K, denoted K lex
tight F under
PT = (T, P).

tight F, iff Po lex

tight F, iff  lex

Example 4.3 (Car Example continued). Consider again the probabilistic knowledge base K = (T, P, (Po)o IP
) of Example 4.1. The
following are some (terminological default and terminological
probabilistic) tight lex-consequences of PT = (T, P):
( HasColor.{red}|Car)[1, 1],
( HasColor.{red}|SportsCar)[1, 1],
(HasFourWheels|Car)[0.9, 1],
( HasColor.{red}|Roadster)[1, 1],
(HasFourWheels|SportsCar)[0.9, 1],
(HasFourWheels|Roadster)[0.9, 1].

Hence, in addition to the sentences (1) to (3) directly encoded in
P, we also conclude generally, roadsters do not have a red color,
sports cars have four wheels with a probability of at least 0.9,
and roadsters have four wheels with a probability of at least 0.9.
Observe here that the default property of not having a red color and
the probabilistic property of having four wheels with a probability
of at least 0.9 are inherited from cars down to roadsters. Roughly,
the tight lex-consequences of PT = (T, P) are given by all those conditional constraints that (a) are either in P, or (b) can be constructed
by inheritance along subconcept relationships from the ones in P
and are not overridden by more specific pieces of knowledge in P.
The following conditional constraints for the probabilistic
individual Johns car are some (assertional probabilistic) tight lexconsequences of K = (T, P, (Po)o IP
), which informally say that
Johns car is a sports car, has a red color, and has four wheels with
probabilities of at least 0.8, 0.8, and 0.72, respectively:
(SportsCar|)[0.8, 1],
( HasColor.{red}|)[0.8, 1],
(HasFourWheels|)[0.72, 1].

4.3. Main reasoning problems

4.4. Main applications

The main reasoning problems in P- SHOIN(D) are summarized
by the following decision and computation problems (where every
lower and upper bound in the PTBox PT = (T, P), the probabilistic knowledge base K = (T, P, (Po)o IP
), and the set of conditional
constraints F is rational):

PTBox Consistency (PTCon): Given a PTBox PT = (T, P), decide

whether PT is consistent.

Probabilistic Knowledge

Base Consistency (PKBCon): Given a probabilistic knowledge base
), decide whether K

K = (T, P, (Po)o IP
is consistent.

Tight Lexicographic
Entailment (TLexEnt): Given a PTBox PT = (T, P), a finite
set of conditional constraints F, and
two c-concepts  and  , compute
the rational numbers l, u [0, 1] such
that F lex

tight ( |)[l, u] under PT.

tight (R.{o

Another

Some important special cases of TLexEnt are given as fol-
lows: (PCSub) given a consistent PTBox PT and two c-concepts
 and  , compute the rational numbers l, u [0, 1] such that
PT lex
tight ( |)[l, u]; (PCRSub) given a consistent PTBox PT, a
c-concept , a classical individual o IC, and an abstract role
R RA, compute the rational numbers l, u [0, 1] such that
tight (R.{o}|)[l, u]; (PCMem) given a consistent probabilisPT lex
tic knowledge base K, a probabilistic individual o IP, and a
c-concept  , compute l, u [0, 1] such that K lex
tight ( |)[l, u]
for o; and (PRMem) given a consistent probabilistic knowledge
  IC, a probabilistic individual
base K, a classical individual o
o IP, and an abstract role R RA, compute l, u [0, 1] such that
K lex

}|)[l, u] for o.
important decision problem in P-SHOIN(D)

is
Probabilistic Concept Satisfiability (PCSat): Given a consistent
PTBox PT and a c-concept , decide whether PT /lex(|)[0, 0].
This problem is reducible to CSat (see Section 3.3), since
(T, P) /lex(|)[0, 0] iff T /  .
There exists an algorithm for deciding whether a PTBox (respec-
tively, probabilistic knowledge base) in P-SHOIN(D) is consistent,
which is based on a reduction to deciding whether a classical knowledge base in SHOIN(D) is satisfiable and to deciding
whether a system of linear constraints is solvable. More specifi-
cally, one has to solve a sequence of solvability problems of systems
of linear constraints, whose variables are computed by deciding
classical knowledge base satisfiability in SHOIN(D) (see [90] for
further details). This shows that the two consistency problems in
P-SHOIN(D) are both decidable. Furthermore, there is a similar
algorithm for computing tight intervals under lexicographic entailment in P-SHOIN(D), which is based on a reduction to deciding
classical knowledge base satisfiability in SHOIN(D) and to solving
linear optimization problems (see [90]). Thus, also lexicographic
entailment in P-SHOIN(D) is computable. As for the computational
complexity, deciding the two consistency problems in P-SHOIN(D)
is complete for the complexity class NEXP, while computing tight
intervals under lexicographic entailment in P- SHOIN(D) belongs
to FPNEXP[90].
Although there is no implementation of P- SHOIN(D) to date,
there are already implementations of its predecessor P- SHOQ(D)
(see [99]) and of a probabilistic description logic based on probabilistic default reasoning as in [85,87] (see [66]).

As pointed out in [14,15], there is a plethora of applications with
an urgent need for handling probabilistic knowledge in ontologies,
especially in areas like medicine, biology, defense, and astronomy.
Furthermore, there are strong arguments for the critical need of
dealing with probabilistic uncertainty in ontologies in the Semantic
Web, some of which are briefly summarized as follows.
 In addition to being logically related, the concepts of an ontology
are generally also probabilistically related. For example, two concepts either may be logically related via a subset or disjointness
relationship, or they may show a certain degree of overlap. Probabilistic ontologies allow for quantifying these degrees of overlap,
reasoning about them, and using them in semantic-web applica-
tions. In particular, probabilistic ontologies are successfully used
in information retrieval for an increased recall [146,59] (see also
below). The degrees of concept overlap may also be exploited in
personalization and recommender systems.
 Rather than consisting of one standardized overall ontology,
the Semantic Web will consist of a huge collection of different
ontologies. Hence, in semantic-web applications such as automated reasoning and information retrieval, one has to align
the concepts of different ontologies, which is called ontology
matching / mapping [30]. In general, the concepts of two different ontologies do not match exactly, and we have to deal with
degrees of concept overlap as above, which are determined by
automatic or semi-automatic tools or experts. These degrees of
concept overlap are then represented in probabilistic ontologies,
which thus allows for inference about the degrees of overlap
between other concepts and about probabilistic instance relationships [104,98] (see also Section 4.5.2).
 Like the current Web, the Semantic Web will necessarily contain
ambiguous and controversial pieces of information in different
web sources. This can be handled via probabilistic data integration by associating with every web source a probability describing
its degree of reliability [148,45]. As resulting pieces of data, such a
probabilistic data integration process necessarily produces probabilistic facts, that is, probabilistic knowledge at the instance
level. Such probabilistic instance relationships can be encoded in
probabilistic ontologies and there be enhanced by further classical and/or terminological probabilistic knowledge, which then
allows for inference about other probabilistic instance relation-
ships.

An important application for probabilistic ontologies (and thus
probabilistic description logics and ontology languages) is especially information retrieval. In particular, Subrahmanians group
[146,59] explores the use of probabilistic ontologies in relational databases. They propose to extend relations by associating
with every attribute a constrained probabilistic ontology, which
describes relationships between terms occurring in the domain of
that attribute. An extension of the relational algebra then allows
for an increased recall (which is the proportion of documents relevant to a search query in the collection of all returned documents)
in information retrieval. In closely related work, Mantay et al. [95]
propose a probabilistic least common subsumer operation, which
is based on a probabilistic extension of the description logic ALN.
They show that applying this approach in information retrieval
allows for reducing the amount of retrieved data and thus for avoiding information flood. Another closely related work by Holi and
Hyv  onen [48,49] shows how degrees of overlap between concepts
can be modeled and computed efficiently using Bayesian networks
based on RDF(S) ontologies. Such degrees of overlap indicate how
well an individual data item matches the query concept, and can

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

thus be used for measuring the relevance in information retrieval
tasks. Finally, Weikum et al. [151] and Thomas and Sheth [142]
describe the use of probabilistic ontologies in information retrieval
from a more general perspective.

4.5. Other probabilistic ontology languages

To our knowledge, there are no other approaches to probabilistic description logics for the Semantic Web in the literature.
Furthermore, although there are several previous approaches to
probabilistic description logics without semantic web background,
P- SHOIN(D) is the most expressive probabilistic description logic,
both in terms of the generalized classical description logic and in
terms of the supported forms of terminological and assertional
probabilistic knowledge. That is, previous probabilistic description
logics generalize less expressive classical description logics, and
they only allow for some facets of the terminological and assertional probabilistic knowledge of this paper, but not for all of them
at the same time. There are also several probabilistic extensions of
web ontology languages in the literature. In this section, we give an
overview of all these approaches.

4.5.1. Probabilistic description logics

Other approaches to probabilistic description logics can be classified according to the generalized classical description logics, the
supported forms of probabilistic knowledge, the underlying probabilistic semantics, and the reasoning techniques.

One of the earliest approaches to probabilistic description logics
is due to Heinsohn [47], who presents a probabilistic extension of
the description logic ALC, which allows to represent terminological probabilistic knowledge about concepts and roles, and which
is based on the notion of logical entailment in probabilistic logics,
similar to [100,2,34,84]. Heinsohn [47], however, does not allow
for assertional (classical or probabilistic) knowledge about concept
and role instances. The main reasoning problems are deciding the
consistency of probabilistic terminological knowledge bases and
computing logically entailed tight probability intervals. Heinsohn
proposes a sound and complete global reasoning technique based
on classical reasoning in ALC and linear programming, as well as
a sound but incomplete local reasoning technique based on the
iterative application of local inference rules.

Another early approach to probabilistic description logics is due
to Jaeger [61], who also proposes a probabilistic extension of the
description logic ALC, which allows for terminological probabilistic
knowledge about concepts and roles, and assertional probabilistic knowledge about concept instances, but does not support
assertional probabilistic knowledge about role instances (but he
mentions a possible extension in this direction). The entailment of
terminological probabilistic knowledge from terminological probabilistic knowledge is based on the notion of logical entailment in
probabilistic logic, while the entailment of assertional probabilistic knowledge from terminological and assertional probabilistic
knowledge is based on a cross-entropy minimization relative to terminological probabilistic knowledge. The main reasoning problems
are terminological probabilistic consistency and inference, which
are solved by linear programming, and assertional probabilistic
consistency and inference, which are solved by an approximation
algorithm.
The recent work by D  urig and Studer [29] presents a further
probabilistic extension of ALC, which is based on a model-theoretic
semantics as in probabilistic logics, but which only allows for assertional probabilistic knowledge about concept and role instances,
and not for terminological probabilistic knowledge. The paper
also explores independence assumptions for assertional probabilistic knowledge. The main reasoning problem is deciding the

consistency of assertional probabilistic knowledge, but neither an
algorithm nor a decidability result is given.

Jaegers recent work [62] focuses on interpreting probabilistic
concept subsumption and probabilistic role quantification through
statistical sampling distributions, and develops a probabilistic version of the guarded fragment of first-order logic. The semantics is
different from the semantics of all the other probabilistic description logics in this paper, since it is based on probability distributions
over the domain, and not on the more commonly used probability
distributions over a set of possible worlds. The paper proposes a
sound Gentzen-style sequent calculus for the logic, but it neither
proves the completeness of this calculus nor decidability in general.
Koller et al.s work [68] presents the probabilistic description
logic P-Classic, which is a probabilistic generalization (of a vari-
ant) of the description logic Classic. Similar to Heinsohns work
[47], it allows for encoding terminological probabilistic knowledge
about concepts, roles, and attributes (via so-called p-classes), but
it does not support assertional (classical or probabilistic) knowledge about instances of concepts and roles. However, in contrast
to [47], its probabilistic semantics is based on a reduction to
Bayesian networks. The main reasoning problem is to determine the
exact probabilities for conditionals between concept expressions in
canonical form. This problem is solved by a reduction to inference in
Bayesian networks. As an important feature of P-Classic, the above
problem can be solved in polynomial time, when the underlying
Bayesian network is a polytree. Note that a recent implementation
of P-Classicis described in [65].
Closely related work by Yelland [153] proposes a probabilistic
extension of a description logic close to FL, whose probabilistic
semantics is also based on a reduction to Bayesian networks, and
it applies this approach to market analysis. The approach allows
for encoding terminological probabilistic knowledge about concepts and roles, but it does not support assertional (classical or
probabilistic) knowledge about instances of concepts and roles.
Like in Koller et al.s work [68], the main reasoning problem is to
determine the exact probabilities for conditionals between con-
cepts, which is solved by a reduction to inference in Bayesian
networks.

4.5.2. Probabilistic web ontology languages

The literature contains several probabilistic generalizations of
web ontology languages. Many of these approaches focus especially
on combining the web ontology language OWL with probabilistic
formalisms based on Bayesian networks.

In particular, da Costa [14], da Costa and Laskey [15], and da
Costa et al. [16] suggest a probabilistic generalization of OWL, called
PR-OWL, whose probabilistic semantics is based on multi-entity
Bayesian networks (MEBNs). The latter are a Bayesian logic that
combines first-order logic with Bayesian networks. Roughly speak-
ing, PR-OWL represents knowledge as parameterized fragments of
Bayesian networks. Hence, it can encode probability distributions
on the interpretations of an associated first-order theory as well as
repeated structure.

In [18,19], Ding et al. propose a probabilistic generalization of
OWL, called BayesOWL, which is based on standard Bayesian net-
works. BayesOWL provides a set of rules and procedures for the
direct translation of an OWL ontology into a Bayesian network,
and it also provides a method for incorporating available probability constraints when constructing the Bayesian network. The
generated Bayesian network, which preserves the semantics of the
original ontology and which is consistent with all the given probability constraints, supports ontology reasoning, both within and
across ontologies, as Bayesian inferences. In [104,19], Ding et al.
also describe an application of the BayesOWL approach in ontology
mapping.

In closely related work, Mitra et al. [98] describe an implemented technique, called Omen, to enhancing existing ontology
mappings by using a Bayesian network to represent the influences
between potential concept mappings across ontologies. More con-
cretely, Omen is based on a simple ontology model similar to RDF
Schema. It uses a set of meta-rules that capture the influence of
the ontology structure and the semantics of ontology relations, and
matches nodes that are neighbors of already matched nodes in the
two ontologies.

Yang and Calmet [152] present an integration of the web ontology language OWL with Bayesian networks, called OntoBayes. The
approach makes use of probability and dependency-annotated
OWL to represent uncertain information in Bayesian networks. The
work also describes an application in risk analysis for insurance and
natural disaster management. Pool and Aikin [107] also provide
a method for representing uncertainty in OWL ontologies, while
Fukushige [35] proposes a basic framework for representing probabilistic relationships in RDF. Nottelmann and Fuhr [101] present
two probabilistic extensions of variants of OWL Lite, along with a
mapping to locally stratified probabilistic Datalog.

Another important work is due to Udrea et al. [147], who present
a probabilistic generalization of RDF, which allows for representing
terminological probabilistic knowledge about classes and assertional probabilistic knowledge about properties of individuals. They
provide a technique for assertional probabilistic inference in acyclic
probabilistic RDF theories, which is based on the notion of logical
entailment in probabilistic logic, coupled with a local probabilistic
semantics. They also provide a prototype implementation of their
algorithms.

5. Possibilistic uncertainty and description logics

Similar to probabilistic extensions of description logics, possibilistic extensions of description logics have been developed by
Hollunder [53] and Dubois et al. [22]. In the sequel, we implicitly
assume the description logic SHOIN(D) as underlying description
logic, but any other (decidable) description logic can be used as
well.

5.1. Syntax

A possibilistic axiom is of the form P    l or N    l, where   is a
classical description logic axiom, and l is a real number from [0, 1]. A
possibilistic RBox (respectively, TBox, ABox) is a finite set of possibilistic axioms P    l or N    l, where   is an RBox (respectively, TBox,
ABox) axiom. A possibilistic knowledge base K = (R,T,A) consists
of a possibilistic RBox R, a possibilistic TBox T, and a possibilistic
ABox A. The following example from [53] illustrates possibilistic
knowledge bases.

Example 5.1 (Car example continued). The following possibilistic
knowledge base K = (R,T,A) encodes some possibilistic knowledge about cars and rich people. Let R = . The TBox T represents
the possibilistic terminological knowledge that every person owning a Porsche is either rich or a car fanatic with a necessity of at least
0.8 and every rich person is a golfer with a possibility of at least
0.7:
T = {Nowns.Porsche  richPerson  carFanatic  0.8,

P richPerson  golfer  0.7}.

Furthermore, the ABox A expresses the possibilistic assertional
knowledge that Tom owns a 911 with necessity 1, a 911 is a
Porsche with necessity 1, and Tom is not a car fanatic with a

necessity of at least 0.7:
A = {N (Tom, 911) : owns  1, N 911 : Porsche  1,

N Tom : carFanatic  0.7}.

5.2. Semantics

Let  denote the set of all classical description logic interpre-
tations. A possibilistic interpretation is a mapping  :   [0, 1]. In
the sequel, we assume that  is normalized, that is, that (I) = 1
for some I. The possibility of a description logic axiom   in a
possibilistic interpretation , denoted Poss( ), is then defined by
Poss( ) = max{(I)|I,I   } (where max  = 0), and the necessity of  , denoted Nec( ), is defined by Nec( ) = 1  Poss( ).
A possibilistic interpretation  satisfies a possibilistic axiom
P    l (respectively, N    l), or  is a model of P    l (respectively,
N    l), denoted   P    l (respectively,   N    l), iff Poss( ) 
l (respectively, Nec( )  l). The notion of satisfiability of possibilistic
knowledge bases and the notions of logical and tight logical consequences of possibilistic axioms from possibilistic knowledge bases
are then defined as usual [53,22].

Example 5.2 (Car example continued). Consider again the possibilistic knowledge base K of Example 5.2. It is not difficult to verify
that K is satisfiable and logically implies that Tom is a golfer with
a possibility of at least 0.7 [53], that is,
K  P Tom : golfer  0.7.

5.3. Main reasoning problems

The main reasoning problems related to possibilistic description logics are deciding whether a possibilistic knowledge base is
satisfiable, deciding whether a possibilistic axiom is a logical consequence of a possibilistic knowledge base, and computing the tight
lower and upper bounds entailed by a possibilistic knowledge base
for the necessity and the possibility of a classical description logic
axiom. As shown by Hollunder [53], deciding logical consequences,
and thus also deciding satisfiability and computing tight lower and
upper bounds can be reduced to deciding logical consequences
in classical description logics. A recent implementation of reasoning in possibilistic description logics using KAON22 is reported in
[109,108].

5.4. Main applications

Liau and Yao [80] report on an application of possibilistic
description logics in information retrieval. More concretely, they
define a possibilistic generalization of the description logicALC and
show that it can be used in typical information retrieval problems,
such as query relaxation, query restriction, and exemplar-based
retrieval. Possibilistic description logics can also be used for handling inconsistencies in ontologies [109,108]. Another important
application of possibilistic description logics is the representation
of user preferences in the Semantic Web. For example, the recent
work by Hadjali et al. [38] shows that possibilistic logic can be nicely
used for encoding user preferences in the context of databases.

6. Vagueness and description logics

In this section, we define the syntax and the semantics of a fuzzy
generalization of SHOIN(D), called fuzzy SHOIN(D). We recall here

2 http://kaon2.semanticweb.org/.

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

as Young(x) = ls(x; 10, 30). Then,
YoungPerson = Person  age.Young
denotes young persons.

(5)

6.1.2. Fuzzy modifiers

Fuzzy SHOIN(D) also supports fuzzy modifiers, an interesting feature of fuzzy logics. Fuzzy modifiers, like very, more or less,
and slightly, apply to fuzzy sets to change their membership func-
tion. Formally, a fuzzy modifier m represents a function fm : [0, 1] 
[0, 1]. For example, we may define fvery(x) = x2 and fslightly(x) = 
x.
Fuzzy modifiers have been considered, e.g., in [52,144]. Syntacti-
cally, if M is a new alphabet for fuzzy modifiers, m M is a fuzzy
modifier, and C is a concept in fuzzy SHOIN(D), then m(C) is a
concept in fuzzy SHOIN(D) as well. For example, by referring to
Example 3.1, we may define the concept of sports cars as
SportsCar = Car  max speed.very(High),
(6)
where very is a fuzzy modifier with membership function fvery(x) =
x2, and High is a fuzzy datatype predicate over the domain of speed
expressed in kilometers per hour and may be defined as High(x) =
rs(x; 80, 250).

6.1.3. Fuzzy knowledge bases
We next define fuzzy knowledge bases in fuzzy SHOIN(D). We
first define concepts and fuzzy axioms in fuzzy SHOIN(D).
Concepts in fuzzy SHOIN(D) are defined in nearly the same way
as concepts in SHOIN(D), except that we now also allow fuzzy
modifiers from a set of fuzzy modifiers M as unary operators on
concepts. More concretely, Concepts in fuzzy SHOIN(D) are defined
by induction as follows. Every atomic concept A A is a concept, 
and  are concepts, and if a1, . . . , an  I, then {a1, . . . , an} is a con-

cept (called oneOf). If C, C1, C2 are concepts, R RA  R
A , and m M,
then (C1  C2), (C1  C2), C, and m(C) are concepts (called conjunc-
tion, disjunction, negation, and fuzzy modification, respectively), as
well as R.C, R.C,  nR, and  nR (called exists, value, atleast, and
atmost restriction, respectively) for an integer n  0. If D is an n-
ary datatype predicate and T, T1, . . . , Tn  RD, then T1, . . . , Tn.D,
T1, . . . , Tn.D,  nT, and  nT are concepts (called datatype exists,
value, atleast, and atmost restriction, respectively) for an integer
n  0. We eliminate parentheses as usual. For decidability reasons,
number restrictions are restricted to simple abstract roles.
We define fuzzy axioms, fuzzy RBoxes, fuzzy TBoxes, fuzzy
ABoxes, and fuzzy knowledge bases in fuzzy SHOIN(D) as follows.
A fuzzy RBox R is a finite set of transitivity axioms Trans(R) in
SHOIN(D) and fuzzy role inclusion axioms of the form    n,    n,
  > n, and   < n, where   is a role inclusion axiom in SHOIN(D),
and n [0, 1].
A fuzzy TBox T is a finite set of fuzzy concept inclusion axioms
   n,    n,   > n, and   < n, where   is a concept inclusion axiom
in SHOIN(D), and n [0, 1].
A fuzzy ABox A consists of a finite set of equality and inequality
axioms a = b and a /= b, respectively, and of fuzzy concept and fuzzy
role membership axioms of the form    n,    n,   > n, or   < n,
where   is a concept or role membership axiom in SHOIN(D), and
n [0, 1].
For example, a : C  0.1, (a, b) : R  0.3, R  S  0.4, and C 
D  0.6 are fuzzy axioms. Informally, from a semantical point of
view, a fuzzy axiom    n (respectively,    n,   > n, and   < n)
constrains the membership degree of   to be at least (respectively,
at most, greater than, and less than) n. Hence, jim : YoungPerson 
0.2 says that jim is a YoungPerson with degree at least 0.2. On the
other hand, a fuzzy concept inclusion axiom of the form C  D  n
says that the subsumption degree between C and D is at least n.

Fig. 1. (a) Trapezoidal function trz(x; a, b, c, d), (b) triangular function tri(x; a, b, c),
(c) left-shoulder function ls(x; a, b), and (d) right-shoulder function rs(x; a, b).

the semantics given in [131,134], which is based on arbitrary but
fixed combination functions , , , and  (see Section 2.3). Note
that the language here subsumes the one described in [119], which
has been developed in parallel. After describing the main reasoning
problems in vague description logics, we summarize their main
applications, and we also give an overview on other vague ontology
languages.

6.1. Syntax

We now define the syntax of fuzzy SHOIN(D). We first define
fuzzy datatype theories and fuzzy modifiers, and then fuzzy axioms
and fuzzy knowledge bases.

6.1.1. Fuzzy datatype theories

We have seen that SHOIN(D) allows to reason with datatypes,
such as strings and integers, using the so-called concrete domains.
In the fuzzy generalization, concrete domains and thus datatypes
may be based on fuzzy sets as well. More specifically, a fuzzy
datatype theory D = (D,D) is defined in the same way as a classical datatype theory except that D now assigns to every n-ary
datatype predicate an n-ary fuzzy relation over D. For example,
like in SHOIN(D), the datatype predicate 18 may be a unary crisp
predicate over the natural numbers denoting the set of integers
smaller than or equal to 18, that is, 18 : Natural  [0, 1] and
18(x) =

1 if x  18,
0 otherwise.

Then,
Minor = Person  age.18
defines persons, whose age is less than or equal to 18, that is, it
defines minors.

(4)

As for non-crisp fuzzy datatype predicates, we recall that in
fuzzy set theory and practice, there are many functions for specifying fuzzy set membership degrees. In particular, the triangular,
the trapezoidal, the left-shoulder, and the right-shoulder functions
are simple, but most frequently used to specify fuzzy set membership degrees (see Fig. 1). Using these functions, we may then define,
for example, Young : Natural  [0, 1] to be a fuzzy datatype predicate over the natural numbers denoting the degree of youngness of
a persons age. The fuzzy datatype predicate Young may be defined

A fuzzy axiom is a transitivity, a fuzzy concept inclusion, a fuzzy
role inclusion, a fuzzy concept membership, a fuzzy role member-
ship, an equality, or an inequality axiom. A fuzzy knowledge base
K = (R,T,A) consists of a fuzzy RBox R, a fuzzy TBox T, and a fuzzy
ABox A.

6.2. Semantics

A fuzzy interpretation I = (

We now define the semantics of fuzzy SHOIN(D). The main
idea behind it is that concepts and roles are interpreted as fuzzy
subsets of an interpretations domain. Therefore, axioms in fuzzy
SHOIN(D), rather being satisfied (true) or unsatisfied (false) in an
interpretation, are associated with a degree of truth in [0, 1]. In
the following, let , , , and  be an arbitrary but fixed t-norm,
s-norm, implication function, and negation function, respectively
(see Table 3 for some specific choices). As such, the semantics is a
generalization of Straccia [125] in which Zadeh Logic has been used
as specific interpretation of the connectives (see Table 3).
,I) relative to a fuzzy datatype
theory D = (D,D) consists of a nonempty set 
I (called the
domain), disjoint from D, and of a fuzzy interpretation function
I that coincides with D on every data value, datatype, and fuzzy
datatype predicate, and it assigns:
 to each individual a I an element a

I;
 to each atomic concept A A a function A
I  [0, 1];
I : 
 to each abstract role R RA a function R
I  [0, 1];

I : 
 to each datatype role T  RD a function T
I  D  [0, 1];
I : 
 to each modifier m M the modifier function m
I = fm : [0, 1] 
[0, 1].

The mapping I is extended to all roles and concepts as follows

(where x, y 
(x, y) = R

I):
I(y, x),

I},

, . . . , an

(x),
(x),

(R
I(x) = 1,
I(x) = 0,
1 if x {a1
{a1, . . . , an}I(x) =
0 otherwise,

(x)  C2
(x) = C1
(C1  C2)

(x)  C2
(x) = C1
(C1  C2)

(x) = C
(C)
I(x),

(x) = m
I(x)),
I(C
(m(C))
(R.C)

I(x, y)  C
(x) = sup

y I
(R.C)

(x) = inf
I(x, y)  C
y IR
( nR)
(x) =
y1,...,yn  I,|{y1,...,yn}|=n
(x) =
( nR)

y1,...,yn+1  I,|{y1,...,yn+1}|=n+1
(T1, . . . , Tn.D)

i=1Ti
(T1, . . . , Tn.D)

i=1Ti
We comment briefly some points. The semantics of R.C,
(R.C)

I(x, yi),

n+1

i=1 R

(x, yi)

y1,...,yn  D
y1,...,yn  D

(x) =
(x) =

I(y),
I(y),

(x, y)  C

i=1R

(x, yi)

(y),

sup

sup

inf

inf

(x) = sup
y I

 0,

I(x, yi)
 DD(y1, . . . , yn),
 DD(y1, . . . , yn).

is the result of viewing R.C as the open first-order formula
y.FR(x, y)  FC(y) (where F is the obvious translation of roles and
concepts into first-order logic (FOL) [3]) and the existential quantifier  is viewed as a disjunction over the elements of the domain.

(y)

y IR

(x, y)  C

(x) = inf

Similarly,
(R.C)
is related to the open first-order formulay.FR(x, y)  FC(y), where
the universal quantifier  is viewed as a conjunction over the elements of the domain. However, unlike the classical case, in general,
we do not have that (R.C)
. For example, this holds
in ukasiewicz logic, but not in G  odel logic. Also interesting is that
(see [43]) the axiom  (R.A)  (R.A) has no classical model,
but it has a fuzzy one. Indeed, in [43], it is shown that in G  odel logic
it has no finite model, but it has an infinite fuzzy model.
The semantics of the concept nR is equivalent to
( nR)

Another point concerns the semantics of number restrictions.

(x, yi)  1i<jnyi /= yj,

I = (R.C)

(x) =

sup

i=1R

{y1,...,yn}I

which is the result of viewing nR as the open first-order formula
y1, . . . , yn  n
That is, there are at least n distinct elements that satisfy to some
degree R(x, yi). This also guarantees that R.  ( 1R).

i=1R(x, yi)  1i<jnyi /= yj.

Similarly, the semantics of nR is equivalent to

( nR)

(x) =

inf

{y1,...,yn+1}I

n+1
i=1 R

(x, yi) 

1i<jn+1yi = yj

i=1 R(x, yi)  1i<jn+1yi = yj.

which is the result of viewing nR as the open first-order formula
y1, . . . , yn+1  n+1
Note that not necessarily ( nR)  ( n + 1R) holds. The equivalence is true for Zadeh and ukasiewicz logic, but neither for G  odel
nor for Product logic.
We extend I to all non-fuzzy axioms as follows (where a, b I
and v  D):
I = inf
(C  D)
I = inf
(R  S)
I =
(T  U)
I = C
(a : C)
((a, b) : R)
((a, v) : T)

x  IC
x,y IR
inf
(x,y) ID
I(a
I),
I = R

I(a
I),
, b
I = T
I(a

, vD).

(x)  D

(x, y)  S

(x, y),

(x, y)  U

(x),

(x, y),

Note here that, e.g., the semantics of a concept inclusion axiom
C  D is derived directly from its FOL translation, which is of
the form x.FC(x)  FD(x). This definition is clearly different from
the approaches in which C  D is viewed as x.C(x)  D(x) (e.g.,
[125,122]). This latter approach has the effect that the subsumption relationship is a classical{0, 1}relationship, while in the former
approach, subsumption is determined up to a certain degree in [0,
1].
We next define what it means that a fuzzy interpretation I satisfies a fuzzy axiom E, or I is a model of E, denoted I  E, as follows.
We define: (1) I  Trans(R) iff R
I(z, y)
for all x, y 
I, (2) I    
 n, where 
 {,, >, <}, iff  

 n, (3)
I  a = b iff a
I = b
I. We say that a con-
,I) and
cept C is satisfiable iff there exists an interpretation I = (

an individual x  
I(x) > 0. We say that I satisfies a set
of fuzzy axioms E, or I is a model of E, denoted I  E, iff I  E for all
E E. We say I satisfies a fuzzy knowledge base K = (R,T,A), or I is
a model of K, denoted I  K, iff I is a model of T  R  A. We say K is
satisfiable iff it has a model. A fuzzy axiom E is a logical consequence
of a fuzzy knowledge base K, denoted K  E, iff every model of K
satisfies E.

I, and (4) I  a /= b iff a
I such that C

I(x, y)  supz  I R

I(x, z)  R

I /= b

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

Example 6.1 (Car example continued). Example 3.1 illustrates an
evident difficulty in defining the class of sports cars. Indeed, it is
highly questionable why a car whose maximum speed is 243 km/h
is not a sports car anymore. Essentially, the higher the maximum
speed, the more closely a car is a sports car, which makes the concept of a sports car a fuzzy concept, that is, a vague concept, rather
than a crisp one. In the next section, we see how to represent such
concepts more appropriately. Let us now reconsider Example 3.1,
where all the axioms of the TBox and ABox are asserted with degree
1, that is, are of the form    1. We replace the definition of SportsCar
by the definition in (6). Then, we have that (under ukasiewicz
logic)
K  SportsCar  Car  1, K  mgb : SportsCar  0.28,
K  enzo : SportsCar  1, K  tt : SportsCar  0.92.
Note that the maximal speed limit of the mgb car (170 km/h) induces
the upper limit 0.28 of the membership degree.
Example 6.2. Consider the knowledge base K with the definitions
in (4) and (5). Then, under ukasiewicz logic, we have that (see
[129])
K  Minor  YoungPerson  0.6,
K  YoungPerson  Minor  0.4,
which are relationships not captured with classical SHOIN(D).
An interesting point here is that according to the semantics of
fuzzy SHOIN(D), e.g., a minor is a young person to a certain degree
and is obtained without explicitly mentioning it. This inference
cannot be achieved in classical SHOIN(D). Similarly, referring to
Example 3.1, in fuzzy SHOIN(D), the car tt is a sports car to a certain degree. Therefore, unlike Example 3.1, tt is now closely a sports
car, as it should be.

6.3. Main reasoning problems

In addition to the standard problems of deciding the satisfiability of fuzzy knowledge bases, deciding the satisfiability of
concepts relative to fuzzy knowledge bases, and deciding logical
consequences of fuzzy axioms from fuzzy knowledge bases, two
other important reasoning problems are the best truth value bound
(BTVB) problem and the best satisfiability bound problem, which
we describe in the following.
Given a fuzzy knowledge baseK and a classical axiom  , where  
is neither a transitivity axiom nor an equality or inequality axiom,
it is of interest to compute  s best lower and upper truth value
bounds (best truth value bound). The greatest lower bound of   relative to K, denoted glb(K,  ), is defined by
glb(K,  ) = sup{n|K     n},
while the least upper bound of   relative to K, denoted lub(K,  ), is
defined by
lub(K,  ) = inf{n|K     n},
where sup = 0 and inf  = 1. For example, the logical consequences in Examples 6.1 and 6.2 contain the best truth value
bounds. Furthermore, note that
lub(K, a : C) =  glb(K, a : C),

(7)

that is, the lub can be determined through the glb (and vice versa).
Similarly, lub(K, (a, b) : R) =  glb(K, a : R.{b}). Note also that
K     n iff glb(K,  )  n, and K     n iff lub(K,  )  n.

Fig. 2. The soft price constraints.

Finally, the best satisfiability bound of a concept C relative to K,

denoted glb(K, C), is defined by
glb(K, C) = sup

(x)|I  K}.

sup
x  I

{C

Intuitively, among all models I of K, we determine the maximal
degree of truth that the concept C may have over all individuals

I.
Example 6.3. Consider the knowledge base K in Example 3.1.
Assume that a car seller sells an Audi TT for $31 500, as from the
catalog price. A buyer is looking for a sports car, but wants to pay not
more than around $30 000. In classical description logics no agreement can be found. The problem relies on the crisp condition on the
sellers and the buyers price. A more fine-grained approach would
be to consider prices as concrete fuzzy sets instead. For example,
the seller may consider optimal to sell above $31 500, but can go
down to $30 500. The buyer prefers to spend less than $30 000, but
can go up to $32 000. We may represent these statements by means
of the following axioms (see Fig. 2):
AudiTT = SportsCar  hasPrice.rs(x; 30 500, 31 500),
Query = SportsCar  hasPrice.ls(x; 30 000, 32 000).

Then, we may find out that the highest degree to which the concept C = AudiTT  Query is satisfiable is 0.75 (the possibility that
the AudiTT and the query matches is 0.75). That is, glb(K, C) = 0.75
and corresponds to the point where both requests intersect (that
is, the car may be sold at $31 250).

Problems such as determining the greatest lower bound of
an axiom can be solved by relying on mixed integer linear programming (MILP) (see, e.g., [9,128,130,140,139]). Roughly, the basic
idea is as follows. Consider a fuzzy knowledge base K = (R,T,A).
To determine the greatest lower bound of an axiom, we combine appropriate description logic tableaux rules with methods
developed in the context of many-valued logics [40]. For exam-
ple, to determine, e.g., glb(K, a : C), we consider an expression of
the form a : C  x, where x is a [0, 1]-valued variable. Then, we
construct a tableaux for K = (R,T,A  {a : C  x}) in which the
application of satisfiability preserving rules generates new assertion axioms together with inequations over [0, 1]-valued variables.
These inequations have to hold in order to respect the semantics of
the description logic constructors. Finally, to determine the greatest
lower bound, we minimize the original variable x such that all con-

straints are satisfied.3 Interestingly, under ukasiewicz and Zadeh
logic, we end up with a mixed integer linear programming optimization problem, while under the Product logic, we end up with a
mixed integer quadratically constrained programming (MIQCP) optimization problem [111]. Similarly, glb(K, C1  C2) is the minimal
value of x such that K = (R,T,A  {a : C1  C2  1  x}) is satisfi-
able, where a is new individual. Therefore, the greatest lower bound
problem can be reduced to the minimal satisfiability problem of
a fuzzy knowledge base. Furthermore, glb(K, C) is determined by
the maximal value of x such that (R,T,A  {a : C  x}) is satisfiable,
where a is a new individual. In summary,
glb(K, a : C) = min x such that(R,T,A  {a : C  x}) satisfiable,
glb(K, C1  C2) = min x such that(R,T,A  {a : C1  C2  1  x})
satisfiable,
glb(K, C) = max x such that(R,T,A  {a : C  x}) satisfiable.

6.4. Main applications

Fuzzy logic has numerous practical applications in general (see,
e.g., [67]). Related to fuzzy description logics, we point out that
they have first been proposed for logic-based information retrieval
[96], which originated from the idea to annotate textual documents
with graded description logic sentences, which goes back to [97].
The idea has been reconsidered in [120,141,156]. In particular, (i)
Zhang et al. [156] describe a semantic portal that is based on fuzzy
description logics; (ii) Li et al. [76] present an improved semantic search model by integrating inference and information retrieval
and an implementation in the security domain; (iii) Straccia and
Visco [141] report on a multimedia information retrieval system
based on a fuzzy DLR-Lite description logic, which is capable to deal
with hundreds of thousands of images. DAquin et al. [17] provide
a use case in the medical domain, where fuzzy concrete domains
are used to identify tumor regions in X-ray images. Agarwal and
Lamparter [1] use fuzzy description logics to improve searching
and comparing products in electronic markets. They provide a more
expressive search mechanism that is closer to human reasoning and
that aggregates multiple search criteria to a single value (ranking
of an offer relative to the query), thus enabling a better selection
of offers to be considered for the negotiation. Liu et al. [81] use a
fuzzy description logic to model the management part in project
selection tasks.

6.5. Other vague ontology languages

There are several extensions of description logics respectively
ontology languages using the theory of fuzzy logic in the literature.
They can be classified according to (a) the description logic respectively ontology language that they generalize, (b) the allowed fuzzy
constructs, (c) the underlying fuzzy logics, and (d) their reasoning
algorithms.
The first work is due to Yen [154], who proposes a fuzzy extension of a very restricted sublanguage of ALC, called FL [12,73].
The work includes fuzzy terminological knowledge, but no fuzzy
assertional knowledge, and it is based on Zadeh logic. It already
informally talks about the use of fuzzy modifiers and fuzzy concrete
domains. Though, the unique reasoning facility, the subsumption
test, is a crisp yes/no questioning. Tresp and Molitor [144] consider
a more general extension of fuzzy ALC. Like Yen, they also allow for

3 Informally, suppose the minimal value is  n (if no such value exists, then K is
not satisfiable). We know then that for any interpretation I satisfying K such that
I   n holds. This means
(a : C)
that glb(K, a : C) =  n.

<  n, the starting set is unsatisfiable, and thus (a : C)

fuzzy terminological knowledge along with a special form of fuzzy
modifiers (which are a combination of two linear functions), but
no fuzzy assertional knowledge, and they assume Zadeh logic as
underlying fuzzy logic. The work also presents a sound and complete reasoning algorithm testing the subsumption relationship
using a linear programming oracle.
Another fuzzy extension of ALC is due to Straccia [123,125],
who allows for both fuzzy terminological and fuzzy assertional
knowledge, but not for fuzzy modifiers and fuzzy concrete domains,
and again assumes Zadeh logic as underlying fuzzy logic. Straccia [123,125] also introduces the best truth value bound problem
and provides a sound and complete reasoning algorithm based on
completion rules. In [124], Straccia reports a four-valued variant
of fuzzy ALC. In the same spirit, H  olldobler et al. [50,51] extend
Straccias fuzzy ALC with concept modifiers of the form fm(x) = x,
where  > 0, and present a sound and complete reasoning algorithm (based on completion rules) for the graded subsumption
problem.

Straccias works [127,133,138] are essentially as [125], except
that now the set of possible truth values is a complete lattice rather
than [0, 1].
Sanchez and Tettamanzi [112114] consider a fuzzy extension
of the description logic ALCQ (without assertional component)
under Zadeh logic, and they start addressing the issue of a fuzzy
semantics of quantifiers. Essentially, fuzzy quantifiers allow to
state concepts such as FaithfulCustomer  (Most)buys.LowCalorie-
Food encoding the set of all individuals that mostly by low calorie
food. An algorithm is presented, which calculates the satisfiability
interval for a fuzzy concept.
H  ajek [43,44] considers a fuzzy extension of the description
logic ALC under arbitrary t-norms. He provides in particular algorithms for deciding whether C  D  1 is a tautology and whether
C  D  1 is satisfiable, which are based on a reduction to the
propositional BL logic for which a Hilbert-style axiomatization
exists [42](but see also [44] for the complexity of rational Pavelka
logic, and see [10] for some complexity results on reasoning in fuzzy
description logics).
Straccia [126] provides a translation of fuzzy ALC (with general concept inclusion axioms) into classical ALC. The translation
is modular, and thus expected to be extendable to more expressive
fuzzy description logics as well. The main idea is to translate a fuzzy
assertion of the form a : C  n into a crisp assertion a : Cn, with the
intended meaning a is an instance of C to degree at least n. Then,
concept inclusion axioms are used to correctly relate the Cns. For
example, C0.7  C0.6 is used to encode that whenever an individual
is an instance of C to degree at least 0.7, then it is also an instance
of C to degree at least 0.6. The translation is at most quadratic in
the size of the fuzzy knowledge base. Note that the translation does
not yet work in the presence of fuzzy modifiers and fuzzy concrete
domains. Bobillo et al. [8] extend the approach to a variant of fuzzy
SHOIN. The idea has further been considered in the works [78,79],
which essentially provide a crisp language in which expressions of,
e.g., the form a : R0.8.C0.9 are allowed, with the intended meaning
if a has an R-successor to degree at least 0.8, then this successor
is also an instance of C to degree at least 0.9. The idea has further
been extended to a distributed variant of fuzzy description logics
in [82].

In [94], a fuzzy extension (based on Zadeh logic) of CARIN [74]
is provided, which combines fuzzy description logics with nonrecursive Horn rules.

Other extensions of fuzzy description logics concern their integration with fuzzy logic programs, which however goes beyond
the scope of the present paper (see, e.g., [138,135,133,89,91,149]).
An interesting extension is due to Kang et al. [64], who extends
fuzzy description logics by comparison operators, e.g., to state that

T. Lukasiewicz, U. Straccia / Web Semantics: Science, Services and Agents on the World Wide Web 6 (2008) 291308

Tom is taller than Tim. Another interesting extension is proposed
by Dubois et al. [22], who combine fuzzy description logics with
possibility theory. Essentially, since a : C  n is Boolean (either an
interpretation satisfies it or not), we can build on top of it an uncertainty logic, which is based on possibility theory in [22].

We recall that usually the semantics used for fuzzy description
logics is based on Zadeh logic, but where the concept inclusion is
crisp, that is, C  D is viewed as x.C(x)  D(x). In [52,144], a calculus for fuzzy ALC [115] with fuzzy modifiers and simple TBoxes
under Zadeh logic is reported. No indication for the BTVB problem is given. Straccia [123,125] reports a calculus for fuzzy ALC and
simple TBoxes under Zadeh logic and addresses the BTVB prob-
lem. How the satisfiability problem and the BTVB problem can be
reduced to classical ALC, and thus can be solved by means of tools
like FaCT and RACER is shown in [126]. Results providing a tableaux
calculus for fuzzy SHIN under Zadeh logic (but only allowing for
a restricted form of concept inclusion axioms, which are called
fuzzy inclusion introductions and fuzzy equivalence introductions),
by adapting similar techniques as for the classical counterpart, are
shown in [120,121]. Fuzzy general concept inclusion axioms under
Zadeh logic can be managed as described in [122]. Also interesting is
the work [77], which provides a tableau for fuzzy SHI with general
concept inclusion axioms. Finally, the reasoning techniques for classical SHOIN(D) [57] can be extended to [125], as [120,121,118,117]
already show.

On the other hand, fuzzy tableaux algorithms under Zadeh
semantics do not seem to be suitable to be adapted to other seman-
tics, such as ukasiewicz logic. Even more problematic is the fact
that they are yet unable to deal with fuzzy concrete domains.
Despite these negative results, recently, Straccia [130,128] report a
calculus for fuzzy ALC(D) whenever the connectives, the modifiers,
and the fuzzy datatype predicates are representable as bounded
mixed integer linear programs (MILPs). For example, ukasiewicz
logic satisfies these conditions as well as the membership functions
for fuzzy datatype predicates that we have presented in this paper.
Additionally, modifiers should be a combination of linear functions.
In that case, the calculus consists of a set of constraint propagation rules and an invocation to an oracle for MILP. The method has
been extended to fuzzy SHIF(D) [139](the description logic behind
OWL Lite). The use of MILP for reasoning in fuzzy description logics
is not surprising as their use for automated deduction in manyvalued logics is well known [39,40]. Bobillo and Straccia [9] provide
a calculus for fuzzy ALC(D) under product semantics.

A very recent problem for fuzzy description logics is the top-k
retrieval problem. While in classical semantics, a tuple satisfies or
does not satisfy a query, in fuzzy description logics, a tuple may
satisfy a query to a degree. Hence, for example, given a conjunctive
query over a fuzzy description logic knowledge base, it is of interest
to compute only the top-k answers. While in relational databases,
this problem is a current research area (see, e.g., [31,60,75]), very
few is known for the case of first-order knowledge bases in general (but see [136]) and description logics in particular. The only
works that we are aware of are [132,137,141], which deal with
the problem of finding the top-k result over knowledge bases in a
fuzzy generalization of DL-Lite [13](note that [103] is subsumed by
[137]).

7. Conclusion

Handling uncertainty and vagueness has started to play an
important role in ontology languages for the Semantic Web. In this
paper, we have first provided a brief introduction to uncertainty
and vagueness at the propositional level, and we have summarized
the basics of classical description logics for the Semantic Web. We

have then described the most prominent approaches to handling
probabilistic uncertainty, possibilistic uncertainty, and vagueness
in expressive description logics for the Semantic Web, and we have
given an overview of related approaches.

There are many important aspects that are open for future
research. In particular, an important issue is to develop more scalable formalisms for handling probabilistic uncertainty, possibilistic
uncertainty, and vagueness in ontology languages for the Semantic
Web, especially those scalable formalisms that are also practically
relevant. Another important issue is to provide more implementa-
tions, especially of scalable formalisms. It would also be interesting
to integrate the above forms of uncertainty and vagueness in a single description logic for the Semantic Web. Another interesting
issue for future research is the integration of probabilistic, possi-
bilistic, and fuzzy description logics with rule-based languages for
the Semantic Web.

Acknowledgments

This work has been partially supported by the German Research
Foundation (DFG) under the Heisenberg Programme. We thank the
reviewers of this paper, whose constructive comments helped to
improve our work.
