Web Semantics: Science, Services and Agents

on the World Wide Web 2 (2004) 4979

Semantic annotation, indexing, and retrieval


Atanas Kiryakov

, Borislav Popov, Ivan Terziev,

Dimitar Manov, Damyan Ognyanoff

Ontotext Lab, Sirma AI EAD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria

Received 11 March 2004; received in revised form 20 July 2004; accepted 27 July 2004

Abstract

The Semantic Web realization depends on the availability of a critical mass of metadata for the web content, associated with
the respective formal knowledge about the world. We claim that the Semantic Web, at its current stage of development, is in a
state of a critical need of metadata generation and usage schemata that are specific, well-defined and easy to understand. This
paper introduces our vision for a holistic architecture for semantic annotation, indexing, and retrieval of documents with regard
to extensive semantic repositories. A system (called KIM), implementing this concept, is presented in brief and it is used for the
purposes of evaluation and demonstration.

A particular schema for semantic annotation with respect to real-world entities is proposed. The underlying philosophy is that
a practical semantic annotation is impossible without some particular knowledge modelling commitments. Our understanding
is that a system for such semantic annotation should be based upon a simple model of real-world entity classes, complemented
with extensive instance knowledge. To ensure the efficiency, ease of sharing, and reusability of the metadata, we introduce an
upper-level ontology (of about 250 classes and 100 properties), which starts with some basic philosophical distinctions and then
goes down to the most common entity types (people, companies, cities, etc.). Thus it encodes many of the domain-independent
commonsense concepts and allows straightforward domain-specific extensions. On the basis of the ontology, a large-scale
knowledge base of entity descriptions is bootstrapped, and further extended and maintained. Currently, the knowledge bases
usually scales between 105 and 106 descriptions.

Finally, this paper presents a semantically enhanced information extraction system, which provides automatic semantic
annotation with references to classes in the ontology and to instances. The system has been running over a continuously growing
document collection (currently about 0.5 million news articles), so it has been under constant testing and evaluation for some
time now. On the basis of these semantic annotations, we perform semantic based indexing and retrieval where users can mix
traditional information retrieval (IR) queries and ontology-based ones. We argue that such large-scale, fully automatic methods
are essential for the transformation of the current largely textual web into a Semantic Web.
 2004 Elsevier B.V. All rights reserved.

Keywords: Semantic annotation; Semantic metadata; Information retrieval


Corresponding author. Tel.: +359 29768 303; fax: +359 29768 311.
E-mail addresses: naso@sirma.bg (A. Kiryakov), borislav@sirma.bg (B. Popov), ivanterziev@sirma.bg (I. Terziev), mitac@sirma.bg

(D. Manov), damyan@sirma.bg (D. Ognyanoff).

1570-8268/$  see front matter  2004 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2004.07.005

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

1. Introduction

2. The missing fibers of the Semantic Web

Semantic Web is about adding formal structure and
semantics (metadata and knowledge) to web content
for the purpose of more efficient management and
access. Since the realization of this vision depends on
the presence of a critical mass of metadata, the acquisition of this metadata stands as a major challenge to
the Semantic Web community. Taking into account the
millions of web pages already in existence, the manual
accumulation of such explicit semantics is not considered a feasible approach, despite that it is unavoidable
in some cases. Our vision is that fully automatic
methods for semantic annotation should be researched
and developed. To this end, the necessary design and
modelling questions should be identified and resolved,
and also complementary resources and an infrastructure should be enabled. Should automatic semantic
annotation systems be ensured to get wide acceptance
and usage, then their tasks ought to be clearly defined;
their performance-properly evaluated; and their benefits and limitations  clearly understood by the target
users.

With our work, we aim to create an efficient, robust,
and scalable architecture for automatic semantic annotation (also called semantic tagging [27]), and to
implement this architecture in a component-based platform for semantic-based indexing and retrieval over
large document collections. What is considered our
central innovative contribution is the fact that we offer an end-to-end, extendable system, which addresses
the complete cycle of metadata creation, storage, and
semantic-based search, and which includes a set of
front-ends for online use that offer semantically enhanced browsing.

This paper is structured as follows. Section 2 opens
a general discussion on the Semantic Web. Sections 3
and 4 define the requirements and the tasks in semantic
annotation. Section 5 presents a semantic annotation
model, consisting of a light-weight ontology, a semantic repository, and a metadata storage model. Section 6
defines the different stages in the annotation process.
Our implementation of the KIM platform for semantic annotation, indexing, and retrieval, is presented in
detail in Section 7, including evaluation and end-user
tools. Section 8 reviews related work, followed by Section 9, which concludes the paper by discussing future
extensions and improvements.

We are going to start this discussion with a highlevel analysis of the Semantic Web in order to make our
motivation for the development of the semantic annotation notion, presented in the paper, more clear. In our
viewpoint, the Semantic Web at present offers a combination of a high-level vision and low-level standards.
The situation can be illustrated with the following anal-
ogy. Suppose you are feeling depressed and you know
that this can be improved by some entertainment, and
that the latter is probably going to use up some money.
Unfortunately, surfs, hiking, theaters, bars, rakia, and
any other concepts, related to entertainment, are still
not thought out. And, of course, there are no specific
surfs, theaters, bars, and bottles of rakia, and nobody
considers the mountains as a subject of entertainment.
So, one can end up in a situation where there are some
ways to make and manage money, but no specific ideas
and opportunities on how to spend them exist.

Similarly, because there is a shared perception of
a problematic information overload within the current
WWW (actually, the depression by the analogy, mentioned above), the Semantic Web offers that there
should be metadata to structure it (analogously to the
high-level notion of entertainment) and it should be
in RDF(S), here are the repositories to manage it (the
money and the banks). However, this still fails to provide sufficient guidance for the development and usage of the metadata. At present, in the Semantic Web
country there are legislations and banks, but there is no
economy. There is no production and consumption of
metadata.

Our understanding is that what miss in this aspect are
simple, well-defined, measurable, widely understood
tasks; specific practices and approaches for performing
them; tools, resources, and industry support. But, as
always, the tools come after the tasks.

2.1. Birds eye view of the data model of the web

If we abstract the current web away from the trans-
port, content type, and content formatting aspects, it
could be regarded as a set of documents where there is
some metadata, attached to the documents (title, key-
words, etc.), and there are hyperlinks among the documents (see the left-hand side part of Fig. 1. What does
the Semantic Web add to this picture? Semantic meta-

Fig. 1. The current WWW (left) and the Semantic Web (right).

data of different sorts, both on document level (i.e. attached to the whole document) and on content level
(i.e. attached to particular parts and positions within
the document). As shown in the right-hand side part
of Fig. 1, from a birds eye view, the Semantic Web is
more . . . colorful and rich than the current one.

2.2. What is the semantic metadata about?

On our view, in order to find or define the added
value of the Semantic Web, it is crucial to elaborate a
bit more over the nature of the semantic metadata.

Suppose, we add a tag <2134> to some portion of
a document as follows . . . Abc <2134>xyz</2134>
. . .. Is this metadata useful? Can we call it semantic?
Without further assumptions, the answers are negative.
In order to have metadata useful in a Semantic Web
context, it should mean something, i.e. the symbols (or
references) that constitute it should allow additional
interpretation.
Interpretation means an approach,
allowing the assigning of something additional to the
symbols (i.e. adding information value) with respect to
a certain model or context. In a knowledge representation (KR) context, this is what is considered an assignment of meaning or semantics to symbols, expressions,
etc. It is important to realize that interpretation is only
possible with respect to something; to some domain,
model, context (possible) world. This is the domain
that (the interpretations of) the symbols are about.

In cases when the domain of interpretation is not
clearly defined, this means that it is obvious and/or it
is too fuzzy and complex to be formally named and
discussed. In literature, in the process of writing, the
context of interpreting what is being written, is totally

open-authors often write with the intention to allow
multiple and even ambiguous interpretations. This allows for freedom of interpretation to the reader, which
is nice when there is a human reader. In the context of
the Semantic Web, however, the interpretations should
be performed automatically by machines in strict and
 as far as possible  a most predictable fashion. This
requires a formal definition of the interpretation and,
because of this, a formal definition of the context. Assuming that one and the same context can be modelled
in different ways, allowing different (and potentially
ambiguous) interpretations, what has to be specified is
the conceptualisation  as defined in [32]: a conceptualization is an abstract, simplified view of the world
that we wish to represent for some purpose. This is
where ontologies rise up to act as logical theories for
the formal specification of a conceptualization (again
in [32]).

In this way, it becomes obvious that one cannot
get to useful Semantic Web applications if he/she
only considers annotations in RDF(S), OWL, or some
other language. Annotations could be expressed in
RDF(S), but they are not about RDF(S). The metadata is used to provide some partial formalization of
the content of the documents as a prerequisite for
more comprehensive management. The content on the
web, as a product of human civilization, is mostly
about the world. It covers both phenomena in the
real world or citizens in some more intangible mental spaces, indirectly referring to the reality. Although
the above understanding is quite informal and speculatively presented, we consider it rather important for
the realization of the Semantic Web, and depicted in
Fig. 2.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

Fig. 2. Metadata about the world, not about RDF.

Fig. 3. Metadata referring to world knowledge.

Further, as mentioned above,

the metadata can
hardly refer to (or be interpreted directly with respect
to) the world. Such references cannot be formal and
unambiguous. What the semantic metadata can be expected to refer to directly is a formal model of the world
(see Fig. 3). Section 5.4 provides a discussion on the
way this formal model is usually structured according
to the KR tradition.

2.3. A tinge of philosophy

Modelling knowledge about

the world requires
some assumptions about its nature, as well as about
the nature of the observer who is expected to use,
understand, and rely on the models. These issues have
been studied for thousands of years from the philoso-
phers. Without getting into a serious discussion, we
find the philosophy of Objectivism of Ayn Rand most
close the objectives, capabilities, and practices of the
contemporary KR and its usage in the Semantic Web
context.

Below we copy a brief summary of the Objectivism,

taken from [33]:

Existence and consciousness are facts implicit in every
perception. They are the base of all knowledge (and the
precondition of proof: knowledge presupposes something to know and someone to know it. They are absolutes which cannot be questioned or escaped: every
human utterance, including the denial of these axioms,
implies their use and acceptance.

The third axiom at the base of knowledge  an axiom
true, in Aristotles words, of being qua being  is
the Law of Identity. This law defines the essence of
existence: to be is to be something, a thing is what it
is; and leads to the fundamental principle of all action,
the law of causality. The law of causality states that a
things actions are determined not by chance, but by its
nature, i.e., by what it is.

It is important to observe the interrelation of these three
axioms. Existence is the first axiom. The universe exists
independent of consciousness. Man is able to adapt
his background to his own requirements, but Nature,
to be commanded, must be obeyed (Francis Bacon).
There is no mental process that can change the laws of

nature or erase facts. The function of consciousness is
not to create reality, but to apprehend it. Existence is
Identity, Consciousness is Identification.

The philosophic source of this viewpoint and its major
advocate in the history of philosophy is Aristotle. Its
opponents are all the other major traditions, including Platonism, Christianity, and German idealism. Directly or indirectly, these traditions uphold the notion
that consciousness is the creator of reality. The essence
of this notion is the denial of the axiom that existence
exists.

It

is important

to mention  however, without
trying to judge or advocate on reasons and motivation
 that most of todays information science and KR
approaches implicitly assume the existence of an objective reality. Ontologies aid the sharing of knowledge on
the basis of the assumption that there is a single reality
and the sharing is a matter of aligning the way different
people or systems think about it. The management
of alternative realities is possible in some logic dialects
(starting with the modal logics), but usually much
more complex and computationally expensive. Hypo-
thetically, if KR was following Idealism, then the sort
of quasi-consciousness, developed with KR methods,
would lead to the creation of a quasi-reality. Although
a possible view, the latter is hardly matching the expectations towards the Semantic Web. Thus, although it
is hard to decide on its theoretical soundness, it seems
that the Objectivism is in line with some practical
decisions to be made in Semantic Web applications
of KR.

We introduced the above philosophical comments,
because when one is trying to exhaustively model huge
amounts of world knowledge in a context as general as
the Semantic Web, there are a number of modelling
issues to be resolved, which depend on whether one
assumes existence of objective reality or not, as well as
on other basic philosophical distinctions.

3. Information need definition and satisfaction

One of the obstacles towards the realization of the
Semantic Web is that there is a general vision about it,

but no well-defined information access methods1 exist.
For instance, the classical IR has a single, basic, wellestablished and understood way of defining and satisfying the information need:2 it is defined as a set of words
(tokens) of interest and is satisfied with a list of documents relevant to those words. As regards the domain of
relational databases, the information need is defined as
an SQL query and satisfied, typically, through a result
table.

- How is the information need defined and satisfied

within the Semantic Web?

The above question is stated faultily  like the current web, the Semantic Web could not be expected to
have a single access method, and therefore a single approach for the definition of the information need. The
following questions seem more relevant:

- How does the Semantic Web extend the existing ac-

cess methods?

- What new access methods become feasible?

The answers that could be of any concern should
unite the following elements: conscious user needs
(at least needs, which can be understood and adapted
to replace other ones that are harder to satisfy), a
sound scientific theory, and a robust technology, which
can implement applications based on the theory, and
which at the same time are efficient enough in satisfying the needs. At the end of the day, this is all
about efficiency and expectations management. Let
us take as an example the keyword-based search en-
gines. These are far not perfect: the information need
is poorly defined and imprecisely satisfied  indeed,
one would prefer to ask questions and receive answers,
rather than construct a pseudo-question by carefully
compiling a set of words for the query and then get
a pile of documents back, which would surely answer the pseudo-question, and which at the same time
would probably, somehow, maybe, partially answer
the real question behind the users forehead. How-
ever, search engines are popular because they meet

1 An information access method could be considered as a
paradigm for (i) definition of the information need (the question)
and (ii) the way it is being satisfied (the answer/result type).

2 Of course, this statement reflects and represents an oversimplified view on what IR really does, but still this is the level of understanding that most users are up to, and it allows them to make use of
the technology.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

a number of conditions that are critical for a wide
acceptance of an information access method in web
context:

I. To significantly improve the efficiency of access-

ing the content on the web.

II. To provide that no additional skills, effort, disci-
pline, good will, and correctness are required from
the authors.

III. To offer somehow predictable behavior and per-

formance.

4. Semantic annotation

Semantic annotation is a specific metadata generation and usage schema, aiming to enable new information access methods and to extend the existing ones.
The annotation scheme, offered here, is based on the
conception that named entities (NE, see Section 4.2)
constitute an important part of the semantics of the documents they are mentioned in. Moreover, via the use
of different sorts of redundancy and external or background knowledge, those entities can be coupled with
formal descriptions and thus provide more semantics
and connectivity to the web.

In a nutshell, semantic annotation is about assigning to the entities in the text links to their semantic descriptions (as presented in Fig. 4). This sort of
metadata provides both class and instance information
about the entities. Whether these annotations should
be called semantic, entity or some other way, it is
all a matter of terminology. To the best of our knowl-
edge, there neither exists a well-established term for
this task, nor there is a well-established meaning for the
term semantic annotation. What is more important is
that the automatic semantic annotations enable many
new types of applications: highlighting, indexing and
retrieval, categorization, generation of more advanced
metadata, smooth traversal between unstructured text
and available relevant knowledge. Semantic annotation
is applicable for any sort of text  web pages, regular (non-web) documents, text fields in databases, etc.
Further, knowledge acquisition can be performed on
the basis of the extraction of more complex dependencies  analysis of relationships between entities, event
and situation descriptions, etc.

4.1. Tasks

We hope that the expectations in respect of the Semantic Web would be easier to meet and make happen
should the following basic tasks be properly defined
and solved:

1. Annotate and hyperlink (references to) named enti-

ties in textual (parts of) documents formally.

2. Index and retrieve documents with respect to the

entities referred to.

The first task could be pictured as a canvas where
an advanced blend of paints are used: a basic pressclipping exercise, a typical IE3 task, and automatic
hyper-linking. The resulting annotations basically represent a method for document enrichment and presen-
tation, the results of which can be further used to enable
other access methods.

The second task is just a modification of the classical IR task  documents are retrieved on the basis of
relevance to NEs instead of words. However, the basic
assumption is quite similar  a document is characterized by the bag of tokens (or atomic text entities,
as those are referred to in [14]) which constitute its
content, disregarding its structure. While the basic IR
approach considers the word stems as tokens, there has
been considerable effort for the last decade towards using word-senses or lexical concepts (see [17] and [25])
for indexing and retrieval. Named entities can be seen
as a special sort of a token to be taken care of. What
we present here is one more (pretty much independent)
development direction instead of an alternative of the
contemporary IR trends.

4.2. Named entities

In the natural language processing (NLP) field, and
particularly the information extraction (IE) tradition,
named entities (NE) are considered: people, organiza-
tions, locations, and others, referred to by name [26].
By a wider interpretation, these also include scalar values (numbers, dates, amounts of money), addresses,
etc.

3 Information extraction, a relatively young discipline in the natural language processing (NLP), which conducts partial analysis of
text in order to extract specific information [4].

Fig. 4. Semantic annotation.

NEs should be handled in a different, special way
because of their different nature and semantics4 compared to words (terms, phrases, etc.). While the former
denote particulars (individuals or instances), the latter denote universals (concepts, classes, relations, at-
tributes). While words can be described via the means
of lexical semantics and common sense, the understanding and the managing of named entities require
certain more specific world knowledge.

5. Semantic annotation model and
representation

In this section we discuss the structure and the representation of semantic annotations, including the necessary knowledge and metadata. There are a number of
basic prerequisites for the representation of semantic
annotations:
 an ontology (or taxonomy, at the least), defining the
entity classes; it should be possible for these classes
to be referred to;

4 Without trying to discuss what semantic means in general, we
reduce it to the simplified a model or description of an object which
allows further interpretation (see Section 2.2).

 entity identifiers, which allow those to be distinguished and linked to their semantic descriptions;
 a knowledge base with entity descriptions.

Entity descriptions actually make up the nonontological hemisphere (of formal knowledge) in the
semantic stores brain  they represent the other,
complementary, parallel aspect of the representation
of semantic annotations. Unlike the stringent rules of
entity representation in an ontology, entity descriptions
(usually organized in a body of formal knowledge that
we call a knowledge base  see Section 5.4), provide
a more flexible and broad-spectrum way for the iden-
tification, representation, description, and general interlinking of entities. Entity descriptions are typically
instance knowledge/data, representing two types of entity knowledge-descriptions and relationships. Thus we
have the chance, via the use of entity descriptions, to
extend the amount of knowledge/data that we need in
the respective application domain by populating the
knowledge base with specific entities, which sometimes do not fit the design of the ontology or just need
a more extensive elucidation.

To embed or not to embed?  that is the ques-
tion, which concerns an important choice that should
be made in the representation of annotations. Although
embedded annotations seem easier to maintain, there

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

are a number of arguments, giving evidence that semantic annotations have to be decoupled from the content
they refer to. One key reason for this is the ambition
to allow for dynamic, user-specific, semantic annotations  conversely, embedded annotations become a
part of the content and may not change according to
the interest of the user or to the context of usage.

Further, complex embedded annotations would have
a negative impact on the volume of the content and
could complicate its maintenance  e.g. imagine that
a page with three layers of overlapping semantic annotations needs to be updated without disintegrating
their consistency. Those and a number of other issues,
defending the externally encoded annotation, can be
found in [23], which also presents an interesting parallel to open hypermedia systems.

Once laid down that semantic annotations should
be kept separate from the content, the next question is
whether or not (or to what an extent) should the annotations be coupled with the ontology and the knowledge
base. It is the case that such an integration seems profitable  it would be easier to keep the annotation in
sync with the class and entity descriptions. However,
there are at least three important considerations, as fol-
lows:
 Both the cardinality and the complexity of the annotations differ from those of the entity descriptions
 the annotations are simpler, but their count is
usually much bigger than the one of the entity de-
scriptions. Even considering middle-sized corpora
of documents, the number of annotations could reach
tens of millions of them. Suppose that 10M annotations are stored in an RDF(S) store together with
1M entity descriptions. Suppose also that on average
annotations and entity descriptions are represented
with 10 statements each. The difference, regarding
the inference approaches and the hardware that is
capable of efficient reasoning and access to a 10Mstatement repository and to a 110M-statement repos-
itory, is considerable.
 It would be nice if the world knowledge (ontology
and instance data) and the document-related metadata can be kept independent. This would mean that
for one and the same document, different extraction,
processing, or authoring methods will be able to deliver alternative metadata, referring to one and the
same knowledge store.

Fig. 5. Distributed heterogeneous knowledge.

 Most important, it should be possible that the ownership and the responsibility for the metadata and the
knowledge are distributed. In this way, different parties can develop and separately maintain the content,
the metadata, and the knowledge.

On the basis of the above arguments, what we propose is a decoupled representation and management of
the documents, the metadata (annotations), and the formal knowledge (ontologies and instance data), as this
is all illustrated in Fig. 5.

5.1. Light-weight upper level ontology

We will shortly advocate the appropriateness of using an ontology for the definition of the entity types
 these represent the only widely accepted paradigm
for the management of open, sharable, and reusable
knowledge in a way, which allows automatic interpretation and inference. According to our view, a lightweight ontology (poor on axioms, making no use of
expensive logical operators) is sufficient for the simple definition of the entity classes, their appropriate at-
tributes, and relations. At the same time it allows more
efficient and scalable management of the knowledge
(compared to the heavy-weight semantic approaches).
The drawback of using light-weight ontologies is that
they present a less expressive and less adequate model
of the world, which could fail to predict some facts
or impose some constraints on the possible interpre-
tations. E.g. a light-weight ontology may not include
in itself the axiom that the sets of men and women are
disjoint. Such an ontology, would not help a machine
to decide that if John is a man, then he is not a woman,

and thus he cannot be a mother of Peter. Realizing this
disadvantage, we claim that light-weight ontologies are
likely to be more suitable for semantic annotation, in
most of the cases, for the following reasons: (i) they are
easier to understand, thus, the same applies to metadata
based on them; (ii) these are easier to build, verify, and
maintain; and (iii) those are easier to get consensus
upon.

The ontology to support semantic annotation in a
web context should address a number of general classes
of entities, which use to appear in texts in various do-
mains. Describing these classes together with the most
basic relations and attributes means that an upper-level
ontology should be involved. The experience, gathered
within a number of projects,5 demonstrates that logically extensive upper-level ontologies are extremely
hard to agree on, to build, maintain, understand, and
use. This seems to give enough evidence that a lightweight upper level ontology is what semantic annotations need as a basis.

5.2. Knowledge representation language

According to the analysis of ontology and knowledge representation languages and formats in [9] and
by other authors, it becomes evident that to a great
extent no consensus exists beyond RDF(S), [2]. The
latter is well established in the Semantic Web community as a knowledge representation and interchange
language. The rich diversity of RDF(S) repositories,
APIs, and tools forms a mature environment for the
development of systems, which are grounded in an
RDF(S) representation of their ontological and knowledge resources. Because of the common acceptance of
RDF(S) in the Semantic Web community, it would be
easy to reuse the ontology and KB, as well as to enrich
them with domain-specific extensions. The new OWL
standard [7], offers a clear, relatively consensual and
backward-compatible, path beyond RDF(S), but it still
lacks a sufficient tool support. Our experience shows
(see the Section 7 on KIM) that for the basic purposes
of light-weight ontology definition and entity descrip-
tion, RDF(S) provides sufficient expressiveness. The
most obvious nice-to-have primitives (equality, transitive and symmetric relations, etc.) are well covered

5 I.e., Cyc (http://www.cyc.com) and the Standard Upper Ontol-

in OWL Lite  the simplest first level of OWL. So,
we suggest that RDF(S) is used in a way which allows
easy extension towards OWL  this means the avoiding of primitives and patterns, not included in OWL.
Such examples are the arbitrary meta-class definition
and usage, some reification patterns, and others.

5.3. Metadata encoding and management

The metadata should be stored in a format that allows its efficient management; we are not going to
prescribe a specific format here, but rather to outline
a number of principles and requirements towards the
document and annotation management:
 Documents (and other content) in different formats
should be identifiable and their text content should
be accessible.
 To allow non-embedded annotations over documents
to be stored, managed, and retrieved according to
their positions, features, and references to a KB.
 To allow the embedding of the annotations at least
 To allow the export and the exchange of the annota-

for some of the formats.

tions in different formats.

There are a number of standards and initiatives, related to the encoding and the representation of meta-
data, related to text. Two of the most popular are TEI6
and Tipster.7 Probably the most widely used system
providing Tipster-like support for document annotations is GATE (see Section 7.1).

5.4. Knowledge base

Once we have the entity types, relations, and attributes encoded in an ontology, the next aspect of the
semantic annotation representation are the entity de-
scriptions. It should be possible to identify, describe,
and interconnect the entities in a general, flexible and
standard fashion. We call a body of formal knowledge
about entities a knowledge base (KB)  although a
bit old-fashioned, this term best reflects the representation of non-ontological formal knowledge. A KB is
expected to contain mostly instance knowledge/data,

6 The Text Encoding Initiative, http://www.tei-c.org/.
7 Tipster

Architecture,

http://www.cs.nyu.edu/cs/faculty/

ogy initiative (http://suo.ieee.org/).

grishman/tipster.html.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

so other names can also make a good match for such a
dataset.

We deem that the ontology (defining all classes, re-
lations, and attributes, together with additional constraints and dependencies) is a sort of schema for the
KB and so both should be kept into a semantic store 
any sort of a system for formal knowledge reasoning
and management, which provides the basic operations:
storage and retrieval according to the syntax and semantics of the selected formalism. The store may or
may not provide inference,8 it can implement different
reasoning strategies, etc. Also, there are several more
advanced management features, which are not considered as a must: versioning, access control, transaction
support, locking, client-caching. For an overview of
those, please see [13,12,16,21]. Whether the ontology
and the knowledge base should be kept together  this
is a matter of distributed knowledge representation and
management, which is beyond the scope of this paper.
The KB can host two sorts of entity knowledge (de-

from trusted sources.

scriptions and relationships), as follows:
 Pre-populated  imported or acquired otherwise
 Automatically extracted  discovered in the process
of semantic annotation (i.e. via IE) or using other
knowledge discovery and acquisition methods.

It is up to the specific implementation whether or
not, and to what extent, the KB is to be pre-populated.
For instance, information about entities of general importance (including their aliases) can significantly help
the IE used for automatic semantic annotations  an
extensive proposal about this can be found in the description of the KIM platform below in this paper.

Further, domain and task specific knowledge could
help in the customization of a semantic annotation application  after extending the ontology to match the
application domain, the KB could be pre-populated
with specific entities. For instance, information about
specific markets, customers, products, technologies,
and competitors could be of great help for business
intelligence and press-clipping; or, for company intelligence within UK, it would be important to have a more

exhaustive coverage of UK-based companies and UK
locations. It might also appear beneficial to reduce the
general information that is not applicable in the concrete context and thus construct a more focused KB.

Since state-of-the-art IE (and in particular named
entity recognition, NER) allows the recognition of new
(previously unknown) entities and also of relations between them, it is reasonable to use such techniques for
the enrichment of the KB. Because of the innate impreciseness of these methods, the knowledge, accumulated through them should be distinguishable from the
one that was pre-populated. Thus the extraction of new
metadata can still be grounded in the trusted knowledge
about the world, while the accumulated entities would
be available for indexing, browsing, and navigation.
Recognized entities could be transformed to trusted
ones at some point through a process of semi-automatic
validation. An important part of this enrichment would
be the template extraction of entity relations, which
could be referred to as some kind of content-based
learning of the system. Depending on the texts that are
processed, the respective changes would occur in the
recognized parts of the KB, and thus its projection of
the world would change accordingly (e.g. processing
only sports news articles, the metadata would be both
rich for this domain and poor for the others).

Finally, the symbolic IE processing usually requires
that some lexica are used for pattern recognition and
for other purposes. These are both general entries (such
as various sorts of stop words), as well as ones that are
specific for the entity classes being handled. It is common that IE systems keep these either in applicationspecific formats or directly hard-coded in the source
code. On our view, it is worth representing and managing those in the same format, used for the ontology
and the entity knowledge base  in this way the same
tools (parsers, editors, etc.) can be used to manage both
sorts of knowledge. For this purpose, a part of the ontology (or just a separate one) could be dedicated to the
definition of the types of lexical resources, used by the
natural language technologies involved.

6. Semantic annotation process

8 For instance, there are experts who do not consider as inference
the interpretation of RDF(S) according to its model-theoretic seman-
tics, just because this one is simple compared to semantic and the
inference methods in other languages.

As already mentioned, we focus mainly on the automatic semantic annotation, leaving manual annotation to approaches, which are rather relating to web

content authoring. Even less accurate, the automatic
approaches for metadata acquisition promise scalability and without them the Semantic Web would remain
mostly a vision for a long time. Our experience shows
that the existing state-of-the-art IE systems have the
potential to automate the annotation with reasonable
accuracy and performance.

Although a lot of research and development has been
carried out in the area of automatic IE so far, the lack
of standards and integration with formal knowledge
and ontology management systems has been hindering its usage for semantic annotation. We claim that it
is crucial to encode the extracted knowledge formally
and according to well-known and widely accepted standards for knowledge representation and metadata en-
coding. Such a system should be easily extensible for
domain-specific applications, providing basic means
for addressing the most common types of entities, their
attributes, and the relations among them.

6.1. Extraction

A major problem with the traditional named-entity
recognition approaches is that the annotations produced are not encoded in an open formal system, and
unbound entity types are used. The resources in use
are also traditionally presented in a proprietary form,
without clear semantics. This hampers the reuse of both
the lexical resources and the resulting annotations by
other systems, thus limiting the progress of language
technologies, since the sharing of resources and results
is too expensive.

These problems can be partly resolved by an
ontology-based infrastructure for IE. As proposed
above, the entity types should be defined within an on-
tology, and the entities that are recognized, should be
described in an accompanying KB. Thus an NLP systems with ontology support would share more easily
both their pre-populated knowledge and the results of
their processing, as well as all the different sorts of
lexicons and other commonly used resources.

An important case, demonstrating how ontologies
can be used in IE, are the so-called gazetteer modules,
which are used to look-up strings in a text out of predefined lists. At present, these lists are kept in proprietary
formats. Annotations with some unbound strings, used
as types, are among the typical results from the operation of gazetteers. A better approach presumes that all

the various annotation types and list values be kept in
a semantic store. Thus, the resulting annotation can be
typed by reference to ontology classes and, even fur-
ther, point to a specific lexeme or entity, if appropriate.
Since a huge amount of NLP research has been carried out throughout the recent decades, we propose the
reuse of existing systems with proven maturity and ef-
fectiveness. Such a system should be modified to use
resources kept in a KB, and to produce annotations referring to the latter. Our experience shows that such a
change is not a trivial one. All the processing layers
have to be re-engineered in order to become open towards the semantic repository and to depend on it for
their inputs. However, there are a number of benefits
of such an approach, as follows:
 All the various kinds of resources can be managed
 It becomes easier to manage the different sorts of linguistic knowledge at the proper level of generality.
For instance, a properly structured entity type hierarchy would enable the entities and their references
in the text to be classified in the most precise way,
while still easily matched in more general patterns,
based on subsumption in the type hierarchy. Thus,
one can have a specific mountain annotated and still
match it within a grammar rule which expects any
sort of a location.
 Wherever possible, any available further knowledge
will be accessible directly with a reference from the
annotation to the semantic store. Thus, the available
knowledge for an entity can be used, for instance,
for disambiguation or co-reference resolution tasks.

in a much more standard and uniform way.

A processing layer that is not present in traditional
IE systems can generate and store in the KB the descriptions of newly discovered entities, i.e., it is able
to populate the KB with new instances. The next time
the same entity is encountered in the text, it could be
directly linked to the already extracted description. Fur-
ther, extending the IE task to cover template relations
extraction, another layer could enrich the KB with these
relations.

6.2. Indexing and retrieval

Historically, the issue of some specific handling of
named entities used to be neglected by the information retrieval (IR) community, apart from some shal-

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

low handling for the purpose of questions/answering
tasks. However, a recent large-scale human interaction
study on a personal content IR system of Microsoft
[8] demonstrates that, at least in some cases, ignoring
named entities goes against user needs: The most common query types in our logs were people/places/things,
computers/internet, and health/science. In the peo-
ple/places/things category, names were especially
prevalent. Their importance is highlighted by the fact
that 25% of the queries involved peoples names, which
suggests that people are a powerful memory cue for
personal content. In contrast, general informational
queries are less prevalent.

As the web content is growing rapidly, the demand for more advanced retrieval methods increases
accordingly. Based on semantic annotations, efficient
indexing and retrieval techniques could be developed,
involving an explicit handling of the named entity ref-
erences.

In a nutshell, semantic annotations could be used
to index both NY and N.Y. as occurrence of the
specific entity New York, like if there was just its
unique ID. Since present systems do not involve entity
recognition, they will index on NY, N, and Y,
which demonstrates well some of the problems with
the keyword-based search engines.

Given the metadata-based indexing of the content,
advanced semantic querying should be feasible. In a
query towards a repository of semantically annotated
documents, it should be possible to specify entity type
restrictions, name, and other attribute restrictions, as
well as relations between the entities of interest. For
instance, it should possible to make a query that targets
all documents that refer to Persons that hold some Positions within an Organization, and which also restricts
the names of the entities or some of their attributes (e.g.
a persons gender).

Further, semantic annotations could be used to
match specific references in the text to more general
queries. For instance, a query such as company Redwood Shores could match documents mentioning the
town and specific companies such as ORACLE and
Symbian, but not the word company.

Finally, although the above sketched enhancements
look promising, a lot of research and experiments are
required to determine to what extent and how they could
improve the existing IR systems. It is hard, in a general
context, to predict how semantic indexing will combine

with the symbolic and the statistical methods currently
in use, such as the lexical approach, presented in [17],
and the latent semantic analysis, presented in [15]. For
this purpose, large scale experimental data and evaluation are required.

7. KIM platform: implementing the vision

The knowledge and information management
(KIM) platform has been implemented in order to
embody our vision of semantic annotation, indexing,
and retrieval services and infrastructure. An essential
idea in KIM is the semantic annotation (as illustrated
in Fig. 4). Here we bet on our vision that massive
automatic semantic annotation is the prerequisite for
building up most of the metadata, needed for the
Semantic Web to happen. In order to achieve this, we
reuse existing human language (HLT), and especially
information extraction (IE),
technologies. We use
these technologies that took decades to reach their
current robust state, and we integrate them with the
contemporary Semantic Web technologies for knowledge representation and reasoning. In this way the IE
processing is enriched with (i) a consistent and uniform
representation of both its input resources (lexical infor-
mation, pre-populated knowledge) and results (newly
recognized entities and relations), (ii) it benefits from
the additional knowledge in the semantic repository
(e.g. relations between entities that could be used for
disambiguation). Semantic annotation can be seen as a
classical named-entity (and relations) recognition and
annotation process. But in contrast to most of the existing IE systems, KIM provides for each entity reference
in the text (i) a link (URI) to the most relevant class in
the ontology, and (ii) a link to the specific instance in the
knowledge base. As a result of the automatic semantic
annotation, metadata is generated and associated with
the processed resource. This metadata is not embedded
in the processed document, thus allowing different
semantic annotation tasks to take place, accordingly
resulting in diverse sets of metadata. The rationale
behind this decision could be found in Section 5.

Beside automatic semantic annotation, KIM allows
indexing and retrieval with respect to the generated
metadata. This unlocks a new level of retrieval methods
that are semantically enhanced. Meaning that, the information need could be specified with respect to the se-

mantic repository. The resources needed are described
by (i) the entities that are expected in the document,
(ii) relations between the latter and other entities, and
(iii) attributes of these entities. An example could be
a query that searches for all documents, which mention a person who is a CEO of a European company
in the telecommunications industry sector. These semantic queries could be combined with the traditional
keyword search, used by the IR engines, but they obviously provide much more comprehensive access ca-
pabilities, which are particularly important, given the
ever-growing number of documents in organization in-
tranets, let alone the internet.

KIM consists of Java-based components and allows
an easy integration with custom applications. More
on the architecture and KIM APIs could be found
in the following Section 7.1. There are a number of
front-ends that are part of the KIM platform (and
others could be developed using the APIs). One of
them  the browser plug-in  performs semantic
annotation over an arbitrary web content. The KIM
Web UI is another front-end facility, which provides
semantically-enabled access methods over data-stores
of annotated and indexed documents. And finally,
KIM is equipped with a simple KB Explorer  a
web-based form that could be accessed both from
within the pages, annotated with the plug-in, or from
within the KIM Web UI. It is used to display the
semantic description of a particular entity, and  more
generally  for browsing and exploration of the semantic repository. All these front-ends are presented in
Section 7.6. Also, they are available for demonstration
purposes at http://www.ontotext.com/kim.

7.1. KIM architecture

The KIM platform consists of formal knowledge
resources (KIM Ontology, a knowledge base), KIM
Server (with an API for remote access or embedding),
and front-ends (Section 7.6). The architecture of the
KIM Server (Fig. 6) allows for an easy modification,
extension, and embedding in third-party systems. It
also provides an abstraction layer over the specific
underlying component
implementations, and thus
ensures flexibility in cases of a custom implementation
(or configuration) of KIM with another semantic repos-
itory, metadata storage, or IR engine. Furthermore,
KIM Server components could easily be wrapped

in the shape, expected by another component-based
framework, which minimizes the integration costs.
The KIM Server constitutes of the following major
components: Semantic Repository, semantic annota-
tion, document persistence, indexing, and query. These
are visible as parts of the KIM Server API and they
could be used by third-party systems/applications.

The KIM platform is based on robust open-source
platforms, specialized in three different domains:
RDF(S) repositories, HLT (and especially IE), and IR.
The technologies that KIM builds on have been carefully chosen, so that they are mature enough, scalable,
and platform independent at the same time. The knowledge resources are kept in the Sesame9 RDF(S) repos-
itory, which provides storage and query functionality
infrastructure. The Sesame repository is loaded with
millions of RDF(S) statements. It is being queried by
the semantic search methods to identify the entities according to the restrictions provided, and the result is
further used for the retrieval of the referring documents.
The IE process also relies on the semantic repository
for its initialization and further processing.

The GATE10 platform has been used as a basis for
the IE process and also for the management of content and annotations. It provided the fundamental text
analysis technologies, on top of which we have built
the semantically-aware extensions, specific for the IE
of KIM. The annotations and document management
paradigms were derived from the GATE infrastructure,
though in a slightly simplified form in order to avoid
any dependencies of the KIM clients on anything beyond the KIM API.

The Lucene IR engine was adapted to perform in-
dexing, retrieval and evaluation of content relevance
according to named entities; this enables the semantic
access methods, described in Section 7.5. The usage
of Lucene is an evidence that it is easy to adjust a traditional IR engine to perform indexing with respect
to semantic annotations. This means that there is no
need of completely new technologies for the purposes
of semantic indexing, but rather the systems, providing semantic IR, could (at least partly) be based on the
existing, high-quality IR engines.

9 , RDF(S) repository by Aduna (ex Aidministrator b.v.).
10 General

(GATE),
http://www.gate.ac.uk, leading NLP and IE platform developed in
the University of Sheffield.

engineering

architecture

for

text

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

Fig. 6. Architecture of the KIM platform.

7.2. KIM ontology (KIMO)

The rationale behind the KIM ontology (KIMO-
available at http://www.ontotext.com/kim/kimo.rdfs)
is to provide a minimal but sufficient ontology, suitable
for open-domain, general-purpose semantic annota-
tion. It was designed from scratch for the purposes of
KIM; a number of upper-level resources inspired its
creation and development: OpenCyc, WordNet 1.7,
DOLCE, EuroWordnet Top, and others. In order to
keep the ontology simple and easy to understand, it
is preserved small and na ve with respect to a great
number of philosophical, mathematical, and logical
problems. KIMO is a simplistic upper-level ontology,
starting with some basic philosophic distinctions
between entity types (such as Object-s  existing
entities such as locations and agents, Happening-s
 defining events and situations, and Abstract-ions
that are neither objects nor happenings). Further
on, the ontology goes into more details to such an
extent
the real-world entity types of general
importance are included (meetings, military conflicts,
employment positions, commercial, government and
other organizations, people, and various locations,
etc.). The characteristic attributes and relations for the
featured entity types are defined (e.g. subRegionOf
property for Location-s, hasPosition for
Persons, locatedIn for Organizations,
etc.). Having this ontology as a basis, one could
add domain-specific extensions to it easily, in or-

that

der to profile the semantic annotation for concrete
applications.

The distribution of the entity types, which are most
commonly referred to, varies greatly across domains.
As researched in [19], despite the difference of type
distributions, there are several general entity types
that appear in all corpuses  Person, Location,
Organization, Money (amount), Date, etc. The
proper representation and positioning of those basic
types was one of the objectives behind the design of
KIMO. Further, the ontology defines more specific entity types (e.g. Mountain, as a specific type of loca-
tion).

The extent of specialization of the ontology is determined on the basis of a research of the entity types in a
corpus of general news (including political, sports, and
financial ones). At present, KIMO consists of about
250 entity classes and approximately 100 attributes
and relations. The top classes are Entity, Enti-
tySource, and LexicalResource. The Entity
branch (see Fig. 7) represents the core ontology (the
variety of entity classes), while the other branches
could be considered as an auxiliary ones. The LexicalResource branch is dedicated to the encoding
of various data, related to the IE process, such as company suffixes (AG, Ltd.), first names of persons, etc.
(depicted in Fig. 8). An important class within this
branch is Alias, representing the names of the instances of the Entity class (see Fig. 9). The hasAlias
relation is used to link an Entity to its alternative

This sort of axioms is supported by Sesame and it
provides a consistent mechanism (easily understandable and manageable, too) for custom inference
extensions to the RDF(S) semantics with respect to
a particular ontology. Those axioms can be seen as
an ad hoc though quite a practical way to avoid the
RDF(S) constraints without the need to implement
some specific flavor of OWL or another language.

Finally, one of the objectives of the KIMO development was to make it compliant with Dublin Core,
the ACE annotation types,11 and the ADL Feature
Type Thesaurus.12 This means, that although those
are not directly imported (for consistency reasons),
a formal mapping of the appropriate classes and
primitives is easy, on the basis of (i) compliant design
and (ii) formal notes in the KIMO glosses, which
indicate the appropriate mappings. For instance, in
KIMO, a hasContributor property is defined, with a
domain InformationResource and a range Agent, as an
equivalent of the Contributor element in Dublin Core.
The development philosophy of KIMO is to make it
compliant, in the future, with other popular standards
and ontologies, such as FOAF.

7.3. KIM knowledge base

The entity descriptions are stored in the same
RDF(S) repository where the KIM ontology is. Each
entity carries information about
its specific type,
alias(es) (including a main alias, expressing the official name), attributes (e.g. the latitude of a location),
and relations (e.g. a Location subRegionOf another Location). A simplified schema of the entity
representation is demonstrated in Fig. 9. As regards the
KB, we made two important modelling decisions:

11 The automatic content extraction (ACE) is one of the most influencing information extraction programs, see http://www.itl.nist.gov/
iad/894.01/tests/ace/. A set of entity types is defined within
The ACE 2003 Evaluation Plan (ftp://jaguar.ncsl.nist.gov/ace/doc/
ace evalplan-2003.v1.pdf). Those are: person, organization, a geopolitical entity (GPE), location, facility.

12 Alexandria Digital Library (ADL) is a project at the University
of California, Santa Barbara, http://www.alexandria.sdc.ucsb.edu/
lhill/adlgaz/. The Location branch of KIMO is contains about 80
classes aligned with the ADL Feature Type Thesaurus, which on
its turn is aligned with the geographic feature designators, of the
GNS database of NIMA (National Imagery and Mapping Agency of
United States), at http://www.earth-info.nga.mil/gns/html/.

Fig. 7. The top of KIMO class hierarchy with expanded entity
branch.
names. The official name of an entity is referred to by
the hasMainAlias property. The instances of the
EntitySource class are used to separate the trusted
(pre-populated) information in the KB, from the automatically extracted one. This is indicated by the generatedBy property of the specific entity.

The ontology was coded in RDF(S). In addition, a
number of generative (in the style of the RDFS MT
semantics) axioms are defined, such as:
X, locatedIn, Y andY, subRegionOf, Z 

X, locatedIn, Z

Fig. 8. The lexical resources top class hierarchy.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

Fig. 9. Sample RDF(S) representation of an entity description.

 Each entity has a single, most specific type. This is
to ensure the consistency with the IE process, where
the instance is recognized together with its type.
 There will be no meta-classes, in order to allow an

easy migration to OWL Lite.

KIM World KB has been pre-populated with entities of general importance, that allow enough clues for
the IE process to perform well on inter-domain web
content. Because the building of a domain-independent
general knowledge base is a complex task and, defined
in this way, it does not offer a clear realization strat-
egy, we substituted this task with an easier one, which
seems to serve as a good approximation: to provide a
good coverage of the entities mentioned in international
news (see news collector in Section 7.3.2.1). Here we
mean those publications, which tend to cross the borders of the countries and feed the headlines of the global
news wires. The specific about this domain is that it
covers the most well-known entities in the world.

7.3.1. Pre-population of KIM KB

The KIM World KB consists of more than 200,000
entities, which have been gathered semi-automatically
out of a big number of public data-sources. For each
of the sources, appropriate import procedures are de-
veloped. These procedures are complemented by a
comprehensive strategy regarding the filtering, cross-
checking, and compilation of the data coming from the
different sources.

In its current state, the KIM KB contains about
36,000 locations, including continents, global regions,
and countries (according to FIPS) with their capitals,
4400 cities (including all the cities with a population

over 100,000), mountains, big rivers, oceans, seas, and
even oil fields. Each location has geographic coordinates and several aliases (usually including English,
French, Spanish, and sometimes the local transcription
of the location name), as well as co-positioning relations (e.g. subRegionOf). This spatial knowledge
provides a good basis for location-based services (see
[18] for details).

The organizations of high general importance have
also been pre-populated in the KB. Including the
biggest world organizations (such as UN, NATO,
OPEC), over 140,000 international companies, and 140
stock exchanges, for a total of 147,000 organization
instances. For some of the public companies, there are
position relations of managing personnel. The organizations also have locatedIn relations towards the
corresponding Country instances. The information
about the companies, that is imported additionally, consists of a short description, an URL, a reference to an
industry sector, reported sales, a net income, and a number of employees.

Finally, in order to enable the IE process so that
new entities and relations, which are not a part of the
KB, can be recognized, a collection of lexical resources
(derived from GATE) is also presented in the KB. It
covers organization suffixes, person names, time lexica,
currency prefixes, and others.

In addition to the KB described above, we also produce a smaller version of KIM World KB, which is
a step towards some less restrictive distribution constraints  in terms of both licensing and hardware re-
quirements. This is, of course, achieved by removing
some of the entities, but we also verify that this does
not have a serious impact on the IE accuracy. We ex-

Table 1
Statistics about full and small versions of the KB

RDF-statements

Explicit
After inference

Instances
Entity

Location

Country
Province
City

Organization
Company

Person

Alias

Small KB

Full KB

1,014,409

2,248,576
5,200,017

4, 400

7, 848

4, 417

pect that the small version will also be more usable as
a basis for domain-specific extensions. Table 1 shows
a comparison between the two versions.

7.3.2. Controlling the quality and coverage of
KIM KB

Ensuring the quality of the KB content is not a trivial task at all, and it is impossible for it to be performed
manually (having more than 200,000 pre-populated en-
tities, the manual approach simply does not scale). The
KIM KB is verified iteratively, using an independently
built KB of entities and relations, which have been collected manually. An indirect verification is also performed during the evaluation of the performance of the
KIM IE against a human-annotated corpus.

7.3.2.1. KIM KB population and quality verification.
The coverage of the KIM KB is guaranteed through
the regular processing and analysis of headline news.
Here we employ the news collector  a service which
collects between 500 and 2000 top stories (in English)
per day from about 20 of the most popular global news
sources between. See http://www.news.ontotext.com/.
On top of the news collector corpus, entity ranking
is performed so that the level of popularity of the
specific entities can be detected. This allows for the
proper manual handling of the most popular entities, at
the least, as well as for the early spotting of potential
problems concerning the import strategy and sources.
The ranking algorithm works on entities and treats all

its aliases as equivalent references to the entity. Due to
this it is very sensitive to duplicated instances (when
two aliases are presented as two standalone instances).
We have faced a number of issues, related to instance
identification. Examples range from the use of numbers or stop-words for locations to the use of variations
in punctuation and organization suffixes for Organizations (e.g. The Coca Cola Company and Coca-Cola
are two aliases of the same entity). In order to ensure
a consistent pre-population of the KB, we use some
heuristics, applied in different combinations, depending on our level of trust in the information source,
including the following:
 suppressing of aliases according to various criteriasecondary aliases, matching a primary alias of another entity; word lists: a stop-word list, a list of
common words, a list of approximately 80,000 English words;
 class-specific pre-processing and comparison of
 automatic generation of additional aliases-for in-
stance, by truncating parts of the main alias, i.e. if
Xyz Ltd. is the main alias of a company name, then
we might expect that Xyz is also a relevant one.

aliases;

7.3.2.2. Knowing vs. cognizing-news sources and the
way people communicate via mass media. Strictly
speaking, the usage of news sources for the enrichment
of the KIM KB might seem a debatable choice: one
may argue that news sources around the world are
never entirely neutral, but rather the other way aroundmost of them are quite biased and opinionated to a
certain extent, which varies depending on the country,
the political, social, and professional orientation of the
respective news source, etc. And we have to admit that.
In order to reduce the degree of subjectivity we employ
the above-mentioned methods for instance identification and verification of the quality of the KB content in
terms of the universal verity of the entities and their
relationships. However, news, as well as everything,
anything in our conditional, civilized world, as we
all know it, is influenced (and in many cases-created)
by us, humans. After all, the end users of KIM are
also humans and therefore it is all about everyones
personal cognition and perception of reality. In terms
of approaches to human recognition and perception of
news, language, facts of life, etc., artificial intelligence

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

is not yet as intelligent as we would like it to be in
order to make a significant difference  in terms of
authenticity of the information presented  between,
for example, an enhanced search for specific jobs a
user performs, using the KIM platform, and a habitual
morning acquaintance with the daily news and classifieds in a newspaper. The differences would be many,
of course: in quantity, in the time spent for searching,
and in the precision of the results, but still not in the
quality-no automatic indication would appear if a
certain job offer is not valid, or falsified. The cognition
(and the recognition) of what is universally correct
and true for everybody (or, to go further, of what
objective reality is, and where the virgin spring of
neutral, ideal, authentic reflections of reality is buried)
is an issue that has been facing human minds for
tens of centuries. What is more, presently machines
could not be programmed to think for, or judge
over, matters that todays human civilization has not
yet reached consensus about. (See Section 2.3, for
further comments on the relation between KR, AI and
philosophy.)

The very pieces of information that a news source
may present  directly or not  to the perceiver of in-
formation, are like a subsequent handful of coins in the
money-box of acquired knowledge  one is just happy
to slip them inside the box. However, too few people
would really check the coins for authenticity  i.e.
the out-and-out suspicion, and consecutive judgment,
regarding the verity and trustworthiness of the information received, are all the subject of cognition: it is (in the
most part) an unconscious act, and it greatly depends
on the individual personality, background, and erudition of the perceiver. And yes, this is what a machine
cannot possibly be expected to cope with. According to
Chomsky, who introduced the term,13 to cognize means
to denote a relation a person has to his or her knowl-
edge. Actually, cognizing is said to differ very little
from knowing in the ordinary sense, but there are some
important features of cognizing that set it off from the
standard conception of what it is to know something.
Perhaps the most salient feature of cognizing is that it
is a relation primarily  though apparently not exclusively  associated with implicit or unconscious knowing or knowledge. What distinguishes cognizing per

13 Chomsky, Noam (1980, p. 69). Rules and Representations.

Columbia University Press, New York.

se from ordinary knowing is that in many cases, what
is cognized is inaccessible to consciousness. That is
why the automatic quality verification task for the KIM
KB is so difficult at this stage: the system knows
that The Guardian, for example, is a trusted source of
information, and the risks with automatic processing
towards filling the KB with a certain percentage of
useless junk are still a part of the game, at least for
now.

Hope lives, though, in our belief that a satisfactory
percentage of a certain Kantian universalizability14
is present in the work and the minds of the authors
in mass media around the world  i.e. some of them
may present information in a biased, opinionated, even
deliberately misleading fashion, but still simple human ethics, combined with the inherent, invisible rules
of this profession, would keep this percentage low
enough. Statistically, most news are true, are they
not?

To finalize, the choice of using a corpus of up to date
global news articles is far less arbitrary than it seems.
On the obvious side, one can use these articles as an
easily available source for the extraction of pieces of
common culture. What is less obvious, but more im-
portant, is the following: the global news channels (of
any sort) are those that form and determine the global
common culture. What the majority of people in Europe and America know about Asia is (i) what they
have learned in school and (ii) what Reuters and CNN
have been focusing on over the last years. We cover the
first sort of knowledge with the KIM World Knowledge
Base, and the second one  through an analysis of the
news articles. Other emanations of the common culture
would also be relevant as a source: movies, books, etc.
While at present we are not able to cover all of them, we
can just put some reliance on a conviction that a part of
the artifacts they introduce are, at a reasonable degree,
reflected in the news as well. For instance, Harry Potter was mentioned in about 900 articles in the period
between years 2002 and 2004.

14 Kant, Immanuel. Critique of Practical Reason (1788). Kantian
theories of practical reasoning require that reasons be universaliz-
able: i.e., generally, that it be possible for everyone in like circumstances to act likewise on the basis of a similar reason. Universalizability acts as a filter through which proposed actions and the reasons
for them are passed. Contemporary interest in universalizability is
primarily due to the role it plays in Kantian moral theory, which is
today one of the most prominent positions in ethics.

7.4. KIM information extraction

Table 2
KIM IE evaluation

KIM IE is based on the GATE framework, which
has proved its maturity, extensibility, and task independency for IE and other NL applications. The essence
of the KIM IE is the recognition of named entities
(NE) with respect to KIMO ontology and a knowledge
base of entity descriptions. The entity instances all bear
unique identifiers that allow annotations to be linked
both to the entity type (class) and to the exact individual entity in the KB. For new (previously unknown)
entities, new identifiers are allocated and assigned; then
minimal descriptions are stored in the semantic store.
The annotations are kept separately from the annotated
content, and an API for their management is provided.
Rationale about keeping the metadata separately from
the processed content is discussed in Section 5.

7.4.1. KIM IE evaluation

The default KIM IE application is based on semantic
gazetteers, a shallow analysis of the text, and patternmatching grammars. The evaluation in the table here
was performed with respect to flat NE types (e.g. if
Reuters is recognized as NewsAgency, still on a more
general level it is an Organization). The reason to
evaluate against corpora of flat NE types is that there
are not well established metrics for semantic annota-
tions. Also, there are not any human-annotated corpora
with annotations, according to (at least) a hierarchy of
named-entities, that could be mapped to KIMO (or another ontology) and thus to provide a golden standard
for the evaluation of semantic (entity) annotations.

We used three different corpora for the evaluation
of KIM IE, all of them consisting of news articles in
different domains: general international news, business
news, and UK news. The results obtained are presented
in Table 2. In order to combine the P/R metrics from the
three different corpora, we used as a weight factor the
number of tokens in each corpus divided by the total
number of tokens for three corpora.

7.4.2. Custom KIM IE and traditional IE
approaches

The task of creating a Semantic IE application benefited from the already existing IE components in
GATE, but we had to semantically enable some of
them (e.g. the pattern-matching transducer), or to create completely new components-such as the seman-

Flat NE type

Precision (%)

Recall (%)

F1a (%)

Date
Person
Organization
Location
Percent
Money

a The F1 in the table stands for a special state of the F-measure
(which measures the accuracy of IE, and its formula uses both precision and recall): when the precision and the recall have equal weights
(percent values 50% each), we use the term F1.

tic gazetteer. Another important issue is the possibility
to completely change the IE application that we use
in KIM as well as its extensibility. Any GATE application (IE pipe-line) could be plugged into the KIM
Server. It could include machine learning, as well as
rule-based components (or an arbitrary set of the palette
of NLP components, integrated in GATE). The IE application could also be provided by a completely independent system, given that it is wrapped appropriately
and plugged into KIM.

A substantial difference of the semantic IE process
to the traditional one is the fact that it not only finds
out the (most specific) type of the extracted entity, but
it also identifies it, by linking it to its semantic description in the knowledge base. This allows entities to be
traced across documents and their descriptions to be
enriched through the IE process. It can also have a positive effect (theoretically speaking  this has not been
evaluated yet) on both the precision and the recall of
the IR after indexing with respect to the semantic an-
notations/metadata (see the next section)

In terms of the IE process utilized, the most important differences between KIM and other systems and
approaches are grounded in the fact that it performs
semantic annotation (not just annotation of flat named
entities, i.e. it adds semantics through the annotations)
and provides services on the basis of the results. To
do this in a consistent fashion, it performs information extraction, based on an ontology and a massive
knowledge base, using the slightly modified IE engine
of GATE. In this light, KIM might be declared to perform a quite specific, custom type of IE, compared to
the traditional IE approaches. We are going to elaborate
on this issue in more depth below.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

Generally, what a traditional IE approach provides is
the annotating of the respective body of text by simply
adding a string to every entity, which just indicates that
the respective entity belongs to a certain class of named
entities in the taxonomy used. However, this type of annotation does not involve any semantics-i.e. an annotation is not semantically aware, because it does not point
to an additional semantic description of any kind. It is
oversimplified because the traditional flat NE type sets
consist of several general types (such as organization,
person, date, location, percent, money). Although these
represent the most important domain-independent NE
types, still every average well-educated person could
break down the entities of one and the same type further into more specific classes (e.g. public companies,
sport teams, and syndicates are all well-recognized important sorts of organizations). Therefore, when the results of such a traditional IE annotation are presented
to a human agent for processing (or just for interpreta-
tion), the missing semantics is added naturally by the
human since they just know it by cognition (see Section 7.3.2.2); in the case of an interpretation by some
non-human agent (e.g. a machine/software), then any
desired semantics, considered for incorporation, should
be encoded in a form which allows interpretation, based
on a strict formal rules.

Here we arrive at the point of that le coup de grace
semantique KIM delivers on traditional IE: KIM makes
the big difference by adding semantics to the IE pro-
cess. This is why we call the custom IE of KIM a semantic IE, or semantic annotation instead of just
annotation. KIM links the annotations it produces,
not just to nodes of a taxonomy, but rather to a whole
formal model of the respective domain: the ontology,
having its internal logic, rules, and formal interrela-
tions. What is more, this approach allows for the identification of the concrete entity that takes place along
with the annotation  i.e. if John is a person, we also
know that John is the particular Person X that acts as
an instance of the Person class.

Here follows an extensive description of the way we
have implemented such an IE approach in KIM. As already mentioned, the basis is the relation to a semantic
repository, which contains an ontology (with a number of entity class definitions) and a knowledge base
of entity descriptions. For each entity, KIM provides
a reference in the text, as follows: (i) a link (URI) to
the most specific class in the ontology, and (ii) a link

to the specific instance in the KB. Each extracted NE
is linked to its most specific (priority) class (for in-
stance, the string Arabian Sea would be identified as
a reference to an instance of the Sea class, instead of
the traditional named entity type-Location). Also,
each NE is linked to an individual semantic entity de-
scription, associated for the entity description in the KB
(attributes and relations of the entity). The KB has been
pre-populated with entities of general importance, and
it is iteratively enriched with entity individuals and relations as a result of the IE process. Thus the extracted
named entities could be further used for semantic indexing and retrieval of content with respect to entity
instance and class.

As we have already mentioned, the information extraction process in KIM is based on the GATE platform.
Several generic NLP components for tokenization,
part-of-speech tagging, and others, have been directly
reused by KIM. The KIM gazetteer lookup component
searches through entities aliases (names) and other
lexical resources (suffixes, context words). The latter
serve as clues for the pattern-matching grammar NER
process. The pattern-matching grammars in GATE15
have been modified in order to handle entity class information and to allow the generalization of the rules.
The grounding principle is simple-a reference to an
entity of a more specific class, can match a pattern that
specifies a more general class. For instance, a grammar
rule detecting the pattern of Location that is a
subRegionOf a Country, instead of specifying
multiple rules for each of the concrete location subclasses  City, Province, CapitalCity, etc.).
To make this happen, we enhanced GATE from a
standard IE application up to some custom, semantically biased features, as follows:
 a semantic repository (or a semantic store) was
added, so that each annotation can be linked to a respective entity class, entity identifier, and semantic
description in it; actually, GATE uses this semantic
repository as an input gate, against which the text
corpus is annotated;

15 For the description of the basic GATE pattern-matching model
and the JAPE engine, refer to [4] and [5]. In a nutshell, JAPE allows
regular expression matching, over feature-structure-based annota-
tions. The right hand side of the JAPE rules could contain arbitrary
Java code.

abled;

 GATE components were made semantically en-
 a new semantic IE-specific components were added;
 JAPE pattern-matching grammars were added,
which allow for
semantically aware pattern-
matching, using the class subsumption, defined in
the ontology.

A more elaborate description of the IE process of
KIM can be found in [36]. As a final of the IE discussion
here, we want to stress that there are a number of problems of the traditional IE, which remain locked within
KIM. One of those is the reference matching when there
are alternative (unknown) aliases of one and the same
entity, or more generally, the co-reference resolution
problem. Suppose, there is a text which mentions J.P.
Smith, another one, that mentions J. Smith and two
entities in the knowledge base with main aliases John
Smith and Jeremy Smith. The following questions
are problematic: Do the two references in the document
refer to one and the same entity? If yes, is there a corresponding entity description in the knowledge base and
which one is it? These questions are tough to answer
to in a general context. KIM, like many other systems,
implements a number of heuristics which help in answering them with some reasonable accuracy.

Further, for the purpose of gathering entity descrip-
tions, it is quite interesting to be able to extract relations between entities. The tasks of this sort are again
harder to solve in a general context, compared to the
NER task. KIM implements heuristics for automatic
extraction of two patterns (i) persons having specific
positions with specific organizations and (ii) organization being located in some locations. However, the
accuracy on these tasks is not high, and considerable
efforts are required in order to add new patterns. Again,
like with the co-reference resolution, KIM experiences
the typical IE limitations.

7.5. Indexing and retrieval

KIM provides indexing with respect to the semantic annotations, generated for a document, i.e. indexing with respect to the metadata. This type of indexing
enables new (semantically-enhanced) access methods.
Thus the user could specify queries, which consist of
constraints, regarding the types of entities, relations between the entities, and entity attributes. E.g. one could

specify the NEs that are to be referred to in the documents of interest, with name restrictions (e.g. a Person which name ends with Alabama). An example of
a query consisting of pattern restrictions over entities
could be: give me all documents referring to a Person that hasPosition spokesman within a Com-
pany, locatedIn a Location with name UK. To
answer the query, KIM applies the semantic restrictions
over the entities in the instance base. The resulting set of
entities is matched against the IR (full-text) index. Then
the referring documents are retrieved with relevance
ranking according to these NEs. These queries could
also be combined with a traditional keyword search
procedure and thus benefit from the combination of
both approaches. Technically, the Lucene16 IR engine
is adapted to perform full-text indexing, uniquely addressing each entity and disregarding the alias used in
the text.

If we are to elaborate a bit more on the subject of
indexing, the type of semantic indexing that KIM per-
forms, combines both IR and IE tasks. The first step
in the indexing process is the semantic pre-processing
(annotating) of each document that is to be included in
the corpus of documents for retrieval  this is the IE
part of the game. This pre-processing finds expression
in the appendage, or linking, of an unique internal
string identifier (a semantic annotation) to every slice
of text that we know the meaning of, according
to the ontologies and knowledge bases used. Consider
the text slice Paris is full of tourists, where Paris has
been recognized and annotated with a particular entity URI (e.g. http://www.ontotext.com/kim/kimo.rdfs
CountryCapital T.69).
there is a proce-
dure, which derives another string (shorter, and more
indexing-friendly) from the URI. Suppose that, in
the case of Paris, the unique string derived is a1b2c3.
Then the result of the pre-processing is the string Paris
a1b2c3 is full of tourists.

In KIM,

This metadata is to serve as an index pointer for
the respective entity during the retrieval process. Then
comes the next step: the document is delivered for indexing to the Lucene IR engine along with those ID
strings, and an indexing procedure is performed, including them as ordinary tokens. After that we can perform a search using those ID strings, serving as an

16 http://www.jakarta.apache.org/lucene.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

index  for instance, if there is a query about Paris,
what actually takes place in the system, is that the engine derives again its unique string ID, and then it uses
the standard full-text search for the string. The appropriate actions are taken to deliver the original (not the
scrambled) content back to the user. In this aspect,
the indexing that KIM offers is a bit different from
the standard text indexing, since it utilizes a KIMspecific type of unique identifiers. However, the indexing itself does not make a direct use of the entity
description knowledge base-the latter is only used in
the retrieval process for the matching of structured
queries.

The benefit of this sort of pre-processing is that (i)
one can find a reference to an entity in the text, disregarding which alias was used and (ii) the level of
relevance with respect to entities gets higher. While
the first benefit is obvious, the second could be demonstrated with the follow example. Suppose that there
is a document, where the strings U.K., UK, and
United Kingdom appear exactly once, and the string
Germany appears twice. Without any pre-processing,
the document would appear twice more relevant to the
string Germany than to the string UK.

The retrieval accuracy of KIM has not been evaluated against a traditional IR engine, and this is a topic
that should be researched in the future. However, KIM
has the potential to perform better, not only towards reducing the unrelated documents in the result set while
still retrieving the relevant ones (improvement of the
precision, as with a NE indexing system with flat entity
types as in [20] and [22]), but also towards increasing
the number of relevant documents by ones that do not
contain the alias, used for the entity name restriction,
but which have the same entity, mentioned with another one of its aliases. For example, if you look for
documents that refer to the city of Beijing and you use
a keyword search specifying the city by its name, then
you will miss all documents that only mention Pekin.
On the contrary, given the world knowledge in KIM, the
semantic IR would also find the documents that only
mention Pekin, because (i) the KB knows that Pekin
and Beijing are aliases of the same entity and (ii) the
documents are indexed by the entity identifier. Name
abbreviations and their full forms can serve as another
example (e.g. UK, above).

The IR functionality is available through the API
and through the KIM Web UI. The API allows the

creation of semantic queries, and requesting the doc-
uments, that refer to the restrictions, from a particular
data-store. As a result, some of the features (title, au-
thor, origin, etc.) of the resulting documents are loaded
from the data-store, but the documents are not loaded
completely, so that the expensive processing is not
needlessly delayed. The same functionality has been
made available through the Web.

7.6. KIM front-ends

The KIM Server API allows for the building of different front-end user interfaces. These front-ends could
provide full access to the functionality of the KIM
Server, including its IR functionality, semantic reposi-
tories, semantic annotation services, and document and
metadata management infrastructure. Some front-ends
have already been built within the KIM Platform. These
are the browser plug-in (KIM Plugin), the KIM Web
UI, the KB Explorer (KIM Explorer), and the Graph
View.

We have created a plug-in (Fig. 10) for the MS Internet Explorer browser. The KIM plug-in (available for
download at http://www.ontotext.com/kim) provides
easy delivery of semantic annotations to the end user.
On its first tab  Classes, the plug-in displays the entity
type hierarchy (a branch of the KIM ontology). For each
of the entity types there is an associated color used for
highlighting the annotations of this type. Check boxes
for each entity allow the user to select the entity types
of interest. Upon invoking an annotation of the current
(arbitrary) browser content, the plug-in extracts the text
of the currently displayed document and sends it to
an Annotation Server, which in its turn uses the KIM
Server Semantic Annotation API. The servers return
the annotations with their offsets, type, and instance
information. The annotated entities are highlighted in
the content (in the color of the respective entity type),
and are hyperlinked to the KIM KB Explorer (the additional small panel in Fig. 11). Moreover, when the
mouse cursor is hovered over a semantically annotated entity, tooltips are displayed, showing the type
and unique identifier of the respective entity (e.g. AP
in Fig. 10).

The second tab (Fig. 12) of the plug-in contains a
list of all the entities, which are recognized in the current document, sorted by frequency of appearance. By
clicking on an identified entity in the content, or on

Fig. 10. The plug-in with the top of the KIM ontology shown in the left pane.

Fig. 11. . . . and the KIM Explorer panel over it.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

the small
icon on the left-hand side of the desired
entity in the list of entities in the Entities tab, the user
invokes the KIM KB Explorer (Fig. 11), displaying the
semantic description of the entity in the KB (incl. type,
aliases, relations, and attributes). In this way, the user
can navigate directly from the mentions of entities in
the text to the instances that are linked to them in the
KB.

The Entities tab also allows the user to execute (by
clicking on the little
icon to the right-hand side of the
entity name) a semantic IR query to the default (for the
KIM Server, used by this plug-in) document data-store.
The result is a list of all the documents that have this
particular entity mentioned in them. To gather an impression about how useful this could be, imagine how,
while browsing and annotating, you find a page about
a particular organization, then you go to the Entities
tab of the plug-in, you query the referring documents,
and you land with a result list in hand, which could be
further explored (Fig. 12).

The KIM Web UI is next in the list of KIM front
ends (accessible at http://www.ontotext.com/kim). It
offers IR services over data-stores of semantically annotated and indexed documents. It enables three levels

of complexity of the semantic queries: entity lookup,
predefined patterns, and entity pattern search. All these
queries could be combined with the traditional keyword
search, which is also available in the Web UI, along
with the typical metadata properties for documents, like
authors, title, subtitle, and subject. Any combination of
queries could result in a set of entities that satisfy the
restrictions, or in a set of documents that refer to these
entities.
 The entity lookup allows the definition of query restrictions by the type and the alias of the entity. E.g.
show me all organizations that end on Ltd..
 The predefined patterns search provides a set of
frequently used queries to assist the user. These
queries consist of a predefined pattern frame of entities with specified types (like Person has Position within Organization). The user is
allowed to restrict the entities in the pattern by their
names (e.g. CEO for the position).
 The entity pattern search is the most comprehensive query definition interface that the KIM Web UI
provides. It gives the flexibility to specify the entity types, relations between these entities, and thus
create the entity pattern. Further one could specify

Fig. 12. The entities tab of the plug-in with the referring documents for one of the entities, displayed in the KIM Web UI panel.

Fig. 13. Entity pattern search from the web UI  looking for a telecom company in Korea.

attribute (like alias, longitude, age, etc.) re-
strictions. An example of Pattern Search is provided
in Fig. 13.

also be expanded, in order to visualize their link as
well (this is the case with the Chairman and CEO
node on the screenshot).

The next step, following the specification of the user
need, is the retrieval of the relevant entities or documents (referring to those entities). The result set could
be narrowed further by refining the queries. The content
of the documents from the result set could be examined.
The mentions of the relevant entities are highlighted
and hyperlinked to the KB.

Explorer. In case the result of the query is a set of
entities, then they are linked to the KB Explorer, which
is capable of displaying their semantic descriptions in
the instance base.

Finally, a hyperbolic tree-based graph representation of the semantic repository is also available as an alternative of the KIM Explorer. As presented in Fig. 14,
the graphical explorer visualization is focused on one
entity (Danone Group is demonstrated on the screen-
shot). Its links to and from other entities are presented
via labeled arcs. The attributes of the entity are provided in a separate pane on the right-hand side of the
view. Further, the entities which are not in focus can

7.7. Performance

We performed semantic annotation, indexing, and
storage of the metadata over the documents from
the corpus on a Pentium 4 (2.53 GHz) PC, and we
came by the following throughput metrics: annotation-
8 kb/s; indexing-27 kb/s; storage-6 kb/s; approximately
10 annotations/kb and 80 annotations/s. The speed of
annotation depends on the document size and it tends
to get slower for bigger documents in a sort of logarithmic dependency.

Recently, KIM was extended with a Cluster Archi-
tecture, which allows multiple crawlers and annotators to share one the same semantic repository, index,
and document storage. A commodity equipment-based
test cluster configuration is capable of annotation of
the News Collector corpus (at present consisting of
more than 0.5 million news articles for the period years
20022004) overnight. Semi-synthetic tests of Sesame,
as a semantic store of KIM, demonstrated that it can

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

Fig. 14. A graph view of the description in Danone Group.

handle, without considerable slow down, 1.2 million
entity descriptions, comprising of about 15 million explicit statements, or about 35 millions statements after
forward chaining. The performance and scalability issues are not the focus of this paper; one can find further
details at http://www.ontotex.com/kim/.

7.8. KIM use cases and workflow

In this section we discuss a typical use case for the
KIM Platform, as well as some additional tasks, which
do not relate to end-user actions.

1. Pre-processing  semantic annotation, indexing,

and storage:
1.1 Semantic annotation: The first step is to perform a semantic annotation of the document
(or the corpus of documents) you are interested
in. For instance, if you use the KIM plug-in for
this purpose, you would typically open a web

page, select the entity classes and sub-classes in
the ontology tree, and run the annotation pro-
cess. As shown in Fig. 6, when semantic annotation is requested, the components used are
the KIM Annotation Server, which-through the
KIM Annotation API and using the custom IE
application (see Section 7.4.2)  sends the data
for processing to the GATE engine. As a result,
a list of semantic annotations is generated for
the respective document; also, in most cases
a number of new entities, properties, and attributes are found and, respectively, added to
the Semantic Repository of KIM (these knowledge resources are sent to, and kept in, the
Sesame repository via the Semantic Repository
API  this is done in the background).

1.2 Indexing/storage: The next step can be either
to carry out indexing of the document by the
semantic annotations generated (where the semantic Index API of the KIM Server plays its

part), or just to store the document(s) in the
KIM Document Persistence (document store)
via the Document Persistence API (i.e. the
GATE Document API in the KIM Server). In
the former case (indexing), the KIM Server
makes use  via the Index API  of the Lucene
IR engine, which was adapted to perform indexing and retrieval with respect to named en-
tities. In the case with storage of annotated doc-
uments, the storage is accomplished, using either the Oracle Serial Datastore, or the Lucene
Datastore.

2. Retrieval:

2.1. Retrieval (semantic querying): Here the main
dish is served  everything is ready for the
actual retrieval of the desired information (and
thus the satisfaction of the information need).
Now you can define a semantic query 
via the construction of a combination of pattern classes, restrictions, and relations among
entities-in order to get the relevant entities
and/or documents that match the query defi-
nition. You can do this via the KIM Web UI
front-end tool. The KIM Query API is used
for query processing.

2.2. Getting the results: At this stage, after the
definition of the query, you should select
the type of results that you want
to get
 entities, or documents. If you decide to
receive just the entities, the system will display all the entities that satisfy the query
conditions; in case want to see the documents where these entities are mentioned, the
system will retrieve all documents from the
currently used datastore, which have one or
more of the entities mentioned in them. The
documents are derived from the Document
Datastore.

3. Background activities:

3.1. Semantic repository population and enrich-
ment: The semantic repository of KIM consists
of the KIM ontology (KIMO) and the KIM
knowledge base (KB). It is enriched on a regular basis, both through the daily delivery of
documents, performed by the news collector
gazetteer, and via the newly discovered entities and descriptions, found during the semantic annotation process.

3.2. Entity ranking: On top of the corpus that is
gathered this way, entity ranking is performed
so that the level of popularity of the specific
entities can be detected.

An important note about the flexibility of KIM is that
it can be distinguished for the relative autonomy of its
components  they can all be used independently. Of
course, such uses only allow for a partial benefit from
the functionality of the system, which otherwise is a
powerful semantic annotation, indexing, and retrieval
tool (e.g. you would not make use of the semantically
enabled search capabilities of KIM in case you skip the
initial semantic annotation of the respective documentas a result, only standard keyword search would be
possible to carry out).

8. Related work

The semantic annotation of documents with respect
to an ontology and an entity knowledge base is discussed in [3] and [11]  although presenting interesting and ambitious approaches, these do not discuss the
usage of automated methods. The focus of [11] is the
manual semantic annotation for authoring web content,
while [3] targets on the creation of a web-based, open
hypermedia linking service, backed by a conceptual
model of document terminology. The semantic annotation is used also in the S-CREAM project, presented
in [10]. The approach there is interesting for the heavy
involvement of machine learning techniques for an automatic extraction of relations between the entities that
are annotated. A similar approach is also taken within
the MnM project [24], where the semantic annotations
can be placed inline in the document content and refer
to an ontology and a KB server (WebOnto), accessible
through an API. Another related approach is taken in
OOF, [6], which puts an emphasis on the collaborative
ontology development and annotation.

The approach for the extending of an information
retrieval engine with the usage of semantic metadata
has been pioneered in the course of the On-To-
Knowledge17 project. The QuizRDF module, [35],
used to offer enhancements of a standard full-text

17 On-To-Knowledge

is

an EC FP5

project-http://www.

ontoknowledge.org/.

A. Kiryakov et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 4979

search machine with the metadata extracted from
OntoBuilder. In contrast to KIM, there was not a
clear distinction between the sorts of semantics being
extracted and used for indexing: concepts,
terms,
entities. Due to the lack of background knowledge,
the resulting ontologies were relatively shallow. Even
with these problems, the approach demonstrated an
impressive potential for search efficiency improve-
ments. QuizRDF was a major source of inspiration for
the design of KIM.

An interesting named entity (NE) indexing and
question/answering system is presented in [20]. A flat
set of entity types is assigned to tokens and the annotations are incorporated in the content, in order to index
by NE type at a later stage. Once indexed, the content
is queried via NL questions, with NE tagging over the
question used to determine the expected answer type
(e.g. when was the UN established; UN here would be
tagged with ORG, specifying that the expected answer
type is an organization).

A significant amount of research on information extraction (IE18) has been performed in various projects
within the GATE framework (see [4,5,19])  with various resulting open-source tools and resources. We built
on those in order to provide a language technology,
which is open to the Semantic Web standards and tools.
The main drawback of using automated methods
like IE, [29], and wrapper induction, [31], without specific modifications for the Semantic Web, is that none
of these approaches expects an input or produces output
with respect to ontologies. This problem is discussed
extensively in S-CREAM [10] and a set of heuristics
for post-processing and mapping of the IE results to an
ontology was developed. However, such heuristics are
not sufficient for large-scale, domain-independent semantic annotation. Instead, we argue that IE and wrapper induction techniques need to use the ontology more
directly during the process of extraction. An approach
for creating an ontology-based IE system is discussed
in [1], and the KIM platform, hereby presented, has
improved on this work by connecting the IE modules
directly to the knowledge repository and introducing
a new disambiguation stage that determines which instance in the ontology is referred to in the text.

18 Information Extraction is a relatively young discipline in the
Natural Language Processing (NLP), which conducts a partial analysis of text in order to extract specific information.

AeroDAML (see [30]) takes an approach similar to
KIM, but implements it as a research prototype in a
much smaller scale. Perhaps the closest  in terms of
goals and architecture  to KIM is the SemTag system [27], which performs large-scale semantic annotation with respect to the TAP ontology [28]. It first
performs a lookup phase, annotating all possible mentions of instances from the TAP ontology. During the
second, disambiguation phase, SemTag uses a vectorspace model to assign the correct ontological class or
to determine that this mention does not correspond to a
class in TAP. The disambiguation is carried out by comparing the context of the current mention (10 words to
the left and 10 to the right) to the contexts of instances
in TAP with compatible aliases.

The TAP ontology (which includes about 65,000
instances) is very similar in size and structure to the
KIM Ontology and KB (e.g., each instance has a number of lexical aliases). An important characteristic of
both ontologies is that they are very light-weight and
they encode only essential properties of concepts and
instances. In other words, the goal is to cover instances
that are frequent, commonly-known, and searched for
(e.g., capital cities, names of presidents), rather than
encoding an extensive set of axioms enabling deep,
Cyc-style reasoning. As reported in [34], the heavyweight logical approach, undertaken in Cyc,19 is not
appropriate for many NLP tasks.

The difference between TAP and KIM KB is in the
level of ambiguity  TAP has a few entities that share
the same alias, while KIM KB has a lot more, due to its
richer collection of locations. Another important difference between KIM and SemTag is their goal. SemTag
only aims at the accurate classification of the mentions
that were found by matching the lexicalizations in the
ontology. KIM, on the other hand, is also aiming at
finding all mentions, i.e., coverage, as well as accu-
racy. The latter is a harder task because there tends to
be a trade-off between accuracy and coverage. In ad-
dition, SemTag does not attempt to discover and classify new instances, which are not already in the TAP
ontology. In other words, KIM performs two tasks 

19 Here we refer to the development of Cyc of the 1990s. Afterwards the Cyc team (Cycorp, http://www.cyc.com and others) invested considerable efforts to enable and demonstrate NLP and particularly IR applications of Cyc. Unfortunately, we are not aware of
reports on the later work.

ontology population with new instances and semantic
annotation, while SemTag performs semantic annotation only.

The SemTag system is based on a high-performance
parallel architecture  Seeker, where each node annotates about 200 documents per second. The demand
for such a parallelism comes from the big volumes of
data, which needs to be dealt with in many applications and which makes automatic semantic annotation
the only feasible option. A parallel architecture of a
similar kind is currently under development for KIM
and, in general, it is an important ingredient of largescale automatic annotation approaches.

9. Conclusion and future work

This paper presented the notion of semantic
annotation-an original meta-data model allowing
ontology-based named entity annotation, indexing, and
retrieval. A number of issues related to the representation and the usage of the semantic annotation were
addressed. The KIM platform was introduced in order
to demonstrate an implementation of this vision.

The evaluation work that has been done until now
does not provide enough empirical justification about
the feasibility of the approach, technology, and resources being used. The major obstacle is that there
are neither test data nor well-developed metrics for semantic annotation and retrieval.

Although na ve in some aspects, the KIM platform
provides a test bed and proves a number of hypotheses
and design decisions, as follows:
 It is worthwhile to use massive entity knowledge for
semantic annotation. Even without a comprehensive
disambiguation, the precision drawbacks seem ac-
ceptable.
 It is possible to store and query hundreds of thousands of entities together with their descriptions in
an RDF(S) repository (namely, Sesame).
 A simple but an efficient technique for entity-aware
 A few light-weight front end tools can deliver in
intuitive fashion the results of semantic annotation,
indexing, and retrieval.

IR is demonstrated.

The challenges towards the general approach can be

summarized as follows:

 Develop (or adapt) an evaluation metric, which properly measures the performance of a semantic annotation system.
 Experiment with different approaches towards the
disambiguation of named-entity references: an adaptation of a HMM learner, used successfully for nonsemantic disambiguation, is one of the first ideas;
techniques, similar to those used for word-sense disambiguation (namely, lexical-chaining); techniques
for symbolic context management, based on relations from the ontology; etc.
 Evaluation of the semantic IR in KIM against a traditional IR engine, so as to formally measure the
positive effect of semantic indexing (cutting out the
irrelevant results, because of the semantic restric-
tions; and retrieving even more correct results 
e.g. when an entity is mentioned with another alias,
but is still indexed by its unique identifier).
 KB editor that allows the management of the en-
 Document management UI which includes stor-
ing, deleting, semantic annotation and indexing of
documents.
 Merging of knowledge bases and the identification
of alternative entity descriptions. This problem is
well recognized by the developers of the TAP system as well. It can be seen as a data-integration
problem under the assumption regarding a shared
schema, but overlapping and some times conflicting
datasets.

 Extension of the KIM Web UI with:

tity descriptions.

KIM is about to be used as a basis and further developed in the context of a number of research projects,
among which: SEKT (http://www.sekt.semanticweb.
org); SWAN (http://www.deri.ie/projects/swan/), and
PrestoSpace (http://www.prestospace.org/).
