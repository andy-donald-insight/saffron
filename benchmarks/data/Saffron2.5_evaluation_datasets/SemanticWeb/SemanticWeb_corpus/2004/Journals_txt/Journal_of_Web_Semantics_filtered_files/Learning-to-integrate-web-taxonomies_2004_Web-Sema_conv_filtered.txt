Web Semantics: Science, Services and Agents

on the World Wide Web 2 (2004) 131151

Learning to integrate web taxonomies

Dell Zhanga,c,

, Wee Sun Leeb,c,

a Department of Computer Science, School of Computing, S16-05-08, 3 Science Drive 2,

National University of Singapore, Singapore 117543, Singapore

b Department of Computer Science, School of Computing, SOC1-05-26, 3 Science Drive 2,

National University of Singapore, Singapore 117543, Singapore

c Singapore-MIT Alliance, E4-04-10, 4 Engineering Drive 3, Singapore 117576, Singapore

Abstract

We investigate machine learning methods for automatically integrating objects from different taxonomies into a master
taxonomy. This problem is not only currently pervasive on the Web, but is also important to the emerging Semantic Web. A
straightforward approach to automating this process would be to build classifiers through machine learning and then use these
classifiers to classify objects from the source taxonomies into categories of the master taxonomy. However, conventional machine
learning algorithms totally ignore the availability of the source taxonomies. In fact, source and master taxonomies often have
common categories under different names or other more complex semantic overlaps. We introduce two techniques that exploit
the semantic overlap between the source and master taxonomies to build better classifiers for the master taxonomy. The first
technique, Cluster Shrinkage, biases the learning algorithm against splitting source categories by making objects in the same
category appear more similar to each other. The second technique, Co-Bootstrapping, tries to facilitate the exploitation of intertaxonomy relationships by providing category indicator functions as additional features for the objects. Our experiments with
real-world Web data show that these proposed add-on techniques can enhance various machine learning algorithms to achieve
substantial improvements in performance for taxonomy integration.
 2004 Elsevier B.V. All rights reserved.

Keywords: Semantic Web; Ontology mapping; Taxonomy integration; Classification; Machine learning

1. Introduction

A taxonomy, or directory or catalog, is a division of a
set of objects (documents, images, products, goods, ser-
vices, etc.) into a set of categories. There are a tremendous number of taxonomies on the Web, and we often
need to integrate objects from various taxonomies into
a master taxonomy.

This problem is currently pervasive on the Web,
given that many websites are aggregators of informa-

Corresponding author. Tel.: +65 68744251; fax: +65 67794580.
Co-corresponding author. Tel.: +65 68744526;

fax: +65 67794580.

E-mail addresses: dell.z@ieee.org (D. Zhang),

leews@comp.nus.edu.sg (W.S. Lee).

1570-8268/$  see front matter  2004 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2004.10.001

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

tion from various other websites [1]. For example, a
Web marketplace like Amazon1 may want to combine
goods from multiple vendors catalogs into its own;
a Web portal like NCSTRL2 may want to combine
documents from multiple libraries directories into its
own; a company may want to merge its service taxonomy with its partners; a researcher may want to
merge his/her bookmark taxonomy with his/her peers;
Singapore-MIT Alliance3, an innovative engineering
education and research collaboration among MIT, NUS
and NTU, has a need to integrate the academic resource
(course, seminar, report, software, etc.) taxonomies of
these three universities.

This problem is also important to the emerging
Semantic Web [2], where data has structures and
ontologies describe the semantics of the data, thus
better enabling computers and people to work in
cooperation. On the Semantic Web, data often come
from many different ontologies, and information
processing across ontologies is not possible without
knowing the semantic mappings between them. Since,
taxonomies are central components of ontologies,
ontology mapping necessarily involves finding the
correspondences between two taxonomies, which is
often based on integrating objects from one taxonomy
into the other and vice versa [3,4].

If all taxonomy creators and users agreed on a universal standard, taxonomy integration would not be so
difficult. But the Web has evolved without central edi-
torship. Hence, the correspondences between two taxonomies are inevitably noisy and fuzzy. For illustration,
consider the taxonomies of two Web portals Google4
and Yahoo5: what is Arts/Music/Styles/ in one may
be Entertainment/Music/Genres/ in the other, category Computers and Internet/Software/Freeware/
and category Computers/Open Source/Software/
have similar contents but show non-trivial differ-
ences, and so on. It is unclear if a universal standard will appear outside specific domains, and even
for
there is a need to integrate
objects from legacy taxonomies into the standard
taxonomy.

those domains,

1 http://www.amazon.com/.
2 http://www.ncstrl.org/.
3 http://web.mit.edu/sma/.
4 http://www.google.com/.
5 http://www.yahoo.com/.

Manual

taxonomy integration is tedious, error-
prone, and clearly not possible at the Web scale. A
straightforward approach to automating this process
would be to formulate it as a classification problem,
a problem that has been well-studied in the machine
learning area [5]. Conventional machine learning algorithms would simply train classifiers on the master
taxonomy data, while totally ignoring the source taxonomy data. However, the availability of the source
taxonomy data could be helpful to build better classifiers for the master taxonomy if there is some semantic overlap between the source and master taxonomies,
particularly when the number of training examples is
not very large.

One of the simplest assumptions that can be helpful for improving learning is that each source category
is likely to be mapped entirely into a master category.
Under this assumption, it is helpful to bias the learning algorithm to prefer functions that do not split objects of the source categories into different master cat-
egories. Agrawal and Srikant proposed the Enhanced
Na ve Bayes algorithm [1], which modifies the Na ve
Bayes algorithm [5] such that if most of the objects
from a source category are classified into a single master category, it intends to classify more objects from
that source category into that master category. This has
the effect of reducing the number of objects that are
classified differently from the majority class within a
source category. In this paper, we describe a different
mechanism, Cluster Shrinkage, which biases learning
algorithms that use object distance/similarity measures
from splitting the objects in a source category into different master categories. This is done by changing the
distance/similarity measure so that objects in the same
source category become closer to each other. We applied this technique to the Transductive Support Vector
Machine [68] and the Spectral Graph Transducer [9].
While useful, assuming that each source category
is mapped entirely into a master category is clearly
simplistic. Potentially useful semantic relationships between a master category C and a source category S in-
cludes:

 C = S (identical): an object belongs to C if and only
 C S =  (mutual exclusion): if an object belongs to

if it belongs to S;

S it cannot belong to C;

also belong to C;

 C S (superset): any object that belonging to S must
 C S (subset): any object not belonging to S also
 C and S overlap but neither is a superset of the

cannot belong to C; and

other.

In addition, semantic relationships may involve
multiple source and master categories. For example,
a master category C may be a subset of the union of
two source categories Sa and Sb, so, if an object does not
belong to either Sa or Sb, it cannot belong to C. The realworld semantic relationships are noisy and fuzzy, but
they can still provide valuable information for classifi-
cation. For example, knowing that most (80%) objects
in a source category S belong to one master category Ca
and the rest (20%) of the examples belong to another
master category Cb is obviously helpful. The difficulty
is that knowledge about those semantic relationships
is not explicit but hidden in the data. One method of
exploiting these relationships without explicitly knowing them is to include indicator variables for source
categories as additional features for the objects so that
machine learning algorithms can utilize such informa-
tion. Unfortunately the values of these variables are not
known for the objects in the master taxonomy, which
we use for learning. We propose an iterative technique
that alternatively uses information in the source and
master taxonomies to construct the required features for
taxonomies. We call this technique Co-Bootstrapping
as it bootstraps on the information from the classifier of the other taxonomy in order to improve itself
at each iteration. We applied this technique to the AdaBoost algorithm [1012] and the Support Vector Machine [13,14].

Our experiments on the datasets from two of todays main Web taxonomies (Google/Dmoz and Ya-
hoo) show that all the proposed techniques can enhance various machine learning algorithms to achieve
substantial improvements in performance for taxonomy integration. The Co-Bootstrapping technique,
which may be able to exploit more source-master
relationships than the Cluster Shrinkage technique,
appears to give better performance although we
have not done extensive experimentation on this
comparison.

The rest of this paper is organized as follows. In Section 2, we give the formal problem statement. In Sec-

tions 35, we describe Enhanced Na ve Bayes, Cluster
Shrinkage and Co-Bootstrapping. In Section 6, we conduct experimental evaluations. In Section 7, we review
the related work. In Section 8, we discuss hierarchical taxonomies and other extensions. In Section 9, we
make concluding remarks.

2. Problem statement

Taxonomies are often organized as hierarchies. In
this work, we assume for simplicity that any objects
assigned to an interior node really belong to a leaf node,
which is an offspring of that interior node. Since, we
now have all objects only at leaf nodes, we can flatten
the hierarchical taxonomy to a single level and treat
it as a set of categories [1]. Possible extensions that
may be able to exploit the presence of hierarchies are
discussed in Section 8.

Now we formally define the taxonomy integration

problem that we are solving. Given two taxonomies:
 A master taxonomy M with a set of categories C1,
C2, . . ., CM each containing a set of objects; and
 A source taxonomy N with a set of categories S1, S2,
. . ., SN each containing a set of objects,

we need to find the correct categories in M for each
object in N.

The objects in M are called master objects, and the
objects in N are called source objects. C1, C2, . . ., CM
are called master categories, and S1, S2, . . ., SM are
called source categories.

To formulate taxonomy integration as a classification problem, we take master categories C1, C2, . . .,
CM as classes, the master objects as training examples
and the source objects as test examples, so that taxonomy integration can be automatically accomplished
by predicting the classes of each test example. Such
a classification problem has several prominent charac-
teristics:
 It is essentially a multi-class multi-label classification problem, in the sense that there could be more
than two possible classes (multi-class) and one object could belong to more than one class (multi-
label);
 The test examples are already known to the machine

learning algorithm; and

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

 The test examples are already labeled with a set of
categories which are usually not identical with but
relevant to the set of categories to be assigned.

For machine learning algorithms that only lead to
binary (positive/negative) classifiers, multi-class multilabel classification could be achieved via the popular
one-vs.-rest ensemble method: creating a binary classifier for each class Ci that takes the objects belonging
to Ci as positive examples and all other objects as negative examples. For the binary classifier corresponding
to Ci, we denote the class of negative examples (objects
that do not belong to Ci) by  Ci.

Some of the algorithms for this problem require a
tune set, i.e., a set of objects whose source categories
and master categories are all known, to find the optimal
values of its parameters. To achieve good performance
using those algorithms, there should be an adequate
number of objects in the tune set. If the set of master
objects and the set of source objects have a decent size
intersection, then this intersection can act as a tune set.
Otherwise, a tune set can only be made available by
selecting a subset of source objects and manually labeling their master categories. Selection of the source
objects to be placed in the tune set can be done via
random sampling or active learning [1].

In this paper, we focus on text data, i.e., objects in
taxonomies are actually documents. Other types of data
can be treated using similar techniques.

3. The Enhanced Na ve Bayes technique

In this section, we describe Agrawal and Srikants
Enhanced Na ve Bayes (ENB) algorithm [1] for taxonomy integration.

3.1. Na ve Bayes

Na ve Bayes (NB) is a well-known text classification algorithm [5]. NB tries to fit a generative model
for documents using training examples and then apply
this model to classify test examples. The generative
model of NB assumes that a document is generated by
first choosing its class according to a prior distribution
of classes, and then producing its words independently
according to a (typically multinomial) distribution of
terms conditioned on the chosen class [15].

Given a test document d, NB classifies d into a class
Ci if and only if Pr[Ci|d] > Pr[  Ci|d]. The posterior
probability Pr[C/d] can be computed via Bayess rule:
Pr[C|d] = Pr[C, d]
Pr[d]

= Pr[C]Pr[d|C]

Pr[d]

 Pr[C]Pr[d|C].

Ci

The probability Pr[C] can be estimated by:

Pr[C] = |C|
|Ci| ,
where |C| is the number of training documents in C.
With the na ve assumption that the words of d occur
independently given the class C, the probability Pr[d/C]
can be estimated by:
Pr[d|C] =

(Pr[w|C])n(d,w),

w d

where n(d, w) is the number of occurrences of w in d.
The probability Pr[w|C] can be estimated by:
Pr[w|C] =

n(C, w) + 
wi  V (n(C, wi) + )

where n(C, w) is the number of occurrences of w in
training documents in C, V the vocabulary of terms,
and 0 <  1, the Lidstones smoothing parameter [16].
Taking logs, we see that NB is actually a linear classi-

fier:
log Pr[C|d]  log

(Pr[w|C])n(d,w)
(n(d, w)  log Pr[w|C]) + log Pr[C].

Pr[C]

w d

w d

3.2. Enhanced Na ve Bayes

Suppose that the classifier predicts that 90% of the
objects in the source category S fall in the master category C and the other 10% fall in other master categories.
Those 10% that fall in the other master categories are
probably prediction errors [1]. We can exploit this idea
to enhance classification performance by making the
master categories that own a large percentage of objects from S receive more objects from S and at the
same time making the master categories that own a
small percentage of objects from S receive fewer objects from S. One effect of doing this is that it reduces

the likelihood of splitting source categories, particularly those with most objects classified into one master
category.

Agrawal and Srikant recently proposed the Enhanced Na ve Bayes (ENB) algorithm [1] according
to the above idea. They assumed that one document
only belongs to one master category. Here, we relax
this assumption and extend their ENB algorithm to
multi-class multi-label classification via the one-vs.-
rest method (as stated in Section 2).

Given a source document d in a source category S,
ENB classifies it into a master category Ci if and only
if Pr[Ci|d, S] > Pr[  Ci|d, S]. The posterior probability
Pr[C|d, S] can be computed as:
Pr[C|d, S] = Pr[C, d, S]
Pr[d, S]

= Pr[S]Pr[C, d|S]

Pr[d, S]

 Pr[C, d|S].

ENB invokes a simplification that assumes d and S

are independent given C, therefore,
Pr[C, d|S] = Pr[C|S]Pr[d|S, C] = Pr[C|S]Pr[d|C]

= Pr[C|S]

(Pr[w|C])n(d,w).

w d

The probability Pr[w|C] can be estimated in the same
way as in NB.
For the probability Pr[C|S], a simple estimation
could be achieved by first using trained NB classifiers
to tentatively classify source objects (test examples)
into master categories (classes) and then calculating,

= |C  S|
C|C  S| ,

Pr[C|S] = |C  S|
|S|
where |S| is the number of documents in S and |C S|
is the number of documents in S that are classified into
C by NB. However, this simple estimation is based on
NBs classification result and is, therefore not so reli-
able. As an illustration, consider a scenario where we
know that the categorizations of M and N are exactly
identical. With a perfect classifier, the above estimation should be 1 for the true class and 0 for all other
classes. With a classifier that is 90% accurate, the above
estimation would be 0.9 for the true class and 0.1 distributed over all other classes. In this case, we would
like to increase Pr[C|S] for the class having most examples from S and decrease Pr[C|S] for all other classes.
More generally, we would like to use a parameter   0

|C|  |C  S|
C (|C|  |C  S|)

 |C|  |C  S|.

that reflects the degree of semantic overlap between M
and N to decide the strength of such induction bias. In
addition, we would like to use Pr[C] to smooth the estimation of Pr[C|S]. ENB uses an estimation formula
that satisfies theses goals:
Pr[C|S] =

Taking logs, we see that ENB is still a linear classi-

w d

fier:
log Pr[C|d, S]  log

Pr[C|S]

(Pr[w|C])n(d,w)
(n(d, w)  log Pr[w|C]) + log Pr[C|S].
When a source document d belongs to multiple
source categories S(1), S(2), . . ., S(g), the above ENB
formulas should be amended by substituting S(1), S(2),
. . ., S(g) for S.

w d

Comparing the classification functions of ENB and
NB, it is obvious that all ENB does is to shift the classification thresholds of its base NB classifiers, no more
and no less. When  = 0, ENB is same as NB. When
 > 0, we have the following result from [1] which indicates that ENB tends to re-classify the documents from
S into the classes with more S-documents.
Theorem 1 ([1]). Let |Cx  S| denote the number of
documents in S that are classified into Cxby NB. For
any document d in a source category S, suppose ENB
with parameter 1  0 predicts its class to beCx1and
ENB with parameter 2  0 predicts its class to beCx2.
If 1 > 2, then|Cx1

 S|  |Cx2

 S|.

From the above theorem, we see that a document d
in a source category S can only switch from a class with
fewer S-documents to a class with more S-documents
and not the other way around as ENB increases its parameter . This has the effect of reducing the number
of objects that are classified differently from the majority class within a source category. A good value for
 can be found using a tune set.

4. The Cluster Shrinkage technique

In this section, we describe the Cluster Shrinkage
technique in detail, and show how it can enhance the
Transductive Support Vector Machine [68] and the

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

Spectral Graph Transducer [9] for taxonomy integra-
tion.

4.1. Vector Space Model

Documents (text objects) can be represented as vectors using Vector Space Model [17]. For each document d in a collection of documents D, it is first preprocessed by removal of stop-words and stemming,
and then represented as a feature vector x = (x1, x2,
. . ., xm), where xi indicates the importance weight
of term wi (the ith distinct word occurred in D).
Following the popular TF IDF weighting scheme,
we set the value of xi to the product of the term
frequency TF(wi, d) and the inverse document frequency IDF(wi), i.e., TF(wi, d)  IDF(wi). The term
frequency TF(wi, d) means the number of occurrences
of wi in d. The inverse document frequency is defined as IDF(wi) = log(|D|/DF(wi)), where |D| is
the total number of documents in D, and DF(wi)
is the number of documents in which wi occur. Fi-
nally, all feature vectors are normalized to have unit
length.

4.2. Support Vector Machine

Support Vector Machine (SVM) [13,14] is a powerful machine learning algorithm which has shown outstanding classification performance in practice. It is
based on a solid theoretical foundationstructural risk
minimization [18].

The

classification function of

In its simplest linear form, an SVM is a hyperplane
that separates the positive and negative training examples with maximum margin, as shown in Fig. 1. Large
margin between positive and negative examples has
been proven to lead to good generalization [18].
an SVM is
f(x) =wx + b, where wx
is the dot product
between w (the normal vector to the hyperplane)
and x (the feature vector representing an example).
The margin for an input vector xi is yif(xi) where
yi  {1, 1} is the correct class label for xi. In the
linear case, the margin is geometrically the distance
from the hyperplane to the nearest positive and
negative examples. Seeking the maximum margin can be expressed as a quadratic optimization

Fig. 1. An SVM is a hyperplane that separates the positive and negative training examples with maximum margin. The examples closest
to the hyperplane are called support vectors (marked with circles).

problem:
minw w  w,

yi(w  xi + b)  1,

s.t.

i.

When positive and negative examples are linearly
inseparable, soft-margin SVM tries to solve a modified optimization problem that allows but penalizes the
examples falling on the wrong side of the hyperplane.

4.3. Transductive learning

Regular learning algorithms try to induce a general
classification function, which has high accuracy on the
whole distribution of examples. However, this so-called
inductive learning setting is often unnecessarily com-
plex. For the classification problem in taxonomy integration context, the set of test examples to be classified are already known to the learning algorithm. In
fact, we do not care about the general classification
function, but rather attempt to achieve good classification performance on that particular set of test exam-
ples. This is exactly the goal of transductive learning
[6].

The transductive learning task is defined on a fixed
array of n examples X = (x1, x2, . . ., xn). Each example
has a desired classification Y = (y1, y2, . . ., yn), where
yi  {+1, 1} for binary classification. Given the labels for a subset Yl  [1, . . ., n] of |Yl| = l < n (training)
examples, a transductive learning algorithm attempts

to predict the labels of the remaining (test) examples
in X as accurately as possible.

Why can transductive learning algorithms outperform inductive learning algorithms? Transductive
learning algorithms can observe the examples in the
test set and potentially exploit structure in their distri-
bution. For example, there usually exists a clustering
structure of examples: the examples in same class tend
to be close to each other in feature space, and such kind
of knowledge is helpful to learning, especially when
there are only a small number of training examples [8].
Most machine learning algorithms assume that both
the training and test examples come from the identical
data distribution. This assumption does not necessarily
hold in the case of taxonomy integration. Intuitively,
transductive learning algorithms seem to be more robust than inductive learning algorithms to the violation
of this assumption, since transductive learning algorithms takes the test examples into account for learn-
ing. This interesting issue needs to be stressed in the
future.

4.4. Transductive Support Vector Machine

Transductive Support Vector Machine (TSVM),
which was introduced by [6] and later refined by [7,8],
extends SVM to transductive learning setting. A TSVM
is essentially a hyperplane that separates the positive
and negative training examples with maximum margin on both training and test examples, as shown in
Fig. 2 (adopted from [8]). It is the clustering structure
of examples that TSVM exploits as prior knowledge to
improve classification performance [8].

TSVM requires a parameter p to specify the fraction of test examples to be classified into the positive
class [8]. To estimate the value of p, we first run SVM
and get p (the fraction of test examples predicted to be
positive by SVM), then we set the value of p to a
smoothed version of p:   p + (1  )  0.5, where
0   1.

4.5. Spectral Graph Transducer

Recently, Joachims introduced a new transductive
learning algorithm, Spectral Graph Transducer (SGT)
[9], which can be seen as a transductive version of the
k nearest-neighbor (kNN) classifier.

Fig. 2. A TSVM is essentially a hyperplane that separates the positive and negative training examples with maximum margin on both
training and test examples (cf. Fig. 1). The test examples are represented by dots.

T , where,

SGT works in three steps. The first step is to build
the k nearest-neighbor (kNN) graph G on the set of
examples X. The kNN graph G is similarity-weighted
and symmetrized: its adjacency matrix is defined as
A = A + A


The function sim(,) can be any reasonable similarity measure. In the following, we will use a common
similarity function:

xk  kNN(xi)sim(xi, xk)

if xj  kNN(xi)

sim(xi, xj)

ij =

sim(xi, xj) = cos  = xi  xj
||xi||||xj|| ,

else

matrix with Bii =

where  represents the angle between xi and xj. The
second step is to decompose G into spectrum, specifi-
cally, compute the smallest 2 to d + 1 eigenvalues and
corresponding eigenvectors of Gs normalized Lapla-
1(B A), where B is the diagonal degree
cian L = B
jAij. The third step is to classify
the examples. Given a set of training labels Yl, SGT
makes predictions by solving the following optimization problem which minimizes the normalized graph
cut with constraints:

cut(G+, G

miny

|{i : yi = +1}||{i : yi = 1}| ,

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

Fig. 3. SGT does classification through minimizing the normalized
graph cuts with constraints.

yi = +1,
yi = 1,
y = {+1,1}n

if i Yl and positive
if i Yl and negative

s.t.


i G+

) =

where G+ and G
denote the set of examples (vertices)
with yi = +1 and yi =1, respectively, and the cut-value
cut(G+, G
j  G Aij is the sum of the
edge weights across the cut (bi-partitioning) defined
by G+ and G
. Although this optimization problem is
known to be NP-hard, there are highly efficient techniques based on the spectrum of the graph that give
good approximation to the global optimal solution [9].
For example, consider a classification problem with
six examples X = x1, x2, x3, x4, x5, x6) whose kNN
graph G is shown in Fig. 3 (adopted from [9]) with
line thickness indicating edge weight. Given a set of
training labels Yl = {1, 6}: y1 = +1 and y6 =1, SGT
predicts y2 and y3 to be positive, whereas, predicts y4
and y5 to be negative, because cutting G into G+ = {x1,
= {x4, x5, x6} gives the minimal nor-
x2, x3} and G
malized cut-value while keeping x1  G+ and x6  G


Unlike most other transductive learning algorithms,
SGT does not need any additional heuristics to avoid
unbalanced splits [9]. Furthermore, since SGT has an
approximation that can be solved globally with efficient
spectral techniques, it is more robust and promising
than existing transductive learning techniques [9].


4.6. Cluster Shrinkage

One intuition about taxonomy integration is that if
two objects belong to the same source category S, they
are more likely to belong to the same master category C
than to be assigned into different master categories. In
other words, the original categorization should be preserved to some degree. We hereby propose the Cluster
Shrinkage (CS) technique, which attempts to enhance
the classification by treating all source and master categories as clusters and then shrinking them.

Fig. 4. The distance-based Cluster Shrinkage (CS) technique.

We use slightly different CS techniques

for

SVM/TSVM and SGT.

4.6.1. Distance-based Cluster Shrinkage

The CS technique for SVM/TSVM contracts a
cluster (category) by reducing the distances among

its objects. As depicted in Fig. 4,
this could be
achieved through relocating each object x S to
= c+(1 )x, where c = (
x Sx)/|S| is the cen-

ter of S and 0  1. The formula of x
is actually a
linear interpolation of x and c with parameter .
Denoting the Euclidean distance between two examples (vectors) with function d(, ), we give the following theorem which indicates that the distance-based
CS lets all objects in the same category become closer
to each other.
Theorem 2. For any pair of objects x1  S and x2  S,
suppose the center of S is c, the distance-based CS technique makes x1and x2becomex
2, respectively,
then,
d(x

2)  d(x1, x2).
, x

1andx

Since,

1 = c + (1 )x1

Proof.
2 = c + (1 )x2, we get,

d(x

and

, x
2)
= ||x

|| = ||(c + (1  )x1)

(c + (1  )x2)|| = ||(1  )(x1  x2)||
= (1  )||x1  x2|| = (1  )d(x1, x2).

Since, 0  1, we get 0 1  1, so,
d(x

2)  d(x1, x2).
, x

It is possible for an object x to belong to multiple
categories S(1), S(2), . . ., S(g) whose centers are c(1),
c(2), . . ., c(g), respectively. This may pose problems for
the Cluster Shrinkage technique. One possible method
of dealing with this is to create generalized categories
where each generalized category is identified with a set
of original categories. For example, we can conceptually create a generalized category S
that corresponds
to the set of original categories S(1), S(2), . . ., S(g), and

if and only if it belongs to S(1),
allocate an object to S
S(2), . . ., S(g). Cluster Shrinkage can then be done on
generalized categories in the same way as before. Un-
fortunately, using generalized categories may result in
many very small clusters. As a compromise, we retain
the original cluster center for an object that belongs
to a single category and use c = (
c(h))/g as the
cluster center for an object that belongs to multiple categories S(1), S(2), . . ., S(g).

h=1

Inductive learning algorithms like SVM do not take
test examples into account for construction of classi-
fiers, therefore performing CS on the source taxonomy
data (test examples) has no opportunity to change the
classification functions. That is to say, the CS technique
cannot provide much help to inductive learning algo-
rithms. In contrast, TSVM seeks the maximum margin
hyperplane (the thickest slab) in both training and test
examples, therefore making objects in a category S become closer to each other tells TSVM to avoid splitting
S. Consequently performing CS on the source taxonomy data (test examples) guides TSVM to preserve
the original categorization to some degree, while doing
classification. Furthermore, we found that performing
CS on the master taxonomy data (training examples)
as well as the source taxonomy data lets TSVM reduce its dependence on the master taxonomy data and
puts more emphasis on taking advantage of the source
taxonomy data. Therefore, we perform CS on both the
source and master taxonomies to enhance TSVM for
taxonomy integration, as shown in Fig. 5.

One salient property of SVM/TSVM is that the only
operation it requires is the computation of dot products
or kernels between pairs of examples [13]. For linear
kernel SVM/TSVM, the distance-based CS technique
could be implemented implicitly by just substituting
the kernel function k with the following one:

Fig. 5. A CS-TSVM attempts to preserve the original categorization
of the source taxonomy to some degree, while doing classification
(cf. Fig. 2). A dot labeled with number i represents an object in the
source category Si.

CS-k(x1, x2)

= x
= (c1 + (1  )x1)  (c2 + (1  )x2)
= 2c1  c2 + (1  )2x1  x2 + (1  )

(c1  x2 + c2  x1)

= 2k(c1, c2) + (1  )2k(x1, x2)
+(1  )(k(c1, x2) + k(c2, x1)).

For non-linear kernel SVM/TSVM, similar formula
could also be derived. The CS technique enhanced
SVM/TSVM is referred to as CS-SVM/CS-TSVM.
The parameter 0  1 controls the strength of
the clustering structure of examples. Increasing  results in more influence of the original categorization information on classification. When  = 1, CS-
SVM/CS-TSVM classifies all objects belonging to one
source category S as a whole into same master cate-
gories. When  = 0, CS-SVM/CS-TSVM backs off to
SVM/TSVM. As long as the value of  is set appro-
priately, CS-SVM/CS-TSVM should never be worse
than SVM/TSVM because it includes SVM/TSVM as
a special case. A good value for  can be found using
a tune set.

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

4.6.2. Similarity-based Cluster Shrinkage

The CS technique for SGT contracts a cluster (cat-
egory) by increasing the similarities among its objects.
This could be achieved through substituting the regular similarity sim(, ) between two examples xi  Si
and xj  Sj with the CS similarity CS-sim(xi, xj) =
  sim(ci, cj) + (1  )  sim(xi, xj), where ci and cj
are the centers of Si and Sj, respectively, 0   1.

Theorem 3. For any pair of objects xiand xjin the
same category S, CS-sim(xi, xj) sim(xi, xj).

Proof. Suppose the center of S is c, we get:
CS-sim(xi, xj) =   sim(c, c) + (1  )  sim(xi, xj).
Since, sim(c, c) sim(xi, xj) and   0, we get:
  sim(c, c)    sim(xi, xj),
  sim(c, c) + (1  )  sim(xi, xj)    sim(xi, xj)

therefore

+ (1  )  sim(xi, xj),

i.e.
CS-sim(xi, xj)  sim(xi, xj).

From the above theorem, we see that the similaritybased CS technique increases the similarities between
pairs of examples that are known in the same category,
consequently puts more weight to the edge between
them in the kNN graph. Since, SGT seeks the minimum
normalized graph cut, stronger connection among examples in a category S directs SGT to avoid splitting S,
in other words, to preserve the original categorization
of the taxonomies to some degree while doing classifi-
cation. Due to the same reason as in Section 4.6.1, we
perform CS on both the source and master taxonomies
to enhance SGT for taxonomy integration.

The CS technique enhanced SGT is referred to as
CS-SGT. The parameter  in CS-SGT plays the same
role as the parameter  in CS-SVM/CS-TSVM.

5. The Co-Bootstrapping technique

In this section, we describe the Co-Bootstrapping
technique in detail, and show how it can enhance the
AdaBoost algorithm [1012] and the Support Vector
Machine [13,14] for taxonomy integration.

5.1. AdaBoost

Boosting [19,20] is a general method for combining
classifiers, usually weak hypotheses (moderately accurate classification rules), into a highly accurate classi-
fier.
Documents (text objects) can be represented using
a set of term-features FT = {fT 1, fT 2, ..., fTn}. The
term-feature fTh(1 h n) of a given object x is a binary feature indicating the presence or absence of wh
(the hth distinct word in the document collection) in x,
i.e.,
fTh =

if wh  x
if wh / x

Let X denote the domain of possible objects, and let
Y be a set of k possible classes. A labeled example is
a pair (x, Y) where x X is an object and Y  Y is the
set of classes which x belongs to. Define Y[l] for l  Y
to be:
Y[l] =

+1

if l  Y
if l / Y

A hypothesis is a real-valued function h : X  Y 
R. The sign of h(x, l) is a prediction of Y[l] for x, i.e.,
whether object x is contained in class l. The magnitude
of h(x, l) is interpreted as a measure of confidence in
the prediction.

Based on a binary feature f, we are interested in weak
hypotheses h, which are simple decision stumps of the
form:
h(x, l) =
where c1l, c0l  R.

if f = 1
if f = 0

c1l
c0l

The most popular boosting algorithm is AdaBoost
which was introduced in 1995 by Freund and Schapire
[10]. Our work is based on a multi-class multi-label
version of AdaBoost, AdaBoost.MH [11,12], which is
described in Fig. 6.
Given m training examples (x1, Y1), . . ., (xm, Ym)
where each xi  X, Yi  Y, AdaBoost.MH dynamically
maintains a distribution Dt over all objects and classes.
Initially this distribution D1 is uniform. In the tth round,
the optimal weak hypothesis ht is selected based on the
set of training examples and the current distribution
Dt. Then a parameter t is chosen, and the distribution

Fig. 6. The boosting algorithm AdaBoost.MH.

Dt is updated in a manner that puts more weights on
difficult examples (object-class pairs) that are misclassified by ht. Please refer to [11,12] for the details
on computing optimal ht and t. This procedure repeats
for T rounds. The final hypothesis H(x, l) is actually a
weighted vote of weak hypotheses
tht(x, l), and
the final prediction can be computed according to the
sign of H(x, l).

t=1

5.2. Co-Bootstrapping

If we have indicator functions for each category of
N, we can imagine taking those indicator functions as
features when we learn the classifier for M. This allows us to exploit the semantic relationships among
the categories of M and N without explicitly figuring
out what the semantic relationships are. More specif-
ically, for each object in M, we augment the ordinary
term-features with a set of category-features FN =

{fN1, fN2, ..., fNN} derived from N. The categoryfeature fNj(1  j  N) of a given object x is a binary
feature indicating whether x belongs to category Sj (the
jth category of N), i.e.,
if x S
fNj =
if x / Sj

In the same way, we can get a set of categoryfeatures FM = {fM1, fM2, ..., fMM} derived from M
to be used for supplementing the features of objects in
N. The remaining problem is to obtain these indicator
functions, which are initially not available.

The Co-Bootstrapping technique overcomes the
above obstacle by utilizing the bootstrapping idea, as
shown in Figs. 7 and 8. Let BT
r (F ) denote a classifier for taxonomy Ts categorization based on feature set F at step r. Initially, we build a classifiers

0 (FT ) based on only term-features, then use it to
classify the objects in M (the training examples) into

Fig. 7. The Co-Bootstrapping (CB) algorithm.

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

vector and the category-feature vector into a heterogeneous feature vector for each object. In this case, we put
equal combining weights on different kinds of features
by default.

The CB technique enhanced AdaBoost (AB) is referred to as CB-AB. The CB technique enhanced SVM
is referred to as CB-SVM.

6. Experiments

We have conducted experiments to evaluate the
effectiveness of the proposed techniques. The used
datasets, softwares, and some other supplementary information are listed in http://www.comp.nus.edu.sg/
smadellz/publications/ltiwt supp.html.

Fig. 8. The Co-Bootstrapping (CB) process.

6.1. Datasets

1 (FT  FM). The new classifier BN
0 (FT ) because BN

the categories of N, thus we can predict the value
of each category-feature fNj  FN for each object
x M. At the next step, we will be able to build
1 (FT  FN) using the predicted values of FN of the

training examples. Similarly, we can build BM
0 (FT )
1 (FT  FM)
and BN
1 (FT  FM)
ought to be better than BN
leverages more knowledge. Hence, we can predict the
value of each category-feature fNj  FN for each object
x M more accurately using BN
1 (FT  FM) instead of
2 (FT  FN).

0 (FT ), and afterwards we can build BM
2 (FT  FN) is very likely to be better than
Also, BM
1 (FT  FN) because BM
2 (FT  FN) is based on a

more accurate prediction of FN. This process can be
repeated iteratively in a ping-pong manner. We call
this technique Co-Bootstrapping6 (CB), since the two
r (FT  FM) collaboclassifiers BM
rate to bootstrap themselves together.

(FT  FN) and BN

The CB technique can be used to enhance AdaBoost
(AB) by using both the term-features and the category-
features. Since, AdaBoost is able to combine heterogeneous weak hypotheses automatically, therefore alleviates the problem of how to weight term-features and
category-features [22]. The CB technique can also be
used to enhance SVM by combining the term-feature

We have collected five datasets from two of todays
main Web taxonomies: Google/Dmoz and Yahoo. One
dataset includes the slice of Googles taxonomy and
the slice of Yahoos taxonomy about websites on one
specific topic, as shown in Table 1. In each slice of tax-
onomy, we take only the top-level directories as cate-
gories, e.g., the Movie slice of Googles taxonomy
has categories like Action, Comedy, Horror, etc.
For each dataset, we show in Table 2, the number of
categories occurred in Google and Yahoo, respectively.
In each category, we take all items listed on the corresponding directory page and its sub-directory pages
as its objects. An object (list item) corresponds to a
website on the World Wide Web, which is usually described by its URL, its title, and optionally a short annotation about its content, as illustrated in Fig. 9. Here,
each object is considered as a text document composed
of its title and annotation7.

For each dataset, we show in Table 3 the number of
objects occurred in Google (G), Yahoo (Y), either of
them (G Y), and both of them (G Y), respectively.
The set of objects in G Y covers only a small portion (usually less than 10%) of the set of objects in
Google or Yahoo alone, which suggests the great benefit of automatically integrating them. This observation
is consistent with [1].

6 This idea is named Cross-Training in [21].

7 Note that this is different with [1,21] which take actual Web

pages as objects.

Table 1
The datasets

Book
Disease
Movie
Music
News

Google

Yahoo

/Top/Shopping/Publications/Books/
/Top/Health/Conditions and Diseases/
/Top/Arts/Movies/Genres/
/Top/Arts/Music/Styles/
/Top/News/By Subject/

/Business and Economy/Shopping and Services/Books/Bookstores/
/Health/Diseases and Conditions/
/Entertainment/Movies and Film/Genres/
/Entertainment/Music/Genres/
/News and Media/

Table 2
The number of categories

6.2. Tasks

Google

Yahoo

Book
Disease
Movie
Music
News

Table 3
The number of objects

Book
Disease
Movie
Music
News

Google

Yahoo

G Y

G Y

The number of categories per object in these datasets
is 1.54 on average. This observation confirms our previous statement in Section 2 that an object may belong
to multiple categories. Hence, multi-label classification
methods and performance measures should be used.

The category distributions in all theses datasets are
highly skewed. For example, in Googles Book taxon-
omy, the most common category contains 21% objects,
but 88% categories contain fewer than 3% objects and
49% categories contain fewer than 1% objects. The
category distributions of the Book dataset are shown
in Fig. 10. In fact, skewed category distributions have
been commonly observed in real-world text categorization applications [23].

For each dataset, we pose two symmetric taxonomy
integration tasks: G Y (integrating objects from Yahoo into Google) and Y G (integrating objects from
Google into Yahoo).
As stated in Section 2, we formulate each task as
a classification problem. The objects in G Y can be
used as test examples, because their categories in both
taxonomies are known to us [1]. We hide the test examples master categories but expose their source categories to the learning algorithm in training phase, and
then compare their hidden master categories with the
predictions of the learning algorithm in test phase. Suppose the number of the test examples is n. For G Y
tasks, we randomly sample n objects from the set GY
as training examples. For Y G tasks, we randomly
sample n objects from the set YG as training exam-
ples. This is to simulate the common situation that the
sizes of M and N are roughly in same magnitude. For
each task, we do such random sampling five times,
and report the classification performance averaged over
these five random samplings.

6.3. Features

All documents are based on bag-of-words representation and pre-processed by removal of stop-words
and stemming. For Na ve Bayes-based algorithms,
each document is simply considered as independently
occurring words (as described in Section 3.1). For
SVM/SGT-based algorithms, each document is trans-

Fig. 9. An object (listed item) corresponds a website, which is usually described by its URL, its title, and optionally a short annotation about its
content.

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

Fig. 10. The category distribution of the Book dataset.

formed into a vector (as described in Section 4.1). For
CB-SVM, both the term-features and category-features
are separately normalized to have equal length, and
then concatenated and normalized to make up the unitlength feature vector. For AdaBoost-based algorithms,
each document is just treated as a set of binary termfeatures (as described in Section 5.1), because our experiments revealed that in this situation TF IDF term
weighting is not helpful to AdaBoost that is implemented on decision stumps.

We do not perform further feature selection, because
it has been pointed out that in text categorization very
few features (terms) are really irrelevant, thus aggressive feature selection may result in a loss of information
[14]. Generally, speaking, SVM/SGT and AdaBoost
can work well in high dimensional space, while Na ve
Bayes may benefit from appropriate dimensionality reduction as reported in [24]. A careful study on the effect of feature selection in this scenario is left for future
work.

6.4. Measures

F = 2pr/(p + r), where precision is the proportion of correctly predicted positive examples among all predicted
positive examples, and recall is the proportion of correctly predicted positive examples among all positive
examples. The F-scores can be computed for the binary
decisions on each individual category first and then
be averaged over categories. Or they can be computed
globally over all the M n binary decisions where M
is the number of categories in consideration (the number of categories in M) and n is the number of total
test examples (the number of objects in N). The former way is called macro-averaging and the latter way
is called micro-averaging [23]. It is understood that
the micro-averaged F-score (miF) tends to be dominated by the classification performance on common
categories, and that the macro-averaged F-score (maF)
is more influenced by the classification performance on
rare categories [23]. Since, the category distributions
are highly skewed (Section 6.1), providing both kinds
of scores is more informative than providing either
alone.

6.5. Settings

As stated in Section 2, it is natural to accomplish taxonomy integration tasks via building multi-class multilabel classifiers. To measure classification performance
for each class (category in M), we use the standard
F-score (F1 measure) [17]. The F-score is defined as
the harmonic average of precision (p) and recall (r),

We use our own implementation of NB. The Lidstones smoothing parameter  in NB is set to an appropriate value 0.1 [16]. The performance of ENB would
be greatly affected by its parameter . We run ENB
with a series of exponentially increasing values of :

Fig. 11. Comparing the performances of NB and ENB.

(0, 1, 3, 10, 30, 100, 300, 1000) [1] for each taxonomy integration task, and report the best experimental
results.

We use SVMlight8 [8,14] for the implementation
of SVM/TSVM. We take linear kernel, and accept all
default values of parameters except j and p. The
parameter j is set to the ratio of negative training
examples over positive training examples, thus balance the cost of training errors on positive and negative examples. The parameter p for TSVM that
means the fraction of test examples to be classified
into the positive class is set to   p + (1  )  0.5,
where p is the fraction of test examples predicted
to be positive by SVM, and  = 0.99. In all our CS-
SVM/CS-TSVM experiments, the parameter  is set
to 0.5.

We use SGTlight9 [9] for the implementation of
SGT, with the following parameters: -k 10, -d 100,
-c 1000 t f p s. In all our CS-SGT experiments, the
parameter  is set to 0.2.

Fine-tuning  or  using tune sets may generate
better results than sticking with a pre-fixed value. In
other words, the performance superiority of applying
the CS technique is under-estimated in our experiments
but performance inferiority would not be reliably es-
tablished.

8 http://svmlight.joachims.org/.
9 http://sgt.joachims.org/.

We use BoosTexter10 [11] for the implementation of
AdaBoost. We set the boosting rounds T = 1000. BoosTexter uses only decision stumps as weak hypothe-
ses/learners. Using more advanced learning algorithms
such decision trees as weak learners in AdaBoost is
likely to produce better performance.

The Co-Bootstrapping iteration number is set to

R = 8 for both CB-AB and CB-SVM experiments.

6.6. Results

Our first set of results establishes the superiority of
the enhanced learning algorithms compared with the
corresponding base learning algorithms. Comparisons
between ENB, CS-TSVM, CS-SGT, CB-AB and CBSVM with their base learning algorithms are shown in
Figs. 1115, respectively. We see that enhanced learning algorithms achieve much better performance than
their base learning algorithms.

Our experiments also reveal some additional in-
sights. Interestingly, Cluster Shrinkage does not help
inductive SVM as shown in Fig. 16, demonstrating that
the transductive setting is important for
the Cluster Shrinkage technique to work. Comparing
Figs. 12 and 16, we also see that even without Cluster
Shrinkage, the basic transductive SVM works better
than the inductive SVM for this problem.

Fig. 17 shows that the performances of CB-AB increase along with the number of Co-Bootstrapping it10 http://www.research.att.com/schapire/BoosTexter/.

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

Fig. 12. Comparing the performances of TSVM and CS-TSVM.

erations, on the Book dataset. This shows that the two
AdaBoost classifiers learned from the two taxonomies
do mutually help each other until they become stable.
Finally, Fig. 18 compares the performances of all the
enhanced learning algorithms. The enhanced learning
algorithms based on SVM appears to perform the best,
corresponding well with SVM being the best performing base learning algorithm on this dataset. CB-SVM
appears to be the best performing algorithm in this limited experiment where we have not varied the parameter
settings. This suggests that Co-Bootstrapping is able to
exploit more of the taxonomy relationships compared
with Cluster Shrinkage. Multi-labeled objects may also
pose problems for the Cluster Shrinkage techniques as

these objects are drawn towards the midpoint of all the
cluster centers that they belong to and may end up more
isolated than before shrinkage. However, we have not
done any controlled experiments to verify these issues.
In terms of computational efficiency, NB, SVM and
AB are all quite fast, and NB is the fastest among them.
Transductive learning techniques also tend to be more
computationally expensive compared with their inductive versions. TSVM uses computationally expensive
greedy search to get a local optimal solution. In con-
trast, SGT approximates the normalized cut cost function that it optimizes so that a solution can be found
efficiently. In practice, SGT is substantially faster than
TSVM. Cluster Shrinkage only requires using modified

Fig. 13. Comparing the performances of SGT and CS-SGT.

Fig. 14. Comparing the performances of AB and CB-AB.

Fig. 15. Comparing the performances of SVM and CB-SVM.

kernels or similarities, therefore would not pose extra
computational burden. Co-Bootstrapping requires iterations of the base algorithms although a small number
of iterations (we used 8) is probably enough.

Some transductive learning algorithms, such as
TSVM produce hypotheses that can be used to classify new unseen objects. In such cases, the classifiers
produced can continue to be used even if the source taxonomies continue to be populated, just like in most inductive learning algorithms. Other transductive learning algorithms, such as SGT is not designed to classify
new unseen objects and may be more appropriate for
integrating legacy taxonomies that will not change fur-
ther.

7. Related work

Most of the recent research efforts related to taxonomy integration are in the context of ontology mapping
on Semantic Web. An ontology specifies a conceptualization of a domain in terms of concepts, attributes,
and relations [25]. The concepts in an ontology are usually organized into a taxonomy: each concept is represented by a category and associated with a set of objects
(called the extension of that concept). The basic goal of
ontology mapping is to identify (typically one-to-one)
semantic correspondences between the taxonomies of
two given ontologies: for each concept (category) in
one taxonomy, find the most similar concept (category)

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

Fig. 16. Comparing the performances of SVM and CS-SVM.

Fig. 17. The performances of CB-AB increase along with the number
of Co-Bootstrapping iterations, on the Book dataset.

in the other taxonomy. Many ontology mapping systems derive similarities between concepts (categories)
based on their extensions (objects) [3,4,26]. For exam-
ple, a reasonable similarity measure for a pair of concepts (categories) C and S is the Jaccard coefficient:
Jaccard-sim(C, S) =|C S|/|C S| [27]. To compute
such content-based similarities, ontology mapping systems need to first integrate objects from one taxonomy
into the other and vice versa, i.e., do taxonomy integra-
tion. So, our work can be used for this kind of contentbased ontology mapping.

As stated in Section 2, taxonomy integration can be
formulated as a classification problem. The Rocchio
algorithm [17,28] has been applied to this problem in

Fig. 18. Comparing the performances of the different enhanced al-
gorithms.

[3]; and the Na ve Bayes (NB) algorithm [5] has been
applied to this problem in [4], without exploiting information in the source taxonomies.

In [1], Agrawal and Srikant proposed the Enhanced
Na ve Bayes (ENB) approach to taxonomy integra-

tion, which we have analyzed in depth and compared
with our proposed approaches. In [21], Sarawagi et
al. independently proposed the Co-Bootstrapping technique (which they named Cross-Training) to enhance
SVM for taxonomy integration, as well as an Expectation Maximization (EM)-based approach EM2D (2-
dimensional Expectation Maximization).

In [22], AdaBoost is selected as the framework
to combine term-features and automatically extracted
semantic-features in the context of text categorization.
We also choose AdaBoost to combine heterogeneous
features (term-features and category-features), but it is
for a different problem (taxonomy integration) and it
works in a different way (Co-Bootstrapping).

Recently, there has been considerable interest in
learning from a small number of labeled examples
together with a large number of unlabeled exam-
ples. This is called semi-supervised learning. Representative works on this topic include employing
the Expectation Maximization (EM) technique [29]
and Co-Training [3033]. Basically, Co-Training attempts to utilize unlabeled data to help classification through exploiting a particular form of redundancy in data: each object is described by multiple
views (disjoint feature sets), which are compatible
and uncorrelated (conditionally independent) [30]. CoBootstrapping is similar to Co-Training in the sense
that both techniques iteratively train two classifiers,
which mutually reinforce each other. Nevertheless,
Co-Bootstrapping and Co-Training are essentially targeted for different kinds of problems, as shown in
Table 4.

8. Hierarchical taxonomies and other
extensions

As most taxonomies are hierarchical in nature, it
would be important to extend the techniques proposed
in this paper to handle hierarchical taxonomies effec-
tively. Although, it is possible to flatten the hierarchy to
a single level, past studies have shown that exploiting
the hierarchical structure can lead to better classification results [34,35]. While we have not investigated
techniques of incorporating the information in the hi-
erarchies, it is possible to suggest various simple extensions to the current techniques.

To handle hierarchies in SVM/TSVM, it is possible
to use the technique proposed in [35]. One could also
incorporate a hierarchical version of Cluster Shrinkage
that is similar to the idea explored in [36]: transforming
each objects position to a weighted average of its original position, its parent-clusters center, its grandparentclusters center, . . ., all the way up to the root. For ex-
ample, consider a two-level taxonomy H where Sjk is a
sub-category of Sj. Suppose the center of Sjk is cjk and
the center of Sj is cjk. For each x Sik  Si, one reasonable way to achieve hierarchical CS is as follows:
ik = ci + (1 )cik using a parameter
first compute c
ik + (1 )x
0  1, and then replace x with x

using a parameter 0  1.

= c

To extend Co-Bootstrapping, it is useful to consider
hierarchies as trees. One simple technique to exploit
the hierarchy in Co-Bootstrapping would then be to
include indicator functions for both internal and leaf
nodes, where the indicator function for an internal node

Table 4
Comparison between Co-Training and Co-Bootstrapping

Co-Training

Classes

One set of classes

Features

Two disjoint sets of features: V1 and V2

Co-Bootstrapping

Two sets of classes
(1) The set of source categories
(2) The set of master categories

Two sets of features
(1) Conventional-features plus source category-features
(2) Conventional-features plus master category-features

Assumption

V1 and V2 are compatible and uncorrelated (conditionally

independent)

The source and master taxonomies have some semantic overlap, i.e.,
they are somewhat correlated

D. Zhang, W.S. Lee / Web Semantics: Science, Services and Agents on the World Wide Web 2 (2004) 131151

would indicate whether the object belongs to the subtree rooted at that internal node. This allows the learning algorithm to also exploit relationships between subtrees in the taxonomies rather than just between leaves.
Assuming that each internal node has more than one
child, the number of indicator functions used will be at
most two times the number of indicator functions used
in the non-hierarchical technique as the number of internal nodes is always less than the number of leaves.
Many issues remain to be explored. Even within
these algorithms, there are variations that we have not
experimented with, such as combining transductive
learning algorithms with Co-Bootstrapping. We also
do not yet have a good characterization of when the
techniques work (cf. compatibility and conditional independence conditions in Co-Training).

Cluster Shrinkage still takes a rather asymmetrical
view as ENB with a clear distinction between source
and master classifications, Co-Bootstrapping takes a
symmetrical view, relying on a Co-Training like idea
of two classifications informing each other. The experiments show that both techniques can improve the base
learning algorithms substantially. The experiments also
suggest that Co-Bootstrapping with Support Vector
Machines works the best although more work will have
to be done before definitive conclusions can be drawn.

Acknowledgments

We would like to thank the anonymous reviewers for
their helpful comments and many useful suggestions.

9. Conclusion
