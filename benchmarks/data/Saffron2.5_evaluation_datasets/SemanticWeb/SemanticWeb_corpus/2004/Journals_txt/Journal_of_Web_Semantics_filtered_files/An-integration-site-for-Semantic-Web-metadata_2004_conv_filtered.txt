Web Semantics: Science, Services and Agents

on the World Wide Web 1 (2004) 229234

An integration site for Semantic Web metadata


Andreas Harth

DERI at NUI Galway and USC Information Sciences Institute, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292, USA

Received 3 December 2003; received in revised form 9 December 2003; accepted 10 December 2003

Abstract

The Semantic Web has motivated many communities to provide data in a machine-readable format. However, the available
data has not been utilized to far to the extent possible. The data, which has been created by a large number of people, is dispersed
across the Web. Creating the data without central coordination results in RDF of varying quality and makes it obligatory to
cleanse the collected data before integration. The SECO system presented in this paper harvests RDF files from the Web and
consolidates the different data sets into a coherent representation aligned along an internal schema. SECO provides interfaces
for humans to browse and for software agents to query the data repository. In this paper, we describe the characteristics of RDF
data available online, the architecture and implementation of the SECO application, and discuss some of the experienced gained
while collecting and integrating RDF on the Web.
 2004 Elsevier B.V. All rights reserved.

Keywords: Information integration; Semantic web

1. Introduction

The Semantic Web has motivated grassroots efforts
to develop and publish ontology specifications, and
various other people on the Web provided hand-crafted
instance data in RDF for these ontologies. Examples
include Friend Of A Friend (FOAF) [1] that is used
in the Semantic Web community to describe people
and their relationships, and RDF Site Summary 1.0
[2] that is used in the Weblog community to syndicate news items. Both formats are encoded in RDF,
are based on relatively compact ontologies, and enjoy
wide community support. The distributed RDF files
on the Web, connected by links, represent a Web of

 http://seco.semanticweb.org/.


Tel.: +1-310-448-8473; fax: +1-310-822-0751.
E-mail address: aharth@isi.edu (A. Harth).

data that has been built by a collaborative effort of
thousands of people.

So far, the RDF data on the Web has been utilized
inadequately, mostly because it is difficult to locate
and combine the files and query the resulting information since the RDF files are dispersed across several
thousand Web sites. News aggregators such as Google
News [3] scrape the news items out of HTML pages
but are not using RSS feeds, RSS aggregators such as
AmphetaDesk [4] are client applications, and FOAF
aggregators such as PeopleAggregator [5] solely cover
a fraction of all FOAF data available. Most impor-
tantly, the aggregators lack a query interface for software agents to access the integrated data.

SECO, the application presented in this paper, acts
as a mediator [6] that aggregates arbitrary RDF files
from the Web and constructs a user interface in HTML
from the integrated data sets using a three-stage

1570-8268/$  see front matter  2004 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2003.12.002

A. Harth / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2004) 229234

approach. Software agents can query the RDF data set
using a remote query interface over HTTP. In the fol-
lowing, we describe the qualities of the data currently
available on the Semantic Web, show the architecture
of SECO, explain its implementation, and conclude
with discussion and future work.

2. A Web of data

To mimic of the structure of the existing hypertext
Webdocuments and links between documentsthe
rdfs:seeAlso property is used, especially in
FOAF datasets. The usage of the rdfs:seeAlso
link to connect RDF documents makes it possible to
use an RDF crawler for traversing the Web of RDF
files. Fig. 1 illustrates three RDF files (large circles)
that are connected via rdfs:seeAlso properties.
The files contain statements consisting of resources
(solid circles) and properties (arrows). Resources with
the same URI in different files, depicted in green,
provide a way to connect the various RDF data sets.

3. SECO architecture

SECO consists of a crawler, wrappers, a trans-
former, a user interface, a remote query interface,

and a data repository as depicted in Fig. 2. The data
repository consists of four different sets of RDF data:
the MetaModel contains metadata about the harvested
files,
the SourceModel contains the original RDF
triples from the RDF files on the Web, the TargetModel contains cleaned data, and the UsageModel
contains site usage data from the user interface. The
data in the MetaModel, the UsageModel, and the TargetModel is specified by ontologies, while the data in
the SourceModel can be of any schema and is therefore not specified. The crawler collects data from
RDF files and from legacy data sources that are converted into RDF by wrappers and stores the incoming
data in the SourceModel. The transformer takes the
data from the SourceModel, cleans it, and stores the
results in the TargetModel, which is accessed by the
user interface to produce HTML and by the remote
query interface to answer queries.

3.1. Crawler and wrappers

The RDF crawler traverses the Web of data by
following rdfs:seeAlso links. By relying on the
links inside RDF files, the crawler can visit a large
portion of the Semantic Web without the need to find
the location of RDF files from sources that cannot be
easily processed with the RDF toolkit. The locations

Fig. 1. Three RDF files, connected via rdfs:seeAlso.

Table 1
Legacy data sources that are covered by wrappers

Type

Email
Google API
RSS 0.92 and 2.0
Web pages

Format

RfC822

Protocol

IMAP4
SOAP Web Service

RDF syntax, and then stores the RDF statements of
the files in the SourceModel using the RDF reification
mechanism to track their provenance.

3.2. Transformer

This component transforms statements from the
SourceModel to the TargetModel by conducting the
following steps:

1. The first step copies statements from the SourceModel to the TargetModel omitting statements that
contain redundant information. The class names
are mapped to the class names of the TargetModel if such a mapping is defined, otherwise the
rdf:type statements are omitted.

2. The second step is the cleansing stage where
date literals are normalized and string literals are
augmented with xml:lang information based
on statements in the original RDF file to provide
multi-language support.

3. Then mandatory properties, if necessary with default values, are added because the user interface
needs a minimal set of properties to be able to display an instance. In case a future RDQL query processor supports optional properties this step can be
left out.

4. Finally, FOAF instances are fused [7] based on the
foaf:mbox sha1sum property, which has been
computed in the previous step if was not already
present in the original RDF file. Within FOAF a
person is uniquely identified based on the SHA1
hash sum [8] over the foaf:mbox property to
hide the real email address from spammers.

3.3. Remote query interface

The remote query interface in SECO is an implementation of the RDF Net API [9] and allows software

Fig. 2. Components and data sets of SECO.

of the wrappers, which are used by the crawler to
access legacy data sources, have to be provided
manually.

A basic set of customized wrappers to convert the
content of legacy data sources into RDF is part of
SECO. The wrapper provided are accessible via HTTP
and return an RDF representation of the data from
the underlying datastore, which can be included into
the data repository by the crawler in the same way
as native RDF data. Table 1 summarizes the types,
original formats and original protocols of the legacy
data sources that can be included into the crawling
process.

Metadata related to the Web crawling process, such
as the date of the last visit of a file, the HTTP return
code, or a copy of the robots.txt file from sites,
is stored in the MetaModel. The crawling is carried
out continuously. Properties within the RDF files can
determine the update frequency of the files, otherwise
a default value is assumed. The crawler gathers RDF
files from the Web, validates for correct XML and

A. Harth / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2004) 229234

Fig. 3. Screenshot of the editors page.

agents to issue queries in the RDF Query Language
(RDQL) [10] over HTTP.

3.4. User interface

The user interface consists of HTML pages that
are generated from the TargetModel. Once logged in,
both users and editors can browse, rate, and sort the
news items, while editors are also able to select their
choice of news items for the publicly accessible front
page. Users rating news items, the admin selecting
a news item for the front page, and other actions
performed through the user interface are logged into
the UsageModel. The screenshot in Fig. 3 shows the
editors page with three news items, where one item
originated from slashdot, one from a German public
television news show, and another one from a personal blog. Users can perform full-text searches on
literals in the TargetModel by using the input box in
the top center of the page.

The front page, the search results page, the details
page, and the personalized page are created in three
steps. First, one or more RDQL queries that fetch
data for the resulting page in the user interface are
executed against the TargetModel. Then the result of
this query is formatted as an XML document. Finally,
this XML document is transformed into a presentation oriented HTML page using an XSLT stylesheet
that has been customized manually to the output of
the query form the previous step. Thus the process
of creating HTML from RDF as depicted in Fig. 4
comprises of three stages: from content over structure
to presentation [11].

Fig. 4. Creating HTML pages from RDF using XML as intermediate representation.

4. Implementation

SECO consists of roughly 12,000 lines of code and
comments embedded in code, excluding libraries and
external packages. The components for the crawler
and wrappers, the transformer, and the user interface
are implemented in 7000 lines of Java code, plus 1500
lines of XSL stylesheets used for creating HTML
pages in the user interface. The implementation was
test-case driven, resulting in 2500 lines of code for
JUnit [12] unit tests. Ontology specifications for the
MetaModel, UsageModel, and TargetModel are made
up of 1000 lines of RDF.

RDF in SECO is processed and stored using the
Jena2 Semantic Web toolkit [13], and is made persistent in the MySQL database [14]. Jena2 provides a
API implemented in Java for accessing and manipulating RDF for which an active developer community
provides code, documentation, and support via a mailing list. Joseki2 [15], an application related to Jena2,
is leveraged in SECO for the remote query interface
functionality. Joseki is implemented as a set of servlets
packaged as a Web application.

All components in SECO that serve HTTPthe
wrappers, the user interface, and the remote query
interfaceare implemented as Web applications and
run inside the Jetty servlet container [16]. In the
user interface, every data driven page is produced
by one or more queries embedded in a servlet. The
queries are customized for the vocabulary used and
are expressed in RDQL, except for the cases where
queries spread several model or require functionality
not available in RDQL such as counting results. In
this case, the queries are implemented calling Jena
methods from the Java code. Both RDQL queries and
Jena methods return XML which is the intermediate
format of the servlet.

XSLT stylesheets are applied to the XML inside
the servlet using the Xalan2 XSL-T processor [17].
The stylesheets are customized to the queries and pass
parameters to the servlet via HTTP Get. The servlet
gets the parameters, parses them, and performs the
necessary operations on the data repository. Strings
displayed in the user interface are language specific
and therefore references to the language-specific part
of the XSLT stylesheet as described in [18].

SECO is built on top of other sub-systems: Xalan2,
Xerces, Jakarta ORO, ICU4J, util.concurrent, Jena2,

Joseki2, Axis,
MySQL jdbc connector.

jtidy, Jetty, Google API, and the

5. Conclusion

We presented the design and the implementation
of SECO, an application that collects and aggregates
data from the Semantic Web. In SECO, both RDF data
and data from legacy data sources using wrappers are
collected, the resulting data is cleansed and can be
queried by software agents via a remote query server.
We presented a three-stage approach to create HTML
from RDF using XML as an intermediate representa-
tion.

An interesting direction to pursue is to investigate
how rules can be used to simplify the transformation process and replace the transformer component
implemented in Java with rules (e.g., using TRIPLE
[19]) and therefore being able to eliminate the
distinction between the SourceModel and Target-
Model.

In future work, we plan to incorporate a mediator
approach with virtual integration into SECO, similar to the approach employed in the BuildingFinder
[20]. The virtual
integration approach has better
scalability properties that will be increasingly of
concern as more ontologies are published and more
RDF data is available online. The software distribution is available at http://seco.semanticweb.org/
downloads/ under a BSD-style license, and we hope
it proves to be a valuable resource for further exper-
iments.

Acknowledgements

Id like to thank the people on the #rdfig IRC
channel for comments, suggestions, and code, especially Matt Biddulph for making the hackscutter
code available. Thanks to Salim Khan for proofread-
ing, Hongsuda Tangmunarunkit for comments on a
draft version of this document, Jose Luis Ambite
for valuable insights, and Stefan Decker for critique
on numerous versions of this paper. Furthermore I
would like to thank the provider of Open Source Java
softwarewithout these invaluable resources SECO
would not have been possible.

A. Harth / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2004) 229234
