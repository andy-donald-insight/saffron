Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Substructure counting graph kernels for machine learning from RDF
data
Gerben Klaas Dirk de Vries a,, Steven de Rooij b,a

a System and Network Engineering Group, Informatics Institute, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands
b Knowledge Representation and Reasoning Group, Department of Computer Science, VU University Amsterdam, De Boelelaan 1081, 1081 HV Amsterdam,
The Netherlands

h i g h l i g h t s
 Systematic graph kernel framework for RDF.
 Fast computation algorithms.
 Low frequency labels and hub removal on RDF to enhance machine learning.

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 15 December 2014
Received in revised form
16 June 2015
Accepted 3 August 2015
Available online 10 August 2015

Keywords:
Graph kernels
Machine learning for RDF
WeisfeilerLehman
Hub removal

In this paper we introduce a framework for learning from RDF data using graph kernels that count substructures in RDF graphs, which systematically covers most of the existing kernels previously defined and
provides a number of new variants. Our definitions include fast kernel variants that are computed directly
on the RDF graph. To improve the performance of these kernels we detail two strategies. The first strategy
involves ignoring the vertex labels that have a low frequency among the instances. Our second strategy
is to remove hubs to simplify the RDF graphs. We test our kernels in a number of classification experiments with real-world RDF datasets. Overall the kernels that count subtrees show the best performance.
However, they are closely followed by simple bag of labels baseline kernels. The direct kernels substantially decrease computation time, while keeping performance the same. For the walks counting kernel
this decrease in computation time is so large that it thereby becomes a computationally viable kernel to
use. Ignoring low frequency labels improves the performance for all datasets. The hub removal algorithm
increases performance on two out of three of our smaller datasets, but has little impact when used on our
larger datasets.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

In recent years graph kernels have been introduced as a promising method to perform data mining and machine learning on
Linked Data and the Semantic Web. These methods take Resource
Description Framework (RDF) data as input.

One main advantage of this approach is that the techniques are
therefore very widely applicable to all kinds of Linked Data. Almost
no assumptions are made on the semantics of the data and its con-
tent, other than that it is in RDF. So, additionally, these methods

 Corresponding author. Tel.: +31 20 525 7522.

E-mail addresses: g.k.d.devries@outlook.com (G.K.D. de Vries),

steven.de.rooij@gmail.com (S. de Rooij).

http://dx.doi.org/10.1016/j.websem.2015.08.002
1570-8268/ 2015 Elsevier B.V. All rights reserved.

require very little knowledge of Semantic Web technologies to be
employed.

Another advantage is the host of existing machine learning al-
gorithms, called kernel methods [1,2], that can be used with these
graph kernels. The most well known of these algorithms is the
Support Vector Machine (SVM) for classification and regression.
However, algorithms exist for ranking [3,4], clustering [5], outlier
detection [6], etc., which can all be used directly with these kernels.
More recently, interest has increased for large scale linear classification [7] for larger datasets, for which a number of graph kernels
can also be used.

In this paper we give a comprehensive overview of graph kernels for learning from RDF data. We introduce a framework for
these kernels, which are based on counting different graph sub-
structures, that encompasses most of the graph kernels previously introduced for RDF, but also introduces new variants. The

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

framework includes fast kernel variants that are computed directly
on the RDF graph. We also detail the necessary adaptation of the
WeisfeilerLehman graph kernel [8] needed to compute a number
of kernels in our framework. Furthermore, we give two strategies
to further improve the machine learning performance with these
kernels. The first strategy ignores vertex labels which have a low
frequency of occurrence among the instances and the second strategy removes hubs to simplify the RDF graphs. All of our kernels
defined in the framework can be used with large scale linear classification methods.

The kernels are studied in a number of classification experiments on different RDF datasets. The goal of these experiments is
to study the influence of the different choices for graph kernels defined in our framework. It turns out that kernel performance differs
per dataset. Overall, kernels that count subtrees in the graphs are
the best choice. However, simple bag of labels baseline kernels also
perform well and are significantly cheaper to compute. The strategy to ignore low frequency labels has a positive effect on performance in all tasks, whereas the hub removal strategy only has a
positive effect in a number of tasks and has no influence for larger
datasets.

The work presented in this paper consolidates and expands our
earlier papers on graph kernels for RDF [9,10] and hub removal
[11].

The rest of this paper is structured as follows. We begin with
an overview of related work. In Section 3 we introduce our kernel
framework and algorithms. Section 4 covers our experiments with
these kernels. We end with conclusions and suggestions for future
work.

2. Related work

Graph kernels, such as those introduced in [12,8,13], are
methods to perform machine learning on graph structured data,
using kernel methods [1,2].

For learning from RDF data, the intersection subtree and intersection graph kernels were introduced in [14]. A fast approximation of the WeisfeilerLehman graph kernel [8], specifically
designed for RDF was introduced in [9]. In [10] a fast and simple graph kernel, similar to the intersection subtree kernel was
defined. In the context of recommender systems using linked
data [15] and [16] introduce kernels based on the labels in the
neighborhood of a vertex. The current paper defines a framework
which systematically covers most of these previously introduced
kernels.

There are a number of papers that cover machine learning from
Linked Data and the Semantic Web using kernel methods, which
focus on the formal/ontological level of the data, and which are
therefore less generally applicable. One of the first papers to introduce kernel methods to learn from the Semantic Web was [17]. In
that paper, kernels are defined manually using task relevant properties of the instances. Other approaches are based on description
logics, such as [18], and inductive logic programming [19].

The specific task of link prediction on RDF graphs is tackled
in several papers. One approach is to use matrix factorization
[20,21]. Another, related approach, is to use tensor factorization [22,23]. Finally, there are also approaches using (tensor) neural networks [24,25] and very recently Gaussian processes [26].
The graph kernel methods of this paper are applicable to a more
wide range of machine learning and data mining tasks than these
link prediction methods. For instance, graph kernels can be used in
clustering, to predict labels/classes that are not links in the graph.
The framework for graph kernels for RDF that we define in this
paper has some similarities to the comparison of propositionalization [27] strategies for Linked Data in [28], in the sense that some
of the propositionalizations are similar to the graph substructures

Fig. 1. Small example RDF graph of an author network.

that our graph kernels work with. However, they do not use kernel methods and use Linked Data as background knowledge to enhance data mining on normal data [29].

For a relatively recent overview of data mining and machine

learning from Linked Data and the Semantic Web, see [30].

3. Graph kernels for RDF

The Resource Description Framework (RDF)1 is the foundation
of Linked Data and the Semantic Web. The central idea is to store
statements about resources in subjectpredicateobject form,
called triples, which define relations between a set of terms. A triple
(s, p, o) in a set of triples T specifies that the subject term s is
related to the object term o by the predicate p.

Often a set of triples T is referred to and visualized as a graph,
where the subject and object terms in the data constitute the
vertices, and a triple (s, p, o) specifies that s and o are connected by
an edge with label p. This graph interpretation breaks down when
predicate terms are also used in the subject or object position,
for example to define properties of the relations themselves [31].
However, most datasets that occur in practice define only a limited
number of facts about the relations, so for simplicity we choose to
interpret predicates in the predicate position and predicates in the
subject or object positions as distinct, even if they have the same
label.

More expressive Semantic Web knowledge representation
formalisms, such as the Web Ontology Language (OWL) and RDF
Schema (RDFS) can be represented in RDF. Therefore, RDF triple-
stores, often include reasoning engines to automatically derive
new triples, if the RDF represents data modeled using these more
expressive formalisms.

Fig. 1 is an illustration of a simple RDF graph of two Persons
(person1 and person2) that authored a couple of papers, one of
them (paper3) together.

Learning from RDF using graph kernels takes the approach that
instances, which are vertices in a large RDF graph, are represented
by their neighborhood, i.e. the triples around them, up to a certain
depth. For example, in Fig. 1, the instances could be person1 and
person2, and their neighborhood of depth 1 would be all the triples
in which they are the subject.

The graph in Fig. 1 is an intuitive visualization of RDF data as a
directed labeled multigraph. But when described formally, labeled
multigraphs are more complicated than necessary for our purpose.
Moreover, such a representation requires us to always distinguish
between edges and vertices, which turns out not to be necessary in
the description of any of our algorithms. So instead we opt to reify

1 http://www.w3.org/standards/semanticweb/.

Different variants of the extraction function J lead to different
graph kernels for RDF graphs. In most of the kernels that we define
the function J is composed of two functions H and F, where H
extracts the relevant neighborhood for an instance i and F extracts
the substructures from that neighborhood. In this case JH,F (G, i) =
F (H(G, i)).

3.1.2. Instance neighborhood

For each instance we consider either the tree or the graph
around the instance i extracted from the RDF graph G, up to depth
d, as its neighborhood.
Let Graphd(G, i) extract the subgraph G = (V, E, l) around an
instance vertex i from graph G up to depth d. The vertices V in this
graph contain all vertices that are reachable in G from i in at most
d steps, and the same is true for the edges E. The label function l
is the same as l, but restricted to the vertices in V. Note that when
d = 0 we only extract the vertex i (and no edges).
The function Treed(G, i) extracts the subtree of depth d rooted
at instance node i from a graph G = (V , E, l). Define a walk of
length d from vertex v0  V as a sequence v0, v1, . . . , vd of vertices such that for each i < d there is an edge (vi, vi+1)  E.
Then Treed(G, i) = (V, E, l), where the new vertex set V consists of all walks of length d or less from vertex i in G; two walks
v, w  V are joined by an edge in E if and only if w extends v
by one additional vertex. Finally the label function l maps a vertex v  V to the original label of the last vertex of the walk, that
is, l(v0, . . . , vn) = l(vn). The root of Treed(G, i) is the vertex sequence with just i, which is identified by its lack of incoming edges.
In other words, Tree constructs the tree up to depth d by following
the walks from the instance vertex i.

3.1.3. Substructures

Given a graph G = (V , E, l) we consider different substructures
to count, creating different kernels. Below we define 3 different
types of substructures.
Bag of Labels. The most basic approach is to count the equivalent
vertices in the neighborhood of the instance. Effectively this boils
down to counting vertices with the same label, hence the name Bag
of Labels (BoL). For a graph G = (V , E, l), we extract the following
substructures:
BoL((V , E, l)) = {({v},, l)|v  V}.
Thus, each substructure in the bag of labels is a very simple graph
with one vertex and no edges.
Walks. To be able to define the extraction function we first need
to extract all walks of a fixed length n starting from a vertex v0. For
a graph G = (V , E, l) and a vertex v0  V , let
Walksn((V , E, l), v0) = {({vi|i  n},{(vi, vi+1)|i < n}, l)|
v0, . . . , vn is a walk in (V , E, l)}.
The extraction function then extracts all walks from the neighborhood G = (V , E, l) up to a given depth n:

(4)

(5)

Walksn((V , E, l)) = 

0kn
vV

Walksk((V , E, l), v).

(6)

Fig. 2. Small example RDF graph of an author network as a bipartite representation
without edge labels.

the edges: we represent the set of triples as vertices which allow
the RDF to be represented by a simple directed graph with vertex
labels.
In the following we use RDF graphs G = (V , E, l) based on sets

of triples T . For such a graph G, the vertices V are defined as:
V = {v|x, y((v, x, y)  T  (x, y, v)  T )}  T .
(1)
So there is a vertex for each unique subject and object, and for each
triple. The edges E are:
E = {(v, (v, p, o))|(v, p, o)  T }  {((s, p, v), v)|(s, p, v)  T }.
(2)
So for each triple, two edges are created. Finally, we define the label
function l which assigns a label to a vertex in V such that l(v) =
v if v is a subject/object, and l((s, p, o)) = p for vertices that
represent triples. Fig. 2 illustrates this representation by visualizing
the graph in Fig. 1 following this bipartite definition.

This definition has the additional advantage that our algo-
rithms, as well as the subsequent discussion, apply to any labeled
simple graphs, regardless of whether or not they are constructed
from a triple store.

3.1. Kernel definitions

Below we define a family of graph kernels for RDF that are
based on the idea of counting common substructures in the
neighborhoods of the instances. The majority of the work on graph
kernels for RDF fits into this family.

3.1.1. Convolution kernel

The definition of the kernel is based on an extraction function J,
which maps an instance vertex i in graph G to a set of graph substructures derived from G. These graph substructures are themselves graphs of the same form. The kernel is based on counting
common, i.e. equivalent, substructures. Two substructures G1 and
G2 are equivalent, denoted G1  G2, if their graphs are equivalent according to the usual definition of graph equivalence, with
the additional requirement that the corresponding vertices have
the same label. Thus, the equivalence class is always completely
determined by the label function.
All the kernels that we define in the rest of this paper are
instances of the following convolution kernel [32]. Let G =
(V , E, l) be an RDF graph and let i, i  V be two instance vertices,
then

kJ (i, i) = 

xiJ(G,i)
xiJ(G,i)

1[xi  xi],

where 1[xi  xi] is 1 if xi  xi and 0 otherwise.

(3)

In [14] paths, i.e. walks that do not contain repeating vertices,
are also considered. However, both that paper and [9] find
very little difference in predictive performance between the two
settings. Therefore, we stick to walks for clarity. Furthermore, in
the case of instance trees, paths and walks are the same.

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

Subtrees. Finally, we consider full subtrees up to depth n of a graph
G. For this definition we use the Tree function defined earlier.
For a graph G = (V , E, l) the full subtrees up to depth n is the

set given by:

Subtreesn((V , E, l)) = 

0kn

{Treek((V , E, l), v)|v  V}.

(7)

Potentially, other, more complicated, substructures can also
be considered, like subgraphs and partial subtrees, but they can
lead to significantly longer computation time and/or combinatorial
explosion.
Root Constraint. For trees G we also consider the constraint in
which only substructures are counted that start from the root
vertex, because it has been used before [14,10] and it can lead to
faster computation. The Root version of an extraction function F
returns only those substructures that start with the root vertex:
F Root (G) = {G  F (G)|the root of G is the root of G}.
(8)
In the bag of labels case, this would make little sense, since this
would lead to only one label. But in the Walks setting this would
mean that all walks start in the root vertex and in the Subtrees
setting that all subtrees have the root vertex as their root vertex.

3.1.4. Possible kernels

These different options can be combined into 8 possible kernels.
For example, given an RDF graph G, the combination of instance
trees with counting walks is:
kJTreed,Walksn

1[xi  xi].

(i, i) =


(9)

xiWalksn(Treed(G,i))
xiWalksn(Treed(G,i))

The full list of kernels is the following.

GraphBoLd = kJGraphd,BoL
TreeBoLd = kJTreed,BoL

GraphWalksd,n = kJGraphd,Walksn
TreeWalksd,n = kJTreed,Walksn

TreeWalksRootd,n = kJTreed,Walksn,Root
GraphSubtreesd,n = kJGraphd,Subtreesn
TreeSubtreesd,n = kJTreed,Subtreesn

TreeSubtreesRootd,n = kJTreed,Subtreesn,Root

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

Graph indicates that instance graphs are extracted from the RDF
data and Trees that we use trees. BoL indicates the bag of labels
substructures, Walks the walks substructures and Subtrees the
subtrees. Finally Root indicates whether we use the root constraint.
All the kernels have the depth parameter d which indicates to
which depth the graphs or trees have been extracted. Kernels 38
also have a parameter n for the multiset of substructures, which is
either the maximum walk length or the maximum subtree depth.
Relation to Existing Kernels for RDF. The TreeWalksRoot kernel is the
same as the intersection path kernel defined in [10], which is very
similar to the intersection subtree kernels from [14] and has almost
identical performance. Both kernels consider only substructures
that contain the root vertex. However, the original intersection
subtree kernels in [14] count (partial) subtrees in the intersection
tree, which are more difficult and computationally expensive to
enumerate and hence the kernel does not easily fit this framework.
The GraphWalks kernel is nearly identical to the intersection
graph walk kernel from [14]. Both kernels count common walks

between two instances. The intersection graph walk kernel first
creates the intersection graph between two instances and then
counts the walks in that graph, whereas the GraphWalks kernel
enumerates the walks in the instance graphs directly and then
counts how many there are in common.

The GraphSubtrees kernel is actually the WeisfeilerLehman
kernel [8] implemented for RDF. However, there is a difference
with respect to repeatedly counting the same subtrees, which we
will deal with in the following section on the computation of the
kernels.

In [16] the GraphBoL kernel is used in the context of a recommender system. Multiple more complicated weighting schemes for
the bag of labels in the neighborhood of vertex are tried in that paper but the variant that is essentially the GraphBoL kernel gives the
best performance. The neighborhood-based graph kernel in [15] is
also a GraphBoL kernel, but with a special weighting scheme taking
into account the frequency and distance to the instance vertex of a
label.

So, previous graph kernel research for RDF fits the framework,
presented above, very well. Furthermore, this framework also
introduces some new variants.

3.2. Kernel computation

N, from the set of all substructures X = 

All of the kernels above can be computed using a map: f : X 
i J(G, i) to indices in a
feature vector. With this map we can construct a feature vector
i for each instance i, which counts the occurrence of specific
substructures for J(G, i). Then the convolution kernel defined
earlier, is simply the dot-product between two feature vectors:
k(i, i) = i  i .
(18)
The advantage of the feature vector representation is that it leads
to efficient computations of the kernels [33] and for larger datasets
the feature vectors can be used directly as input to linear learning
methods, such as linear Support Vector Machines [34]. The sparse
nature of these feature vectors fits very well with these linear Support Vector Machines.

The Subtrees kernels can be computed using the Weisfeiler
Lehman algorithm [8]. This algorithm is efficient because only full
subtrees are considered, since this greatly restricts the number of
possible substructures. However, this leaves open the possibility of
modifying the label function and thereby the equivalence relation,
which we consider in Section 3.4.1.

The walks counted in the Walks kernel can be straightforwardly
enumerated for the instance graphs and trees using a recursive
function. However, we can do this a bit more efficiently using an
algorithm similar to the WeisfeilerLehman algorithm. The bag of
labels kernels are special (base) cases of either of these algorithms.

3.2.1. Counting subtrees using WeisfeilerLehman

The WeisfeilerLehman Subtree graph kernel [8], from now on
the WeisfeilerLehman (WL) kernel, is a state-of-the-art, efficient
kernel for graph comparison. The kernel computes the number
of subtrees shared between two (or more) graphs by using the
WeisfeilerLehman test of graph isomorphism. This algorithm
creates labels representing subtrees in a number of iterations.
However, labels created in different iterations are different, but can
potentially identify the same subtree. To make sure that a subtree
is not counted more often than it occurs, the WL algorithm needs
to be modified.

The rewriting procedure of WeisfeilerLehman kernel for RDF
graphs (or trees) G is given in Algorithm 3.1, which is adapted
from [8]. First, the algorithm creates a multiset label for each vertex
based on the labels of the neighbors of that vertex (steps 1a and
1b). This multiset is sorted and together with the original label

graph rewriting process. Every label ln(v) represents a subtree
rooted in v. By multiplying with the terms (1  (l(v), l(v))) and
(1  (l(v), l(v))) we make sure that a label is only counted
when it is not equal to the label for that vertex in the previous it-
eration. We can do this because labels for equivalent subtrees are
constructed in the same iteration. This addition ensures that the
same subtree is not counted multiple times, which is possible in
the original WeisfeilerLehman definition. So the kernel kWL essentially defines the Subtrees kernels: GraphSubtreesd,n, TreeSubtreesd,n
or TreeSubtreesRootd,n, depending on whether we used graphs or
trees as input and whether we restrict ourselves to only the root
vertex.

The label dictionary f can be used as our mapping function2 to
construct feature vectors  to compute the kernels by taking the
dot-product, or to be used directly. When h = 0 we get a bag of
labels kernel.

3.2.2. Counting walks

In the vein of the WeisfeilerLehman algorithm we can also
count the walks, in the instance graphs or trees, in iterations. This
algorithm is given in Algorithm 3.2. In each iteration n the walks of
length n are determined. This is achieved by first creating a union
of all walk labels from the neighbors of v (steps 1a and 1b). Then
all these labels are prefixed with the original vertex label l(v) (step
2). Each of these concatenated labels is mapped and replaced with
a unique new label (step 3 and 4), as in the WeisfeilerLehman
algorithm. Thus at the end of each iteration, each vertex contains a
set of labels that represent the walks of length n starting from that
vertex. Note that there is no label sorting step in this algorithm and
that the label functions ln map to sets of labels.

Algorithm 3.2 Walk Count
Input
Output

a set of graphs G and number of iterations h
(shared) label functions l0 to lh, which map a vertex v to
a set of labels, and label dictionary f

 for n = 0 to h
 for each (V , E, l)  G
1. Multiset-label determination
 for each v  V
a. if n = 0 set l0(v) = {l(v)}

b. if n > 0 set ln(v) =

uN(v) ln1(u)

2. Concatenation

3. Label compression

 for each v  ln(v), replace v with the concatenation of
v and l(v)
 for each v  ln(v), map v to a new compressed label,
using a function f :   , such that f (v) = f (v )
iff v = v
 for each v  ln(v), replace v with f (v)

4. Relabeling

concatenated into a string, which is the new label (steps 2a and
2b). For each unique string a new (shorter) label is introduced and
this replaces the original vertex label (steps 3 and 4). At the end
of each iteration, each label represents a unique full subtree. The
rewriting process can be efficiently implemented using counting
sort, for which details are given in [8].

Algorithm 3.1 differs from the original algorithm in two places.
First, our graphs have directed edges, which is reflected in the fact
the neighborhood N(v) of a vertex v contains only the vertices
reachable via outgoing edges. Second, as mentioned in the original
algorithm, labels from two iterations can potentially be different
while still representing the same subtree. To make sure that this
does not happen we have added tracking of the neighboring labels
in the previous iteration, via the multiset M(v), to the steps 1a and
1b. When the multiset of the current iteration is identical to that of
the previous iteration, we simply give a vertex the label created in
the previous iteration, instead of a new one, see step 2b.

Algorithm 3.1 WeisfeilerLehman Relabeling
Input
a set of graphs G and number of iterations h
Output
(shared) label functions l0 to lh and label dictionary f
Comments Mn(v) are multisets of labels for a vertex v. N(v) is the
neighborhood of v, which is N(v) = {v | (v, v)  E}.

n(v) are multisets of labels for a vertex v which contain
the previous iteration.

 for n = 0 to h
 for each (V , E, l)  G
1. Multiset-label determination

 for each v  V
a. if n = 0, Mn(v) = l0(v) = l(v) and M
b. if n > 0, M

n(v) = Mn(v) and Mn(v) = {ln1(u)|u 

n(v) = 

N(v)}

2. Sorting each multiset

a. for each Mn(v), sort the elements in Mn(v) in ascending
b. for each sn(v), if n > 0 and M

order and concatenate them into a string sn(v)
n(v) is unequal to Mn(v),
add ln1(v) as a prefix to sn(v), else if n > 0, set sn(v) =
sn1(v)

3. Label compression

 for each sn(v), map sn(v) to a (new) compressed label,
:   , such that f (sn(v)) =
using a function f
f (sn(v)) iff sn(v) = sn(v)
 for each sn(v), set ln(v) = f (sn(v))

4. Relabeling

Using the rewriting techniques of Algorithm 3.1 we can define
the WeisfeilerLehman (WL) kernel. Let Gi,n = (V , E, ln, ln1) and
Gi,n = (V, E, ln, ln1) be the nth iteration rewriting of the graphs
(or trees) Gi and Gi, for instances i, i, using Algorithm 3.1, and h
the number of iterations. Note that we added the labels from the
previous iteration to the graphs (and assume these to be empty
when n = 0). Then the WeisfeilerLehman kernel is defined as:

kh

WL(i, i) = h
where

k,WL((V , E, l, l), (V, E, l, l)) =

k,WL(Gi,n, Gi,n),

n=0

vV
vV

(19)

Using Algorithm 3.2 we define the Walk Count (WC) kernel.
Let Gi,n = (V , E, ln) and Gi,n = (V, E, ln) be the nth iteration
rewriting of the graphs (or trees) Gi and Gi, for instances i, i, using
Algorithm 3.2, and h the number of iterations. Then:

WC(i, i) = h

kh

n=0

[(l(v), l(v))  (1  (l(v), l(v)))  (1  (l(v), l(v)))]

k,WC(Gi, Gi ),

(21)

(20)
and  is the Dirac-kernel which tests for equality. This kernel
counts the common vertex labels in each of the iterations of the

2 For instance when we use consecutive integers as compressed labels.

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

where

k,WC((Vi, Ei, l), (Vi , Ei , l)) = 

(v, v ).

(22)

vln(v),vVi
vln(v),vVi


In this kernel every label  represents a different walk.
Similar to the WL kernel, this kernel defines the Walks kernels:
GraphWalksd,n, TreeWalksd,n or TreeWalksRootd,n, depending on
whether we used graphs or trees as input and whether we restrict
ourselves to only the root vertex. Again, we can use the label
function f to create feature vectors and when h = 0 we have the
bag of labels kernel.

3.3. Direct graph kernels

As mentioned, the intuition of graph kernels for RDF is that the
neighborhood of an instance vertex contains the interesting information about that instance in the form of substructures. Which
substructures are used is determined by the function J. Until now
we have defined this function in two stages, by using a function H
to extract the neighborhood and F to compute the features in this
neighborhood. However, it is also possible to define J without using an extraction function H, but directly on the graph G. To do so
we need to determine which substructures are sufficiently close to
the instance vertex to be included as features for that instance.

F ((V , E, l), i) =

3.3.1. Kernel definitions
Let dist(v, v) define the length of the shortest path in G between the vertices v and v, and let G = (V , E, l) be an RDF graph.
Furthermore, let the function F n(G, v) return a set of substructures
of size n  N, rooted in the vertex v. Then we define a direct extraction function J as follows.
Directd
{x  F n((V , E, l), v)|dist(i, v) + n  d, v  V}.
(23)
Thus, for an instance vertex i in G this function returns all the substructures that are sufficiently close to i. Close is defined as the
sum of the distance between the root vertex of the structure and
the instance vertex and the size of the substructure. So, larger substructures have to have a root vertex closer to the instance ver-
tex. This definition ensures that no vertex v in a substructure has
dist(i, v) > d, since dist(i, v) + dist(v, v)  dist(i, v) and
n  dist(v, v).
For the function F we have two obvious candidates, the Walksn
and Treed functions defined earlier. Note that the Tree function
returns a graph and not a set of graphs. We simply wrap this
function as follows: Treesd(G, v) = {Treed(G, v)}. This leads to the
following two additional kernels.
DirectWalksd = kDirectd
DirectSubtreesd = kDirectd
These two kernels are variants of the Walks and Subtrees kernels
defined earlier, but are directly defined on the RDF graph G, instead
of using subgraphs or subtrees. A DirectBoL variant is also possible,
however, it is easy to see that this is equivalent to the GraphBoL
kernel. Both kernels have only one parameter d instead of the two
parameters d and n that we saw earlier.

Walks

(25)

(24)

Trees

3.3.2. Kernel computation

the algorithms that we have defined earlier.

Both the Direct kernels defined above can be computed using
Let Gn = (V , E, ln, ln1) be the nth-iteration of one RDF graph
rewritten for h iterations using Algorithm 3.1, with l0 to lh the

resulting label functions. Note that we again add the previous
iteration of the label function to the graph. Then we compute the
Direct WeisfeilerLehman kernel between two instances i, i  I,
as:

,WLDirect((Gn, i), (Gn, i)),
kd,n

(26)

WLDirect(i, i) = h

kd,h

n=0

where
,WLDirect(((V , E, l, l), i), ((V , E, l, l), i)) =
kd,h


[(l(v), l(v))  (1  (l(v), l(v)))

dist(i,v)+hd,vV
dist(i,v)+hd,vV

 (1  (l(v), l(v)))].

(27)
This definition is very similar to the earlier definition of the WeisfeilerLehman kernel. The main difference is that the neighborhood of an instance is now defined by the distance function dist.
The WLDirect kernel implements the DirectSubtrees kernel.

This kernel is similar to the fast approximation of the WeisfeilerLehman kernel for RDF introduced in [9]. The main difference is that in the definition in the current paper, repeated counts
of the same subtree are not allowed. Two other differences are that
the direction that the labels follow during the relabeling is reversed and that the iterations of the WL algorithm in the current
paper are not weighted. If we modify the algorithm in [9] to incorporate these differences, then it will result in the same kernel.
Let Gn = (V , E, ln) be the nth-iteration of one RDF graph rewritten for h iterations using Algorithm 3.2, and l0 to lh the resulting
label functions. Then we compute the Direct Walk Count kernel
between two instances i, i  I, as:

,WCDirect((Gn, i), (Gn, i)),
kd,n

kd,h

WCDirect(i, i) = h


n=0

where
,WCDirect(((V , E, l), i), ((V , E, l), i)) =
kd,h
(v, v ).

vln(v),dist(i,v)+hd,vV
vln(v),dist(i,v)+hd,vV


(28)

(29)

Again, the main difference with the earlier Walk Count kernels
is that the neighborhood of an instance is defined using the distance function dist. The WCDirect kernel implements the DirectWalks kernel.

For the computation of both kernels, the RDF graph G can be
restricted to those vertices that are actually within a distance d of
at least one instance vertex i. Furthermore, in the for loop over the
vertices V in Algorithms 3.1 and 3.2, we can exclude vertices v for
which i  I : dist(i, v) + n  d. In other words, we can ignore
vertices that are too far from all instance vertices for the current
iteration n. Also, tracking which vertices are in the neighborhood
of an instance i, i.e. within dist  d, can be done by creating a map
from vertices to integers (indicating the depth at which the vertex
occurs) for each instance.
Computational Complexity.
In [8] it is shown that the runtime for
the WeisfeilerLehman relabeling algorithm on a set of graphs is
O(Nhm), where N is the number of graphs, h is the number of
iterations and m is the number of vertices per graph. For our Direct
kernels, we do not have N graphs, but just a single larger graph.
Our larger graph has a number of vertices k. So for these kernels
the relabeling algorithm has a runtime complexity O(hk). Hence,
in situations with k < Nm, the Direct kernels will be faster. This
scenario is typical for the RDF use-case, where the subgraphs for

each instance share a (large) number of vertices and edges, which
means Nm  k given large enough N.

For the Walk Count kernel the situation is identical, as can
be seen from the large similarity with the WeisfeilerLehman
algorithm.
Fast Tree Kernels. To compute the Tree kernels defined earlier it is
not necessary to explicitly extract each tree from the RDF graph,
it is sufficient to retain a tree-view for each instance and use
the same trick with the distance function as in the definition of
the two Direct kernels. This means that we only need to perform
the WeisfeilerLehman/Walk Count once on the RDF graph G
instead of on all the extracted subtrees, which is significantly
faster. Therefore, in the experiments in this paper we use such an
implementation.

3.4. Dealing with unique labels

Two of the main differences between RDF graphs and the typical
graphs used in graph mining and machine learning from graphs,
are that vertices in RDF graphs have unique labels3 and there is a
large number of different labels overall. This leads to very specific
graph patterns, which do not generalize well. In this section we
present two strategies to deal with these problems.

3.4.1. Removing low frequency labels

Ideally, a form of inexact matching between the too specific
graph patterns could help alleviate this problem. However, this is
potentially very computationally expensive. As is shown in [33]
we can remain efficient as long as we consider equivalence classes
of the substructures. In this section we introduce adaptations of
the kernel computation algorithms that allow for some inexact
matching, by changing the equivalence classes.

For clarity, our adaptations are defined directly on the algorithms that compute the kernels (WeisfeilerLehman and Walk
count). They are based on the idea that labels that occur only in
the neighborhood of an instance for a low number of instances are
not very informative. The frequency of a label  in a set of graphs
(or trees) G is straightforwardly defined as follows:
Freq(, G) = |{(V , E, l)  G|v  V : l(v) = }|.
(30)
Freq simply determines the number of graphs that  occurs in at
least once. Note that l(v) can be the original label function l or the
variants l0, . . . , lh created by iterations of the WeisfeilerLehman
algorithm.

We also define the frequency of a label  when we use Direct
kernels as follows, let (V , E, l) be an RDF graph, I a set of instance
vertices, and r a distance constraint, then
Freq(, (V , E, l), I, r)= |{i  I|v  V : l(v)= , dist(i, v) r}|,
(31)
where r = d  n, with d the size of the neighborhood and n the
current iteration of the Weisfeiler-Lehman algorithm.

We then modify the Weisfeiler-Lehman algorithm in two ways.
These two modifications are given in Algorithm 3.3. For clarity, we
only give the lines that are changed from the original algorithm.
The first modification is that the sets of labels Mn(v) (and M
n(b))
are now sets instead of multisets, so multiple neighbors that have
the same label are reduced to one label. The second modification
is to only use a label in the rewriting when it is above a minimal

3 In our bipartite representation this holds for the vertices based on the subject
and objects.

frequency threshold , which is shown in the new lines 1b and 2b.
For the DirectSubtrees kernel we use the second definition of Freq.
These two changes to the Weisfeiler-Lehman algorithm allow
for inexact matching of subtrees because we only look at labels
(and therefore subtrees) that are frequent enough and we also
ignore multiple instances of the same label in the neighborhood
of vertex.

Algorithm 3.3 Weisfeiler-Lehman Relabeling Modification
Input Minimum frequency threshold 
Comments Mn(v) are sets of labels for a vertex v.

1. Multiset-label determination

 for each v  V
b. if n > 0, M

n(v) = Mn(v) and Mn(v) = {ln1(u)|u 

N(v), Freq(ln1(u), (V , E, ln1))  }

2. Sorting each multiset
b. for each sn(v), if n > 0 and M

n(v) is unequal to Mn(v) and
Freq(ln1(v), (V , E, ln1))  , add ln1(v) as a prefix to
sn(v), else if n > 0, set sn(v) = sn1(v)

(32)

For the Walk Count algorithm we need two definitions for the
frequency of a walk. For a set of graphs G the frequency of a walk
label  is defined as:
FreqWalk(, G) = |{(V , E, l)  G|v  V :   l(v)}|.
This definition is analogous to the definition of the frequency of a
label for the Weisfeiler-Lehman algorithm above.
A similar definition follows for the DirectWalks kernel. Let
(V , E, l) be an RDF graph, I a set of instance vertices, and r = dn
a distance constraint, then the frequency of a walk label  is:
FreqWalk(, (V , E, l), I, r) =
|{i  I|v  V :   l(v), dist(i, v)  r}|.
Algorithm 3.4 gives the modification to the Walk Count algo-
rithm. The change to use sets instead of multisets is not applicable
to the Walk Count algorithm. The modification to use the minimal
frequency threshold  is similar to the Weisfeiler-Lehman algo-
rithm. In line 1b we only include the walk  if its frequency is high
enough and in line 2b we only use the label l(v) if its frequency is
high enough. For the DirectWalks kernel we use the second definition of FreqWalk.

(33)

Algorithm 3.4 Walk Count Modification
Input Minimum frequency threshold 

1. Multiset-label determination

 for each v  V
b. if n > 0 set ln(v) = {  ln1(u) | FreqWalk

(, (V , E, ln1))   , u  N(v)}

2. Concatenation

 if Freq(l(v), (V , E, ln1))  , for each v  ln(v), replace v
with the concatenation of v and l(v)

Essentially, the changes to the Weisfeiler-Lehman and Walk
count algorithms change the equivalence classes induced by the
equivalence relation  used in Eq. (3). For example, suppose we
have the following two walks (as sequences of labels): a, b, c and
b, a, c. If the frequency of the label b is below , then these walks
are now considered equal.

We will refer to the 8 new kernels (i.e. each variant of the Walks

and Subtrees kernel), introduced above, as MinFreq kernels.

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

3.4.2. Hub removal

Another direction for tackling the specific label problem is to
replace these labels with more generic labels. In [11] a number of
hub removal strategies were studied to perform this task. The most
promising approach detected hubs based on the combination of
edge and vertex labels and relabeled connected vertices with the
label of the hub vertex. In this section we introduce a slightly more
generic version of this approach.

Algorithm 3.5 presents our hub removal strategy.4 First, all
subjectpredicate and predicateobject pairs in a set of RDF triples
are counted and sorted by frequency. All pairs with a minimum
frequency k are kept. We furthermore remove the pairs that
have an instance as either subject or object, because we do not
want to potentially remove instance vertices. The remaining pairs
represent new more generic labels, which we want to put as new
labels on the vertices. For the remaining pairs we create a map Freq
from pairs to their frequency. Then we loop over our RDF graph G
and for each vertex v that represents a subject or object, we check
its outgoing and incoming neighbors (with their connected vertex)
for occurrences in our map Freq. If the pair occurs in Freq then we
remove it from the graph. The pair that has the lowest frequency
provides the new label for v, by concatenating the labels of the
two elements of the pair. The intuition behind using the pair with
the lowest frequency is that we want a more generic label for the
vertex, but we do not want it to be overly generic. So we take the
label that is still considered frequent (since it is above k), but not
the most frequent.

Compared to the algorithms in [11], this algorithm is more
rigorous, since it removes all the frequent pairs from the graph.
Furthermore, we use the parameter k to control the edges to
remove by specifying a minimum frequency, instead of removing
the top h number of hubs. This hub removal algorithm tries to
tackle both the problem of a large number of different labels as well
as the problem of unique vertex labels by replacing vertex labels by
more generic labels.

4. Experiments

In this section we present results for five sets of experiments
using the kernels presented above. The first four sets are
classification experiments with Support Vector Machines (SVMs):
the first using the kernels on regular datasets, the second using
the MinFreq kernels, the third using the kernels in combination
with hub removal, and finally using the kernels on unlabeled RDF
graphs. The final set of experiments presents the runtimes for the
different kernels.

The goal of these experiments is to investigate the performance
differences between the kernel options provided by the presented
framework. In earlier experiments [9] we used iteration weighting
in the Weisfeiler-Lehman kernel and class weights in the SVMs.
Also the direction of the Weisfeiler-Lehman algorithm in [9] was
opposite, i.e. labels traveled to the leafs. In this scenario the
WL kernel does not fit our framework. To have as few potential
influencing factors as possible, we do not use these settings in the
experiments in this paper. However, they can have a positive effect
on performance, so they are options to consider when applying the
kernels in a practical application.

Where appropriate, we also include the IntersectionSubTree
and IntersectionPartialSubTree kernels from [14] for completeness,
using settings that are provided in that paper. The GraphWalks

Algorithm 3.5 Hub Removal
Input

a set of triples T , a set of instances I, graph G based on
T , and minimum frequency k

Output graph G with hubs removed
Comments Nout (v) = {v | (v, v)  E} is the out-neighborhood

and Nin(v) = {v | (v, v)  E} is the in-neighborhood

1. count the subjectpredicate (s, p) and predicateobject (p, o)

pairs in T and retain the pairs with a frequency  k

2. remove all pairs for which the subject or object is an instance in

3. for all retained pairs , create a map Freq() which maps  to
its frequency
 for each v  V that represents subject or object
 cmin = 
 for each v  Nout (v)
 v is the vertex for which (v, v)  E
 if Freq(l(v), l(v)) is defined
 Remove v from V and the edges (v, v) and (v, v) from

 if Freq(l(v), l(v)) < cmin
 cmin = Freq(l(v), l(v))
 min = (l(v), l(v))

 for each v  Nin(v)
 v is the vertex for which (v, v)  E
 if Freq(l(v), l(v)) is defined
 Remove v from V and all edges (v, v) and (v, v) from

 if Freq(l(v), l(v)) < cmin
 cmin = Freq(l(v), l(v))
 min = (l(v), l(v))
 if cmin = , set l(v) to the concatenation of min
 remove any orphaned v  V

kernels are so similar to the intersection graph kernels from [14],
that we do not include the latter. Moreover, the intersection graph
kernels are very inefficient to compute.

All of the kernels and experiments are implemented in Java
and are available online.5 For our experiments we use the
SESAME6 triple-store to handle RDF data and the Java versions
of the LibSVM [35] and LibLINEAR7 [34] support vector machine
libraries.

In each classification experiment we optimize over a number
of extraction depths d for the graphs and trees. For the maximum
walk length and maximum subtrees depth n (or equivalently the
number of iterations parameter h) we use n = d. We choose not
to additionally optimize over n during training, because higher
settings include the lower settings and thus the optimization
has very little effect, other than substantially increasing training
time. Furthermore, we test the kernels both in the setting where
additional triples are added to the dataset by RDFS inferencing
by the triple-store and when these are not. Since the differences
in results between these two settings are not very large, we only
report the results for the setting which gave the best performance.
As done earlier in [14,9], we set the labels for the instance
vertices to one identical special root label, since the original label
uniquely identifies each instance, which is not useful. All computed
kernels/feature vectors are normalized before use.

4 Note that we call it hub removal because of historical reasons, but the hubs
that are removed are actually edges (or in our bipartite case the edges that represent
predicates), not vertices.

5 http://www.data2semantics.org/publications/jws-2015/ and
https://github.com/Data2Semantics/mustard.
6 http://www.openrdf.org/.
7 http://liblinear.bwaldvogel.de/.

Table 1
Average number of edges for the extracted instance graphs and trees for the
different datasets. The extraction depth is given by d and inf indicates inferencing
by the triple-store.

Affiliation
Affiliation, inf.
Mutag
Mutag, inf.
Lithogenesis
Lithogenesis, inf.
Theme
Theme, inf.
Category
Category, inf.

d = 6
Graph

Tree

Tree
15 640
24 112

3 824
1 487
2 538
3 720
4 850


d = 2
Graph

d = 4
Graph

Tree

GraphWalks kernel under the d = 6 setting did not finish in time,
so for the GraphWalks kernel we optimized d from the range: 2, 4.
Slightly better performance was achieved in the no inference
settings, so those results are shown in the table. The TreeWalks
kernel achieves the overall best performance. The bag of labels
(BoL) kernels, especially TreeBoL also show very good performance.
The kernels that only allow substructures with the Root, especially
the Subtrees variant, and the IntersectionSubTree kernels clearly
show the worst performance.

4.1.2. Mutagenic prediction

For our next experiment we use the MUTAG dataset, which
is distributed as an example dataset for the DL-Learner9 [37]
toolkit. It contains information about 340 complex molecules that
are potentially carcinogenic, which is given by the isMutagenic
property, which is the property that we try to predict. The largest
class in this dataset contains 211 of the 340 instances. The setup
is the same as in the previous experiment. We remove the
isMutagenic property from the dataset.
Results. Table 2 shows the results for this experiment from the no
inference setting. As in the affiliation prediction experiment, for
the GraphWalks kernel we optimized from the range: 2, 4. From
the results it is very clear that the BoL kernels show the best
performance.

4.1.3. Lithogenesis prediction

For our third experiment we use a dataset from the British
Geological Survey (BGS),10 as we did earlier in [9]. This dataset
contains information about geological measurements in the form
of Named Rock Units. These named rock units have a number of
properties to predict, one of them is the lithogenesis property, for
which we take the four largest classes, leading to 163 instances.11
The setup is the same as in the previous two experiments. We
remove the lithogenesis property and its inverse property from the
dataset during training and testing.
Results. The results are given in Table 2. There are a number
of good scores. The Subtrees kernels show the best performance.
However, the GraphBoL kernel achieves a similar score. What is
interesting is that the Walks kernels clearly perform worse than
the BoL and Subtrees kernels. Like in the affiliation prediction
experiment, the Root and the IntersectionSubTree kernels are the
worst performers.

4.1.4. Theme prediction

For the fourth experiment we again use the BGS dataset. This
time we try to predict the geological theme. The dataset contains
around 12 000 named rock units with a geological theme. We use
the three largest classes.12 The data is highly skewed, since the
largest class contains around 10 000 instances.

The experimental setup is similar to the two experiments
above. However, we take 10 random subsets of 5% of the data and
use 5-fold cross-validation. Furthermore, we use LibLINEAR to train
support vector machines directly on the feature vector representa-
tions. We therefore do not include the IntersectionSubTree kernels,
which do not have such a representation. Their exclusion is further justified by their weak (and similar to the Root kernels) performance in the previous three tasks.

9 http://dl-learner.org.
10 http://data.bgs.ac.uk/.
11 We expanded this experiment to four classes from two in the original version
in [9] to make it more discriminative.
12 We expanded this experiment to three classes from two in the original version
in [9] to make it more discriminative.

4.1. Regular classification

Table 1 presents the average number of edges per instance
graph or tree for the datasets used in the classification experi-
ments. For the Direct kernels, the size of the neighborhood that is
considered is the same as for the instance graphs.

From the table we can see that the number of additional
edges that the tree perspective introduces with respect to the
graph perspective differs quite significantly between datasets. This
difference is an indication of the tree/graph like nature of the
underlying RDF graph. If this RDF graph contains a large number
of cycles, then the instance trees will have far more edges than the
instance graphs.

4.1.1. Affiliation prediction

Our first classification experiment uses the AIFB dataset, used
for the first time in [17] and repeated in following papers. This
dataset contains information about the AIFB research institute:
people, publications, groups; modeled in the SWRC ontology [36].
In total this dataset contains 178 persons that belong to 1 of 5
research groups. One of these groups has only 4 members, which
we ignore. The goal of the experiment is to predict the affiliation for
the remaining 174 persons. For training and testing the affiliation
relation (and the inverse employs relation) are removed from the
dataset.

For each kernel a C-SVC support vector machine from the
LibSVM library is trained. We do 10-fold cross-validation per
kernel, which is repeated 10 times with different randomization
seeds. Within each fold, the d and C parameter are optimized, again
using 10-fold cross-validation. The depth parameter d is optimized
from the range: 2, 4, 6.8 For C we use the range: 1, 10, 102, 103,
which is dynamically extended by a factor 10 if the best
performance is achieved on the edge (e.g. 1 or 103) of the range.
Results. Table 2 shows the results for this experiment (and the following two) in terms of average accuracy, with the standard deviation shown in parentheses. The table shows the neighborhood
extraction method (Graph, Tree or Direct) horizontally versus the
different kinds of substructures (BoL, Walks, Subtrees) vertically.
The kernels that have the Root constraint are shown in a separate column, together with the Intersection SubTree (IST) and Intersection Partial SubTree (IPST) kernels. The best scores overall,
and those scores that have no significant difference with those
scores under a paired t-test with p < 0.05, are indicated in a bold
type face. The best score per column, and those that do not differ
significantly, are shown in italic type face. The computation of the

8 We use only even settings since one triple (s, p, o) consists of two edges in our
graph representation.

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

Table 2
Results for the Affiliation, Mutagenic and Lithogenesis experiments.

Affiliation

BoL
Subtrees
Walks
Lithogenesis

BoL
Subtrees
Walks

Graph
.875(.014)
.888(.007)
.879(.008)

Graph
.859(.011)
.861(.014)
.826(.016)

Tree
.909(.003)
.912(.004)
.920(.007)

Tree
.842(.013)
.844(.013)
.812(.021)

Direct

.886(.009)
.895(.012)

Direct

.864(.013)
.795(.018)

Table 3
Results for the Theme and Category Prediction experiments.

Theme

BoL
Subtrees
Walks
Category

BoL
Subtrees
Walks

Graph
.985(.003)
.988(.002)
.988(.002)

Graph
.920(.004)
.931(.003)
.919(.003)

Tree
.992(.003)
.993(.001)
.990(.003)

Tree
.930(.003)
.931(.003)
.919(.003)

Direct

Root

.990(.003)
.986(.003)

.625(.013)
.984(.005)

Direct

Root

.931(.002)
.916(.003)

.519(.009)
.917(.003)

Results. Table 3 contains the results for the setting with triplestore inferencing. Again, for the GraphWalks we used the range:
2, 4. The TreeSubtrees kernel has the best performance. However,
the TreeBoL kernel achieves very similar performance. Again, the
Root kernels clearly perform worst.

4.1.5. Category prediction

For our fifth classification experiment we use a dataset from
the Amsterdam Museum [38], which contains information about
around 70 000 artifacts in the museums collection. Each artifact
belongs to one of 18 categories, we try to predict this category
for the largest 17 classes. Next to the category relation we also
remove the highly correlated materials relation from the data
during training and testing.

Similar to the previous experiment we perform classification
on 10 random subsets, with 5% of the full dataset. We only use
the range: 2, 4 for the depth parameter, since most kernels were
difficult to compute for d = 6 because of very large instance
graphs/trees. Again, we use LibLINEAR.
Results. We give the results in Table 3 for the setting with inferenc-
ing. Very clearly the best performance is achieved by the Subtrees
kernels in all settings. Once again, the Root kernels show the worst
performance.

4.1.6. Discussion

In three of the five experiments the best performing kernel
is one of the Subtrees variants, only in the affiliation prediction
experiment does the TreeWalks kernel perform better. However,
the improvement of using the more complex kernels over the bag
of labels (BoL) kernels is not large, for two experiments the BoL
kernels achieve a similar score and in one experiment they are even
clearly the best performers.

In all experiments the kernels that consider only substructures
with the root, i.e. the Root kernels and both IntersectionSubTree
kernels, are the worst performers. For a large number of settings
they are outperformed by the simpler BoL kernels.

Mutagenic

BoL
Subtrees
Walks
Root

Subtrees
Walks

Graph
.948(.006)
.914(.005)
.914(.004)

Aff.
.553(.011)
.865(.014)
.778(.010)
.827(.012)

Tree
.951(.004)
.936(.009)
.923(.008)

Mutag.
.626(.000)
.910(.008)
.795(.014)
.932(.001)

Direct

.914(.005)
.914(.004)

Litho.
.579(.006)
.803(.018)
.661(.014)
.804(.020)

The five experiments show no clear preference for one of the
three extraction methods (Graph, Tree and Direct). However, the
Tree kernels clearly perform the best in the affiliation prediction
experiment, which is interesting, because the difference in size between the graph and tree representation for that dataset is the
largest. For the no inference setting for which the results are
shown, for depth 6 the trees have 5 times more edges than the
graphs.

4.2. Classification with removing low frequency labels

For our experiments with the minFreq kernels we use the same
classification tasks as above. In addition to optimizing over the d
parameter we also optimize over the minimal frequency parameter , for which we used different ranges for different experi-
ments. For the affiliation and theme prediction experiments we
used: 0, 1, 2, 4, 8, 16, for lithogenesis prediction: 0, 2, 4, 8, 16, 32,
for category prediction: 0, 1, 2, 4, 8, and finally for mutagenic:
0, 16, 32, 64, 128, 256, the mutagenic task really benefits from the
higher thresholds. For the Subtrees kernels we tested with using
both sets and multisets during the Weisfeiler-Lehman relabeling.
In turned out that only in the lithogenesis prediction task switching to sets was beneficial. Also, in these experiments we do not
set the label of the instances vertices to the same label, since the
MinFreq modification generalizes this idea. The rest of the experimental setup is unchanged.

4.2.1. Results
The results for the 5 experiments are given in Table 4. With
+ or  a positive, resp. negative, significant difference, using a
paired t-test with p < 0.05, with the corresponding score in the
original experiment (i.e. the corresponding non MinFreq kernel) is
given. The Subtrees kernels benefit more from removing the low
frequency labels than the Walks kernels. Especially noteworthy
is the improvement in the mutagenic prediction task, where all
kernels improve. They showed clearly worse performance than
the BoL kernels in the original experiment, but the performance
is closer (and in two cases better) now.

4.2.2. Discussion

In all five experiments the MinFreq modification of the algorithms has a positive effect. Especially the variants of the Subtrees
kernels achieve better performance, albeit relatively marginal in
case of the theme prediction task. The preferred extraction method
does not really change for the MinFreq kernels.

4.3. Classification with hub removal

For our hub removal experiments we again perform the same
classification tasks. The original kernels (i.e. without the MinFreq

Table 4
Results for the 5 experiments using the MinFreq kernels.

Affiliation

Subtrees
Walks
Mutagenic

Subtrees
Walks
Lithogenesis

Subtrees
Walks
Theme

Subtrees
Walks
Category

Subtrees
Walks

Graph
.895+(.009)
.839(.013)

Tree
.914(.006)
.918(.006)

Direct
.900+(.010)
.902(.007)

Root
.581+(016)
.828(.018)

Graph
.937+(.014)
.936+(.007)

Tree
.952+(.007)
.943+(.007)

Direct
.958+(.006)
.944+(.005)

Root
.668+(.005)
.936+(.006)

Graph
.882+(.011)
.809(.020)

Tree
.863+(.012)
.795(.024)

Direct
.881+(.009)
.788(.020)

Root
.593+(.017)
.764(.021)

Graph
.990(.004)
.984(.006)

Tree
.995+(.002)
.986(.003)

Direct
.990(.003)
.984(.004)

Root
.676+(.042)
.980(.008)

Graph
.935+(.003)
.923+(.005)

Tree
.933+(.003)
.921(.004)

Direct
.933+(.003)
.924+(.003)

Root
.498(.024)
.913(.006)

modifications) are now computed on graphs with hubs removed,
using Algorithm 3.5, for the following settings of the minimum
frequency k: 10, 20, 40, 80, 160,. Note that the  setting
means that the graph is unchanged. During training of the support
vector machine we now optimize k as well as d and C.

4.3.1. Results
Table 5 gives the results for the affiliation, mutagenic and lithogenesis prediction experiments. The + and  have the same meaning is in the results table for the MinFreq experiments. For the
affiliation prediction experiment we see a clear improvement for
all the Tree kernels, especially the TreeSubtrees kernel performs
clearly better. For the mutagenic prediction experiment all kernels improve, especially the Walks kernels are interesting, since
they are now the best performers. Finally, in the lithogenesis experiment we see a clear negative impact of hub removal, with
the GraphWalks and GraphSubtrees kernels drastically decreasing
in performance.

The results for the theme and category prediction experiments
are given in Table 6. Hub removal has almost no influence
on performance, apart from quite substantially increasing the
performance of the TreeSubtreesRoot kernel.

4.3.2. Discussion

Only in the first three experiments do we see a clear influence
of the hub removal. Especially in the affiliation and mutagenic prediction task it leads to improved performance for a large number
of kernels. For the lithogenesis prediction experiment the influence
is more mixed, performance is increased for some kernels, but also
severely decreased for two kernels. For the two experiments on
larger dataset hub removal has very little influence.

Our hub removal strategy can, in principle, be combined with
the MinFreq kernels and this could potentially improve performance further. However, it also leads to the combined optimization of the parameters  and k, substantially increasing training
time.

4.4. Graphs without labels

performance than the baseline BoL kernels, the difference is
smaller than might be expected. To get a feeling for the importance
of graph structure in RDF graphs with respect to the classification
tasks presented, we performed the first three of the above
experiments (affiliation, mutagenic and lithogenesis prediction) on
graphs with the vertex labels removed (i.e. all of them are set to the
same label).

4.4.1. Results

The results for these experiments are given in Table 7. Note
that the BoL kernels in all three experiments now perform on
chance level.13 In the affiliation and lithogenesis experiments, the
Subtrees kernels are clearly the best performers, whereas there is
less difference between the Walks and Subtrees in the mutagenic
experiment.

4.4.2. Discussion

These three experiments clearly show that there is information
in the structure of the RDF graphs, as well as the labels. Especially
the Subtrees kernels are able to exploit this information. However,
the amount of information that the graph structure contains on top
of the label information is apparently not very large. Since in the
regular experiments, the difference between the BoL kernels and
the Subtrees kernels is very small.

4.5. Computation time

Finally, we present the computation time of a representative
number of kernels for three different datasets. For each of those
datasets we compute the kernel on a number of fractions of the
entire dataset extracted for d = 6, with inferencing. Note that
the implementations for the two bag of labels (BoL) kernels are
not as efficient as possible, since they are essentially h = 0 Subtrees kernels. They are still included as a reference though. Note
that the computation time shown is for the substructure counting
(e.g. the Weisfeiler-Lehman algorithm) and kernel matrix computation steps of the kernels, all of the kernels also have an instance
extraction step (either to graphs or trees, or to maps that track the
vertices and edges for an instance in case of the Direct kernels). This
step is similar in terms of computation time for all the kernels. So,
the substructure counting step is what determines the differences
in computation time between the kernels.

4.5.1. Lithogenesis prediction

Fig. 3 shows the computation times for the lithogenesis
prediction dataset. This is the only figure which includes the
GraphWalks kernel. Already on this small dataset it takes far more
time to compute than the other kernels. The rest of the kernels are
close together, only the two BoL kernels are clearly faster.

4.5.2. Affiliation prediction

The computation time for the affiliation prediction dataset is
given in Fig. 4. We can see that for larger fractions the DirectSubtrees kernel more and more outperform the GraphSubtrees kernel
as predicted, as does the TreeSubtrees kernel. The Walks are substantially slower on this dataset, since there are a large number of
cycles in the RDF dataset. Note that for this dataset, GraphWalks
only finished computation for the first few fractions, with times
that would be far off this figure with a factor 10 or more.

Even though the kernels that count more complex graph
features (Walks and especially Subtrees) often show better

13 Their accuracy is the same as the fraction of the largest class in the dataset.

G.K.D. de Vries, S. de Rooij / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 7184

Table 5
Results for the Affiliation, Mutagenic and Lithogenesis experiments using Hub Removal.

Affiliation

BoL
Subtrees
Walks
Lithogenesis

BoL
Subtrees
Walks

Graph
.896+(.017)
.896(.014)
.884(.016)

Graph
.859(.012)
.640(.015)
.671(.018)

Tree
.931+(.008)
.936+(.008)
.932+(.008)

Tree
.839(.016)
.839(.017)
.844+(.010)

Direct

.895(.014)
.920+(.010)

Direct

.841(.019)
.838+(.023)

Mutagenic

BoL
Subtrees
Walks
Root

Subtrees
Walks

Graph
.967+(.006)
.962+(.004)
.966+(.007)

Aff.
.663+(.011)
.892+(.020)
.867+(.015)
.866+(.014)

Tree
.961+(.005)
.962+(.006)
.963+(.009)

Mutag.
.944+(.000)
.966+(.006)
.941+(.003)
.947+(.004)

Direct

.962+(.004)
.965+(.007)

Litho.
.790+(.010)
.837+(.020)
.795+(.015)
.769(.019)

Table 6
Results for the Theme and Category Prediction experiments using Hub Removal.

Theme

BoL
Subtrees
Walks
Category

BoL
Subtrees
Walks

Graph
.985(.003)
.988(.004)
.985(.004)

Graph
.920(.004)
.931(.003)
.918(.003)

Tree
.991(.004)
.992(.003)
.990(.004)

Tree
.930(.003)
.930(.004)
.918(.004)

Direct

Root

.990(.004)
.985(.005)

.898+(.018)
.985(.004)

Direct

Root

.930(.003)
.916(.003)

.758+(.012)
.917(.003)

4.5.3. Theme prediction

For the theme prediction dataset computation times are given
in Fig. 5. Since we use feature vectors in this experiment, the
times are for computing the feature vectors. Again, as the fractions
become larger, the difference between the DirectSubtrees and
GraphSubtrees kernel grows. This dataset contains far less cycles
and therefore the TreeSubtrees is very close in computation time
to DirectSubtrees. Again, GraphWalks only finished computation for
the first few fractions, but the times would be far off the figure.

4.5.4. Discussion

From the results of the classification experiment and the
computation times, we can see that the DirectWalks kernel makes
the GraphWalks kernel computationally viable. The large difference
in computation time is due to the exponential growth in the
number of walks in graphs with cycles. Hence the fact that the
GraphWalks kernel is already hard to compute on the small dataset
used in the affiliation prediction experiment.

The DirectSubtrees algorithm also speeds up the GraphSubtrees
kernel by around a factor 3 for the full affiliation and theme
prediction datasets. And as we can see from the figures this factor
grows when datasets become larger.

Fig. 3. Runtimes (ms) for different kernels on the Lithogenesis Prediction dataset,
with errors bars indicating the standard deviation.

Fig. 4. Runtimes (ms) for different kernels on the Affiliation Prediction dataset,
with errors bars indicating the standard deviation.

Table 7
Results for the Affiliation, Mutagenic and Lithogenesis experiments with labels removed.

Affiliation

BoL
Subtrees
Walks
Lithogenesis

BoL
Subtrees
Walks

Graph
.412(.000)
.844(.009)
.549(.014)

Graph
.571(.000)
.809(.019)
.657(.014)

Tree
.412(.000)
.884(.018)
.698(.013)

Tree
.571(.000)
.680(.024)
.569(.004)

Direct

.862(.009)
.647(.013)

Direct

.740(.016)
.581(.017)

Mutagenic

BoL
Subtrees
Walks
Root

Subtrees
Walks

Graph
.621(.000)
.708(.016)
.689(.009)

Aff.
.514(.011)
.692(.011)
.397(.027)

Tree
.621(.000)
.726(.012)
.621(.000)

Mutag.
.624(.016)
.723(.007)
.620(.002)

Direct

.715(.016)
.703(.009)

Litho.
.534(.013)
.564(.011)
.571(.000)

Vector Machines [3,4] the kernels defined in this paper could be
used to perform link prediction. It would be interesting future
research to see how such an approach compares to existing
methods.

Currently, our kernels can only be used on RDF graphs that fit in
main memory. However, the kernels discussed in this framework,
and especially the kernels computed directly on the RDF graph, are
suitable to implement on large graph processing platforms, such as
GraphLab [39] and Pregel [40].

Acknowledgments

This publication was supported by the Dutch national program
COMMIT and by the Netherlands eScience center. The authors
thank Peter Bloem for valuable discussions.
