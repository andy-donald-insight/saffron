Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 142151

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Mining the Web of Linked Data with RapidMiner
Petar Ristoski, Christian Bizer, Heiko Paulheim

Data and Web Science Group, University of Mannheim, B6, 26, 68159 Mannheim, Germany

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 January 2015
Received in revised form
11 May 2015
Accepted 11 June 2015
Available online 8 July 2015

Keywords:
Linked Open Data
Data mining
RapidMiner

Lots of data from different domains are published as Linked Open Data (LOD). While there are quite
a few browsers for such data, as well as intelligent tools for particular purposes, a versatile tool for
deriving additional knowledge by mining the Web of Linked Data is still missing. In this system paper, we
introduce the RapidMiner Linked Open Data extension. The extension hooks into the powerful data mining
and analysis platform RapidMiner, and offers operators for accessing Linked Open Data in RapidMiner,
allowing for using it in sophisticated data analysis workflows without the need for expert knowledge
in SPARQL or RDF. The extension allows for autonomously exploring the Web of Data by following
links, thereby discovering relevant datasets on the fly, as well as for integrating overlapping data found
in different datasets. As an example, we show how statistical data from the World Bank on scientific
publications, published as an RDF data cube, can be automatically linked to further datasets and analyzed
using additional background knowledge from ten different LOD datasets.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

The Web of Linked Data contains a collection of machine
processable, interlinked datasets from various domains, ranging
from general cross-domain knowledge sources to government,
library and media data, which today comprises roughly a thousand
datasets [1,2]. While many domain-specific applications use
Linked Open Data, general-purpose applications rarely go beyond
displaying the mere data, and provide little means of deriving
additional knowledge from the data.

At the same time, sophisticated data mining platforms exist,
which support the user with finding patterns in data, providing
meaningful visualizations, etc. What is missing is a bridge between
the vast amount of data on the one hand, and intelligent data
analysis tools on the other hand. Given a data analysis problem,
a data analyst should be able to automatically find suitable data
from different relevant data sources, which will then be combined
and cleansed, and served to the user for further analysis. This
data collection, preparation, and fusion process is an essential part
of the data analysis workflow [3], however, it is also one of the
most time consuming parts, constituting roughly half of the costs
in data analytics projects [4]. Furthermore, since the step is time
consuming, a data analyst most often makes a heuristic selection of
data sources based on his a priori assumptions, and hence is subject

 Corresponding author.

E-mail address: petar.ristoski@informatik.uni-mannheim.de (P. Ristoski).

http://dx.doi.org/10.1016/j.websem.2015.06.004
1570-8268/ 2015 Elsevier B.V. All rights reserved.

to the selection bias. Despite these issues, automation at that stage
of the data processing step is still rarely achieved.

In this paper, we discuss how the Web of Linked Data can
be mined using the full functionality of the state of the art data
mining environment RapidMiner1 [5]. We introduce an extension
to RapidMiner, which allows for bridging the gap between the Web
of Data and data mining, and which can be used for carrying out
sophisticated analysis tasks on and with Linked Open Data. The
extension provides means to automatically connect local data to
background knowledge from Linked Open Data, or load data from
the desired Linked Open Data source into the RapidMiner platform,
which itself provides more than 400 operators for analyzing data,
including classification, clustering, and association analysis.

RapidMiner is a programming-free data analysis platform,
which allows the user to design data analysis processes in a plug-
and-play fashion by wiring operators. Furthermore, functionality
can be added to RapidMiner by developing extensions, which are
made available on the RapidMiner Marketplace.2 The RapidMiner
Linked Open Data extension adds operators for loading data from
datasets within Linked Open Data, as well as autonomously
following RDF links to other datasets and gathering additional data
from there. Furthermore, the extension supports schema matching
for data gathered from different datasets.

1 http://www.rapidminer.com/.
2 https://marketplace.rapidminer.com/.

As the operators from that extension can be combined with
all RapidMiner built-in operators, as well as those from other
extensions (e.g., for time series analysis), complex data analysis
processes on Linked Open Data can be built. Such processes
can automatically combine and integrate data from different
datasets and support the user in making sense of the integrated
data.

The use case we pursue in this paper starts from a Linked
Open Dataset publishing various World Bank indicators. Among
many others, this dataset captures the number of scientific journal
publications in different countries over a period of more than
25 years. An analyst may be interested in which factors drive a high
increase in that indicator. Thus, she needs to first determine the
trend in the data. Then, additional background knowledge about the
countries is gathered from the Web of Linked Data, which helps
her in identifying relevant factors that may explain a high or low
increase in scientific publications. Such factors are obtained, e.g., by
running a correlation analysis, and the significant correlations can
be visualized for a further analysis, and for determining outliers
from the trend.

The rest of this paper is structured as follows. Section 2
describes the functionality of the RapidMiner Linked Open Data
extension. In Section 3, we show the example use case of scientific
publications in detail, whereas Section 4 briefly showcases other
use cases for which the extension has been employed in the past.
Section 5 presents evaluations of various aspects of the extension.
Section 6 discusses related work, and Section 7 provides an outlook
on future directions pursued with the extension.

2. Description

RapidMiner is a data mining platform, in which data mining and
analysis processes are designed from elementary building blocks,
so called operators. Each operator performs a specific action on
data, e.g., loading and storing data, transforming data, or inferring
a model on data. The user can compose a process from operators by
placing them on a canvas and wiring their input and output ports,
as shown in Fig. 1.

The RapidMiner Linked Open Data extension adds a set of
operators to RapidMiner, which can be used in data mining
processes and combined with RapidMiner built-in operators, as
well as other operators. The operators in the extension fall into
different categories: data import, data linking, feature generation,
schema matching, and feature subset selection.

2.1. Data import

RapidMiner itself provides import operators for different data
formats (e.g., Excel, CSV, XML). The Linked Open Data extension
adds two import operators:
 A SPARQL Importer lets the user specify a SPARQL endpoint or
a local RDF model, and a SPARQL query, and loads the query
results table into RapidMiner. For local RDF models, SPARQL
queries can also be executed with RDFS and different flavors of
OWL inference [6].
 A Data Cube Importer can be used for datasets published using
the RDF Data Cube vocabulary.3 Following the Linked Data
Cube Explorer (LDCX) prototype described in [7], the importer
provides a wizard which lets the user select the dimensions to
use, and creates a pivot table with the selected data.

3 http://www.w3.org/TR/vocab-data-cube/.

2.2. Data linking

In order to combine a local, potentially non-RDF dataset
(e.g., data in a CSV file or a database) with data from the LOD cloud,
links from the local dataset to remote LOD cloud datasets have to
be established first. For that purpose, different linking operators
are implemented in the extension:
 The Pattern-based linker creates URIs based on a string pattern.
If the pattern a dataset uses for constructing its URIs is known,
this is the fastest and most accurate way to construct URIs. For
example, the RDF Book Mashup [8] dataset uses a URI pattern for
books which is based on the ISBN.4
 The Label-based linker searches for resources whose label is
similar to an attribute in the local dataset, e.g., the product
name. It can only be used on datasets providing a SPARQL
interface and is slower than the pattern-based linker, but can
also be applied if the link patterns are not known, or cannot be
constructed automatically.
 The Lookup linker uses a specific search interface5 for the
DBpedia dataset [9]. It also finds resources by alternative names
(e.g., NYC or NY City for New York City). For DBpedia, it usually
provides the best accuracy.
 For processing text, a linker using DBpedia Spotlight6 [10] has
also been included, which identifies multiple DBpedia entities
in a textual attribute.
 The SameAs linker can be used to follow links from one dataset
to another. Since many datasets link to DBpedia, a typical setup
to link to an arbitrary LOD dataset is a two-step approach:
the Lookup linker first establishes links to DBpedia at high
accuracy. Then, owl:sameAs links between DBpedia and the
target dataset are exploited to set the links to the latter.

2.3. Feature generation

For creating new data mining features from Linked Open Data
sources, different strategies are implemented in the extensions
operators:
 The Direct Types generator extracts all types (i.e., objects of
rdf:type) for a linked resource. For datasets such as YAGO,7
those types are often very informative, for example, products
may have concise types such as Smartphone or AndroidDevice.
 The Datatype Properties generator extracts all datatype proper-
ties, i.e., numerical and date information (such as the price and
release date of products).
 The Relations generator creates a binary or a numeric attribute
for each property that connects a resource to other resource.
For example, if a dataset contains awards won by products,
an award attribute would be generated, either as a binary
feature stating whether the product won an award or not, or
a numerical one stating the number of awards.
 The Qualified Relations generator also generates binary or
numeric attributes for properties, but takes the type of the
related resource into account. For example, for a product linked
to a manufacturer of type GermanCompany, a feature stating
whether the product has been manufactured by a German
company or not would be created.

4 In cases where additional processing is required, such as removing dashes in an
ISBN, the operator may be combined with the built-in Generate Attributes operator,
which can perform such operations.
5 http://lookup.dbpedia.org/.
6 http://spotlight.dbpedia.org/.
7 http://yago-knowledge.org.

P. Ristoski et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 142151

Fig. 1. Overview of the process used in the running example, including the nested subprocess in the link explorer operator.

 The Specific Relations generator creates features for a userspecified relation, such as Wikipedia categories included in
DBpedia. It also allows for following property chains spanned
by a user-defined property, e.g., following the skos:broader
relation for Wikipedia categories in DBpedia in order to extract
features for super-categories as well.
 In addition to those automatic generators, it is also possible to
control the attribute generation process in a more fine-grained
manner by issuing specific SPARQL queries using the Custom
SPARQL generator.
Kernel functions are distance functions that hold between two
data instances in some arbitrary space. They are commonly used
in Support Vector Machines (SVMs), in which they allow training
a classifier without explicitly transforming the data into a feature
vector representation [11]. However, kernel functions can be used
to transform, e.g., RDF data into a propositional feature vector.
The Linked Open Data Extension integrates such operators for two
types of kernels from the Data2Semantics Mustard library8:
 RDF Walk Count Kernel counts the different walks in the subgraphs (up to the provided graph depth) around the instances
nodes. The maximum length of the walks can be specified as a
parameter. For this kernel, there are four different variants:
 Fast: This is a fast approximation of counting all the walks in

the subgraph, which is done with the Full setting.

 Root: Only considers walks that start with the instance node

(i.e. the root) [12].

 Tree: Counts all the walks in the subtree that is rooted in the
instance node. This is faster than the Full subgraph version,
since a tree does not contain cycles.

 Full: Counts all the walks in the subgraph.
 RDF WL Sub Tree Kernel counts the different full subtrees in
the subgraphs (up to the provided graph depth) around the
instances nodes, using the WeisfeilerLehman algorithm [13].
The maximum size of the subtrees is controlled by the number
of iterations, which can be a specified parameter. Like for the
RDF Walk Count Kernel, there are four different variants of this
kernel:
 Fast: This is a fast approximation of counting all the subtrees

in the subgraph, which is done with the Full setting [14].

 Root: Only considers subtrees that start with the instance

node (i.e. the root).

 Tree: Counts all the subtrees in the subtree that is rooted in
the instance node. This is faster than the Full subgraph ver-
sion, since a tree does not contain cycles.

 Full: Counts all the subtrees in the subgraph.
All of those feature generation operators can work in three

different modes:
1. using a predefined SPARQL endpoint,
2. dereferencing URIs and processing the RDF returned from an

HTTP request, or

3. using a local RDF model.
While the SPARQL-based variant is usually faster, the dereferencing URIs variant is more versatile, as it can also be applied to
datasets not offering a SPARQL endpoint.

Furthermore, all mentioned generators are able to retrieve
the hierarchical relations between the features, e.g., the direct
types generator examines the rdfs:subClassOf property. Such
hierarchies may be exploited for feature selection (see below). The
extension also offers a graphical visualization of such hierarchies
that might be useful for a better understanding of the data. For
example, an excerpt of the retrieved hierarchy from the YAGO
ontology in DBpedia for instances of type Country is depicted in
Fig. 2.

When generating data mining features from graph based data,
different propositionalization strategies can be applied. For exam-
ple, the standard binary or numeric (i.e., counting) representation
can be used, or more sophisticated representation strategies that
use some graph characteristics might be introduced. The strategy
of creating features has an influence on the data mining result. For
example, proximity-based algorithms like k-NN will behave differently depending on the strategy used to create numerical features,
as the strategy has a direct influence on most distance functions.
So far, the extension supports four strategies for generating a
feature for properties connected with a resource:
 Creating a binary feature for each relation.
 Creating a count feature for each relation, specifying the number
of resources connected by this relation.
 Creating a relative count feature for each relation, specifying the
fraction of resources connected by this relation. For a resource
that has total number of P relations, the relative count value for
a relation p is defined as np
P , where np is the number of relations
of type p.
 Creating a TF-IDF feature for each relation, whose value is
and r|r : p(r, r) denotes the number of resources for
P log
|{r|r:p(r,r)}| , where N is the overall number of resources,

np

which the relation p exists.

8 https://github.com/Data2Semantics/mustard.

In [15], we have shown that the propositionalization strategy has
an influence on the result quality of a data mining process (see also
Section 5.2).

Fig. 2. Hierarchical relations (excerpt) between the features retrieved from the YAGO ontology in DBpedia for instances of type Country, visualized in RapidMiner.

2.4. Feature subset selection

All standard methods for feature subset selection can be used
in conjunction with the RapidMiner Linked Open Data extension,
as well as specialized operators from the Feature Subset Selection
extension.9 Furthermore, the Linked Open Data extension provides
the Simple Hierarchy Filter, which exploits the schema information
of a Linked Open Data source, and often achieves a better compression of the feature set than standard, non-hierarchical operators,
without losing valuable features. The operator implements different hierarchical feature selection strategies [1618]. As shown
in [17], those algorithms often lead to a better compression of the
feature space and a reduction of overfitting by avoiding the selection of too specific features (see also Section 5.3).

2.5. Exploring links

The feature generation algorithms above so far use only one
input URI, and obtain features from that URI. This means that they
are usually restricted to one dataset. For making use of the entire
LOD cloud, the extension provides the Link Explorer meta operator,
which follows links of a given type (by default: owl:sameAs) to
a specified depth, and applies a set of operators to each resource
discovered by that (see Fig. 1). A typical configuration is to use
the link explorer in combination with the datatype properties
generator, which results in following links from one starting point,
and collect all the datatype properties for all linked resources.

Since the datasets that are used by that meta operator are
not known a priori, and there is no reliable way of discovering a
SPARQL endpoint given a resource URI [19], the link explorer only
works by dereferencing URIs, but not by means of SPARQL queries.

2.6. Data integration

When combining data from different LOD sources, those usually
use different schemas. For example, the population of a country
can be contained in different datasets, using a different datatype

property to denote the information. In order to use that data more
effectively, such attributes can be merged into one by applying
schema matching. The extension provides the PARIS LOD Matcher,
which is an adaptation of the PARIS framework [20], and is able
to perform alignment of instances, relations and classes. The
matching operator outputs all discovered correspondences. Those
can then be passed to the Data Fusion operator, which offers various
conflict resolution strategies [21], e.g., majority voting, average,
median, etc.

3. Example use case

In our example use case, we use an RDF data cube with World
Bank economic indicators data10 as a starting point. The data cube
contains time-indexed data for more than 1000 indicators in over
200 countries. As shown in Fig. 1, the process starts with importing
data from that data cube (1). To that end, a wizard is used, which
lets the user select the indicator(s) of interest. The complete data
cube import wizard is shown in Fig. 3. In our example, in the
first step we select the indicator Scientific and technical journal
articles, which reports the number of such articles per country
and year. In the second step, we select the dimensions and the
measures.

As a row dimension, we select the countries, and as column
dimensions, we select the time-indexed values for the number
of scientific and technical journal articles. In the final step of the
import wizard, we are able to select the values for the previously
selected dimensions. After completing the wizard, the resulting
table is generated. The indicator is present for 165 countries, so
our resulting data table contains 165 rows, with columns per year,
depicting the target value from 1960 to 2011. We are interested in
understanding which factors drive a large increase in that indicator.
In the next step, we set links of the data imported from the RDF
cube to other datasets (2). In our example, we use the label-based
linker to find countries in DBpedia which have the same name as
the country in the imported slice of the data cube.

The subsequent step is identifying more relevant datasets by
following RDF links, and getting the data from there. This is carried out by the link explorer operator (3). Starting from DBpedia,

9 http://sourceforge.net/projects/rm-featselext/.

10 http://worldbank.270a.info.

P. Ristoski et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 142151

Fig. 3. Data cube import wizard.

we follow all owl:sameAs links to a depth of 2. Inside the link
explorer, we collect datatype (3a) and also perform the matching
(3b). We end up with data from ten different datasets, i.e., DBpe-
dia,11 LinkedGeoData,12 Eurostat,13 Geonamesm,14 WHOs Global
Health Observatory,15 Linked Energy Data,16 OpenCyc,17 World
Factbook,18 and YAGO.19

The initial set of datatype properties extracted has 1329 at-
tributes, which, after processing by schema matching and conflict
resolution, are fused to 1090 attributes. For example, we find six
different sources stating the population of countries by following
RDF links between datasets, five of which are merged into one
single attribute. A detailed evaluation of the feature consolidation
process is given in Section 5.4.

Once all the attributes have been extracted and matched, the
actual analysis starts. First, the Generate Aggregate operator (4),
which is a RapidMiner built-in operator, computes the increase in
scientific publications from the individual per-year values. Then, a
correlation matrix is computed (5), again with a RapidMiner builtin operator, to find interesting factors that explain an increase in
scientific publications.

From all the additional attributes we found by following links to
different datasets in the Web of Data, we are now able to identify
various attributes that explain a strong increase in scientific
publications:

11 http://dbpedia.org.
12 http://linkedgeodata.org.
13 http://eurostat.linked-statistics.org/ and
http://wifo5-03.informatik.uni-mannheim.de/eurostat/.
14 http://sws.geonames.org/.
15 http://gho.aksw.org/.
16 http://en.openei.org/lod/.
17 http://sw.opencyc.org/.
18 http://wifo5-03.informatik.uni-mannheim.de/factbook/.
19 http://yago-knowledge.org.

 The fragile state index (FSI) and the Human Development Index
(HDI) are aggregate measures comprised of different social,
political and health indicators, and both are good indicators for
the growth of scientific publications.
 The GDP per capita is also strongly correlated with the increase
in scientific publications. This hints at wealthier countries being
able to invest more federal money into science funding.
 For European countries, the number of EU seats shows a significant correlation with the increase in scientific publications.
As larger countries have more seats (e.g., Germany, France, UK),
this may hint at an increasing fraction of EU funding for science
going being attributed to those countries [22].
 Additionally, many climate indicators show a strong correlation
with the increase in scientific publications: precipitation has
a negative correlation with the increase, while hours of sun
and temperature averages are positively correlated. This can be
explained by an unequal distribution of wealth across different
climate zones [23], with the wealthier nations often located in
more moderate climate zones.20
So far, these results have concentrated on one specific world
bank indicator, while, as stated above, there are more than a
thousand. We have conducted similar experiments with other
indicators as well, revealing different findings. For example, we
looked into the savings of energy consumption over the last years.
Here, we can observe, e.g., a correlation with the GDP, showing
that wealthier countries can afford putting more efforts into
saving energy, and also have the economic means to replace old,
energy inefficient infrastructure with new, more energy efficient
facilities.21

20 A more tongue-in-cheek interpretation may be that if the weather is bad,
scientists spend more time in the lab writing journal articles.
21 See.,
energy_cx_jz_0707efficiency_countries.html.

e.g., http://www.forbes.com/2008/07/03/energy-efficiency-japan-biz-

In summary, the experiment shows that
 we can enrich a dataset at hand with external knowledge from
the LOD cloud,
 we can follow RDF links between datasets, and, by that, gather
and combine data from different sources to solve a task at hand,
and
 we can use analysis methods that identify relevant answers to
a question.

4. Further use cases

The RapidMiner Linked Open Data extension has already been

used in several prior use cases.

In [24], we use DBpedia and the RDF Book Mashup dataset
to generate eight different feature sets for building a book
recommender system. We built a hybrid, multi-strategy approach
that combines the results of different base recommenders and
generic recommenders into a final recommendation. The complete
system was implemented within RapidMiner, combining the LOD
extension and the Recommendation Extension.22 The approach
was evaluated on three tasks of the LOD-enabled Recommender
Systems Challenge 2014,23 where it achieved the best overall
performance.

In [25], we use the LOD extension for the purpose of linkage
error detection between LOD datasets. To that end, links between
datasets are projected into a high dimensional feature space, and
different multi-dimensional outlier detection techniques, taken
from RapidMiners Anomaly Detection extension [26], are used to
identify wrong links. The approach was evaluated on two datasets
of owl:sameAs links: DBpedia  Peel Sessions,24 and DBpedia 
DBTropes.25 The LOD extension was used to generate the feature
vectors for each link, i.e., direct types, and all ingoing and outgoing
properties. The evaluation showed promising results, with an area
under the ROC curve up to 0.86 (i.e., wrong links get lower scores
than correct links with a probability of 86%), and an F-measure up
to 0.54.

In another use case, the extension is used to implement
an approach that combines NLP techniques and background
knowledge from DBpedia for finding disputed topics in news
sites. To identify these topics, newspaper articles are annotated
with DBpedia concepts, using the DBpedia Spotlight linking
operator. For each article, a polarity score is computed. Then, the
extracted DBpedia categories are used to identify those categories
revealing significant deviations in polarity across different media.
An experiment with articles from six UK and US news sites has
shown that such deviations can be found for different topics,
ranging from political parties to issues such as drug legislation and
gay marriage [27].

The direct predecessor of the LOD extension, FeGeLOD, has also
been used in several data mining applications. In [28,29], we use
the extension to enrich statistical datasets with information from
Linked Open Data, and show how that background knowledge
can serve as a means to create possible interpretations as well as
advanced visualization of statistical datasets. In [28] we used four
feature sets generated from DBpedia to generate explanation for
two statistical datasets, i.e., Mercer quality of living survey,26 and
the corruption perception dataset by Transparency International.27

22 http://www.e-lico.eu/recommender-extension.html.
23 See http://challenges.2014.eswc-conferences.org/index.php/RecSys for details.
24 http://dbtune.org/bbc/peel/.
25 http://skipforward.opendfki.de/wiki/DBTropes.
26 http://across.co.nz/qualityofliving.htm.

results.

http://www.transparency.org/policy_research/surveys_indices/cpi/2010/

In [29], we use DBpedia, Eurostat, and LinkedGeoData to retrieve features to discover which factors correlate with the unemployment rate in French regions. Later, we used the corresponding
links from DBpedia to GADM28 to automatically retrieve the geographical shape data for each region, and visualize the findings on
a map.

Furthermore, FeGeLOD has been used in text classification
approaches.
In [30], we discuss an approach for automatic
classification of events (e.g., into sports, politics, or pop culture)
retrieved from Wikipedia that using features extracted from
DBpedia. As features for the classification, we have used direct
types and categories of the resources involved in an event. The
rationale is that for example, sports events can be identified by
athletes and/or stadiums being involved, while political events can
be identified by politicians being involved. Using only such binary
features, we are able to achieve an accuracy of 80% for a problem
comprising more than ten different categories. Furthermore, since
we did not use any textual features, but only structured content
from DBpedia, we are able to apply a model trained on the English
dataset on event datasets in other languages as well.

Similarly, in [31] we use background knowledge from DBpedia
to train a classifier for identifying tweets that talk about car
accidents. We show that a classifier trained only on textual features
is prone to overfitting to events on the training set. However, using
DBpedia categories and direct types as features for building the text
classifier remedies the overfitting effect and keeps the accuracy up
on the same level when applying a model learned on data from one
city to data from a different one.

Apart from our own research, the extension has been used in
many research applications by third parties as well. Schulz et al. use
the extension for developing an approach for semantic abstraction
for generalization of tweets classification [32,33]. Shidik et al. [34]
make use of the extension for developing a machine learning
approach for predicting forest fires using LOD as background
knowledge. Lausch et al. [35] list the extension as a tool for
performing data analysis in environmental research. Schaible
et al. [36] use the extension for the task of movie classification
using background knowledge from LOD.

5. Evaluation

Many of the algorithms implemented by the RapidMiner LOD
extension have been evaluated in different settings. In this section,
we point out the key evaluations for the most important features
of the extension, and introduce additional evaluations of specific
aspects of the extension.

5.1. Feature generation

The feature generation strategies have been evaluated in a prior
publication. In [37] we used the Auto MPG dataset,29 a dataset
that captures different characteristics of cars (such as cylinders,
transmission, horsepower), and the target is to predict the fuel
consumption in Miles per Gallon (MPG). The original dataset
contains 398 cars, each having a name, seven data attributes,
and the MPG target attribute. The goal is to predict the fuel
consumption from the characteristics of the car, i.e., a regression
model has to be learned. Using our extension, we added different
new attributes from the DBpedia dataset, i.e., direct types and
categories.

28 http://gadm.geovocab.org/.
29 http://archive.ics.uci.edu/ml/datasets/Auto+MPG.

P. Ristoski et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 142151

For the prediction of the MPG attribute, we showed that
when using the M5Rules algorithm [38], the relative error of the
prediction is only half as large as the error on the original, nonenriched data.

The new attributes also provide insights that are not possible
from the original dataset alone. For example, UK cars have a lower
consumption than others (while the origin attribute contained in
the original dataset only differentiates between America, Europe,
and Asia). Front-wheel-drive cars have a lower consumption
than rear-wheel-drive ones (the corresponding category being
positively negatively with MPG at a level of 0.411), mostly due
to the fact that they are lighter. Furthermore, a correlation with
the cars design can be observed (e.g., hatchbacks having a lower
consumption than station wagons). All those car characteristics are
not included in the original dataset, but added from DBpedia.

5.2. Propositionalization strategies

In [15] we performed an evaluation on different propositionalization strategies on three different data-mining tasks, i.e., classifi-
cation, regression, and outlier detection, using three different data
mining algorithms for each task. The evaluation was performed for
the binary, numerical, relative count, and TF-IDF vector representation on five different feature sets. The evaluation showed that
although the selected propositionalization strategy has a major impact on the data mining results, it is difficult to come up with a general recommendation for one strategy, as it depends on the given
data mining task, the dataset at hand, and the data mining algorithm to be used.

5.3. Feature selection

In [17] we have performed an evaluation of the feature selection
approach in hierarchical feature spaces, on six synthetic and five
real world datasets, using three classification algorithms. Using our
hierarchical feature selection approach, we were able to achieve
a feature space compression of up to 95%, without decreasing
the model performance, or in some cases even increasing it. The
evaluation has shown that the approach outperforms standard
feature selection techniques as well as recent hierarchy-aware
approaches.

5.4. Data integration

As discussed in our example in Section 3, our dataset contained
1329 attributes collected from ten different LOD sources. On that
dataset, we first applied the PARIS LOD Matcher to detect the
matching attributes in the dataset, which were later resolved
using the Data Fusion operator.30 After applying the operator,
858 correspondences were discovered. The correspondences were
resolved using the data fusion operator, using majority voting for
strings and median for numeric values as resolution strategy. The
fused dataset contained 1090 attributes.

We have evaluated both the matching and the fusion approach
on four manually selected properties. First,
for each of the
properties, we manually counted the number of LOD sources that
provide a value for at least one instance in the dataset. Then, we
counted how many of those properties were matched and fused.
The results are shown in Table 1, e.g., for the property population
there are six LOD sources providing the values, but only five were
matched and fused.

30 For the PARIS LOD Matcher, we used the following parameters: literal
similarity distance = levenshtein, number of iteration = 10, alignment acceptance
threshold = 0.2, post literal distance threshold = 0.58 and literal normalization = true.

Table 1
Feature consolidation results.

Property

Type

#sources

Population
Area
Currency
Country
code

Numeric
Numeric
String
String

#fused
sources

Coverage

Precision

100.0%
97.47%
96.86%
95.73%

91.07%
97.40%
97.40%
90.25%

From the results, we can observe that the matching operator
based on the PARIS approach is able to detect the matching
properties with high accuracy. It only failed to detect one match
for the population and area properties, which is the property from
Eurostat. The reason for not detecting the matching property is
that PARIS combines instance and schema level evidence. Since
Eurostat provides mainly data for European countries, but the
dataset at hand contains countries from all over the world, there
is not enough evidence that the property from the Eurostat should
be matched to the properties from the other sources.

Furthermore, we evaluate the quality of the values of the fused
features. For each instance, we manually retrieved the reference
values for each property from Wikipedia. This is a common practice
for such a task [39,40], as there is no gold standard that can be used.
The measures we use for our evaluation are precision and coverage.
Coverage is the percentage of rows from the query table for which a
value is provided. Precision is the percentage of correct values. We
treat a numeric value as a correct match if it does not deviate more
than 10% from the reference value. Table 1 shows the resulting
precision and coverage values.

The reason for a low precision for the property country code
is that for some of the instances the values of the property differ
across different sources, and the property has high sparsity in
some of the sources, like DBpedia. Therefore, the majority voting
strategy for such cases falls back to selecting the first non-null
value provided from some of the sources, as there are no more than
one occurrence of a value for the observed instance.

5.5. Time performances

In this section we perform runtime evaluation of the feature
generation approaches, and the runtime for performing data
analysis on the generated feature sets. All experiments were run
using standard laptop computer with 4 GB of RAM and Intel Core
i7-3540M 3.0 GHz CPU.

5.5.1. Scientific and technical journal articles

We measure the time for feature generation when using the
complete LOD cloud, this includes discovering the owl:sameAs links
and generating the data, the data integration, and the time for data
analysis, i.e., correlation analysis. We do the same evaluation when
using only DBpedia as a source. To measure the runtime for feature
generation, we repeated the process three times, and report the
average.

The results are depicted in Fig. 4. It can be observed that the
largest portion of time is consumed by the feature generation
step, which, in turn, is dominated by network access. Here, factors
beyond the control of the user play an import role: reachability
and responsiveness of the LOD endpoints used, network latencies,
and intentional delays between requests to respect with access
policies, which may impose maximum request rates.

All factors not influenced by factors originating in online data
access, such as local performance of data analysis, can be addressed
by design in RapidMiner, since there, e.g., are cloud computing

Fig. 4. Runtime performances for feature generation, data integration and data
analysis.

services31 as well as the Radoop extension32 for data analysis with
Hadoop, which allow for scaling the analytics operations.

Fig. 5. Features increase rate per strategy (log scale).

5.5.2. Metacritic movies

Fig. 6. Feature generation runtime per strategy (log scale).

Fig. 7. Naive Bayes learning runtime (log scale).

To further analyze the critical part, i.e., the feature generation,
we have conducted a controlled scalability experiment, where we
examine how the number of instances affects the number of generated features by using types, Wikipedia categories, relations, and
qualified relations as features (cf. Section 2.3). We measure the
time for generating the feature sets, and the time for learning three
different classification methods, i.e., Naive Bayes, k-Nearest Neighbors (with k = 3), and Support Vector Machines. Since we are only
interested in runtime measurements, not qualitative results of the
data mining process, we do not perform parameter optimization.
The runtime measures reflect the total time in seconds needed for
performing 10-fold cross validation.

For the purpose of the evaluation, we use the Metacritic Movies
dataset.33 The Metacritic Movies dataset is retrieved from Meta-
critic.com,34 which contains an average rating of all time reviews
for a list of movies. Each movie is linked to DBpedia using the
movies title and the movies director. The initial dataset contained
around 10,000 movies, from which we selected 1000 movies from
the top of the list, and 1000 movies from the bottom of the list.
To use the dataset for classification, we discretize the target variable into good and bad, using equal frequency binning. We perform the evaluation on five stratified sub-samples of the complete
dataset with different sizes, i.e., 100, 500, 1000, 1500 and 2000.

In the results, we can see that there are three groups of
generators, which show a different behavior. The smallest number
of features are created by relations, a medium number by types
and categories, and the largest number by qualified relations.
However, as shown in Fig. 5, the number of features do not increase
disproportionally as the number of instances is increased; and
there is even a convergence to a stable state for most generators.
The runtimes for feature generation also follow a linear increase, as
shown in Fig. 6. Figs. 79 show that the runtimes for data analysis
are likewise influenced by the number of features (and hence the
number of instances), however, their contribution to the overall
runtime is low, as discussed above.

6. Related work

The use of Linked Open Data in data mining has been proposed
before, and implementations as RapidMiner extensions as well as
proprietary toolkits exist.

31 https://rapidminer.com/products/cloud/.
32 https://rapidminer.com/products/radoop/.

datasets/MetacriticMovies/.
34 http://www.metacritic.com/browse/movies/score/metascore/all.

http://data.dws.informatik.uni-mannheim.de/rmlod/LOD_ML_Datasets/data/

Fig. 8. k-Nearest Neighbors learning runtime (log scale).

The direct predecessor of the RapidMiner LOD extension is the
FeGeLOD toolkit [41], a data preprocessing toolkit based on the
Weka platform [42], which contains basic versions of some of the
operators offered by the LOD extension.

Different means to mine data in Linked Open Datasets have
been proposed, e.g., an extension for RapidMiner [43], as well
as standalone systems like LiDDM [44]. In those systems, data
can be imported from public SPARQL endpoints using custom

P. Ristoski et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 142151

Furthermore, they often require expert knowledge on Semantic
Web technology, e.g., for formulating custom SPARQL queries,
which is not necessary for our extension. Furthermore, automatic
fusion of data from different sources is only scarcely supported.

The main use case for application discussed in this paper,
i.e., extending a given table with data from the web in a data
analysis environment, has also been discussed for other web data
than LOD. In RapidMiner, the Finance and Economics Extension36
offers live access to stock market tickers and other data sources.
For their Excel and BI products, Microsoft offers the Power Query
extension, which also allows extending tables with tables from the
web, and also offers means for data consolidation [54]. A publicly
available prototype which provides for extending local tables with
pre-crawled data from HTML tables and Microdata annotations
in web pages as well as Linked Data is the Mannheim Search Join
Engine [55].

7. Conclusion and outlook

In this paper, we have introduced the RapidMiner Linked Open
Data extension. It provides a set of operators for augmenting
existing datasets with additional attributes from open data
sources, which often leads to better predictive and descriptive
models. The RapidMiner Linked Open Data extension provides
operators that allow for adding such attributes in an automatic,
unsupervised manner.

There are different directions of research that are currently
pursued in order to improve the extension. Besides developing new
algorithms for the functionality already included (e.g., for linking
and feature selection), there are also some new functionalities
currently being investigated.

The current implementation is rather strict in its sequential
process, i.e., it generates attributes first and filters and reconciles
them later. Depending on the generation strategy used, this can
lead to a large number of features being generated only to be
discarded in the subsequent step. To avoid that overhead and
improve the performance, we are working on mechanisms that
decide on or estimate the utility of attribute already during
creation and are capable of stopping the generation of attributes
earlier if they seem to be useless.

Furthermore, more sophisticated propositionalization strategies might be developed. For example, the target variable from the
local dataset can be used for developing supervised weighting ap-
proaches, as used for text mining in [56]. Furthermore, we can use
the graph properties for calculating feature weights, e.g., the fan-in
and fan-out values of the graph nodes can give a better representation of the popularity of the resources included in the features.
Such a popularity score might be a good indicator of the features
relevance for the data mining task. More sophisticated popularity
scores can be calculated using some of the standard graph ranking
algorithms, e.g., PageRank and HITS.

For feature selection, we have only regarded simple hierarchies
so far. If features are organized in a complex ontology, there
are other relations as well, which may be exploited for feature
selection. Generalizing the approach to arbitrary relations between
features is also a relevant direction of future work.

With respect to data integration, we have only used an off-
the-shelf ontology matching approach to identify identical features
across multiple LOD sources. In the future, more sophisticated
matching approaches can be developed. Those should be able
to handle data types beyond strings and numericals, i.e., dates,
coordinates, and various units of measurement. Furthermore, the

Fig. 9. Support Vector Machines learning runtime (log scale).

queries, but no means to join that data with local data are given.
Cheng et al. [45] proposes an approach for automated feature
generation after the user has specified the type of features. To do
so, similar to the previous approaches, the users have to specify
the SPARQL query, which makes this approach supervised rather
than unsupervised. Mynarz et al. [46] have considered using user
specified SPARQL queries in combination with SPARQL aggregates.
Similarly, the RapidMiner SemWeb Extension [47] is an extension for importing RDF from local files into RapidMiner, using custom SPARQL queries. As discussed above, RDF is a general graph
format, which leads to the problem of set-valued features when
transforming the data into the relational form used in RapidMiner.
To cope with that issue, the extension provides different operators
to transform the set-valued data into a lower-dimensional projec-
tion, which can be processed by standard RapidMiner operators.

Linked Open Data may also be loaded with the RMOnto [48]
extension, which is similar to the SemWeb extension, but comes
with a set of tailored relational learning algorithms and kernel
functions. Together, these form a powerful package of operators,
but it is difficult to combine them with built-in RapidMiner
operators, as well as operators from other extensions.

Kauppinen et al. have developed the SPARQL package for
R35 [49], which allows importing LOD data in the well known
environment for statistical computing R.

Kernel

functions compute the distance between two data
instances, by counting common substructures in the graphs of
the instances, i.e. walks, paths and threes. Graph kernels are
used in kernel-based data mining algorithms, e.g., support vector
machines. In the past, many graph kernels have been proposed that
are tailored towards specific application [50], or towards specific
semantic representation [51]. Only few approaches are general
enough to be applied on any given RDF data, regardless of the data
mining task. Losch et al. [52] introduce two general RDF graph
kernels, based on intersection graphs and intersection trees. Later,
the intersection tree path kernel was simplified by Vries et al. [12].
In another work, Vries et al. [14] introduce an approximation of the
state-of-the-art WeisfeilerLehman graph kernel algorithm aimed
at improving the computation time of the kernel when applied to
RDF. Furthermore, the kernel implementation allows for explicit
calculation of the instances feature vectors, instead of pairwise
similarities. Two variants of such kernel-based feature vectors are
implemented in the Linked Open Data extension.

Tiddi et al. [53] introduced the Dedalo framework that traverses
LOD to find commonalities that form explanations for items of a
cluster. Given a supervised data mining task, such an approach
could be easily adapted and used as a feature generation approach.
All of those approaches miss a functionality to link local data to
remote Linked Open Data, as in the use case discussed in Section 3.

35 http://linkedscience.org/tools/sparql-package-for-r/.

productId=rmx_quantx1.

https://marketplace.rapid-i.com/UpdateServer/faces/product_details.xhtml?

data source provenance information could be used in the fusion
process to give more weight to data with higher quality and data
that is up to date.

More information about the RapidMiner LOD extension, detailed user manual, and example processes can be found on the
extension website.37 The extension is available for free download
from the RapidMiner marketplace38 under the AGPL license.39

Acknowledgment

The work presented in this paper has been partly funded by
the German Research Foundation (DFG) under grant number PA
2373/1-1 (Mine@LOD).
