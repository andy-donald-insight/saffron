Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

A novel approach to generate MCQs from domain ontology:
Considering DL semantics and open-world assumption
Vinu E.V., Sreenivasa Kumar P.

Artificial Intelligence and Databases Lab, Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 5 May 2014
Received in revised form
15 May 2015
Accepted 19 May 2015
Available online 11 June 2015

Keywords:
OWL ontologies
Semantic web
Multiple choice questions
Automatic question generation

Ontologies are structures, used for knowledge representation, which model domain knowledge in the
form of concepts, roles, instances and their relationships. This knowledge can be exploited by an assessment system in the form of multiple choice questions (MCQs). The existing approaches, which use ontologies expressed in the Web Ontology Language (OWL) for MCQ generation, are limited to simple concept
related questions  What is C? or Which of the following is an example of C? (where C is a concept
symbol)  or analogy type questions involving roles. There are no efforts in the literature which make
use of the terminological axioms in the ontology such as existential, universal and cardinality restrictions
on concepts and roles for MCQ generation. Also, there are no systematic methods for generating incorrect
answers (distractors) from ontologies. Distractor generation process has to be given much importance,
since the generated distractors determine the quality and hardness of an MCQ. We propose two new MCQ
generation approaches, which generate MCQs that are very useful and realistic in conducting assessment
tests, and the corresponding distractor generating techniques. Our distractor generation techniques, unlike other methods, consider the open-world assumption, so that the generated MCQs will always be valid
(falsity of distractors is ensured). Furthermore, we present a measure to determine the difficulty level (a
value between 0 and 1) of the generated MCQs. The proposed system is implemented, and experiments
on specific ontologies have shown the effectiveness of the approaches. We also did an empirical study by
generating question items from a real-world ontology and validated our results with the help of domain
experts.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

Automated assessment systems serve as a method to measure
the level of learning as well as to provide a faster solution for large
scale assessments. Many tests like TOEFL, IELTS, GRE and GMAT are
dependent on online assessment systems to make the assessment
task easier. Such systems mainly use multiple choice questions
rather than subjective questions for conducting the test.

Using Multiple Choice Questions (MCQs) for assessments has
both merits and demerits. They are preferred for assessing broad
range of knowledge. This is mainly because they require less administrative overhead and provide instant feedback to test takers.
However, studies by Barbara Gross [1] and Sidick et al. [2] show
that, developing effective objective type questions is time consuming and requires domain expertise to generate good quality MCQs.

 Corresponding author.

E-mail addresses: vinuev@cse.iitm.ac.in (Vinu E.V.), psk@cse.iitm.ac.in

(Sreenivasa K.P.).

URL: http://aidblab.cse.iitm.ac.in/psk/ (Sreenivasa K.P.).

http://dx.doi.org/10.1016/j.websem.2015.05.005
1570-8268/ 2015 Elsevier B.V. All rights reserved.

So, there is a need for an automated method for MCQ generation
from a given knowledge source.

Recently, a handful of studies [39] explored the use of structured domain knowledge in the form of description logic based
ontologies to automatically generate MCQs. This would enable online assessment systems to utilize existing knowledge bases for the
assessment of learners knowledge and skills. But, there are challenges involved in generating MCQs from these ontologies. Some
of the challenges that the existing approaches tried to address are:
(i) How to frame interesting and good quality questions from on-
tologies? (ii) How to generate proper incorrect answers (distrac-
tors) for the framed question? (iii) How to control the difficulty
level of the generated questions? Although the previous efforts
were not in vain, there are substantial shortcomings in fully exploiting the formalized knowledge in an ontology for MCQ gener-
ation. In this paper, we show that, with a better understanding of
the semantics of a given ontology (expressed in Web Ontology Lan-
guage), the three challenges can be addressed more elegantly.
Challenge 1. Framing interesting and good quality questions. In the
literature, the approaches that use ontologies have the limitation

that they generate simple concept related questions  What is
C? or Which of the following is an example of C? (where C is
a concept symbol)  or analogy type questions involving roles.
These questions are very basic [10] and do not contain any domain
related specifics. In other words, the approaches which generate
such questions, do not appropriately make use of the axiomatized
knowledge in an ontology. Furthermore, restrictions (existential,
universal and cardinality) on concepts and roles in ontologies are
not utilized properly for question generation in any of the current
approaches.

Consider a movie ontology with statements,

Movie(braveHeart)
MovieDA  Movie  isDirectedBy.Actor
MovieDA(braveHeart)
With respect to this, we can frame a question about the instance
braveHeart: Choose a movie directed by an actor? Our approach
in this paper is an effort in this direction.
Challenge 2. Proper distractor generation. Under the closed-world
assumption (CWA), we can choose any instance which is different
from the instance braveHeart as a distractor for the question in
the example above. But, Web Ontology Language (OWL) adheres
to the open-world assumption (OWA): statements which are not
logical consequences of a given knowledge base are not necessarily
considered false. Therefore, not all distractors which are generated
under CWA can be guaranteed as true distractors.

We observed that most of the existing MCQ generation techniques [7] randomly select instances which do not belong to the
class of the correct answer as distractors. The incorrectness of the
distractors cannot be ensured by this random selection method,
which in turn made it necessary to manually check the correctness of the question items before making use of them. We address
this issue by proposing a systematic method to generate only those
question items which are valid under OWA.
Challenge 3. Control the difficulty level of the generated MCQ. MCQs
of varying difficulty level are necessary to assess the depth of
knowledge of a learner (student). We introduce a measure to
find out the difficulty level of the generated MCQs based on the
similarity-based theory suggested by Alsubait et al. [11].

In this paper, we propose two approaches (i) node-label-
set based approach (ii) edge-label-set based approach to generate (two) interesting types of MCQs. We adopt description logic
specifications of the ontology to generate the so called label-sets
(node-label-sets and edge-label-sets). A measure to estimate the
difficulty level of generated MCQs is also proposed by means of
these label-sets. We study the feasibility of our approaches by
implementing them and generating MCQs from some sample on-
tologies. In Appendix A, we list some of the MCQs, which are generated from Geographical Entity ontology. To validate our new
approaches and difficulty measure, we generated question items
from a real-world ontology and got them evaluated by domain ex-
perts. Statistics of our empirical evaluation validate our arguments
and are detailed in Section 6. The new notations and abbreviations
that we introduced in this paper are listed in Appendix B along with
their meaning.

2. Related work

Papasalouros et al. [5] suggested 11 strategies based on classes,
properties and terminologies of ontologies for framing MCQs
and the corresponding distracting answers. Their MCQ generation
methods lack proper theoretical support for when to use which
strategy, and the stem of all the generated questions remains the
same (Choose the correct sentence).

Cubric and Tosic [4] and M. Cubric [6] generated MCQs of
knowledge level (Which of the following definition describes the

concept C?), comprehension level (Which one of the following
response pairs relates in the same way as a and b in the relation
R?), application level (Which one of the following examples
demonstrates the concept C?) and analysis level (Analyze the text
x and decide which one of the following words is a correct replacement
for the blank space in x.). Their work is an extension of the approach
by Holohan et al. [12], by introducing stems that use annotation
information in the ontology. Strategies similar to Papasalouross
strategies are adopted to find the distractors for the generated
question statements.

Another MCQ generation method is by Alsubait et al. [3]. They
presented an approach called similarity-based approach for generating analogy type questions. In their question generation algo-
rithm, a set of parameters are introduced to control the difficulty
level of the generated questions. They argue that the difficulty level
of a question item can be increased by finding the distractors which
are similar to the correct answer(s). The approach which the authors illustrate is limited to analogy type questions.

Other than the above MCQ generation approaches, there are
works like Abacha et al. [13], Ben Abacha and Zweigenbaum [14]
and Aitko et al. [9], which make use of simple ontology statements:
concept inclusions, role hierarchy and (concept and role) asser-
tions, to generate basic domain related questions.

In addition to the above MCQ generation approaches, a few
researchers worked on rule-based methods for question answer
generation. The work by Zoumpatianos et al. [8] uses Semantic
Web Rule Language (SWRL), a combination of OWL with RuleML,1
to generate MCQs.

3. Preliminaries

In this section, we describe: MCQ, the Description Logic (DL)
SHIQ based ontologies (SHIQ ontologies) and an example
ontology (Harry-Potter-Book ontology).

3.1. Multiple choice questions

A multiple choice question (MCQ) is a type of question in
which students are asked to choose correct answers from a set of
alternatives, in response to a question-statement. MCQ tests are
mainly used to evaluate whether (or not) a student has attained a
certain learning objective.
Definition 1. MCQ is a 3-tuple S, K , D, where, S is a statement
that introduces the problem, K is a non-empty set of correct
solutions to S, and D is a non-empty set of incorrect solutions to S.
Here, S, K and D are called Stem, Keys and Distractors, respectively.
In this work, we only consider MCQs with 1 key and 3 distractors

(total 4 choices), which is a common format used in MCQ tests.

3.2. SHIQ DL and SHIQ ontologies

The Description logic SHIQ is based on an extension of
the well-known logic ALC [15], with added support for role
hierarchies, inverse roles, transitive roles, and qualifying number
restrictions [16].

We assume NC and NR as countably infinite disjoint sets of
atomic concepts and atomic roles respectively. A SHIQ role is either
R  NR or an inverse role R with R  NR. To avoid considering
roles such as (R), we define a function Inv(.) which returns the
inverse of a role: Inv(R) = R and Inv(R) = R.

1 http://wiki.ruleml.org/index.php/RuleML_Home (last accessed 11th Dec. 2014).

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Table 1
The syntax and semantics of SHIQ concept types.
Semantics
AI (given)

I \ CI
CI  DI
CI  DI
{ x  I | y.x, y  RI  y  CI }
{ x  I | y.x, y  RI  y  CI }
{ x  I | #{y | x, y  RI}  n}
{ x  I | #{y | x, y  RI}  m}

Name
Atomic concept
Top concept
Bottom concept
Negation
Conjunction
Disjunction
Existential restriction
Universal restriction
Min cardinality
Max cardinality

Syntax

C  D
C  D
R.C
R.C
nS.C
mS.C

TBox

Table 2
The syntax and semantics of SHIQ ontology axioms.
Syntax
R  S
Tran(R)
C  D
C  D
C(a)
R(a, b)
a  b

Name
Role hierarchy
Role transitivity
Concept inclusion
Concept equality
Concept assertion
Role assertion
Inequality assertion

ABox

Semantics
RI  SI
RI  RI  RI
CI  DI
CI = DI
aI  CI
aI , bI  RI
aI = bI

1 O R

The set of concepts in SHIQ is recursively defined using the
constructors in Table 1, where A  NC , C, D are concepts, R, S are
roles, and n, m are positive integers. A SHIQ ontology  denoted
as a pair O = (T , A), where T denotes terminological axioms (also
known as TBox) and A represents assertional axioms (also known
as ABox)  is a set of axioms of the type specified in Table 2. A role
R in O is transitive if Tran(R)  O or Tran(R)  O. Given an O,
let R1 O R2 be the smallest transitive reflexive relation between
roles such that R1  R2  O implies R1 O R2 and R
2 . For
a SHIQ ontology O, the role S in every concept of the form nS.C
andmS.C in O, should be simple, that is, RO S holds for no transitive role R [17].
The semantics of SHIQ is defined using interpretations. An
interpretation is a pair I = (I , .I ) where I is a non-empty set
called the domain of the interpretation and .I is the interpretation
function. The function .I assigns a set AI  I to every A 
NC, and assigns a relation rI  I  I to every r  NR.
The interpretation of the inverse role r is (r)I := {x, y |
y, x  rI}. The interpretation is extended to concepts and axioms
according to the rightmost column of Tables 1 and 2 respectively,
where #V denotes the cardinality of a set V .
We write I |= , if the interpretation I satisfies the axiom 
(or  is true in the interpretation). I is a model of an ontology O
(written I |= O) if I satisfies every axiom in O. If we say  is
entailed by O, or  is a logical consequence of O (written O |= ),
then every model of O satisfies . A concept C is subsumed by D
w.r.t. O if O |= C  D, and C is unsatisfiable w.r.t. O if O |= C  .
Classification is the task of computing all subsumptions A  B
between atomic concepts such that A, B  NC and O |= A 
B; similarly, property classification of O is the computation of all
subsumptions between properties R  S such that R, S  NR and
O |= R  S. By inferred ontology we mean the ontology O obtained
after classification and property classification, with all the newly
entailed role and concept assertions.

3.3. Harry-Potter-Book ontology

We use a synthetic ontology called Harry-Potter-Book ontology
(O) as a running example in this paper. This ontology, which
models characters in Harry Potter story,2 was developed by

2 http://en.wikipedia.org/wiki/Harry_Potter (last accessed 11th May 2015).

knowledge engineers from our research team. Tables 3 and 4
show the required portions of the TBox and ABox of the ontology
respectively. Some of the explicit knowledge formalized in the
ontology is explained below. Harry Potter, Hermione Granger,
Ron Weasley, Draco Malfoy and Neville Longbottom are students of
Hogwarts School. All Hogwarts students should have exactly one
Creature as Pet (axiom (1) of Table 3). Owl, Toad, Cat and Rat are
the common pets (they are disjoint concepts) (axioms (2025)).

We have concepts like Muggles, Human, Wizard, Pureblood and
Halfblood in our ontology. Muggles are not Wizards (axiom (13)).
All Muggles are Humans (axiom (9)). Pureblood, Halfblood and
Muggle are disjoint concepts (axioms (1618)). Hogwarts students
belong to either Gryffindor house or Slytherin house (axiom (11)).
Harry Potter owns an owl named Hedwig as his pet. Ron Weasleys
pet Scrabbers is a rat. A toad named Trevor is Nevilles pet.
Hermiones pet is Crookshanks, a cat.

3.4. Label-sets

To generate MCQs based on each instance and each pair of
instances, we associate a set, containing the constraints they
satisfy, along with each of them. We call these sets as label-sets in
general. A Label-set of an individual instance is called a node-label-
set and a label-set of a pair of instances is called an edge-label-set.
Node-label-set. The label-set of an instance is the set which contains
the class expressions and (existential, universal and cardinality)
restrictions satisfied by that instance.

Definition 2. The node-label-set of an instance x (represented as
LO(x)) is defined as follows (where C is a concept name and R is a
role name in ontology O, and m and n are positive integers).

LO(x) = C|O |= C(x)  R.C|O |= R.C(x) 
R.C|O |= R.C(x)  nR.C|O |=nR.C(x) 
mR.C|O |= mR.C(x)

Edge-label-set. The label-set of a pair of instances (x, y) is the set
that contains all the property relationships (role names) from the
first instance to the second instance. It is represented as LO(x, y).
Definition 3. LO(x, y) is formally defined as (where R is a role in
ontology O): LO(x, y) = {R|O |= R(x, y)}

The practical method that we adopted for generating these

label-sets is explained in the next section.

3.5. Label-set generation techniques

Node-label-set generation. We generate the label-set of an instance
x from an ontology O by first creating the corresponding inferred
ontology O (using a reasoner). From O we find all the concept
names and (existential, universal and cardinality) restrictions
satisfied by the instance as follows:

Step 1: All the concept names which are satisfied by x are
obtained by a simple SPARQL query. For example, the set of concept
names, which we obtained from O, corresponding to the instance
harryPotter is {HogStudent, Student, Human, Wizard,
Halfblood, Gryffindor}.

Step 2: In order to get the restrictions satisfied by x, we access
the class definition axioms of the concepts which are obtained
in the first step, and then consider the existential, universal
and cardinality restrictions on the right hand side of the class
definitions to form the label-set.

The right hand side of the axioms in its conjunctive normal form
(CNF) is used for enriching the label-set. Those clauses in the CNF
which do not contain any disjunction are directly included in the
label-set. If a clause is a disjunction of expressions (denoted as
D-Clause), then they are handled in parts, as follows:

Table 3
The TBox axioms of Harry-Potter-Book ontology.
(1) HogStudent  Student  hasPet.Creature  hasPet.Pet   1hasPet.Creature
(2) Pet  Creature  isPetOf.HogStudent  isPetOf.HogStudent (3) HogStudent  Student
(4) Student  Human
(7) Toad  Pet
(11) HogStudent  Gryffindor  Slytherin
(13) Muggle  Wizard   (14) Pet  Student  
(16) Halfblood Muggle (17) Pureblood Muggle (18) Pureblood  Halfblood 
(19) Gryffindor Slytherin  (20) Owl Toad 
(22) Owl  Cat  
(25) Rat  Cat  

(6) Rat  Pet
(9) Muggle  Human(10) Cat Pet
(12) DrumstrangStud  HogStudent  
(15) Pet  Human  
(21) Owl  Rat  
(24) Toad  Cat  
(27) hasHelped  knows

(23) Toad  Rat  
(26) hasFriend  knows

(5) Owl  Pet
(8) Pet  Creature

Table 4
The ABox axioms of Harry-Potter-Book ontology.

DrumstrangStud(viktorKrum)

HogStudent(harryPotter)
HogStudent(hermioneGranger)
HogStudent(nevilleLbottom)
HogStudent(dracoMalfoy)
HogStudent(ronWeasley)

Gryffindor(ronWeasley)
Gryffindor(harryPotter)
Gryffindor(hermioneGranger)
Gryffindor(nevilleLbottom)

Muggle(hermioneGranger)
Pureblood(dracoMalfoy)
Pureblood(ronWeasley)
Halfblood(harryPotter)
Weasley(ronWeasley)
hasPet.Cat(hermioneGranger)
hasPet.Rat(ronWeasley)
hasPet.Owl(harryPotter)
hasPet.Toad(nevilleLbottom)
hasPet.Toad(dracoMalfoy)
Toad(trevor)
Owl(hedwig)
Owl(errol)

Cat(crookshanks)
Rat(scrabbers)

Slytherin(dracoMalfoy)
Slytherin(tomRiddle)

hasPet(harryPotter, hedwig)
hasPet(nevilleLbottom,trevor)
hasPet(ronWeasley,scrabbers)
hasPet(hermioneGranger,crookshanks)

Wizard(viktorKrum)
Wizard(harryPotter)
Wizard(dracoMalfoy)
Wizard(tomRiddle)
Wizard(nevilleLbottom)

knows(nevilleLbottom,harryPotter)
knows(harryPotter,tomRiddle)
knows(dracoMalfoy,harryPotter)

hasFriend(harryPotter,hermioneGranger)
hasFriend(harryPotter,ronWeasley)
hasHelped(harryPotter,hermioneGranger)
errol  hedwig

LO(x)  LO(x)  {R.C }

LO(x)  LO(x)  {R.C }

1 for each expression exp in D-Clause
2 if exp is of the form R.C
if O |= R(x, y)  C(y)

5 elseif exp is of the form R.C
if O |= R(x, y)  C(y)

8 elseif exp is of the form nR.C

12 elseif exp is of the form mR.C

Set of Individuals I :={ y|O |= R(x, y)  C(y) }
if (# Distinct Individuals in I)  n
LO(x)  LO(x)  {nR.C }

Set of Individuals J :={ y|O |= R(x, y)  C(y) }
if (# Distinct Individuals in J)  m
LO(x)  LO(x)  {mR.C }

Continuing with our example of harryPotter, enrichment
of its label-set is done by obtaining existential, universal and
cardinality restrictions on each of the concept names in the
label-set (i.e., the set { hasPet.Pet, hasPet.Creature, 
1hasPet.Creature } .)
The stand alone restrictions of an instance  the restrictions
which are not a part of any class definitions, but defined explicitly
for an instance  is also included in the label-set, by using the
Jena Ontology API3 or the OWL API.4 In our example ontology, for
harryPotter, hasPet.Owl is a stand alone restriction. More
examples of node-label-set are given in Table 5.
We maintain the disjoint concepts of each of the concept names

in a label-set LO(x) as a set called DisjointLO(x). For exam-
ple, DisjointLO(harryPotter) is {Pet, Owl, Rat, Toad,
Definition 4. DisjointLO(x) = D| O |= (D  C  ), where
C, D are concept names in O, and C  LO(x).

Cat, Muggle, Pureblood, Slytherin}. We later use the
Disjoint-sets of the label-sets for generating distractors.

Edge-label-set generation. The edge-label-set of a pair of instances
(x, y) can be easily generated from O using a simple SPARQL query.
example, LO(harryPotter, hermioneGranger) =
{ hasFriend, hasHelped, knows }. More examples are given
in the forthcoming sections.

For

4. Proposed MCQ generation approaches

Once we get the label-sets of all the instances (node-label-sets)
and all the pairs of instances (edge-label-sets) in a given ontology,
we can generate MCQs based on the following two approaches.

4.1. Node-label-set based approach

In this approach, we frame a question based on the node-label-
set of an instance (with that instance as the key). All the logical
expressions in a label-set need not necessarily be useful in framing a stem; they can be further reduced. We can remove redundant entities (if any), and combine two or more expressions to form
a human-understandable condition for generating a meaningful
stem. We achieve this by the following Label-set-Reduction pro-
cess.

We also discuss our distractor generating method, and a
measure to compute the difficulty level of the generated MCQ in
detail below.

4.1.1. Label-set-Reduction
Removing named super concepts. Consider the node-label-sets in
Table 5, we can see that they have all the concept names which

3 http://jena.apache.org (last accessed 11th Dec. 2014).
4 http://owlapi.sourceforge.net (last accessed 11th Dec. 2014).

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Table 5
Node-Label-set of instances in Harry-Potter-Book ontology.

LO (harryPotter)

LO(ronWeasley)

LO(hermioneGranger)

L(hedwig)
L(trevor)
L(nevilleLbottom)

L(tomRiddle)

= { HogStudent, Student, Human, Wizard, HalfBlood, Gryffindor,
 hasPet.Pet, hasPet.Owl, hasPet.Creature, 1hasPet.Creature }
= { HogStudent, Student, Human, Weasley, Gryffindor, Pureblood,
 hasPet.Pet, hasPet.Rat, hasPet.Creature,1hasPet.Creature }
= { HogStudent, Student, Human, Muggle, Gryffindor,  hasPet.Pet,  hasPet.Cat,  hasPet.Creature,
1hasPet.Creature }
= { Owl, Pet, Creature, isPetOf.HogStudent, isPetOf.HogStudent }
= { Toad, Pet, Creature, isPetOf.HogStudent, isPetOf.HogStudent }
= { HogStudent, Student, Wizard, Gryffindor,  hasPet.Pet,  hasPet.Toad,  hasPet.Creature,
1hasPet.Creature }
= { Wizard, Slytherin }

Table 6
Reduction rules.

Phase no.

Rule no.

Restriction-1
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
Ru.Cu
nuRu.Cu
nRu.Cu or Ru.Cu
IRu.Cu or Ru.Cu
IRu.Cu or Ru.Cu
!Ru.Cu
!Ru.Cu

Restriction-2
Rv .Cv
Rv .Cv
Rv .Cv
Rv .Cv
Rv .Cv
Rv .Cv
Rv .Cv
Rv .Cv
nRv .Cv
nvRv .Cv
 nRv .Cv
!Rv .Cv
!Rv .Cv
!Rv .Cv
!Rv .Cv

Condition
Cu  Cv  Ru  Rv
Cu  Cv  Rv  Ru
Cv  Cu  Rv  Ru
Cv  Cu  Ru  Rv
Ru  Rv  Cu  Cv
Cv  Cu  Ru  Rv
Cu  Cv  Rv  Ru
Cu  Cv  Ru  Rv
Cv  Cu  Rv  Ru  n  1
Cu  Cv  Ru  Rv  nu  nv
Cu  Cv  Ru  Rv  n = 1
Cu  Cv  Ru  Rv
Cv  Cu  Ru  Rv
Cu  Cv  Ru  Rv
Cv  Cu  Ru  Rv

Result
Ru.Cu
Ru.Cu
Rv .Cv
Ru.Cv ,Rv .Cv
IRu.Cu
IRu.Cu, Rv .Cv
IRu.Cu, IRv .Cu
Ru.Cu, Rv .Cv
nRv .Cv
nuRu.Cu
!Ru.Cu
!Ru.Cu
!Ru.Cv
!Ru.Cu, !Rv .Cu
!Ru.Cv, !Rv .Cv

contain the corresponding instances. Some of the concepts within
these label-sets are hierarchically related (in classsuper-class
relationship) in the ontology, resulting in redundant labels. For
example, consider the label-set LO(harryPotter), it contains the
concepts HogStudent and Student. Since it can be inferred from
the concept HogStudent that harryPotter is also a Student,
we can say that Student is a redundant information (label) in
the label-set. We remove such redundant labels by finding out the
relationship between concepts in the label-set and excluding all
the concept names which subsume at least one other concept in
the label-set. Therefore, from the label-set of harryPotter, the
concepts Human and Student can be removed because of the TBox
axioms (4) and (3) (in Table 3) respectively.
Definition 5. Let S  LO(x) be a maximum subset of named
concepts in LO(x) such that u  S, v  LO(x) and v  u. Then,
label-set after reduction is LO(x)\S.

The presence of redundant concept names in a node-label-set is
mainly because, we do a classification on the ontology prior to the
label-set generation.

The next step in the reduction process of the label-set is the
reduction of terms with quantified and cardinality restrictions. We
propose a set of reduction rules (in the next paragraph) to achieve
this task. We call the new node-label-sets after the two Label-set-
Reduction steps as the Reduced-node-label-sets (Rnls, for short).
In terms of node-label-set of an instance x in ontology O, Rnls is

represented as RLO(x).

Reduction rules. To extend the Label-set-Reduction to incorporate
quantified and qualified cardinality expressions, we have formulated a set of reduction rules (Table 6). These rules are applied by
taking two restrictions (addressed as a pair) at a time from the
label-set. We consider all the possible (size two) combinations, for

testing the applicability of the rules. Those pairs, which do not follow the antecedent part of the rules  also referred as pairs which
are of the form {Restriction-1, Restriction-2}  that are given in Table 6, cannot be reduced, and hence they are used as they are in the
Rnls.
We have introduced two notations I and ! in our reduction

rules in Table 6.
Definition 6. For a concept C and a role R, the semantics of

IR.C is defined using the interpretation I = (I , .I ) asx 
I |y.x, y  RI  y  CI  (z.x, z  RI  z  CI )

(see Section 3.2 for details).
We have defined the rules 57 (in Table 6), based on the
definition of IR.C. Consider the pair {isPetOf.HogStudent,
isPetOf.HogStudent}, we can combine (reduce) the two
restrictions using rule-5, to get IisPetOf.HogStudent. If the
restrictions like R.C and R.C appear together in a label-set,
the former guarantees the presence of an edge R and makes it
necessary that the latter should be satisfied by the condition other
than vacuously true case. This observation is useful while framing
the stem from a label-set.

Definition 7. The semantics of !R.C is defined asx  I |y.
x, y  RI  y  CI y1.x, y1  RI  y2.x, y2  RI 

y1 = y2
Based on Definition 7, we have defined rules 1115. For
example, the restrictions: hasPet.Owl and 1hasPet.Owl,
can be combined to form !hasPet.Owl, using rule-11.
The procedure Generate-Reduced-Node-Label-Set illustrates
the reduction process. Line 2 corresponds to the first part of
the reduction process, where the procedure calls Named-Super-
Concepts to find all the concept names that subsume at least one

concept in the label-set. Lines 310 illustrate how the reduction
rules are being applied on a label-set. These reduction rules are
applied in five phases. Each phase (denoted as p  P) reduces the
set of carefully chosen restriction patterns, the resulting patterns
of which are used for further reduction in the next phase.

Generate-Reduced-Node-Label-Set(L)

// Input: L = LO(x).

// Output: R = RLO(x).

1 P = Phases, list of the set of pre-defined rules
2 R = L\Named-Super-Concepts(L)
3 for each phase p  P

11 return R

R2 = 
for each (u, v)  R  R and u = v
N = Apply-Rule(r,{u, v})
R = R  N
R2 = R2  {u, v}

for each rule r  p and r applies to (u, v)

R = R\R2

Named-Super-Concepts(L)
12 S =  // Named concepts with named sub concepts
13 for each named concept a  L

16 return S

for each named concept b  L where b  a

S = S  {a}

Phase-1: Considers restrictions of the formR.C alone, and rule-
1 is applied to all the possible pairs in the Rnls. Phase-2: This phase
handles restriction of the form R.C, and rules 24 are applied
in a similar way to the resulting set of Phase-1. Phase-3: Applies
rules 511 to the applicable pairs in the result set of the previous
phase. Phase-4: Uses rules 1213. Phase-5: Applies rules 1415 to
all possible pairs in the result set of Phase-4. If a restriction pair
is being reduced (by calling Apply-Rule) by a reduction rule r of
a particular phase p, then those restrictions are removed from the
label-set while applying the rules in the successive phases.

hasPet.Owl)

(hasPet.Creature,

there are n

 ways for choosing a pair. In our example, we

This phase-wise reduction reduces the restrictions which are
of the same type first, and then considers the other possible
combinations. To illustrate our rationale for phase-wise reduc-
tion, consider an instance with label-set {hasPet.Creature,
hasPet.Creature, hasPet.Owl}. If we are not following
for n number of terms,
any particular order for reduction,
can take any of the 3 pairs (hasPet.Creature, hasPet.
and
Creature),
(hasPet.Creature, hasPet.Owl) for starting the reduction.
Fig. 1 shows the reduction process corresponding to each of the 3
pairs. In the figure, (1) and (3) require additional rules other than
the basic reduction rules given in Table 6, to proceed to a single
reduced term, and (2) follows our phase based reduction and requires no additional rules.

rules on the restrictions in RLO(harryPotter) is shown in
restrictions in RLO(harryPotter). So, the diagram shows the

Fig. 2. Phase-1 and Phase-2 rules are not applicable on any of the

A diagrammatic representation of application of the reduction

reduction phases 3 to 5. In the figure, the pairs of restrictions
which satisfy the conditions in Table 6 are reduced based on the
corresponding rules. The directed lines in Phase-3 (in the figure)
are paired such that each pair represents the applicability of a rule.
In Phase-4, the restrictions are connected pairwise and the arrow
head points to the resultant restriction. In Phase-5, rules 1415 are
repeatedly applied to find the final result set.

Fig. 1. The reduction rules are applied to the five restrictions by taking them in
three different orders: (1), (2) and (3). (1) and (3) require additional rules to achieve
a single reduced term, and (2) follows our phase based reduction order and requires
no additional rules.

4.1.2. Stem-set generation heuristics

Stem-sets are the sets which are used for framing the question
statements of MCQs. The Rnls of an instance is normally used as
its stem-set for framing a question. Example 1 shows the stem
generated from Rnls of harryPotter. But there are cases where
we cannot use a Rnls as it is for a stem generation.
Case 1. A Rnls of an instance can contain restrictions of the form
R.C (universal quantifier). These universal quantifiers are re-
strictions, which the instance may satisfy under vacuously true
condition (otherwise, they would have been reduced by the reduction rules in Table 6). We observed that using such restrictions in framing the stem affects the clarity of the question. For
example, consider the Reduced-node-label-set of two instances
namely john and bob. Let their Reduced-node-label-sets be
R(LO(john)) = {Engineer,hasChild.Doctor} and R(LO
(bob)) = {Engineer,IhasChild.Doctor} respectively. The
question framed from the latter Rnls is Choose an Engineer, having only Doctor as child. This stem does not mislead the person
undertaking the test, since the answer to the question can be
unambiguously chosen as bob. This unambiguous selection is
because of the fact that the linguistic interpretation and the logical statement (the Rnls) of the stem remain the same. But the
question generated from the former Rnls misleads the test taker,
since it asks to choose an engineer whose children are all doc-
tors. But, there can be a case that john satisfied the restriction
 hasChild.Doctor only by vacuously true manner. In this case,
the linguistic interpretation of the logical statement under vacuously true case may confuse the test taker. Therefore, we made a
design decision to remove such universal restrictions that may be
satisfied by vacuously true condition from the Rnls to generate the
corresponding stem-set. So, the stem-set of the instance john becomes {Engineer}, and the stem-set of the instance bob remains
the same as that of its Rnls.
Case 2. The restrictions which contain bottom or top concept can
be removed from the Rnls while generating the stem-sets. That is,
the terms like hasChild., hasPet. can be removed from
the Rnls. The terms with bottom or top concept add vagueness

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Fig. 2. Application of reduction rules on the restrictions in RLO(harryPotter).

to the generated stems of the MCQs. We made a design decision
to remove such restriction because,  and  in a restriction are
usually used to define obvious knowledge of a class. For example,
Human  hasParent., Owl  hasPet.; these axioms
when used for framing stem will look like: Choose a human having
some one as parent and Choose the owl having no pet respectively.
In most of the similar cases, it is very obvious (for a human) to
understand the restriction from the concept name itself. In other
words, the constraints of such type are not specific enough to
become a part of the stem.

Example 1. Choose a Hogwarts
Student, a Wizard, a
Gryffindor and a Halfblood, having exactly one Owl
as Pet.

Options

a. Harry Potter
b. Hermione Granger
c. Tom Riddle
d. Hedwig

Human-readable stem generation. We adopted a simple template
based method along with a set of natural language processing
(NLP) tools to convert a stem-set to a human-readable stem.
We use a template similar to the following regular expression
(abbreviated as regex) for framing the stems:

Choose(a|the) ClassRestriction(,|and)?+
PropertyRestriction(,|and)?

In the above regex, ClassRestriction corresponds to the concept
expressions in the label-set. If the labels (we mean rdfs:label
property) of the class names are not available in the ontology,
the local names of the URIs are utilized as the ClassRestriction.
The property related class restrictions in a label-set are used
for assigning values to PropertyRestriction. The property related
constraints in a label-set are treated in parts. We first tokenize
the property names in the constraints. Tokenizing includes wordsegmentation5 and processing of camel-case, underscores, spaces,
punctuations, etc. Then, the verbs in the segmented phrases
(denoted as R-verb) are identified (using the Natural Language
Tool Kit6) and, are changed to suitable pre-defined morphological
word forms. For example, the property has_Pet is segmented to
the list [has, pet ], and they are then converted to [ having, pet ].
This new list of words are then incorporated in a constraint-specific
body, to form the corresponding PropertyRestriction. Consider

the restriction of the form IR.C, the constraint-specific body
associated with this restriction is <R-verb> only
some
<C> as <R-noun>; therefore, Ihas_Pet.Owl (here R-verb
= having and R-noun = pet) becomes having only some owl as
pet. Considering the limited scope of this article, we refrain from
discussing further on how to convert the other property related
restriction types of a stem-set. Interested readers can visit our
website7 for more details.

4.1.3. Distractor generation procedure

In the previous section, we have seen that the MCQ stems
are generated using the stem-set of each of the instances in an
ontology. The instance corresponding to the stem-set will become
the correct answer. Distractors are the set of incorrect options of an
MCQ. Given an instance x (the correct answer of an MCQ), we use its
Rnls to identify the set of all instances which can be considered as
distractors. This set of all possible distractors is denoted as Dpotential.
We use the equation given below to determine Dpotential (where d is
an instance different from x).

RLO(x)  DisjointLO(d) = .

(1)
To recall from Definition 4, the Disjoint-set of a label-set is the
set which is obtained by taking the union of the set of disjoint
concepts of each of the concept names in the node-label-set. Eq.
(1) guarantees that there will be at least one concept in the stem
forming set (stem-set) which is not satisfied by the distractor
instance. This equation also prevents the possibility of a distractor
becoming a right answer under OWA. We call the instance d that
satisfies Eq. (1) as a candidate for the distractor of the generated
MCQ. The procedure Generate-Distractors-Method generates
possible distractors by taking an instance as input.

In Example 1, to find the distractors of the correct answer
harryPotter, we considered instances like hermioneGranger
and hedwig which satisfy Eq. (1). In the illustration below, we
show that hermioneGranger and hedwig are not equivalent to
harryPotter, and they belong to at least one concept which is
disjoint with a concept in the stem-set.
Cat, Wizard, Halfblood, Pureblood, Slytherin}

DisjointLO(hermioneGranger) = {Pet, Owl, Rat, Toad,
DisjointLO(hedwig) = {Rat, Toad, Cat, Muggle,
RLO(harryPotter)  DisjointLO(hermioneGranger)
RLO(harryPotter)  DisjointLO(hedwig)

Gryffindor, Slytherin, Student, HogStudent,
Human}
= {Halfblood, Wizard}
= {HogStudent, Gryffindor}

5 Word-segmentation is done by using Python WordSegment
(https://pypi.
python.org/pypi/wordsegment  last accessed 11th May 2015), an Apache2 licensed
module for English word segmentation.
6 Python NLTK: http://www.nltk.org/ (last accessed 11th May 2015).

7 https://sites.google.com/site/ontoworks/projects (last accessed 15th May
2015).

Generate-Distractors-Method(x)

// Input: x, an instance
// Output: Dpotential, set of possible distractors

1 Dpotential = 
2 for each instance d in O

3 if RLO(x)  DisjointLO(d) = 

Dpotential = Dpotential  {d}

5 return Dpotential

From the definition of MCQ (Definition 1), we need at least
three distractors in the Dpotential set of an instance, to consider
its Rnls for MCQ generation. Therefore, it is necessary to choose
only those Rnls that satisfy this condition, for generating stem-sets.

Thus, with a stem-set S generated from RLO(x), satisfying the

above condition, we use S to generate an MCQ with x as key and
Dpotential set as its distractors.

4.1.4. Difficulty level of MCQs

A notion to measure the difficulty level of ontology based
MCQs was introduced first by Cubric and Tosic [4]. Later, Alsubait
et al. [11] extended this idea and proposed a similarity-based
theory for controlling the difficulty of ontology generated MCQs.
In Alsubait et al. [3], the theory has been applied on analogy type
MCQs. In Alsubait et al. [18], the authors have experimentally
verified their approach in a student-course setup. The practical
solution which they suggested to find out the difficulty level of
an MCQ is w.r.t. the similarity of the distractors to the key. If the
distractors are very similar to the key, students may find it very
difficult to answer the question, and hence it can be concluded that
the question is difficult.

We adopt the above similarity-based theory to calculate the
difficulty level of the generated questions. But the conceptual
similarity measure [19] they have used does not satisfy our
requirements. This is mainly because, we are defining MCQs and
the relationship between instances (key and distractors) in terms
of Rnls. In addition, the new notations which we introduced (I and
!) motivated us to define a new similarity measure. The scope of
the other similarity measures in the literature [2022] is ignored
for the same reasons.
Label-set Similarity Ratio. We introduce a similarity measure called
Label-set Similarity Ratio (LSR) to find out how closely an instance
is related to other instances w.r.t. their Rnls.
Definition 8. Label-set Similarity Ratio (LSR). Given two Reduced-
node-label-sets U, V ; the LSR of U and V (denoted by LSR(U, V ))
is the ratio of number of elements in U which can be associated to
some elements in V , to the total number of elements in U, i.e.,

LSR(U, V ) = #{ u | u  U  vV , u  v }
Association between an element u  U to v  V (denoted by

#U

u  v) is based on the conditions mentioned below.

Since we are interested in the Reduced-node-label-sets (U and
V ) of a SHIQ ontology, the elements of U (or V ) can be a concept
name or a term of the form R.C or IR.C or nR.C or nR.C or
!R.C.
We associate u  U to v  V based on the following conditions:
1. A concept u U is associated to a concept v  V , if u v (concept
subsumption).
2. If u  U is of the form Ru.Cu, we associate it to a v ( V ) of the
formRv.Cv orIRv.Cv or!Rv.Cv such that Ru  Rv and Cu  Cv.
3. The terms of the form IRu.Cu in U are associated to the terms
of the form IRv.Cv or !Rv.Cv in V with Ru  Rv and Cu  Cv.
4. If u is of the form nuRu.Cu, we associate it to a v of the form
nvRv.Cv, such that nv  nu, Ru  Rv and Cu  Cv.

5. If u is of the form nuRu.Cu, we associate it to a v of the form
6. If u is of the form !Ru.Cu, we associate it to a v of the form

nvRv.Cv, such that nv  nu, Ru  Rv and Cu  Cv.
!Rv.Cv, such that Ru  Rv and Cu  Cv.
For example, consider

the Rnls of harryPotter and
hermioneGranger in Table 7. There are two associations from
the first label-set to the second. These two associations (shown
below) are because of the first association condition (concept sub-
sumption). The restriction! hasPet.Owl cannot be associated to
! hasPet.Cat; according to the association condition 6, the former can be associated to the latter only if the concept in the latter,
Cat, subsumes Owl.

RLO(harryPotter) RLO(hermioneGranger)

HogStudent  HogStudent

Muggle

Wizard
Halfblood
Gryffindor  Gryffindor
! hasPet.Owl
! hasPet.Cat

Let U = RLO(harryPotter) and
V = RLO(hermioneGranger); then, #U = 5 and #V = 4.
LSR(U, V ) = #{ Gryffindor, HogStudent}

= 2

#U

Difficulty level calculation. The difficulty level of a generated MCQ
can be calculated based on how the Rnls of its distractors are
related to the Rnls of the correct answer. If the distractors are
closely related to the correct answer, the difficulty level of the MCQ
is (intuitively) high. The closeness between the two Rnls (U and
V ) is measured in terms of their Label-set Similarity Ratio.
Definition 9. Given two Rnls U and V , Closeness is the mean of
LSR(U, V ) and LSR(V , U).
Closeness (U, V ) = LSR(U, V ) + LSR(V , U)

(2)

The difficulty level of an MCQ (Q ) with s as the correct answer

and {x1, x2, . . . , xw} (where w = 0) as distractors is defined as:
Difficulty(Q (s, x1, x2, . . . , xw))
= 1

RLO(s), RLO(xi)
node-label-sets RLO(harryPotter) and
RLO(hermioneGranger) (U and V respectively) is obtained as

Considering our previous example, Closeness of the Reduced-

Closeness

i=1

(3)

follows:
LSR(U, V ) = 0.4;
Closeness(U, V ) = (0.4 + 0.5)/2 = 0.45.

LSR(V , U) = 0.5

Example 2. Choose a Hogwarts
Student, a Wizard, a
Gryffindor and a Halfblood, having exactly one Owl
as Pet.

Options

Closeness
a. Harry Potter

b. Hermione Granger 0.450

c. Tom Riddle
d. Hedwig

Difficulty

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Table 7
Reduced-node-label-sets from Harry-Potter-Book ontology.

RLO(harryPotter)
RLO(ronWeasley)
RLO(hermioneGranger)
RLO(hedwig)
RLO(trevor)
RLO(nevilleLbottom)
RLO(tomRiddle)

= { HogStudent, Wizard, HalfBlood, Gryffindor,!hasPet.Owl}
= { HogStudent, Weasley, Gryffindor, Pureblood,!hasPet.Rat}
= { HogStudent, Muggle, Gryffindor,!hasPet.Cat}
= { Owl,IisPetOf.HogStudent}
= { Toad,IisPetOf.HogStudent}
= { HogStudent, Wizard,!hasPet.Toad}
= { Wizard, Slytherin}

Table 8
Some edge-label-sets of Harry-Potter-Book ontology.

Edge-label-set
LO(harryPotter, ronWeasley)
LO(harryPotter, hermioneGranger)
LO(harryPotter, hedwig)
LO(trevor, nevilleLbottom)

= { hasFriend }
= { knows, hasFriend, hasHelped }
= { hasPet }
= { isPetOf }

P1


{ isPetOf }

P2


{ hasPet }


Options

Closeness

a. Harry Potter

b. Ron Weasley
c. Hermione Granger 0.450

d. Hedwig

Difficulty

Options

Closeness

a. Harry Potter

b. Draco Malfoy
c. Ron Weasley

d. Hermione Granger 0.450

Difficulty

In the above example, we have shown three sets of options
for the MCQ. The difficulty level of the sets are 0.266, 0.283
and 0.358 respectively. Since we assume that the difficulty level
of an MCQ is related to the closeness of its options, the third
option-set makes the MCQ more difficult than the other two
sets. Intuitively, the third set of options is indeed the most
difficult one among all the three sets. This is because, the three
distractors Draco Malfoy, Ron Weasley and Hermione Granger are
closely related to Harry Potter w.r.t. the conditions mentioned in
the stem-set. But, in the other sets of options an owl named
Hedwig is appearing as a distracting answer. Hedwig, being an
odd one among the other options, can be easily identified as a
wrong answer. Clearly, the presence of Hedwig as an option is
reducing the difficulty values of the MCQs which are generated
using the first two sets of options. Later in Section 6, we investigate
the correlation between the difficulty levels estimated from our
Difficulty measure and those determined by domain experts for the
MCQs generated from a real-world ontology.

4.2. Edge-label-set based MCQ generation

If an instance x is related to another instance y by a role r in O,

. (Key: y)

we can frame a question of (fill in the blanks) type:
Stem: x r
A correct answer (say, y) can be easily obtained by identifying the
axioms which can be entailed by the ontology (i.e., O |= r(x, y)).
But to select a wrong option (say, z), it does not always mean
that O |= r(x, z). To guarantee that z is indeed a wrong answer
(under OWA), we need to entail r(x, z). This will ensure the
validity of the generated question item. By valid question items we
mean those MCQs whose all distractors are wrong answers. In what

follows, we illustrate a practical solution to generate valid question
items from edge-label-sets.

Let us consider the edge-label-set of the pair (harryPotter,

ronWeasley) from Table 8.

LO(harryPotter, ronWeasley) = {hasFriend }
We can generate the stem:
Harry Potter has friend

from the above label-set. Some of the possible options are
(a) Ron Weasley, (b) Viktor Krum, (c) Tom Riddle, (d) Hedwig.
The option a is a correct answer because of the assertion
hasFriend(harryPotter,ronWeasley). But the options b, c
and d cannot be guaranteed as true distractors, since O |=
hasFriend(harryPotter, viktorKrum),
hasFriend
(harryPotter, tomRiddle), hasFriend(harryPotter,
hedwig). Therefore, we cannot guarantee that this MCQ is valid. A
systematic method to generate valid MCQs is by considering only
those roles whose range is limited to some concepts or a few instances for stem generation.

We can identify the roles which are suitable for generating
the stem of a valid MCQ from a given edge-label-sets using the
procedure Role-Selection.

For a given edge-label-set LO(a, b), Role-Selection chooses the
roles in LO(a, b) one by one (lines 38) and checks if they form a
part of the restrictions of the form IR.C or !R.C in the Rnls of the
instance a. If such a restriction exists and if C  LO(b), then R can
be considered for framing a stem of the form: a R

Role-Selection(E)

// Input: E =LO(a, b), edge-label-set
// Output: Role sets P1 and P2
1 P1 = P2 =  // Predicate sets
2 for each role R  E
for each concept C in O

P1 = P1  {R}

P2 = P2  {R}

8 return (P1, P2)

if IR.C  R(LO(a)) and C  LO(b)
if !R.C  R(LO(a)) and C  LO(b)

If the role R appears as a part of IR.C, and C  LO(b), then we
store the role R in list P1 (lines 45). Similarly, the roles which are
appearing in the restrictions of the form!R.C are stored in P2 (lines
67). We handle these two lists separately for MCQ generation. The
reason for maintaining two lists is explained below. Table 8 shows
list of roles in P1 and P2 returned by Role-Selection, by giving
corresponding edge-label-set as input.

4.2.1. MCQ generation from edge-label-set and list P2
Consider the edge-label-set LO(harryPotter, hedwig) =
{ hasPet}. From Table 7, we get, the restriction !hasPet.Owl
is an element of R(LO(harryPotter)). Since the concept Owl in
the restriction is contained in LO(hedwig), we can frame a stem
based on LO(harryPotter, hedwig) and hasPet. Example 3
shows the generated MCQ.
According to Role-Selection, given LO(harryPotter,
hedwig) as input, hasPet will get stored in P2. Significance of
this list is that all the roles of the input edge-label-set which
are having a strict role restriction are stored in it. For example,
Harry Potter has exactly one pet which is an owl named Hedwig.
So intuitively, we only need to choose all other instances (includ-
ing other owls) which are different from the instance hedwig as
distractors. Therefore, we can use the procedure discussed in the
previous section (Generate-Distractors-Method) for finding the
distractors.
Given an edge-label-set LO(a, b) with P2 = , we generate
distractors by giving b as the input to Generate-Distractors-
Method. The explicit inequalities of individuals in the ontology
can be also utilized in this case as a condition for choosing the
distractor set. Therefore, Eq. (1) (in Section 4.1.3) which is used
in Generate-Distractors-Method (line 3) can be modified as
follows:
Dpotential =

RLO(x)  DisjointLO(d) = 


(4)


x  d

or

The relevance of this distractor generation method can be
explained by the following simple example. In the above discussion
of Example 3, we have seen that hedwig is an (belongs to the
concept) Owl; intuitively, those Owl instances which are different
from hedwig form the most apt distractors. Our procedure
returns such instances also in the possible distractor set, even
if the inequalities of the instances are not explicitly given in
the ontology. To elaborate it further, assume that the Owl class
has two disjoint subclasses: BlackOwl and WhiteOwl. hedwig
belongs to WhiteOwl, and another instance browner belongs to
BlackOwl. Also, assume that the axiom hedwig  browner, is
not present in the ontology. Since the restriction !hasPet.Owl
is on Owl, the obvious way to choose the complete distractors is
by finding all instances which belong to Owl. But, in Generate-
Distractors-Method, we consider an instance as a distractor if

R(L(x))  DisjointL(d) = . Therefore, the disjointness of

classes BlackOwl and WhiteOwl is enough to make browner a
distractor.

Example 3. Harry Potter has Pet

Closeness
Options
a. Hedwig

b. Crookshanks 0.500

c. Errol

d. Trevor

Difficulty

Closeness
Options
a. Hedwig

b. Ron Weasley

c. Hermione Granger 0.000

d. Scabbers
Difficulty

respectively. We use the same metrics defined in the previous
section for calculating Closeness and Difficulty. According to those
metrics, the first set of options makes the MCQ more difficult than
the second set. Intuitively, since the options in the first set belong
to some subclass of Creature class, the test takers will find it
difficult to guess the correct answer (0.25 probability for guessing
the correct answer). The second set contains Ron Weasley and
Hermione Granger as two of the options. Since those two options
can be easily categorized as wrong answers, the probability of
guessing the correct will be increased to 0.5. Hence, the difficulty
level of the MCQ which uses the second set of options should be
less when compared to the MCQ generated with the first set.

4.2.2. MCQ generation from edge-label-set and list P1

We consider the edge-label-set

means that the RLO(trevor) contains a restriction of the form
IisPetOf.HogStudent  RLO(trevor). This restriction on

LO(trevor, nevilleLbottom) in Table 8 for our illustration.
The list P1 of this label-set contains the role isPetOf. This
IisPetOf.C (where C is some concept). From Table 7, we get,
isPetOf is not that strict when compared to the restriction
in the previous case (!hasPet.Owl). IisPetOf.HogStudent
states that there should be at least one isPetOf relation from
the instance trevor, and the range of the relation should be
HogStudent. Therefore, when we frame an MCQ using the edge-
label-set LO(trevor, nevilleLbottom), the only possible way
to choose a distractor is by finding the instances in the complement
of the concept HogStudent. In general, given a restriction concept
C, the possible distractor set Dpotential can be calculated as follows:

Dpotential =d C  DisjointLO(d) .

(5)

Example 4 is generated using our example edge-label-set and

the role isPetOf.

Example 4. Trevor is the pet of

Options

Closeness

a. Neville Lbottom 1.000

b. Viktor Krum

c. Hedwig

d. Errol

Difficulty

Options

Closeness

a. Neville Lbottom 1.000
b. Hedwig

c. Crookshanks

d. Scabbers
Difficulty

In the above example, the first set of options makes the MCQ
more difficult than the second set of options. The Closeness values
of some options are zero because of the fact that the Reduced-
node-label-sets of such option instances cannot be related to the
Rnls of the correct answer instance (we consider those instances
as unrelated ones).

5. Experiments

In the above example, we show two sets of options for the
MCQ. The difficulty levels of the options are 0.583 and 0.166

In order to support the approaches presented in this paper, we
implemented our techniques in Java 1.7 using the Jena framework

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Table 9
Specifications of ontologies which are used for experiments.

Ontology
Harry-Potter-Book
People & Pet
Plant-Protection
Geographical Entity

Classes

Obj-Props

Indvs

(2.11.0) as a portable library. We use the Jena Ontology API8 to
explore concepts and to look up restrictions in the ontologies.

We consider four ontologies for our experimentation.

1. Harry-Potter-Book ontology9 is developed by our research group

at IIT Madras, India.

2. People & Pet ontology10 is developed by Sean Bechhofer et al. for

ISWC 2003 tutorial on OWL.

3. Plant-Protection ontology11 models knowledge about cerealplant protection; it was developed by International Center of
Agricultural Research in the Dry Areas, University of Aleppo,
Syria.

4. Geographical Entity ontology12 (GEO ontology) is based on
geopolitical organizations and their subdivisions. This ontology
is developed by William Hogan, University of Florida, US.

The specifications of the four ontologies are presented in
Table 9. Some necessary disjointness between concepts were not
there in the ontologies (2) and (4). So, we added a few axioms
to those ontologies to make them suitable to generate expected
results. In GEO ontology, we have added axioms to state that the
concepts: group of dependencies, group of major
administrative subdivision, nation, geopolitical dependency and major administrative subdivision are disjoint. We added axioms to People & Pet
ontology to state that cat,
person,
sheep and tiger are disjoint concepts. We also included
the axiom (male  female  ) and the axioms to state that
publication, vehicle and animal are disjoint concepts.
Three assertions: Male(joe), Male(walt) and Male(fred),
are added to the ontology, to generate distractors which are closely
related to the correct answer.

giraffe,

duck,

Example 5. Choose a person and an oldlady, having only
cat as pet.

Joe

Options

Closeness

a. Minne (T)

(F)
b.

c. Walt
(F)
(F)

d. Fred
Difficulty 0.461

Example 6. Minne has pet

8 http://jena.apache.org (last accessed 11th Dec. 2014).
9 https://sites.google.com/site/ontoworks/ontologies (last accessed 11th Dec.
2014).
10 http://www.cs.man.ac.uk/~horrocks/ISWC2003/Tutorial/people+pets.owl.rdf
(last accessed 11th Dec. 2014).
11 https://sites.google.com/site/ppontology/ (last accessed 11th Dec. 2014).
12 http://repos.frontology.org/geographical-entity-ontology/src
11th Dec. 2014).

accessed

(last

Options

Closeness

a. Tom (T)

(F)
b. Louie

(F)
c. Fido
(F)

d.
Joe
Difficulty 0.500

Example 7. Joe has pet

Options

Closeness

(T)
a. Fido

b. Louie
(F)

c. Tom (F)

d. Huey
(F)
Difficulty 0.750

Example 8. Choose a White van man and a dog owner,
drives a van and a white thing, reads only tabloid,
having some cat as pet.

Options

a. Mick
b. Fred
c. Fido
d.
Joe

Closeness

(T)

(F)

(F)
(F)

Difficulty 0.313

Example 9. Choose a dog owner, having exactly one dog
as pet.

Options
Joe

a.
(T)
b. Dewey (F)
c. Minne (F)
d. Tom (F)

Closeness

Difficulty 0.125

For a comparative study, in Table 10, we present sample MCQs
which are generated from the People & Pet ontology by using
the existing approaches. Each option is marked with (T) or (F)
to represent the key and the distractor respectively. Question 1
in the table is generated using the approach proposed in Cubric
and Tosic [4] and M. Tosic and M. Cubric [6]. Questions 24 are
generated using class, property and terminology based strategies
(respectively) described in Papasalouros et al. [5]. Question 5 is
an analogy type question from the approach in [3]. Examples 59
show sample MCQs which are generated using our approaches
from the same ontology.

Table 11 shows the count of the MCQs generated per ontology
using our two approaches. Sample questions generated from GEO
ontology are given in Appendix A. A detailed list of MCQs generated
by our approaches is available at an external source.13 It should
be noted that, even though we generate large number of MCQs,
the stems of many of these MCQs are the same. This is because, an
ontology can have many instances with the same Rnls.

13 https://docs.google.com/document/d/10tkswD7-Z7hcBsUroxWhzuY0zse
5SUSYfKSbVI4T2Lk/edit?usp=sharing (last accessed 21th Dec. 2014).

Fig. 3. A sample question from PP ontology which is used for empirical study.

Table 10
MCQ examples generated from People & Pet ontology using existing approaches.

1. Which one of the following response pairs is related in the same way as Walt and Huey in the relation hasPet?

2. Choose the correct answer.

a. Walt hasPet Fido (T)
b. Minne hasPet Fido (F)
c. Minne hasPet Dewey (F)
d. Kevin hasPet Louie (F)

5. Haulage Truck Driver : Driver

a. Bus : Vehicle (T)
b. Quality Broadsheet : Newspaper (F)
c. Giraffe : Sheep (F)
d. Giraffe : Cat Liker (F)

a. Joe hasPet Fido (T)
b. Fred hasPet Huey (F)
c. Walt hasPet Louie (F)
d. Minne hasPet Tibbs (F)

3. Choose the correct answer.

a. Fido is a dog (T)
b. Tom is a dog (F)
c. Fluggy is a dog (F)
d. Louie is a dog (F)

4. Choose the correct answer.

a. Man is a Person (T)
b. Cat is a Person (F)
c. Duck is a Person (F)
d. Giraffe is a Person (F)

Table 11
No. of questions generated by proposed approaches.

Ontology

Harry-Potter-Book
People & Pet
Plant-Protection
Geographical Entity

Approach based on
node-label-set

edge-label-set

6. Empirical evaluation

In this section, we describe the empirical evaluation done to (1)
check the usefulness of the question items which are generated
from our two MCQ generation approaches and (2) verify the
effectiveness of the proposed measure for estimating question
difficulty.

6.1. Ontology and question-set

We consider MCQs generated from Plant-Protection ontology
(PP ontology) for our empirical evaluation. A total of 656 questions
have been generated from the ontology. Manual evaluation of all
the generated questions is difficult. We grouped the questions
which are based on the same label-set and, randomly selected one
question from each group to form a question-set. A question-set of
31 questions is generated from the ontology for evaluation.

Participants of the evaluation are seven experts of plant disease
or agriculture related domains. Four of the participants have
PhD and three have masters degree in the plant protection
related areas. Further details of the participants are given in the
acknowledgment section.

6.2. Evaluation method

We presented to the participants the prepared question-set.
Each question in the question-set is provided with three check
boxes along with the following guidelines:

 Useful. Select this option if the stem (question statement) of
the MCQ is useful in conducting a domain related assessment test.
 Not useful, but domain related. Select this option if the stem is
not useful in conducting a domain related assessment test, but it is
related to the domain.

 Not useful and not domain related. Select this option if the stem

is neither useful nor related to the domain.

They need to select one of the above check boxes. This helps in

identifying the usefulness of the generated MCQs.

Each question has a set of answer options. To check the quality
and hardness of the option-set we provide three check boxes:
 High  Medium and  Low. The participants need to tick one
among the three check boxes. Some of the question items may
contain more than one set of option-sets. In that case, they need
to relatively grade the option-sets. A sample question from the
question-set is given in Fig. 3.

A question which has been rated as Useful by at least four out
of the seven participants is considered to be a real-useful question,
for conducting assessment tests. The hardness score of an optionset is determined in the same way from the participants rating,
by considering the majority. We compare the obtained hardness
rating with the pre-calculated hardness which is based on our
Difficulty measure, to judge the effectiveness of our measure.

While calculating the hardness using the Difficulty measure, a
question item is given a low score if the calculated difficulty is in
the range 00.33. Similarly, a score of medium and high is given,
if the difficulties are in the range 0.33< to 0.66 and 0.66< to 1.0
respectively.

6.3. Results

From Table 12, it is evident that a large number of questions
in the question-set are useful in conducting a domain related assessment test. Out of the 23 questions which are generated using
node-label-set based approach, the stems of 8 questions contain
role restriction constraints (like: having only some microorganism
as factor). The majority of the participants have rated all these 8
questions as useful. The question item with stem Choose an air
pollutant. is the only question which is marked as Not useful, but
domain related. Some questions like Choose a drainage. are categorized as Not useful and not domain related ones. Therefore, there is
a need to screen the generated MCQs prior to administrating them
on a real-world assessment test. Our analysis of the results and oral
feedback from the participants show that the MCQs with role restrictions are very useful and realistic in conducting tests.

The effectiveness of the proposed Difficulty measure is studied
by finding out the number of option-sets whose hardness scores

Vinu E.V., Sreenivasa K.P. / Web Semantics: Science, Services and Agents on the World Wide Web 34 (2015) 4054

Table 12
Statistics to find the usefulness of questions generated from PP ontology.

Approach

Node-label-set based
Edge-label-set based

Count of questions
Total

Useful

Not useful, but domain related

Not useful & not domain related

Table 13
Statistics to find the effectiveness of Difficulty measure.

Proposed Difficulty Measure
Domain Experts Opinion
No of option sets correctly classified by the proposed measure.

No. of option-sets with hardness
High
Low

Medium

Total

are same when (1) calculated using the Difficulty measure, and (2)
determined by the domain experts. In total, we used 75 option-sets
(since most of the questions in the question-set have more than
one option-sets) for our evaluation.

In Table 13, the first row shows the number of option-sets that
are rated high, medium and low using the proposed difficulty mea-
sure. The second row shows what the domain experts determined.
The last row gives the count of option-sets that are classified correctly by our approach. We observe that the hardness scores of
49 out of 75 option-sets (nearly 65%) are correctly calculated by
our Difficulty measure. This highlights the employability of the
proposed measure in determining the hardness of the generated
MCQs. In the cases where the calculated score deviated from the
predicted score, we observed that, irrespective of the similarity of
the distractors with the key, there are clues in the correct answer
instances which help in identifying the key easily, making the hardness of the MCQ as low. One such example MCQ is given below,
where the hardness score is calculated as Medium, but the domain
experts have given a Low score because of the word Virus in the
key.

Example 10. Choose a virus.

Options

a. Barley Stripe Mosaic Virus
b. Pyrenophora Graminea
c. Xanthomonas sp.
d. Sclerophthora Rayssiae

(T)
(F)
(F)
(F)
Difficulty

Closeness

The plant-protection question-set and the response-sheets of

the participants are available at our project web-page.14

7. Conclusion and future work

We proposed the generation techniques of two new categories
of multiple choice question types based on the node-label-
sets and edge-label-sets of the instances in an ontology. We
introduced a technique called Label-set-Reduction to make the
label-sets suitable for generating MCQs by converting them to
a reduced form (called Reduced-node-label-sets). We suggested
a systematic method to find the distractors for the generated
MCQs. The similarity of these distractors with the correct answer
is determined using a closeness measure, which is in turn used to

14 https://sites.google.com/site/ontoworks/projects (last accessed 15th May
2015).

estimate the hardness level of the MCQs. We considered the openworld assumption and DL semantics in our approaches to make
the MCQs valid and realistic. The feasibility of the approaches are
studied by experimenting them on a synthetic ontology and on
three ontologies which are available online. Our experiments prove
that the proposed approaches, when used with semantically-rich
ontologies, can provide successful cases. We also did an empirical
study by generating question items from a real-world ontology,
and the result of domain experts evaluation of the generated MCQs
was encouraging.

While the proposed approaches work well in defining the semantics of questions, the problem of generating syntactically correct question items is only partially tackled. In order to overcome
syntactic problems, more sophisticated natural language generation techniques should be utilized in future implementations of the
presented approaches.

In this paper, we assumed that the difficulty level of an MCQ is
purely dependent on how closely its correct answer is related to its
distractors. We considered all stems with same level of importance
and assumed that the option-set makes one question more difficult
(or easier) than the other. Nevertheless, the question statement
itself can make an MCQ difficult (or easier). For example, consider
the stem of Example 3 with stem Harry Potter has pet
., we
can decrease the difficulty level of this question by adding the class
information of the pet of Harry Potter in the stem (Harry Potter has
pet

, an Owl.).

Another observation we made during our work is about the
relevance of the generated MCQ w.r.t. the domain of the ontology.
For example, consider the question Choose a drainage. which
is generated from Plant-Protection ontology. This question is
categorized as not useful in conducting assessment tests and not
related to the domain, by the domain experts. Similarly, when
considering the questions (given in Appendix A) generated from
GEO ontology, Example A.5 appears to be a less relevant MCQ
(when compared with Examples A.1 to A.4), since its stem Choose
a nation. is less related to the domain: geopolitical organization,
but more related to a generic ontology domain which talks about
countries, continents and nations. An automated method for
finding the relevant concepts and roles in an ontology is required
to enhance the applicability of the proposed approaches.

Acknowledgments

This research has been funded by IIT Madras and the Ministry
of Human Resource Development, Government of India. We
express our gratitude to the participants of our evaluation
process: Dr. S. Gnanasambabdan (Director of Plant Protection,
Quarantine & Storage), Ministry of Agriculture, Gov. of India;
Dr. S. Nazreen Hassan (Asst. Professor), Mr. J. Delince and Mr.
J.M. Samraj, Department of Social Sciences AC & RI, Killikulam,

Tamil Nadu, India; Ms. Deepthi. S (Deputy Manager), Vegetable
and Fruit Promotion Council Keralam (VFPCK), Kerala, India; Dr.
K. Sreekumar (Professor) and students, College of Agriculture,
Vellayani, Trivandrum, Kerala,
India. We are thankful to the
anonymous reviewers as well as the internal reviewers (Mr. S.
Baskaran, Mrs. Subhashree B, Mr. Dileep Kumar and Mrs. Athira Vinu)
of our paper, for their valuable suggestions.

Appendix A. MCQ examples from GEO ontology

Sample MCQs generated from Geographical Entity Ontology
using node-label-set and edge-label-set based approaches are
given below:

Example A.1. Choose a geopolitical
member of exactly one sovereign state.

dependency,

Options
Jersey
Illinois

Closeness

a.
(T)

b.
(F)

c. Maryland (F)
(F)

d. Hawaii
Difficulty 0.500

Options
India

(T)
a.
b. Bermuda (F)
(F)
c. Aruba
(F)
d.
Jersey
Difficulty

Closeness

Observe that, difficulty level is zero for Examples 4 and 5, this
is because GEO ontology is not semantically rich enough to relate
the correct instance and the distractor instances.

Appendix B. Notations

LO(x)
LO(x, y)

RLO(x)

!R.C
IR.C
Rnls

Dpotential

Node-label-set of instance x in
ontology O
Edge-label-set of instances x, y in
ontology O
Reduced-node-label-set of x in
ontology O
R.C  1R.C
R.C  R.C
Reduced-node-label-set
Label-set Similarity Ratio
Set of possible distractors

Example A.2. Choose a geopolitical
member of exactly one sovereign state.

dependency,
