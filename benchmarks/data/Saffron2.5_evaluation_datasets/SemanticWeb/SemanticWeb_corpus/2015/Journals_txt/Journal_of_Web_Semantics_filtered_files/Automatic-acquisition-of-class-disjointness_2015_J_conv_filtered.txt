Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Automatic acquisition of class disjointness
Johanna Volker, Daniel Fleischhacker, Heiner Stuckenschmidt

Data & Web Science Group, University of Mannheim, Germany

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 15 December 2014
Received in revised form
17 June 2015
Accepted 6 July 2015
Available online 23 July 2015

Keywords:
Ontology learning
Disjointness
Machine learning
Association rule mining
Semantic web
Linked data

Although it is widely acknowledged that adding class disjointness to ontologies enables a wide range of
interesting applications, this type of axiom is rarely used on todays Semantic Web. This is due to the
enormous skill and effort required to make the necessary modeling decisions. Automatically generating
disjointness axioms could lower the barrier of entry and lead to a wider spread adoption. Different methods have been proposed for this automatic generation. These include supervised, top-down approaches
which base their results on heterogeneous types of evidence and unsupervised, bottom-up approaches
which rely solely on the instance data available for the ontology. However, current literature is missing
a thorough comparison of these approaches. In this article, we provide this comparison by presenting
two fundamentally different state-of-the-art approaches and evaluating their relative ability to enrich a
well-known, multi-purpose ontology with class disjointness. To do so, we introduce a high-quality gold
standard for class disjointness. We describe the creation of this standard in detail and provide a thorough
analysis. Finally, we also present improvements to both approaches, based in part on discoveries made
during our analysis and evaluation.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

One of the most fundamental ideas of Semantic Web technologies is the use of ontologies as background knowledge for information processing. The web ontology language has been created as
a rich logical language for representing this knowledge. Ever since
the invention of OWL, there has been a discussion about the degree
of formalization of ontologies on the Web. Supporters of complex
formal ontologies point to the benefits of rigorous formal underpinnings which enforce a certain degree of agreement that can be
tested using logical reasoning. The ability to check the consistency
of ontological definitions has been cited as one of the biggest benefits of formal ontologies [1]. This benefit, however, comes at a price.
Being able to check consistency, usually requires rich logical specifications of classes, which have to be created before the model can
be verified. In particular, the descriptions to be checked have to
use some kind of negation, either in terms of the logical negation
operator or hidden in other operators such a number restrictions.
Building such rich axiomatizations is an error-prone and cumbersome task. Avoiding modeling errors, requires not only a deep understanding of the domain to be modeled, but also of the logic used
to formalize it [2]. This problem, also referred to as the knowledge

 Corresponding author.

E-mail address: daniel@informatik.uni-mannheim.de (D. Fleischhacker).

http://dx.doi.org/10.1016/j.websem.2015.07.001
1570-8268/ 2015 Elsevier B.V. All rights reserved.

acquisition bottleneck is often cited as a main argument against the
use of rich formal ontologies. There have been attempts to tackle
the knowledge acquisition bottleneck by automatically generating
ontologies from texts. So far however, methods for learning ontologies from texts, focus on the creation of so-called lightweight on-
tologies, which mainly consist of a concept hierarchy and relations
between concepts [3]. Although there are first attempts at creating
expressive ontologies from text [4] and data [5], it will not be possible to fully automate the creation of richly axiomatized ontologies
for some time.

However, as we will show in the remainder of this article, we
can support the acquisition of class (or property) disjointness, a
limited form of negation. Conceptually, disjointness is a semantic
relation between concepts, indicating that the concepts cannot
have common instantiations, i.e., their extensions must always
be disjoint. In combination with other semantic relations, in
particular subsumption, disjointness provides a basis for checking
the consistency of conceptual structures. In the following, we
briefly discuss the role of disjointness in conceptual modeling,
present a logical formalization of disjointness and discuss the use
of disjointness as an operator in Semantic Web languages.

Disjointness has been an essential ingredient of ontological
modeling from the very beginning on. In fact, ontology, in the original sense of the word tries to capture basic distinctions of reality
and sort objects into (disjoint) categories. Aristotles taxonomy of
substances from the third century is a good example for the use of

disjointness as a basic principle [6]. Aristotle uses inherent properties  so-called differentiae  to sort objects into disjoint classes. In
particular, substances are distinguished into material and immaterial substances constituting the concepts body and spirit. Bodies are distinguished into animate and inanimate bodies leading to
the disjoint concepts living and mineral and so forth. This means,
that the whole taxonomy for categorizing objects is based on the
notion of disjointness, as categories located at the same level of the
taxonomy are disjoint by definition.

Ontologies, as they are used in the context of the Semantic Web
today, although motivated by philosophical ontology, are heavily
influenced by more pragmatic approaches to conceptual modeling.
As a result, it is not the case that sibling classes in an ontology can
automatically be assumed to be disjoint. Classes in an ontology do
not always represent categories in the sense of ontological analysis,
but often represent roles that certain objects play in a domain. A
person, for example can be an author of a publication and at the
same time the reviewer of a different one. Thus, the two classes
author and reviewer, which that are likely to appear on the same
level of a class hierarchy, are by no means disjoint. The example of
authors and reviewers also shows that the notion of disjointness
is often context-dependent: We can say that nobody can be the
author and the reviewer of the same paper or be married and
unmarried at the same time.

Unfortunately, the definition of disjointness and the way it is
perceived by humans is not always consistent. For example, when
being asked about the disjointness of the two pairs of concepts
author/reviewer and married/unmarried, most people will
say that the latter concepts are disjoint and the former are not,
although in both cases we can argue that the disjointness is relative
to a certain context. An in-depth investigation of this problem is
out of the scope of this paper. Instead, our investigation uses the
opinions of domain experts to determine whether classes should
be considered disjoint or not.

However, such intricacies make manually modeling disjointness a non-trivial task which might also be the reason why it is not
yet widely used in real-world datasets. This limited deployment
is also visible in the Linked Open Data cloud, a network of interlinked datasets, many of which are accompanied by some kind of
schema. According to the LODStats project,1 only 6 out of 365 properly crawled datasets contain class disjointness statements, which
means that only 1.7% of the datasets contain disjointness. More-
over, even these datasets only feature 49 disjointness statements
in total. Glimm et al. [7] performed a survey on the Billion Triple
Challenge 2011 dataset which discovers class disjointness as one
of the top-20 OWL primitives employed in the dataset but nevertheless its usage in absolute numbers is low.

Since the Web of Data is driven by user-created content, it is
not an option to force users to create more expressive ontologies,
as this would pose a high entry barrier for contributors. Instead,
the usage of automatic methods to enrich ontologies could help to
close to gap between more expressivity without placing the burden
of its full complexity on the casual contributor. In particular, employing statistical methods to generate more complex ontologies
from the vast amount of data already available could help to ease
the transition from the current Web of Data to the Semantic Web.
In the next section, we formally introduce disjointness as a
modeling primitive (cf. Section 2). We then present two different
methods for the automated acquisition of disjointness axioms from
Semantic Web data:

In Section 3, we report on recent experiments in the field
of statistical schema induction [8] and the discovery of class
disjointness axioms from facts, i.e., class membership assertions, in

1 http://stats.lod2.eu.

a knowledge base. In Section 4, we describe a supervised machine
learning approach, which does not presume the existence of facts,
but only requires schema-level descriptions of the classes.

The remainder of this article is dedicated to answering the

following research questions:
1. Which problems do human ontology engineers face when

modeling class disjointness?

2. Which of these two approaches works better? Does the induc-
tive, bottom-up discovery of disjointness axioms from given
facts (cf. Section 3) outperform the supervised approach to
learning disjointness, which relies on schema-level information
(cf. Section 4)?

3. Does a supervised approach to learning disjointness work
across datasets, so that we can train a classifier once and use
it to enrich any other ontology with disjointness axioms?
In order to answer these questions and to enable a systematic
evaluation of both types of approaches, we built a gold standard of
manually created disjointness axioms (cf. Section 5).

Section 6 describes the experimental setup and the results of
our comparative evaluation. We take a closer look at the generated axioms to identify the strengths and weaknesses of both ap-
proaches, and we suggest an extension to our previously developed
framework [9], which helps to overcome a major problem that we
identified during the evaluation (cf. Section 6.2.1).

In the light of our findings, we discuss some related work (cf.
Section 7), before we conclude with a summary and an outlook to
future work (cf. Section 8).

2. Class disjointness

Terminological knowledge usually groups objects of the world
that have certain properties in common. A description of the shared
properties is called a class definition. Classes can be arranged into a
subclasssuperclass hierarchy. Classes can be defined in two ways,
by enumeration of its members or by stating that it is a refinement
of a complex logical expressions. The specific logical operators
to express such logical definitions can vary between ontology
languages; the general definitions we give here abstract from these
specific operators. Further relations can be specified in order to
establish structures between classes. Terminological knowledge
considers binary relations that can either be defined by restricting
their domain and range or by declaring them to be a sub-relation of
an existing one. In order to capture the actual information content
of a knowledge base, we allow to specify single objects, also called
instances. In our view on terminological knowledge, instances can
be defined by stating their membership in a class. Further, we can
define instances of binary relations by stating that two objects form
such a pair.

We can define semantics and logical consequence of a
terminological knowledge base using an interpretation mapping
.I into an abstract domain  such that:
 cI   for all class definitions c in the way defined above
 rI     for all relation definitions r
 oI   for all object definitions o.
This type of denotational semantics is inspired by description
logics, however, are not specific about operators that can be used
to build class definitions, which are of central interest of these
logics. Based on the mapping .I, we formally define the notion of
disjointness as follows:

Definition 1 (Disjointness). Two classes C and D are said to be
disjoint iff CI  DI = .

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

While this definition directly captures the intuition of disjoint-
ness, it is hard to directly test disjointness based on this definition.
A more practical definition of disjointness can be achieved by relating disjointness to negation and subsumption in description logics,
which are defined as follows:
negation CI =   CI
subsumption I |	 C  D iff CI  DI.
From these definitions, a more practical criterion for disjointness follows directly: Two classes C and D are disjoint iff I |	 C 
D.
Disjointness in OWL. The notion of disjointness has been part of
Semantic Web languages from the beginning on. More specifically,
disjointness is included in the web ontology language OWL and
more recently OWL 2 [10]. In both cases, disjointness can be stated
in terms of an axiom
DisjointClasses(C1, . . . , Cn).

This statement declares pairwise disjointness relations between all the classes mentioned. The semantics of disjointness in
OWL and OWL 2 are exactly the one given above.

It has been recognized, that class disjointness is a useful special
case of negation which can be used instead of full negation to
define language fragments that are more simple or more efficient
to implement than the full OWL 2 language. Looking at the
specification of OWL 2 language profiles [11] we can see that
disjointness is part of all currently existing OWL profiles, while full
negation is typically not part of these profiles. This underlines the
practical importance of disjointness as a form of negation that is
well suited for practical applications.

3. Learning disjointness from linked data

The methods that we present in the remainder of this section
are based on the paradigm of statistical inductive learning, i.e., they
are based on the assumption that schema-level knowledge can be
derived from an analysis of existing class membership (or rdf:type)
assertionse.g., by association rule mining or the computation
of statistical correlation values. In this respect, our approaches
bear some resemblance with previous work on concept learning
in description logics [12]. Even several features used in the
LeDA framework (cf. Section 4), including the taxonomic overlap
in terms of existing or automatically acquired individuals, can
be considered inductive or extensional. However, in contrast
to the LeDA framework, the approach proposed here solely
relies on instances in the dataset to derive previously unknown
disjointness relations between classes. Furthermore, while LeDA
is based on supervised classification and thus needs labeled data
to train on, the approach described in the following is based on
unsupervised methods which makes it applicable in scenarios
where no labeled data is available. This approach particularly
benefits from large amounts of data as in the case of Linked Data.
Linked Data consists of a number of RDF datasets which contain
large amounts of instance data and are interconnected by means
of properties between instances of different repositories. However,
most of these datasets fall short in providing expressive schemas.
Furthermore, getting labeled data for training and additional
data for applying the different features used in LeDA are not
always readily available for these datasets. Thus, an unsupervised
approach only relying on the class assertions contained in the
actual dataset is best suited for Linked Data. After describing the
basic approach in this section, we will take a closer look at the
generated disjointness axioms, and see how LeDA compares to the
purely inductive methods in Section 5.

Fig. 1. Process for inductively learning disjointness axioms.

3.1. Preliminaries

Our approach of statistical schema induction, as first introduced
by [8], is based on association rule mining. Association rules are
representations of common implication patterns discovered in
large and usually sparse datasets like the transaction databases of
international supermarket chains whose size typically is up to 1010
transactions and 106 attributes. Developed for such large datasets,
association rule mining approaches are particularly suited for
working on potentially huge Linked Data sources.
Formally, association rule mining approaches work on a
transaction database D = (t1, t2, . . .) containing transactions. Each
transaction tj  I is a subset of the set I = {i1, i2, . . .} of all
possible items. Association rules are rules A  B with A, B  I. For
generating association rules, the first step is to determine frequent
itemsets in the transaction database, i.e., items whose support
exceeds a given support threshold. For this purpose, support of an
itemset X is given by
supp(X ) = |{tj  D : X  tj}|
and might also be used in a normalized fashion relative to
the number of transactions in the database. There are several
algorithms for finding frequent itemsets in a transaction database,
including the well-known Apriori algorithm [13].
generated. For the rule A  B, its confidence is given by
conf (A  B) = supp(A  B)

Given these frequent itemsets, actual association rules can be

supp(A)

which is the conditional probability of an itemset B given the
occurrence of itemset A. In addition to confidence, there is a
number of other measures of interestingness, like lift [14], which
are used to filter and choose generated association rules.
In this work, we do not want to generate rules like A  B,
which would to a degree resemble subsumption and equivalence
relations in ontologies and has already been done by Volker and
Niepert [8]. Instead, we want to generate negative association rules
where either A or B is negated like A  B. It is important to
note that negative association rules, despite being similar to logical implications, do not capture the same logical meaning of implication or, in our special case, of disjointness. Association rules
are not definitive rules but uncertain ones so there may be some
transactions which violate their proposition. This fact is partly represented by the confidence values which incorporate the fraction
of transactions transgressing the association rule.

3.2. Approach

The approach for learning disjointness by means of statistical
schema induction is the same as our previously described
approach [15], partly extended regarding the axiom generation as
described later. The complete process as shown in Fig. 1 is divided
into five steps which we describe briefly in the following.

Since the RDF query language SPARQL [16] is the de facto
standard for accessing remote RDF stores and provides the main
means for retrieving the required data in our implementation, we
also provide the corresponding SPARQL queries for each of the data
retrieval tasks. We explain the steps of the approach by means of
an illustrative example using the RDF data shown in Fig. 2.

Table 1
Transaction table for example data.

Panama Canal
Lake Michigan
Rhine


Table 2
Extended transaction table for example data.

Panama Canal
Lake Michigan
Rhine


C B L S R


Fig. 2. RDF Example DataEllipses are instances, rectangles represent rdfs:
Class and arrows show rdf:type relations.

Terminology extraction: The first step is the extraction of
terminology from the input data. For the use case of learning
class disjointness axioms, we only extract the different classes
used throughout the dataset. This can be done by executing the
appropriate SPARQL query:

SELECT DISTINCT ?c WHERE {

[] rdf:type ?c

It retrieves all concepts in the knowledge base that have at
least one instance explicitly assigned. Depending on the dataset,
the implicit assignment of classes must also be considered, e.g., for
classes defined as subclasses of other classes but without an
explicit type assertion to rdfs:Class. In our example, we retrieve
the five (explicitly) defined classes Canal, BodyOfWater, Lake,
Stream and River.
Instance extraction: For each class in the previously created
list, we now retrieve its extension, i.e., all instances assigned to
the given class. For SPARQL-based access and given a class URI
$CLASSURI this can be also done using the SPARQL query:

SELECT DISTINCT ?i WHERE {
?i rdf:type <$CLASSURI>.

This returns all instances which have types assigned in the
knowledge base and thus all those instances relevant for our
approach. Again, during this phase implicit assignments, such as
from the usage of properties combined with domain or range
assertions, might have to be considered. By performing this query
for all five classes, we get a set containing the three instances
Panama Canal, Lake Michigan and Rhine.
Transaction table creation: In this step, the transaction tables on
which the actual association rule mining takes place are compiled.
This is done by iterating over all instances and retrieving all classes
each instance is assigned to. For each instance $INSTANCE, this is
done using the following SPARQL query:

SELECT DISTINCT ?c WHERE {
<$INSTANCE> rdf:type ?c.

The results of these queries are then stored into the so-called
transaction tables in which each line represents one instance from
the dataset and each column the membership in a specific class.
The transaction table for the running example is shown in Table 1.
Obviously, the steps so far could be merged for better
performance. However, the separation of the different steps allows
us to reuse data generated for one type of axioms for other axiom
types which might depend on the same data.

Association rule mining: Up to this point, we would be able
to deduce positive knowledge from the generated transaction
tables, e.g., association rules like A  B which resemble rdfs:
subClassOf axioms. Applying the normal association rule mining
approach described above, we would not be able to gather negative
rules like A  B. To broaden the discovery of rules into this
direction, we introduce additional concepts that represent the
complements of each class and include these concepts in our
transaction tables. If the corresponding SPARQL query observes
an instance as not belonging to a class A the membership in the
complement class of A (A) is written into the transaction table.
It is important to note that this is not in line with the open-world
assumption (OWA) of RDF and OWL which does not allow us to
infer that an instance belongs to a complement class A just from
the knowledge that the instance is not assigned to A. However,
for our approach presented here, we argue that the influence of
this breach of the OWA is negligible because the more instance
data we have available, the more likely we find evidence against
a disjointness which helps to compensate for some inaccuracies
introduced by presuming a closed-world-like scenario. We thus
rely on the huge amount of instance data a typical Linked Data
datasets provides us with and the experimental results presented
later-on will confirm this assumption. Based on this method, it
is possible to discover negative association rules the same way
as positive ones. For our example, this leads to the extended
transaction table shown in Table 2.

On the resulting transaction tables, the actual association rule
mining is performed using the association rule mining tool by
Borgelt and Kruse [17] based on the Apriori algorithm for detecting
frequent itemsets.

In previous work [15], we also examined an alternative way for
the inductive discovery of disjointness axioms based on correlations between two classes as well as one approach using a different
algorithm for mining negative association rules and compared it to
our association rule mining-based approach presented here. How-
ever, the approach presented here turned out to provide a better
balance between precision and recall than both other approaches.
For our example data, we find association rules like Stream 
Canal which has a support of 0.66 and a confidence of 0.5 or
Lake  River which has a support of 0.33 and a confidence
of 1.
Axiom generation: In this phase, the actual disjointness axioms
are generated based on a user-defined confidence level cmin >
0. We only consider association rules that can be interpreted as
disjointness, i.e., satisfy the pattern A  B where A and B are
distinct classes.

Since association rules are, in contrast to disjointness axioms,
not symmetrical, it is common to find the two directions of
rules having different confidence values. During the generation of
axioms, we parse the generated association rules and group them
into pairs containing the same classes, thus, for classes A and B the

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

Table 3
Overview of logical features.

Feature
fsub
fdist
fpropobj
foverlapc

Description
Subsumption
Semantic distance
Similarity based on object properties
Taxonomic overlap w.r.t. subclasses

obtain a classifier which captures the implicit rules and regularities determining the degree of association among particular combinations of feature values and the tagging of a positive or negative
examples. With the help of this classifier, we can then decide for
new, previously unseen pairs of classes or, more specifically, their
vector-based representations whether those should be labeled as
disjoint or not disjoint.

In the following, we give some examples of the features used
by the current version of our framework called LeDA (Learn-
ing Disjointness Axioms), and outline differences to previous work
where necessary. For further details the reader is referred to our
paper [9].

4.1.1. Logical features

Logical features rely on the structural information provided by
the ontology. Their values can reflect the taxonomic relationships
between classes, or the way they are used in object property
definitions, for example. Table 3 shows the logical features used
by the current version of LeDA. Note that we omit features here
whose values cannot be computed from the schema alone, as we
later want to compare the bottom-up approach described in the
previous section with a pure top-down approach that requires no
instance data.

In description logics, two classes are disjoint iff the intersection
of their sets of instances, also known as their taxonomic overlap,
must be empty.2 Hence, the taxonomic overlap of two classes
is considered not empty as long as there could be common
individuals within the domain that is modeled by the ontology.
foverlapc measures the taxonomic overlap of two classes in terms
of common subclasses.
It is based on the Jaccard similarity
coefficient [19].3 C denotes the set of all the atomic classes in the
ontology, while I refers to the set of named individuals.

(C1, C2) = |{C  C|C  C1  C2}|
|{C  C|C  C1  C2}| .

This feature is complemented by fsub, which can be considered
a particular case of taxonomic overlap, while at the same time
capturing negative information such as class complements or
already existing disjointness contained in the ontology. The value
of fsub for any pair of classes C1 and C2 equals 1 if C1  C2 or C2  C1
is entailed by the ontology, and 0 if C1  C2 can be inferred.
Otherwise, the feature value remains undefined.

fsub(C1, C2) =

undefined

O |	 C1  C2  C2  C1
O |	 C1  C2
otherwise.

association rules A  B and B  A are grouped together. For
each pair of rules, the minimum confidence level of both rules is
compared to cmin and the corresponding disjointness axioms A 
B is generated if this confidence level is greater or equal to cmin.
This prevents our approach from generating disjointness axioms
between pairs of classes which actually subsume each other. For
example, consider data for classes C and D with |D|  |C| and
the extension of D is a superset of the extension of C. Obviously,
this data would present a typical case of the subsumption relation
C  D, but association rule mining on this dataset would result in a
rule D  C with a confidence close to 1 and a rule C  D with
a confidence of 0. Thus, when only considering one direction of the
rule pair, we would generate a wrong disjointness axiom between
the classes C and D with a non-zero confidence though the data
does not actually support this statement.

Hence, we generate disjointness axioms for each grouped
pair of rules and assign the minimum confidence value of the
corresponding rules to the axiom.

3.3. Previous experiments

In previous publications, we already presented our method of
learning disjointness axioms and also evaluated these methods.
First [15], we extended the basic approach of statistical schema
induction [8] to also cover the generation of disjoint classes axioms.
This has been done using the method of negative association rule
mining as described above as well as a different implementation
of association rule mining capable of directly creating negative
association rules. In addition, correlation coefficients between
classes were evaluated as a way of discovering disjoint classes.
As previously stated, during these experiments the approach
described in this section showed the most beneficial compromise
between precision and recall.

Further pushing towards the unsupervised creation of expressive schemas from data, we presented an extension to our methods which covers most of OWL 2 [18]. Particularly focused on the
generation of property axioms, we examined the possibility to create property disjointness from data and evaluated our results by
means of an expert evaluation as well as by using a crowd-sourced
evaluation approach.

When not enough instance data is available that would facilitate
the application of purely extensional or inductive methods such
as the one described in Section 3, methods for learning class disjointness axioms must rely on schema-level knowledge. It is then
possible to consider, for example, the labels or comments associated with classes or their logical definitions. Depending on the size
and richness of the schema, additional background knowledge may
be required. In the following, we present an approach to acquiring
class disjointness axioms from heterogeneous resources, such as
natural language text and a machine-readable dictionary.

4.1. Approach

Our approach to learning disjointness from heterogeneous resources is based on supervised machine learning. It therefore requires training data, i.e., pairs of classes tagged as either disjoint
(positive example) or not disjoint (negative example), provided by
a human ontology engineer. Each pair of classes is described by a
vector of feature values denoting, for example, the lexical similarity
of the two class labels or the closeness of these classes in the taxonomic hierarchy of the schema. From this type of training data, we

4. Learning disjointness from heterogeneous resources

foverlapc

(1)

(2)

The semantic distance (fdist) between two classes C1 and C2 is the
minimum length of a path consisting of subsumption relationships
between atomic classes that connects C1 and C2, or undefined if no
such path exists.

2 Note that the individuals of a class do not necessarily have to be explicitly
named in the ontology.
3 jaccard(A, B) = |A  B|/|A  B|.

Table 4
Overview of lexical features.

Feature
fqgrams
fjarowinkler
flevenshtein
fhyponymy
fwnpp
fwnpath
fwnjc
fwnlc
fwnlesk
fwnlin
fwnresnik
fwnwp

4.1.2. Lexical features

Description
Label similarity (QGrams)
Label similarity (JaroWinkler)
Label similarity (Levenshtein)
Hyponymy according to WordNet
Label similarity (PatwardhanPedersen)
Label similarity (Path)
Label similarity (JiangConrath)
Label similarity (LeacockChodorow)
Label similarity (Lesk)
Label similarity (Lin)
Label similarity (Resnik)
Label similarity (WuPalmer)

These features are based on lexical information associated with
the (atomic) classes of a given ontology. LeDA only uses the
rdfs:label properties and local names of the classes, but in general lexical information could also stem from annotation properties
such as comments, which provide natural language descriptions of
the classes. An overview of the lexical features considered by LeDA
is given by Table 4.

The relationship between two classes is in many cases reflected
by their labels, especially when their labels share a common prefix or postfix. This is because the right-most constituent of an
English noun phrase4 can be assumed to be the lexical head that determines the syntactic category and usually indicates the semantic
type of the noun phrase. A common prefix, on the other hand, often
represents a nominal or attribute adjunct which describes some semantic characteristics of the noun phrase referent.

Therefore, in order to compute the string similarity of the two

class labels, we used three different similarity measures:
 Levenshtein. The Levenshtein distance [20] measures the edit
distance of two strings, i.e., it returns the number of insertion,
deletion and substitution operations that are required to
transform one string into the other.
 QGrams. The basic idea underlying the QGrams metric is that
two strings have a small edit distance if they have many
q-grams in common. A q-gram [21] is a substring of the original
string with length q. Our implementation of the QGrams feature
is based on the SimMetrics library,5 with q = 3.
 JaroWinkler. The JaroWinkler distance is a variant of the Jaro
distance metric taking into account the number of matching
characters, the number of transpositions and the length of a
common prefix.

Moreover, hyponymy relationships between class labels were
used as indicators for possible class subsumption (fhyponymy). In
order to compute the lexical similarity of two classes (their labels,
to be precise), we used the WordNet similarity API by Patwardhan
and Pedersen6 [22] together with WordNet [23] (version 3.1).

4.1.3. Corpus-based features

In addition to the logical and lexical features based on the
ontology or WordNet as described above, we considered several
features that take into account background knowledge in the form
of an automatically acquired text corpus (see Table 5).

Table 5
Overview of corpus-based features, ontology-based measures applied to learned
ontology.
Feature
f c
ccosine
f c
dist
f c
sub
f c
overlapc
f c
overlapi

Description
Cosine distance between Wikipedia articles
Semantic distance (learned ontology)
Subsumption (learned ontology)
Taxonomic overlap w.r.t. subclasses
Taxonomic overlap w.r.t. instances

By querying the English Wikipedia7 for each class label,8
we obtained an initial set of articles, some of which were
disambiguation pages. We followed all content links and applied
a simple word sense disambiguation method in order to obtain
the most relevant article for each class: For each class label, we
considered the article to be most relevant, which had, relative to its
length, the highest terminological overlap with all of the labels
used in the ontology. The resulting corpus of Wikipedia articles
was fed into Text2Onto [24] to generate an additional background
ontology for PROTON and the DBpedia ontology. Each of the two
ontologies consists of classes, individuals, subsumption and class
membership axioms.

Based on this newly acquired background knowledge, we
define the features subsumption (f c
dist), and
taxonomic overlap of subclasses (f c
overlapc ), which directly correspond
In addition, we use
to their counterparts listed by Table 3.
individuals (f c
cosine),
which we computed by comparing the Wikipedia article associated
with the two classes.

overlapi) as well as document-based similarity (f c

sub), semantic distance (f c

(C1, C2) = |{i  I|C1(i)  C2(i)}|
|{i  I|C1(i)  C2(i)}| .

f c
overlapi

(3)

4.2. Previous experiments

In previous experiments, we demonstrated the principle feasibility of automatically generating disjointness axioms, and outlined possible applications in the field of ontology engineering and
evaluation. Early work includes a prototypical extension to the
Text2Onto ontology learning framework [25], which implements
a simple heuristic for enriching ontologies with class disjointness.
Each of the learned axioms was associated with an uncertainty
value, and we proposed to automatically resolve any inconsistencies resulting from errors made by the ontology learning approach.
Later, we showed that learned disjointness axioms can also help
to improve the quality of ontology mappings [26]. Those experiments were made with the LeDA (Learning Disjointness Axioms)
framework [4], which differs from Text2Onto in so far as it relies
on multiple, lexical and logical indicators for class disjointness.

5. Gold standard of class disjointness axioms

In this section, we report on a comparative evaluation of the
previously described approaches. We used the GoldMiner frame-
work, which implements the approach presented in Section 3
as well as several configurations of LeDA (cf. Section 4), in order to enrich the DBpedia ontology with class disjointness ax-
ioms. The quality of the results was then assessed by comparing
them to a manually created class disjointness gold standard. We

4 At least in English, people seem to prefer noun phrases for labeling classes.
5 http://www.dcs.shef.ac.uk/~sam/simmetrics.html.
6 http://www.d.umn.edu/~tpederse/similarity.html.

7 http://en.wikipedia.org (2013-02-04).
8 Labels written as one word, though consisting of nominal compounds or other
types of complex noun phrases had to be normalized with respect to Wikipedia
naming conventions.

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

first describe DBpedia, the dataset we evaluated our approaches
on (Section 5.1). Afterwards, in Section 5.2, we give details on the
creation of the gold standard, including the methodology and some
insights gained during the creation. Then we explain the settings
and configurations used in our tools to perform the experiments
(cf. 6) before finally evaluating these results using the created gold
standard.

5.1. Dataset

We tested our approaches on the DBpedia dataset.9 DBpedia
is a Linked Data dataset constituting the most central point of
the Linked Data cloud [27]. The most recent version of DBpedia
(DBpedia 2014) contains 4.22 million typed instances which have
additional data assigned by means of RDF properties. The older
DBpedia 3.7 dataset, on which we are performing our experiments
in the following, contains 1.83 million typed instances. Being
of such huge size is mostly owed to the fact that DBpedia is
extracted from Wikipedia where each article becomes a DBpedia
instance.

The DBpedia extraction process specifically utilizes the info
boxes contained in many Wikipedia articles (see Fig. 3, for
example). These info boxes show information about the entity
described in the corresponding Wikipedia article in a structured
way, i.e., an info box has different fields each with a defined
meaning. Info boxes are created by the Wikipedia authors using
info box templates. These templates are specific for groups of
entities, e.g., there is an info box template for persons which
contains information like birth date and citizenship.

In DBpedia, each Wikipedia article with an info box is represented by an RDF instance. By employing the info box template
types, these instances get RDF types assigned. For this purpose,
the DBpedia ontology10 has been created. It consists of 319 classes
and 1543 properties for DBpedia 3.7. The different types of info
box templates are manually mapped to matching ontology classes.
Instances extracted from an article which contains a certain info
box are assigned to the ontology classes the info box template is
mapped to. Furthermore, the info box properties are mapped to
the ontology properties.

The DBpedia ontology only contains subsumption relations
for classes and properties as well as some domain and range
assignments for properties, but no disjointness axioms. Thus, it
is well-suited as an evaluation dataset for generating disjointness
axioms using our approaches.

5.2. Methodology

In order to assess the results of our approaches on the DBpedia
dataset and the corresponding ontology, we manually created a
gold standard of class disjointness in the DBpedia ontology. In the
following, we first describe the methodology we applied during the
creation. Afterwards we will give an overview of the resulting gold
standard.

To create a gold standard, we let three ontology engineers
extend the DBpedia ontology by disjointness axioms. The goal was
a completely axiomatized ontology, i.e., disjointness axioms should
be added such that each pair of classes in the ontology is disjoint if
and only if there is a disjointness axiom between both classes in the
gold standard ontology. The methodology the ontology engineers
applied when creating the gold standard is sketched in Fig. 4 and
described in more detail below.

Fig. 3. Wikipedia Info Box about Arnold Schwarzenegger.

Each engineer was provided with a cleaned version of the
DBpedia ontology.11 During the cleaning step, we removed all
classes from the ontology which did not belong to the DBpedia
ontology namespace,12 the RDF or RDF Schema namespace or to
the OWL namespace. Furthermore, we filtered out all non-class
axioms.

Based on the filtered ontology, the engineers started adding
disjointness axioms using Protege13 in a top-down manner, first
assessing the disjointness of the top-most classes to their siblings
and repeating the same on the next level of concepts. This is similar
to the disjoint siblings assumption introduced by Schlobach [28]

9 http://dbpedia.org.
10 http://dbpedia.org/Ontology.

11 DBpedia ontology version 3.7.
12 http://dbpedia.org/ontology/.
13 http://protege.stanford.edu.

Table 6
Basic gold standard statistics.

Pall

0 votes
1 vote
2 votes
3 votes

Pdirect

Pindirect

Table 7
Gold standard inter annotator agreement.

Observed
Fleiss

Pall

Pdirect

Pindirect

Fig. 4. Gold standard creation methodology.

with the exception, that only manually assessed disjointness
axioms are added to the ontology. The next step in the process, was
to check for each possible pair of classes in the ontology whether
they are disjoint and create a list of concept pairs with their
disjointness state. This materialization step helped the engineers
to see the actual implications of the disjointness axioms added in
the previous step. In the generated list, they were able to directly
specify whether a disjointness axiom should be added between
both classes or not. Based on this list, additional disjointness
axioms were then added to the ontology. After this step, the whole
process started again by materializing the resulting disjointness
axioms to allow the engineers to check the results again and correct
class pairs erroneously set to be disjoint. This process was repeated
until each engineer was satisfied. For all three annotators, this
was achieved after three full cycles through the methodology as
outlined above.

Overall, we think this workflow has several advantages over
raw pair-based approaches as used by Volker et al. [9]. First, by also
including the taxonomy the engineers were able to consider the
subclass relations when determining whether to set a class pair as
disjoint. In addition, by having direct access to the ontology itself,
the engineers could also access the comments and additional labels
contained in the ontology which might clarify the meaning of
specific classes further. Second, by including a materialization step
in the workflow, inferred disjointness axioms are made apparent to
the engineers. We think these aspects considerably outweigh the
problem of having to rely on the subsumption hierarchy potentially
being faulty itself.

5.3. Analysis

After completing the gold standard, we took a closer look at the
result. Basic statistics about the distribution of votes on axioms are
given in Table 6.

Obviously, since the gold standard was created by three
engineers, the number of votes for disjointness ranges from zero,
i.e., all engineers consider the two concepts not to be disjoint, to
three, i.e., all engineers consider the given pair to be disjoint. The

Fig. 5.

Inter annotator agreement for subtrees of selected classes.

distribution of votes has been computed for three different sets of
class pairs. The set Pall contains all possible pairs of classes in the
ontology (e.g., {A, B}). Pdirect contains all pairs {A, B} where A and B
are direct siblings, i.e., have a common direct superclass. The third
set of pairs contains all pairs {A, B} for which neither A  B nor
B  A holds. This set is called Pindirect in the following.
As we can see from these statistics, all annotators agreed on
the disjointness or non-disjointness for the majority of pairs.
Furthermore, the agreement is considerably lower for the direct
siblings case compared to the all pairs case. This is also apparent
from the values of the inter annotator agreement values provided
in Table 7.

Since these values are computed for the whole ontology they
do not provide us with information as to whether specific class
pairs are harder to assess than others when annotating them. To
get more insights into the complexity of specific class pairs, we
also computed the inter-annotator agreement on subtrees of the
ontology instead of the whole hierarchy tree. Some results of this
analysis are depicted in Fig. 5. In this figure, the given kappa value
shows the agreement reached for class pairs where both classes are
in the subtree spanned by the concept shown in the diagram.

As we can see in this diagram, the inter-annotator agreements
for different subtrees differ widely. High agreements are mostly
achieved for subtrees containing classes which can be separated
accurately,
like the species subtree consisting of a biological
classifications. Since biological
the
disjointness of the classes representing the biological classes is

taxonomies are disjoint,

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

Table 8
Majority and disjoint sibling baselines.

Majority
dbpedia50

dbpedia100

Disjoint Siblings
dbpedia50

dbpedia100

Precision
Recall
F-Measure

in turn makes it very hard even for ontology engineers to create a
coherent disjointness axiomatization based on the ontology. But
even when leaving these problems aside, we also discovered a
number of problems which are already known from other ontology
modeling contexts not focused on the modeling of disjointness
axioms.

6. Experiments and results

clearly defined. The low agreement for the subtree of Person is
also striking. Further analysis of this agreement showed that the
agreement without chance correction is at 0.99 here. This is caused
by almost all subclasses being marked as pairwise non-disjoint
by all annotators. Only for few pairs there was disagreement,
however, due to the high ratio of disjoint pairs and the chance
correction, the resulting -values were extremely low.

We also created a list of all cases of disagreements between
the annotators, i.e., all pairs for which we only had one or two
votes for disjointness. This list contained 718 class pairs. To find
out more about the reasons for these disagreements, we passed
the resulting list back to the annotators and let them discuss
the individual cases. From the notes taken during the annotators
discussion, we tried to categorize the disagreements by the aspect
leading to the disagreement. This categorization is similar to the
categorization done by [9]. In the following, we describe the most
common categories in more detail.

Many disagreements were caused by problems finding the
right interpretation of the concepts in the ontology. Most of
the time, the contents of the ontology itself led to confusion,
e.g., when the RailwayLine class is subsumed by the class
ArchitecturalStructure but at the same time the comment
of the former states that it should not be mistaken for railway
track which is obviously the interpretation leading to the sub-
sumption. Another similar case is the class Infrastructure
which should be named InfrastructureAsset since it only
describes parts of the infrastructure and not the (rather ab-
stract) infrastructure itself. Similarly, for spatial concepts like in
AdministrativeRegion or ProtectedArea, the annotators
disagreed in their judgments because of the missing additional
documentation further describing the intended interpretation.

Another common problem, which we discovered, was similar
to the common problem about properties changing with time
mentioned above. It occurred for classes which are assigned to an
instance because it has a specific property but where the instance
might lose this property in the course of time. During the creation
of the gold standard, one example for this was the class Theater
which is a subclass of Building. Some annotators remarked
that this class could hardly be disjoint to Museum which is also
subsumed by Building since a theater that is no longer in use
could be used as a museum. This problem seems to be close to the
well-known aspect of rigidity as also discussed by the OntoClean
approach [29] where the modeling of food classes is discussed
which also posed some problems to our annotators.

Further disagreements were caused by concepts where the
intensions were disjoint while the extensions were equal. An
example we found in the ontology for this case are the concepts
Band and MilitaryUnit. Some annotators argued towards
disjointness while others pointed out that there are military bands
being established as military units for themselves.

All in all, from the discussions during this phase, we found out
that the main problem in many cases might come from the fact
that the ontology was created in a crowd-sourced scenario. Thus,
the very limited documentation of the ontology classes and the
overall modeling strategy together with the fact that many creators
of the ontology are probably not experienced ontology engineers,
seems to have led to a mixture of different modeling approaches
which maybe even contradict each other to a certain degree. This

Based on the gold standard described in the previous chapter,
we evaluated the results generated by both GoldMiner14 and
LeDA.15 Each tool was used to create a set of disjointness axioms
for the concepts contained in the DBpedia ontology.16 While
GoldMiner only used the actual DBpedia instance data to generate
disjointness axioms, LeDA used several data sources for computing
the classification features as described in Chapter 4. Since both
approaches consist of several steps and thus are hardly comparable
on a runtime level, we do not provide numbers on the runtime
here.

LeDA uses a supervised learning approach, which means we had
to train it first. We used an ADTree [30] classifier as implemented
by the Weka toolkit [31] which performed well
in previous
experiments. The training was done on the PROTON ontology
(PROTo ONtology)17 using the gold standard created by Volker
et al. [9]. For creating this gold standard, the different PROTON
modules were merged into a single ontology. The PROTON gold
standard exists in two variants to which we will refer as proton50
and proton100. proton50 contains all pairs for which at least 50%
of the annotators considered them being disjoint, equivalently,
proton100 contains all pairs considered to be disjoint by all
annotators. For evaluation purposes, we created the datasets
dbpedia50 and dbpedia100 based on the votes of the gold standard
annotators containing only decisions for which at least 50% or all
annotators agreed on, respectively.

GoldMiner used a local mirror of the DBpedia dataset to retrieve
the relevant instances and generate disjointness axioms. The
results were not filtered by a threshold, since symmetry filtering
as described in Section 3 led to set of axioms which was very clean
and uniform regarding the confidence values. The only constraint
applied during the association rule mining phase was a support
threshold of 2, i.e., only rules were generated which matched for
at least two transactions in the transaction tables.

For comparison purposes, we provide two values for the disjointness axiom generation task which can serve as rough base-
lines. First, we determined the class the majority of classifications
in the gold standard belongs to. In our cases, the majority of class
pairs is annotated as being disjoint. Given this, we were able to
compute the precision and recall values when just setting all class
pairs to disjoint in the ontology. This leads to the precision and recall values given in the left part of Table 8. For the second baseline
approach, we applied the disjoint siblings assumption [28]. Thus,
we set all sibling classes in the ontology to disjoint which resulted
in the precision and recall values shown in right part of Table 8.

Both baselines would not be suitable for being directly used
on an ontology for generating disjointness axioms. Obviously, the
main problem of both baselines is that they do not adapt to the
actual considered pairs. For example, in subtrees like the person

14 The latest version of GoldMiner is available from https://github.com/
dfleischhacker/goldminer.
15 The latest version of LeDA is available from https://code.google.com/p/leda-
project/.
16 The experiments were performed on the DBpedia data version 3.7 with the
corresponding DBpedia ontology version 3.7.
17 http://www.ontotext.com/proton-ontology.

Table 9
Results for GoldMiner on Pall.

Precision
Recall
F-Measure

Table 10
Results for GoldMiner on Pdirect.

Precision
Recall
F-Measure

Table 11
Results for GoldMiner on Pindirect.

Precision
Recall
F-Measure

dbpedia50

dbpedia50

dbpedia50

dbpedia100

dbpedia100

dbpedia100

subtree which might yield a distribution of disjoint to non-disjoint
pairs different from other parts of the ontology, their performance
suffers greatly. In addition, the majority baseline requires statistics
on the actual distribution of disjoint pairs to non-disjoint pairs for
which a manual evaluation of the pairs has to be performed.

6.1. GoldMiner

The results from evaluating the disjointness axioms generated
by GoldMiner on all class pairs are given in Table 9. Here we see that
all class disjointness axioms were discovered and a high precision
was achieved for the generated axioms. Comparing the evaluation
results on both gold standard datasets, dbpedia50 and dbpedia100,
we see a slight increase in the precision for the latter.

By considering the results for the direct sibling pair set
(Table 10), the overall precision is much lower than for all
pairs. This is to be expected since the problem of determining
disjointness between sibling classes is much harder than for an
arbitrary pair of classes. However, we see that the precision is
considerably higher when evaluating on the dbpedia100 dataset
than when evaluating on the dbpedia50 dataset. This shows that
GoldMiner performs better for those pairs of classes for which all
annotators agreed compared to those where disagreement was
found. Thus, GoldMiner seems to have problems determining the
disjointness of classes, especially when the decision is not fully
clear to humans either. This is also visible in the set of all pairs but
to a much smaller degree. The evaluation on Pindirect does not reveal
significant additional information since it is in line with the overall
results (see Table 11).

As described above, we only evaluated a single confidence
threshold in GoldMiner, since the actual distribution of confidence
value is extremely concentrated towards a value of 1.0, i.e., the
confidence values of all generated axioms are higher than 0.99.
This is caused by the symmetry handling of GoldMiner, filtering
out large amounts of lower confidence axioms, combined with the
property of the DBpedia ontology that many subclass relations are
materialized on the instance level.

6.2. LeDA

As we saw in the previous section, the results of GoldMiner
are sufficient overall, but for the sibling case we are only able to
predict about two thirds of the disjointness axioms correctly. This
is mostly caused by GoldMiners reliance on instance data which

proton50

Table 12
Results of ten-fold-cross evaluation on datasets. Precision, recall and F-measure
values determined for positive disjointness assignments.
Lexical

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

Corpus

Logical

All

dbpedia100

dbpedia50

proton100

Table 13
Results for LeDA trained on the proton50 dataset. Precision, recall and F-measure
values determined for positive disjointness assignments.
Lexical

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

Corpus

Logical

All

dbpedia100

dbpedia50

leads to results suffering from missing or insufficient data. Thus, we
also evaluated the approach implemented by the LeDA tool which
considers external data and hence variations in one dataset can be
assumed to have less influence on the actual results.

For LeDA, we performed both a ten-fold-cross evaluation on
the PROTON as well as the DBpedia datasets and the already
described case where LeDA was trained on PROTON and evaluated
on DBpedia. Additionally, we evaluated the results using different
feature sets to get a closer look at how differently the feature sets
behave.

The ten-fold-cross evaluation on PROTON is similar to the one

done by Volker et al. [9]. The results are shown in Table 12.

As we see from the results here, the precision and the recall for
finding disjointness axioms are fairly high for this setting. The best
overall performing features in these experiments are the logical
features that are based on characteristics of the ontology itself.

After testing the performance of LeDA being trained on a part of
the same dataset it is evaluated on, we performed the experiments
for training on one of the PROTON datasets and evaluating on the
DBpedia dataset. The results of these experiments are shown in
Table 13 for training on proton50 and in Table 14 for training on
proton100.

The most striking results are visible when trained on the
whole feature set. In these cases, the recall and consequently the
F-measure values are low even when the precision is high. We
did some more examination in this direction, also having a look
at the low recall values for the logical features trained on proton50.
Finally, we found the root case for these results to be a property
of the employed DBpedia ontology. The version of the DBpedia
ontology used by us contained direct subclass statements to the
top-level OWL concept, i.e., owl:Thing, for all classes in the
ontology. Having this statement explicitly given in the ontology,
the ontology distance feature described in Chapter 4 always

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

Table 14
Results for LeDA trained on the proton100 dataset. Precision, recall and F-measure
values determined for positive disjointness assignments.
Lexical

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

Corpus

Logical

All

dbpedia100

dbpedia50

determined the same distance value for all classes in the ontology
since the connection using owl:Thing as the intermediate
concept always lead to the lowest distance between two concepts.
Thus, this feature did not result in any gain in knowledge for the
DBpedia ontology. At the same time, the features information gain
was one of the highest of all features for the PROTON datasets
which led to learning a classifier prioritizing the ontology distance
feature. Therefore, the classifier learned on the PROTON datasets
and used on a DBpedia dataset resulted in the low recall values.
By excluding the ontology distance feature from the feature set,
the F-measure is raised to 0.96 (Precision 0.95, Recall 0.98). A
similar peculiarity is visible in Table 14. Here, we see that all three
feature categories on their own lead to good results in detecting
disjointness between classes. However, as soon as all three feature
sets are used at the same time, the recall breaks down leading to a
very low F-measure.

6.2.1. Transfer learning

This shows that the features employed by LeDA are highly
dependent on the data used for training and classification. Because
of this, it is not possible to select a specific set of features as being
the best for all cases. Rather, it is necessary to select the feature
set specifically for the combination of training and classification
datasets. Methods for doing this are subject to the area of transfer
learning.

Transfer learning in general deals with the question of how
to use (supervised) machine learning techniques when training
and classification dataset have significant differences regarding
some of their characteristics like their value distributions or feature
spaces. Without further adjustments, classification methods will
perform poorly when applied to such dissimilar datasets. Using
transfer learning methods, it is possible to compensate for some
of this low performance by adapting the datasets to make their
features more similar. Depending on the nature of the dissimilarity
in the datasets, like being of different domains or being used for
different tasks, a number of transfer learning approaches have been
explored which have been surveyed by Pan and Yang [32]. They
also provide a categorization of transfer learning tasks depending
on the availability of labeled data in the source and target domain
as well as the similarity of the considered tasks. According to
this categorization, the transfer learning scenario that we are
examining in this work belongs to the category of Domain Adaption.
For ontology learning approaches based on supervised machine
learning, transfer learning is of great importance. There are many
aspects in which the modeling of ontologies could differ. While
some of these discrepancies might be addressable by normalizing
value ranges, some features might carry more fundamental
deviations. For example, including inferred relations between
classes as in the DBpedia ontologies used in our experiments leads
to highly different distributions of feature values for the ontology
similarity feature. One feature set shows high value variability
whereas the corresponding feature values in the other feature
set are practically constant. Furthermore, different languages used

Table 15
Ranking of feature distribution divergence between proton100 and dbpedia100.

Feature name
f c
sub
f c
overlapc
fhyponymy
fsub
foverlapc
fjarowinkler
fqgrams
fwnwp
flevenshtein
f c
overlapi
fwnlin
fwnpp
fwnresnik
fwnlc
f c
dist
fwnpath
fwnjc
fwnlesk
fdist
fpropobj

Divergence

DKL(P  Q ) =

ln

 P(i)


Q (i)

P(i).

for labeling classes or different levels of abstraction used during
modeling could lead to similar problems.

Given enough knowledge about both the training and the
classification ontology, including their additional data, it would
be possible to manually select the most promising features while
disabling those expected to perform badly. However, in most
cases, this needs further examination of the classification ontology,
which might not always be possible. Since transfer learning works
on the automatically generated feature data only, it enables the
adaption of the feature sets without manual intervention and
therefore makes the approach more robust against modeling
differences.

As also mentioned by Uguroglu et al. [33], one possible solution for selecting feature sets is ranking the features by using their
KullbackLeibler divergence [34] across the two datasets and use,
e.g., the top 10 features of this ranking. Though this method is
not used in general since being computationally too expensive,
for our limited size datasets, this is no large hindrance. The KullbackLeibler divergence is the divergence between two probability
distributions, for discrete distributions it is defined by

More specifically, we used the symmetrized divergence (DKL(C 
T ) + DKL(T  C) for C resp. T being the dataset distributions). The
dataset distributions are computed on the full feature set after normalizing the values to a common value interval. To prevent problems with values not occurring in one of both datasets and thus
having a value count of 0, we use additive smoothing. Table 15
shows a ranking of feature distribution divergences between the
dataset proton100 and dbpedia100 based on which we would take
the ten top-ranked features for this dataset combination.

6.2.2. Improved results

Using the top 10 ranked features for each dataset combination,
we exclude (amongst others) the ontology distance feature which
is one of the lowest ranked features. The results based on this
feature set are shown in Table 16. The additional feature selection
helps to increase the F-measure compared to the results gained by
using the full feature set. Comparing to the other feature sets, it
leads to an F-measure which performs well but not the best. The KL
divergence based feature selection helps to exclude features which
lead to extremely bad performance but it does not seem to be able

Table 16
Results for LeDA using the ten features top ranked using KL divergence between
datasets on Pall. Precision, recall and F-measure values determined for positive
disjointness assignments.

dbpedia50

dbpedia100

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

proton50

proton100

Table 17
Results for LeDA using the ten features top ranked using KL divergence between
datasets on Pdirect. Precision, recall and F-measure values determined for positive
disjointness assignments.

dbpedia50

dbpedia100

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

proton50

proton100

Table 18
Results for LeDA using the ten features top ranked using KL divergence between
datasets on Pindirect. Precision, recall and F-measure values determined for positive
disjointness assignments.

dbpedia50

dbpedia100

Precision
Recall
F-Measure
Accuracy overall
Precision
Recall
F-Measure
Accuracy overall

proton50

proton100

to find the best possible feature set. Nevertheless, according to our
results, it provides a proper way to automatically pick a suitable
feature set.

As seen in the evaluation of the created gold standard, the
decision whether two classes are disjoint or not is harder for sibling
classes than for an arbitrary class pair, Table 17 shows the precision
and recall values for the disjointness on direct sibling pairs resp. in
Table 18 for the all sibling pair set. As expected, the quality of the
generated disjointness axioms for direct siblings is clearly worse
than the results on all pairs. The values for the all sibling pair set
are much higher than for the direct sibling pair set which hints
that LeDA is not always getting the disjointness between two direct
siblings correctly but at one or more levels beneath those classes
in the ontology, the subclasses of one class are recognized as being
disjoint to the other class.

6.3. Discussion

Upon closer inspection, the comparison of the generated axioms
with the DBpedia gold standard reveals several weaknesses of the
approaches presented in Sections 3 and 4, respectively. Some of
these are discussed in the following.

6.3.1. GoldMiner vs. experts

In the results of the experiments, we particularly see the great
dependence of GoldMiner on the instance data. Since it only infers

typical patterns out of the available data, all problems that were
discovered during the evaluation can be directly traced back to
particularities in the dataset.
Problem 1. The instance data is incomplete.

Whenever a knowledge base is incomplete with regard to class
membership assertions, classes which are actually not disjoint are
likely to have non-overlapping extensions. This problem occurs
frequently in the case of DBpedia, where each instance is assigned
to the class which corresponds to its infobox type, and it thus
never happens that, e.g., sibling classes are asserted for the same
instance. Examples of classes with non-overlapping extensions in
DBpedia which are erroneously considered disjoint by GoldMiner
are dbo:MusicalWork and dbo:Musical, dbo:Artist and
dbo:Model, dbo:Scientist and dbo:Wrestler.
Problem 2. Classes become incoherent.

In the gold standard,

the classes dbo:Building and
dbo:Organization have been set as disjoint by the ontology
engineers. However, in the original DBpedia ontology the class
dbo:Library is defined as a subclass of both dbo:Building and
dbo:Organization. Thus, dbo:Library is incoherent in the
gold standard ontology and thus set as disjoint to all other classes
of the ontology by means of logical inference. In contrast, the
DBpedia dataset itself has several instances of this class that are
also assigned to both superclasses which leads GoldMiner to not
learning disjointness between both classes and hence to disagree
with the gold standard. All in all, this difference illustrates to a
degree the conflict between theoretical correctness and practical
applicability. While the gold standard was created having certain
ideas of how to model specific scenarios in the most logically and
theoretically precise way, the GoldMiner-generated ontology just
tries to formalize the typical patterns of the data thus resulting in
an ontology more directly applicable by non-experts.

6.3.2. LeDA vs. experts

The results of LeDA are distinguished by a great number of false
positives. Given a classification model that is based on the top-10
features (see Table 15), LeDA makes 38,903 predictions. 2037 of
these differ from the gold standard (100% agreement, see Table 6),
and 1935 of the errors are false positives.
Problem 1. Disjoint classes with similar labels are classified as non-
disjoint.

One of the most reliable indicators of non-disjointness is
explicit or inferred subsumption, a special case of taxonomic overlap [9]. As argued, e.g., by Vojtech et al. [35], class subsumption is typically reflected by hyponymy relations between class
labels. In particular, we can observe that the names of classes
and their subclasses (e.g. dbo:Work and dbo:MusicalWork)
often share a common head noun or, more generally speak-
ing, a common suffix. Since this also holds for the PROTON
ontology, which we used for training the classifier, it is not astonishing that not only class subsumption (fsub), but also label similarity (fqgrams, fjarowinkler and flevenshtein) and hyponymy according
to WordNet (fhyponymy) are among the highest ranked features for
this dataset. This leads to undesired classification results when semantically dissimilar classes have similar labels like, for example,
dbo:Planet and dbo:Plant, ordbo:AutomobileEngine and
dbo:Automobile.
Problem 2. Ambiguous class labels lead to incorrect classification
results.

Our classification-based approach to learning disjointness is
intensional or top-down in so far as it does not presume
the existence of knowledge based instances. However, the fact
that it uses no elaborate word sense disambiguation methods
to determine the correct sense of a class label can cause
classification errors. For example, dbo:RadioStation is not

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

actually a subclass of dbo:Station as suggested by LeDA,
because dbo:Station does not refer to a broadcasting station,
but to a bus station.

6.3.3. GoldMiner vs. LeDA

The main difference between GoldMiner and LeDA lies in their
requirements regarding their input data. LeDA employs different
types of data from the actual input dataset as well as data from
external data sources like used in the corpus-based features. In
contrast, GoldMiner only relies on instances contained in the input
dataset. Thus, in cases where the dataset does not contain any
instances, e.g., the PROTON ontology, GoldMiner cannot generate
any disjointness axioms. Though LeDA also has a feature relying
on instances in its instance overlap feature, it can forgo the usage
of instances due to the possibility of using other features instead.
However, LeDA being a supervised system, requires annotated
data for training that is at least similar to some degree to the
input dataset. Furthermore, some of the external datasets need
some manual effort before they can be provided to LeDA as
input. Nevertheless, for instance-less datasets, GoldMiner has no
possibility to come up with reasonable results which puts LeDA
first in such cases.

These differences also materialize in the results of our
experiments. For the main part of the class pairs, both LeDA and
GoldMiner classify the same way. Only for a total number of 1555
class pairs GoldMiner and LeDA assigned different disjointness
states. The vast majority (1367) of these instances was disjoint
according to the gold standard while only 188 were manually
annotated as not being disjoint. LeDA only classified correctly in
163 of these cases while GoldMiner reached the correct decision in
1392 cases.

A problem typical for LeDA is demonstrated by the class pair
dbo:MusicalArtist and Musical for which non-disjointness
was inferred. This was most likely caused by the high similarity
of the class labels, which even share the substring Musical,
and a high degree of overlap between the external corpora
describing both concepts. Combined with the unavailability of
the instance overlap feature, this led to many features delivering
positive evidence for non-disjointness in the classification process.
This shows that LeDA has problems to distinguish relatedness
of concepts and semantic similarity. Though this should be
compensated by the inclusion of the WordNet and ontology-based
features, for certain cases, the influence of the other features seems
to outweigh those. This problem also appears for the class pair
dbo:College and dbo:University that was wrongly classified
as being non-disjoint supposedly due to being very related and also
similar based on their purpose.

A typical problem of GoldMiner is directly induced by only relying on instances. In all cases where GoldMiner wrongly classified a
pair of instances as disjoint, this can be directly traced back to the
dataset not containing the class assertions required to derive the
non-disjointness. For example, the classes dbo:Journalist and
dbo:Writer that were classified as disjoint by the human annotators but not by GoldMiner, do not have any common instances
in the DBpedia dataset. Thus, GoldMiner derives a disjointness axiom due to the lack of counter-examples. This hints to a conflict of
GoldMiners statistical approach, which to some degree assumes
the dataset to be complete with regard to the knowledge to deduce,
and the open world assumption inhered in Linked Data. However,
the well overall performance shown in the experiments illustrate
that this problem only gets relevant for a few corner cases given
that the full dataset is large enough. For large datasets the chances
for non-disjoint class pairs to accidentally not overlap decrease and
thus the influence of such cases on the final result gets smaller.

7. Related work

A simple, yet efficient heuristic for introducing disjointness axioms into existing ontologies has been proposed by Schlobach [28].
His approach, known as semantic clarification aims to make logical
debugging techniques applicable to lightweight ontologies, whose
high degree of underspecification prevents modeling errors from
causing inconsistencies. The method of semantic clarification relies on the strong disjointness assumption [36], which postulates disjointness among sibling classes, as well as the pinpointing
technique for discovering and fixing the causes of logical incoher-
ence. In an evaluation based on six real-life ontologies, the majority of the erroneously introduced disjointness axioms  between
69% and 13% of all the added axioms, depending on the considered
ontology  were removed in the final debugging step. This leaves
31%, 87% correct disjointness axioms, respectively. Merging several
of the ontologies further improved these results (i.e. from 31% to
50%), as the additional axioms obviously helped to identify exceptions to the previously made disjointness assumption. However,
as we saw when creating our gold standard and when using an approach similar to the one by Schlobach as a baseline, its precision
falls short on specific subsets of the full dataset, such as the subtree of the Person class, since it cannot adapt to different char-
acteristics. Thus, this approach is particularly qualified for cases
where neither manual work nor more complex methods should be
applied and where its simplicity outweighs the potentially introduced errors.

More recently, there also were works on learning more
expressive ontologies in the context of Linked Data. The DL-Learner
tool by Lehmann [12] uses Inductive Logic Programming (ILP) for
learning class descriptions, which can include disjointness, from
instance data thus treating the problem as a supervised learning
problem. Because DL-Learner relies on the usage of a reasoning
component, it is not directly applicable to datasets of a size typical
for Linked Data. Hellmann et al. [5] tried to tackle this problem
by considering subsets of the full amount of data in order to
make using the basic approach feasible again. An approach that is
better suited for the application on large datasets was presented
by Buhmann and Lehmann [37]. They use predefined SPARQL
queries to detect patterns in the data that indicate specific axioms
holding for the dataset. For example, patterns such as two classes
having only a small overlap with respect to instances or having
no overlap at all are used as indicators of class disjointness. This
work is very similar to our works on inductively learning class
disjointness [15], which forms the basis for this article, and on
learning additional types of property axioms [18]. In further work,
Buhmann and Lehmann [38] proposed methods for generating
more complex axiom types by identifying frequently used, general
axiom patterns like A  B  r.C that could be instantiated
by specific datasets through axioms such as SoccerPlayer 
Person  team.SoccerClub. Based on the identified patterns,
the method tests different instantiations of the patterns regarding
their support in the dataset and then proposes those of high
support as valid axioms.

Topper et al. [39] move towards the application of learned class
disjointness axioms by using them on the DBpedia dataset for the
purpose of detecting errors in the dataset. Their learning algorithm
relies on the Vector Space Model from the area of Information Retrieval which they employ for computing the similarity of classes
based on being used with the same properties in the dataset. They
regard classes as being disjoint when they are not reaching a certain similarity threshold. These learned disjointness axioms combined with additionally determined domain and range restrictions
for the properties are then applied to the data for finding wrong
links between instances. Another approach for learning ontology
axioms including disjointness has been presented by Zhu et al. [40].

Called BelNet, it relies on representing the TBox of the ontology as
a Bayesian Network where each named ontology class designates
a node in the network and (inferable) subsumption relations are
shown as vertices. The instance data of the ontologys ABox is used
to determine the probabilities of belonging to the different classes.
A structure learning approach is applied to resulting Bayesian
Network (BN), which tries to optimize the network in order to
produce a BN better suited for representing the dependencies discovered in the dataset. The optimized BN representation is then
used to determine the confidence of axioms like subsumption or
disjointness. Zhu et al. evaluate their approach on several ontologies and compare it to both DL-Learner and GoldMiner. According
to their evaluation, BelNet performs better in learning the relevant
axiom types than GoldMiner regarding the achieved precision and
recall. They also state that GoldMiner is producing many irrelevant
results. For the learning of disjointness relations, we reduced the
generated noise in this work by extending the disjointness learning with symmetry handling as introduced in Section 3.

The benefits of disjointness axioms in various situations are
well documented in the literature. As already mentioned, the existence of correct disjoint statements opens the possibility of using
logical reasoning to verify the consistency of an ontology. Use cases
that benefit from the ability to check the consistency of a model
include conceptual modeling, semi-automatic ontology learning
and ontology matching. The icom tool by Franconi [41] and colleagues is a good example for the benefits of using disjointness axioms in combination with a complex axiomatization of extended
ER diagrams for detecting inconsistencies in conceptual models.
Haase and Volker [42] use disjointness axioms as a basis for logical reasoning to improve ontologies that have been automatically
extracted from texts. Meilicke et al. [43] showed that disjointness
axioms provide a basis for debugging mappings between heterogeneous ontologies improving the matching result by up to 18%.
Meilicke also showed that being able to verify mappings is essential for typical applications of ontology mappings [44].

However, an automatic assessment or comparison of previous
results is very difficult due to the lack of gold standards which
could serve as benchmark datasets. We hope to fill this gap by
providing a gold standard of class disjointness axioms as well as a
comprehensive report on how it was created. Also note that, while
the majority of previous methods for learning disjointness focus
on the instance level and thus belong to the family of inductive
approaches, we also considered a top-down approach relying on
schema-level information. Our comprehensive comparison of both
types of approaches provides insights regarding their advantages
and weaknesses, and we hope that these insights will inform future
choice regarding semi-automatic ontology engineering.

8. Conclusion and future work

In this article, we presented two approaches to learning class
disjointness axioms: GoldMiner can be seen as a bottom-up or
extensional approach since it solely relies on the instances in the
dataset and uses association rule mining to discover instancelevel patterns of a knowledge base. These patterns are then
employed to suggest axioms, e.g., class disjointness axioms. LeDA,
on the other hand, is best described as a top-down or intensional
approach, because its decisions are purely based on logical and
lexical descriptions of the classes, not on their instances. In
order to evaluate and compare these two approaches, we built
a gold standard of class disjointness axioms, which is based on
the DBpedia ontology. The gold standard as well as the revised
implementations of both GoldMiner and LeDA and the results of
our experiments are publicly available.18

18 https://github.com/dwslab/jws-disjointness.

First, we performed experiments with the GoldMiner approach.
In these experiments, inductively learning disjointness axioms
were shown to perform sufficiently well. However, a number of
class pairs wrongly classified as disjoint were discovered during
the evaluation. Closer examination revealed these problems to be
caused by GoldMiners sole reliance on instances as the only input
data which led to incorrectly learned axioms, when specific classes
only had a small extension or the instances themselves were erroneously assigned. Consequently, we tested LeDA which did not
rely on instance data but employs different types of features. We
showed that a combination of lexical, logical and corpus-based features leads to the best overall performance, provided that training
and test data are based on the same ontology. However, our experiments did not confirm previous results [26], which indicated
that a classifier for learning disjointness trained on one ontology
can be applied to other ontologies. In particular, we observed a
significant decrease of precision, when applying LeDA that had
previously been trained on PROTON to the DBpedia dataset. As a
possible solution to this problem, we proposed a transfer learning
approach based on feature selection. This extension led to a considerably improved accuracy in generating disjointness axioms without the need to manually select an adequate set of features. A closer
inspection of the disjointness axioms generated by the LeDA approach revealed general drawbacks of the top-down approach. In
particular, lexical and corpus-based features can lead to misclassifications when classes have similar or semantically ambiguous
labels.

Overall, the intensional approach taken by LeDA leads to
better results than the extensional method used in GoldMiner.
However, this comes at the price of additional manual effort
which has to be invested in advance. Most importantly, it requires
a manually annotated training ontology for which the same
features are available that are used for the target ontology
later-on. In addition, many of the features employed by LeDA
require additional external information which has to be collected
before it can be applied. Especially for the corpus-based features,
this requires several steps ranging from identifying documents
containing information relevant for the concepts in the ontologies
to generating background ontologies for both training and target
ontology by means of the Text2Onto tool.

In contrast, the extensional approach of GoldMiner does not
require this additional manual effort, since all required information
is already contained in the dataset. This particularly qualifies
GoldMiner for applications where it is more important to get
results without much manual
intervention or when external
data is hard to gather. Furthermore, by solely relying on the
information available in the dataset, the extensional approach
shows characteristics important for full automatic processing of
arbitrary instance datasets. Another advantage directly resulting
from employing the available instance data is the fact that all
generated axioms are directly grounded on the data. While axioms
proposed by LeDA are influenced by a wide variety of information
from different sources, errors in GoldMiner-generated axioms can
provide a direct insight into the typical patterns in the dataset.
Thus, the extensional approach can be understood as generating
more concise descriptions of the datasets typical characteristics.
Future work includes the development of a single integrated
approach to learning class disjointness, which combines instancebased with schema-level evidence. In addition, we are planning
to look deeper into applications of (learned or manually created)
disjointness axioms, such as logical and pattern-based debugging
of ontologies [43], and to extend our analysis of modeling errors
and their practical relevance to property disjointness axioms. In
order to make our methods available to a wider audience, we
are going to implement a plugin for the Protege ontology editor
which supports both an efficient acquisition of training data,

J. Volker et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 124139

and the semi-automatic enrichment of ontologies with learned
disjointness axioms.

In summary, our contributions in this article are as follows.
We evaluated two state-of-the-art methods for generating class
disjointness axioms which differ fundamentally with respect to the
employed machine learning techniques and the exploited type of
data. For this purpose, we created a new gold standard of manually
acquired class disjointness axioms. In contrast to previous work,
we have put an emphasis on the description of the creation process
and provide details on the insights gained during the creation.
Based on this gold standard, our direct comparison of both learning
methods allowed us to assess the strengths and weaknesses of
both approaches. In particular, our findings contradicted previous
assumptions regarding the influence of ontology characteristics on
the results of the supervised learning-based approach. Therefore,
we suggested transfer learning as a means to cope with the
heterogeneity of ontologies which has not been done previously
in the context of ontology learning. The evaluation data, as well
as the revised and extended implementations of our methods for
learning disjointness, are publicly available.

Independent from the learning method, the results were good
enough to be of actual use in application scenarios. Given that
the automatic support greatly reduces the effort it takes to create
disjointness axioms, people will no longer find an excuse for not
adding class disjointness to their ontology. At the same time,
the availability of ontologies which contain negation like class
disjointness broadens the range of possible applications. One of
the most basic is the ability to check the ontology and its data for
consistency and, in addition, apply debugging techniques to detect
errors which might help to raise the overall data quality. Given
the ability to generate descriptions of typical patterns contained
in the dataset, the inductive method also allows more interactive
debugging scenarios were humans are directly integrated into the
errors detection process and are given the possibility of fixing
discovered problems on the fly. Moreover, the higher expressivity
gives more opportunities to infer knowledge from the available
data, which could be particularly worthwhile in the context of
query answering. Instead of solely relying on positive knowledge,
the presence of negation in ontology allows us to properly infer
and handle negative knowledge for datasets and, hence, allows
previously unseen levels of precision. Considering such promising
scenarios, we hope that the results of our work foster a more
widespread use of class disjointness in the Web of Data and thus
bring more expressivity into the Semantic Web.

Acknowledgments

The work reported in this paper was partially funded by the DFG
project GOLD (Generating Ontologies from Linked Data). Johanna
Volker is supported by a Margarete-von-Wrangell scholarship
of the European Social Fund (ESF) and the Ministry of Science,
Research and the Arts Baden-Wurttemberg.
