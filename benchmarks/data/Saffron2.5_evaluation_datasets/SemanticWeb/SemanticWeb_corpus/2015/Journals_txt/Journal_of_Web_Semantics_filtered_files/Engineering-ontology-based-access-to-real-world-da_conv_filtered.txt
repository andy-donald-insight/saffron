Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Engineering ontology-based access to real-world data sources
Martin G. Skjveland, Martin Giese, Dag Hovland, Espen H. Lian, Arild Waaler

Department of Informatics, University of Oslo, Pb. 1080 Blindern, 0316 Oslo, Norway

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 16 October 2013
Received in revised form
21 December 2014
Accepted 13 March 2015
Available online 24 March 2015

Keywords:
Ontology-based data access
Engineering process
Case study
Query evaluation

The preparation of existing real-world datasets for publication as high-quality semantic web data is a
complex task that requires the concerted execution of a variety of processing steps using a range of
different tools. Faced with both changing input data and evolving requirements on the produced output,
we face a significant engineering task for schema and data transformation. We argue that to achieve a
robust and flexible transformation process, a high-level declarative description is needed, that can be
used to drive the entire tool chain. We have implemented this idea for the deployment of ontology-based
data access (OBDA) solutions, where semantically annotated views that integrate multiple data sources
on different formats are created, based on an ontology and a collection of mappings. Furthermore, we
exemplify our approach and show how a single declarative description helps to orchestrate a complete
tool chain, beginning with the download of datasets, and through to the installation of the datasets for a
variety of tool applications, including data and query transformation processes and reasoning services.
Our case study is based on several publicly available tabular and relational datasets concerning the
operations of the petroleum industry in Norway. We include a discussion of the relative performance
of the used tools on our case study, and an overview of lessons learnt for practical deployment of OBDA
on real-world datasets.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

Building a large software system from scratch is a complex
task. What used to be a matter of writing code into a text file, and
then running a compiler, is now an undertaking that can involve
numerous steps, including code generation, downloading specific
versions of hundreds of required libraries, automated testing, and
more. As this process became more complex, tools emerged to
support it, like MAKE [1], ANT [2] and MAVEN [3], that are based on
configuration files that abstract away from some of the details and
that describe the construction process in a more declarative way.
The more complicated these build processes became, the more
declarative and abstract also the configuration files.

Compare this to the task of preparing information for publication as RDF [4]. This used to amount to writing some triples into a
text file. For larger sets, a triple store can be used, that grants access
through a SPARQL [5] endpoint, and possibly some Linked Data [6]
front-end. But in the present work, we are interested in ontologybased data access (OBDA), in a sense to be made precise below,

 Corresponding author.

E-mail addresses: martige@ifi.uio.no (M.G. Skjveland), martingi@ifi.uio.no

(M. Giese), hovland@ifi.uio.no (D. Hovland), elian@ifi.uio.no (E.H. Lian),
arild@ifi.uio.no (A. Waaler).

http://dx.doi.org/10.1016/j.websem.2015.03.002
1570-8268/ 2015 Elsevier B.V. All rights reserved.

as a sophisticated method for publishing high-quality semantic
datasets. Using OBDA on real-world problems, we quickly notice
that also semantic technologies have departed from the state of in-
nocence: done properly, the process will involve a variety of steps
for cleansing, transforming, and mapping data, generating (boot-
strapping) ontologies for existing data schemata, improving them,
managing specifications of mappings that relate ontology concepts
to the data, and more. For all these different tasks, the OBDA system
designers are left with an array of tools spanning from simple text
editors to complex ontology editors and large-scale triple stores
with automated reasoning support. Most often, this preparation of
data and ontology infrastructure is not a one-shot effort: data and
schemata will be updated, new data cleaning tasks become neces-
sary, further improvements of the ontology called for, etc. One is
then faced with a tangle of diverging schemata, ontologies, configuration files and scripts. The management of the whole transformation and installation process becomes a significant engineering
challenge in its own right.

In this work we suggest learning from the software engineering
discipline: to ensure a flexible and robust transformation process,
a high-level declarative description of the transformation process
is required, that can be used to drive the entire tool chain. This
configuration specification is itself an engineering artefact, that
concentrates in one place all the information that is needed to
make the individual tools cooperate. As such, it can benefit from

the full range of engineering possibilities, like explicit, localised
documentation, version control, the possibility to devise tools to
support the creation of the configuration, and more.

Moreover, once the declarative description is in place it
provides us with a framework for prototyping and testing
the relative performance of different OBDA architectures and
components. Consistent with the design of the architecture one
must select components for use from a number of available
candidates. But how do we know what components fit their
purpose best? The performance of these components is typically
evaluated in more or less idealised laboratory tests with
both data and queries provided by database benchmark tools
such as the Lehigh University Benchmark [7] and the Berlin
SPARQL Benchmark [8],1 for instance the studies [911]. Recently,
benchmarks tailored for specific types of OBDA systems have
been developed as well [12,13]. Although these benchmarks
provide a comprehensive test bed for the evaluation of technical
performance, they still lack the natural variance and imperfection
usually encountered in real-world datasets. Hence, benchmarking
gives limited information about how the components will serve
in a real-world deployment like the one we are addressing.
Also, it is difficult to know up-front how the components will
perform in combination. What gives more information is to test the
performance of the various components on typical user queries for
the datasets at hand.

This is exactly what we did in our case study. More precisely,
we developed a declarative description of the various stages in the
OBDA data engineering process. We then applied that description
in a proof of concept OBDA implementation to a data access
scenario over real-world datasets with sample queries supplied
from professional end users. In doing so we also prototyped
different OBDA architectures and components and evaluated these
on the sample queries. Before we go into more details we first
define what we mean by OBDA and review some main points related
to OBDA architectures.

1.1. Materialised vs. virtual OBDA

We will in this article refer to OBDA as an approach to data access
where users interact with, potentially multiple and disparate, data
sources via SPARQL queries using a vocabulary specified in an OWL
[14] ontology. The key reasoning tasks in an OBDA system are
related to the evaluation of SPARQL queries over data exposed as RDF.
There are two main approaches to OBDA, depending on whether
the RDF view is materialised or not. In both cases a transformation
process is at the kernel of the system: in the materialised approach
source data is transformed from the source systems upfront, while
in the non-materialised, or virtual, approach, SPARQL queries posed
by users are transformed into queries over the source data at query
execution time.

The materialised approach is illustrated in Fig. 1(a). The source
data is here transformed into a separate physical storage system,
typically a triple store, that implements a materialised RDF view of
the source data; in the figure the data transformation process is
illustrated by the two arrows labelled data from the source data
to the RDF store. This transformation process is executed before the
system is ready to answer any user query. A query processor then
evaluates a user query q over the RDF store rather than over the
source data, possibly exploiting the ontology and interacting with
the RDF store in various ways.

Since query answering in this case requires no interaction
with the systems hosting the source data, an OBDA system with

1 For a list of relevant benchmarking tools, see
RdfStoreBenchmarking.

http://www.w3.org/wiki/

a materialised RDF view is not constrained by the limitations of
these systems. In particular this opens the possibility of adding
information to the RDF store that is inferred from the source data
and the ontology, such as explicating type information and identity
relationships between individuals. Since this saturation of the RDF
store can be performed up-front in a forward chaining reasoning
process, the OBDA system may at run-time be able to answer
powerful queries very efficiently. Moreover, the OBDA system can
then also support queries that the source data system cannot easily
handle. Consider, as an example, queries that require different
records to be semantically identified as representing the same
entity. Such queries can be processed by reasoning over an OWL
ontology. However, the same type of queries are not directly
supported by most RDBMS in use today as these systems are usually
governed by the unique name assumption where different records
are by assumption taken to represent distinct entities.

The virtual, i.e., non-materialised, approach to OBDA is illustrated in Fig. 1(b). Since the RDF view of source is virtual, there is
no duplication of the source data. In this case the query processor
makes use of the ontology to transform a user query q into a query
qO. The latter query is simpler than q in the sense that it can be
interpreted without reference to implicit information in the ontol-
ogy. The query qO results from a reasoning process where inferences from the ontology are computed at query execution time by
means of query rewriting, a backwards-chaining reasoning tech-
nique. In the next stage of the transformation process, a transformer uses information encoded in so-called mappings that relate
the source data to its RDF exposure. This stage unfolds qO into a
query qDB that makes no reference to any concepts of the ontology,
and that can be executed by the source data system.

When deciding on the architecture for an OBDA system, the

following points are particularly relevant:
 Is source data frequently updated? If so, the cost of keeping a
materialised view synchronised with the source data should be
considered. A virtual approach has significant advantages.

 Are powerful data transformations needed for the RDF view?

Then a materialised approach is needed.

 Is support needed for fragments of OWL for which query
rewriting is not a complete deduction method? A materialised
approach may support an ontology language with sufficient
expressivity.

 Is the response time for query execution critical? Then one
should prototype different architectures upfront and evaluate
them on typical user queries as the results are in many cases hard
to predict.

In cases where some of the source data systems do not offer
any query interface, a typical case in point would be CSV files,
a direct virtual approach is not an option. In cases where a full
transformation into RDF requires several transformation steps, one
can gain some of the advantages of a virtual approach by just a
lightweight transformation of the data into a system that supports
query answering and then treating this system as the source.

1.2. OBDA components

Over the last decade, the design of appropriate ontology languages and the complexity of their associated reasoning problems
have been extensively addressed [1518]. Several components that
can be used in the implementation of OBDA systems are by now also
available. However, the engineering aspects of designing OBDA systems have not been given corresponding attention, and there is yet
little experience derived from practical deployments of OBDA components to support access to real-world datasets in daily use. A major motivation for our work is to contribute to fill this gap.

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

(a) The basic architecture of an OBDA implementing a materialised RDF view.

(b) The basic architecture of an OBDA system implementing a virtual RDF view.

Fig. 1. Basic OBDA system architectures.

In the following we review various architectures of OBDA
systems, focusing on different choices for the transformation
and reasoning processes. Relevant OBDA tools to play a part of
a complete OBDA system are triple stores (virtual and physical),
reasoners, and bootstrappers, and tools that transform data into RDF,
often called RDF izers.2 Some tools can fill more than one category.
Virtual triple stores tools are D2RQ [1921], Ultrawrap [10],
XL Wrap [22], SPARQL 2XQuery [23], Spyder [24], Mastro [25], and
Ontop [26]. These are OBDA tools that leave the data where it
is, and transform the queries into the language of the present
storage engine, e.g., SQL or XQuery. D2RQ and XL Wrap do not
support reasoning on the ontological
level, while the others
support different levels of ontological reasoning. These tools can
also be used to export source data as RDF store, though that is not
their primary usage. For research into virtual triple stores over
relational databases, see for instance [2730]. Physical triple stores
are plentiful, we mention only two, Fuseki [31] and Stardog [32], of
which only Stardog supports ontological reasoning natively.3

Ontology bootstrapping is a technique that uses a relational
schema to create an ontology with classes, data and object proper-
ties, domain and range axioms, and other axioms. Populating the
ontology with data values may also be part of the process. The idea
of creating RDF views of relational databases was already described
by the designers of the semantic web, see Berners-Lees design note
from 1998 [33]. This was the basis for dbview.py [34]. The same
year, a description of mapping relational data automatically into
an ontology was done by Stojanovic et al. [35]. Sequeda et al. [36]
give a more recent overview of the research in this area. Implicit
in these tools is some mapping between the relational data and
the ontology, in newer tools these mappings are represented in the
R2RML [37] format. We mention the bootstrapper RDB ToOnto [38]
and also the database engine Virtuoso [39], which includes bootstrapping features.

2 A list of RDF izers can be found at http://www.w3.org/wiki/ConverterToRdf.
3 See http://www.w3.org/wiki/LargeTripleStores for a list of triple stores that are
known to tackle large datasets.

1.3. Case study

Our case study, addressed in Section 3, targets data sources in
daily use in the oil & gas industry and can serve as a motivation for
the contributions in this article. The case study contains a number
of practical challenges; moreover, these challenges are of a kind
that the designer of a practical OBDA system will in many cases have
to cope with:
 Source data in a variety of formats. In our case, we had to cope
with just under 100 CSV files, some with a rich schema and little
data, others with a small schema and much data, in addition to
one poorly structured relational database.

 Source data in systems which do not offer a query interface. In

our case, the CSV files were in this category.

 Frequently updated source data. In our case most of the CSV files
were generated from a source that is updated on a daily basis.
Even the schema of the CSV files is updated from time to time,
sometimes at frequencies of less than a month.

 No available ontology covering the domain. In our case we
could use two relevant domain ontologies (for stratigraphy
and for geometry); otherwise we had to generate ontologies
from schemata and integrate them with the domain ontologies
guided by a well-established upper ontology [40]. This resulted
in an ontology with approximately 400 classes and 700 roles.

 No available mappings between sources and the ontology.
This is the usual situation; in our case study we generated
approximately 1700 mappings.

We prototyped systems with both a materialised and a virtual
architecture, where the latter dumped the CSV files into a relational
database using a simple transformation. We then selected 42
typical queries for evaluation; 21 of these were supplied by
working domain experts reflecting information they need in their
daily work. To make this study easy to adapt and extend, we
decided to test only freely available software components. As OBDA
tools we used Ontop [26], Fuseki [31], D2RQ [21] and Stardog [32],
and MySQL [41] was used as the underlying relational database
engine.

With our specific combination of test data, test queries, and

choice of tools, it turned out that:

 The virtual approach could handle 5 out of the 21 expert
queries, while the materialised approach with reasoning
support could handle 20 out of 21, and all queries without
reasoning enabled.

 The virtual approach could handle 12 out of the 21 technical
queries, while the materialised approach with reasoning enabled could handle 18 out of 21, but only 14 returned results
when reasoning was not enabled.

For several of the queries, the virtual approach suffered from
the fact that parts of the SPARQL language had not yet been
implemented in Ontop at the time of evaluation.4 Another
interesting observation is that for the technical queries, enabling
different profiles of OWL 2 in 5 of the cases returned different result
sets. This calls for a careful analysis of what OWL profile is the best
choice for the case at hand.

Given the frequent update of source data in this particular
case study, a virtual approach is far better from the point of
view of synchronisation. This holds also in cases where the source
data is CSV files, due to the fact that loading CSV files into a
relational database is a much cheaper operation than a full RDF
materialisation. On the other hand, we learn from the evaluation
that the materialised approach can handle more of the queries.
To implement an operative OBDA system based on the evaluation
results of the prototypes, a designer must weigh the cost of keeping
a materialised RDF view synchronised with the source data against
the lack of support for some of the queries in the virtual approach.
These insights would have been impossible to know up-front.
Moreover, changes in the datasets, schemata, ontologies, maturity
of the used tools, etc., are likely to change the outcome. This
only strengthens our point that a declarative specification of the
data engineering process is required to guarantee its ability to be
reproduced and adapted.

1.4. The structure of this article

In Section 2, we introduce our proposed language for the
declarative specification of data engineering for OBDA installations,
and describe the process of generating all required components
from such a specification. Section 3 describes the use of this
infrastructure on real-world datasets, and the challenges this
implies, concluding with an empirical evaluation of the generated
OBDA installations. We evaluate the usefulness of the OBDA systems
with a series of SPARQL queries. We argue that this is the natural
way of doing such an evaluation, as the query interface is the
single point of interaction with the system that the user has.
Section 4 concludes the paper. We include two appendices:
Appendix A contains full details of the specification language and
its interpretation, while Appendix B lists the queries used in the
evaluation.

2. A declarative description for engineering ontology-based
data access

The concrete data engineering challenge we address stems
from the desire to compare the applicability of the different OBDA

4 According to the developers of Ontop, implementing these features is future
work.

approaches shown in Fig. 1 on a collection of raw data that is not
yet prepared for semantic publishing. The datasets are described
in detail in Section 3; for now it suffices to say that parts of the
data reside in a static relational database, while other parts are
in CSV format downloaded via HTTP. Also, parts of the datasets
are regularly updated, both the schema structure and the data
contents. For our comparison, the datasets need to be prepared
for both of the architectures in Fig. 1. In the case of Fig. 1(a), this
means transforming the datasets into RDF and loading them into
a triple store. In the case of Fig. 1(b), the datasets that are not
readily available via a query interface, i.e., the CSV files, need to be
converted and loaded into a relational database. In both cases, an
ontology and mappings have to be produced that are suitable for
both architectures, preferably leveraging the existing information
in the data schemata.

Repeatedly performing all the necessary transformation steps
manually is clearly far too laborious and error prone. But also a
custom-built automated process, based on ad-hoc scripts where
the transformations are programmatically specified is error prone,
and scales badly. We therefore quickly identified the need for
a central generic specification that can be used to drive the
construction of the different OBDA system components for all data
sources. With a single declarative specification that describes how
to create a relational database, an ontology and mappings from
the source data, it is easy to develop and customise the different
individual components in a consistent and holistic manner, while
keeping all parts of the combined OBDA system synchronised. The
outputs of the transformations are produced by software that can
be applied to any OBDA engineering specification. Supporting new
output formats required by specific tools or modifying existing
output produced by the specification is simple and transparent as
this can be controlled by small independent software programs.
This fosters an environment that is suitable for rapid prototyping
and development of complete OBDA systems.

This section describes in detail what information is encoded
in the generic specification, and how the various required OBDA
system components are generated from it.

2.1. Overview of the OBDA engineering specification

The OBDA engineering specification consists of:

 a language for describing the source data and the resulting OBDA

 a set of rules that translate the specification and the source data

components, and

into the specified output.

We have implemented the rules and orchestrated them to
an automated transformation process for the specification that
covers our requirements for the comparison of OBDA tools. In
its current form, the specification language and implementation
support source data in tabular files or relational databases. The
specification gives a generic description of the data sources and
details how the sources are to be translated and published as
semantic web data. By developing simple tools that implement
the specification and take the source data and an accompanying
specification instance as input we are able to produce:
 SQL scripts that create the structure and contents of tabular data

files as a relational database,

 an ontology that reflects the semantics of the database and the

source data, and

 mappings between the relational database and the ontology.
Fig. 2 presents an overview of the required transformations
between the three representation formats of the data, i.e., from the
original tabular files to a relational database, and the mappings
from the database to an ontology. The vertical arrows in Fig. 2
illustrate the lifting steps from tabular files to relational databases

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 2. Overview of the transformation model of the OBDA engineering specification showing how the constructs of the three representation formats are related. The main
constructs of each representation format are placed on the same level on the horizontal axis. The placement on the horizontal axis indicates which part of the specification,
i.e., which specification map, is responsible for controlling the transformation. Vertical arrows indicate transformations or lifting steps between constructs of different
representation formats. Horizontal arrows indicate a reference between constructs within the same format.

to semantic data. The horizontal arrows are part of the data model
of the respective representation format. The labels at the top
situate the concepts in the specification language. These labels refer
to specification maps, which are explained in the section below. We
explain the figure by first giving a high-level overview of the metamodel for each of the data representation formats, and how the
constructs of the three representation formats are related via the
lifting steps.

As to the lifting from tabular files to databases, each tabular file
is assumed to contain one or more fields.5 These are mapped to a
relational database as follows:
 Each file is mapped one-to-one to a database table.
 Each field is mapped one-to-one to a database column.
We assume that a database table contains one or more columns,
and that a table has exactly one primary key, given by a list of one
or more column names. A foreign key interacts with exactly two
database columns. These constructs are lifted into ontologies as
follows:
 A database table is mapped to zero or more classes.
 A column is mapped to zero or more properties and restriction

 A foreign key is mapped to zero or more properties and

axioms.

restriction axioms.

 A primary key is optionally mapped to one ontology key.
An ontology consists of a set of classes, properties, individuals, and
axioms. An ontology key is key for one class, and is specified with
a list of one or more properties. A restriction axiom specifies the
use of one property in relation to one or more classes. A subclass
axiom relates two classes.

Fig. 2 also indicates what parts of a lifted format may be
produced in a lifting step and how complete the specification of
the lifting can be. The figure shows the degree of freedom that
is possible (or necessary) in each transformation step to produce
output of high quality, and thus how much can be automated.
For the lifting of tabular files to a relational database, tables and
columns are completely specified by one-to-one relationships and
is therefore possible to fully automate. However, primary and
foreign database keys are not possible to lift from the tabular files
since this information is not represented in the tabular files, as is
indicated in the figure by the lack of inwards arrows to the primary

5 We use the word field to address the columns in a tabular file, and reserve
the word column for columns in a relational database table.

and foreign key nodes. Database keys must hence be explicitly
set in the specification. The lifting step to produce an ontology
allows for considerable customisation, as seen by the cardinalities
on the lifting relations between the database and the ontology;
no database construct is required to be mapped to the ontology,
and most database constructs may result in multiple ontology
constructs. Subclass axioms are not represented in the database
and must be specified especially for the ontology construction
step. The degree of possible customisation in this step reflects
the fact that constructing semantic models from relational models
is an effort that in many cases requires significant ontological
engineering.

We have constructed scripts that can bootstrap an OBDA
engineering specification based on the available schema data from
the sources and according to the lifting relationships in Fig. 2.
For the lifting step of tabular data to the relational model, the
bootstrapper generates database table and column names, leaving
it to the user to specify keys and further details of the table and
column definition. For the step of specifying an ontology and
mappings, the bootstrapper will produce a lifting based on the
direct mapping specification [42], as to give the user a startingpoint for further customisation.

The direct mapping specification is a precise description of how
to construct an ontology from a database. It prescribes roughly that
 each table is mapped to a class,
 each table primary key is used to set the URI pattern of the

instances of the corresponding class,

 each foreign key is mapped to an object property,
 each column is mapped to a data property,
 the range of the property is set according to the datatype of the

 the class and property URIs are built from the respective table

database column,

and column names.

In total, this makes a very crude mapping, in essence creating
an ontology representation of the data model of the database,
while our intention is to construct an ontology that reflects the
semantics of the database contents. Some of the problems of
using direct mapping in this respect are given in Fig. 3, which
will be explained after the constructs of the OBDA engineering
specification are given. A remedy to these problems could of course
be to custom-build the ontology and allow mappings expressed
as arbitrary queries over the source data. The latter is possible
in R2RML mapping language [37], and also other proprietary
mapping languages. However, ontology development is on its own
a difficult and expensive task. Now add the task of constructing

Fig. 3. Modelling features of the OBDA engineering specification, contrasted to problems that can occur from using direct mapping as the only method for ontology
development from databases.

and maintaining mappings that fit the ontology and sourcesfor
sources that evolve, and the problem quickly becomes intractable.
This is partly because of the sheer size of the required mappings
for complex databases and hence their corresponding ontology,
but also because tool support for mapping management is very
scarce. Fortunately, some of the problems that are related to direct
mapping can be fixed with relatively simple means. To overcome
these problems, our specification language supports a variety of
customisation possibilities based on simple modelling patterns
that extend the direct mapping approach and that are declared in
a compact and declarative manner.

2.2. The specification language

The main concepts of the specification language reflect the
key modelling constructs used in both the tabular/relational and
the RDF /ontological views of the data. This corresponds to the
constructs, at the schema level, needed to represent data models
generated at the respective lifting steps. There are five types of
specifications, these are also seen in the horizontal axis of Fig. 2:
1. Entity maps relate tabular files, relational tables and ontology
classes. They specify how a tabular file is mapped to a relational
table, and furthermore how the table is mapped to ontology
classes.

2. Identifier maps specify database primary keys and URI patterns
for ontology individuals, and relate the primary keys to their
corresponding ontology keys.

3. Attribute maps relate tabular file fields, relational columns and
ontology properties. They specify how a field in a tabular file is
mapped to database column, and additionally how the column
is mapped to an ontology property and axioms.

4. Relation maps specify foreign keys and relate these to their

corresponding object properties and axioms.

5. Subtype maps specify subclass information in the ontology

which is generated from values in the source data.
Fig. 4 details what information the specification contains
for each of the five specification types. Fig. 5 gives some excerpts from the specification used in our case study. As an

example, consider the Entity map and line 1 of Fig. 5(a),
which contains an entity map instance. Tabular files are by
default mapped to tables with the same name, excluding
the file extension. This map thus specifies that the tabular
file field_operator_hst.csv shall be mapped to the table with the same name as the file, field_operator_hst,
and, furthermore, that the table is mapped to the OWL class
npdv:FieldOperator. The human readable label
for the
Operators, and four
table and class will be Field:
attributes,
fldOperatorDateUpdated,
fldOperatorFrom, fldOperatorTo, are selected for mapping
over to the ontology description of this class. Furthermore, mappings that describe how to construct ontology individuals from the
database contents may be generated. The details of these translations will be given later in this section.

dateSyncNPD,

The design of the language has grown out of the need
for an efficient and clear representation of multiple formats.
The specification language allows for representing relatively
complex modelling and mapping structures from simple and
compact specifications. Moreover, the structure of the language
ensures that similar constructs are grouped while others are
clearly separated in order to reduce update anomaly problems
of the specification, which again increases the usability of the
specification language. Fig. 3 lists the most important modelling
features of the engineering specification and relates them to the
problems of using the direct mapping specification as a single
ontology construction method. Of these we highlight the ability
to assign URIs for all classes, properties and individuals, and the
detailed control of how database tables and columns are mapped
to classes and properties of different types as absolutely necessary
for producing a semantic model of reasonable quality from the
relational schema. Of the more advanced features the specification
allows for transforming information that not directly supported by
the relational model, like specialisation and equality relationships,
but which is represented in designated columns, as appropriate
axioms in the ontology and mappings.

How the specification works is explained in detail in the
remainder of this section, where we in turn present the three main
outputs that are defined from the specification: the declaration

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 4. Schematic overview of the OBDA engineering specification, showing the five specification map types and the fields they comprise. Each field is uniquely identified
with an alphanumeric label, e.g., E1, and a name, which is unique for the map type where the field occurs. An ellipsis (...) denotes that the field may contain multiple values.
Fields written in italic indicate that the field is a reference to different map of the specification; this is also shown with an arrow connecting the two maps. A field is marked
with D, O or M if it is used in the construction of respectively database schema, ontology, or mappings. The right-most names given for each field is its shortname. This name
is unique for a map and is sometimes used to improve the readability when referring to the field. Example instances of parts of the specification are found in Fig. 5.

of a relational database, an ontology and mappings between the
database and ontology. A running example taken from our case
study will illustrate the different outputs of the process. For the
sake of readability, this section will give an informal description of
the transformation process, referring to Appendix A for the formal
definition. Appendix A.1 gives the syntactic requirements for a
valid specification, and Appendix A.2 contains rules for translating
a specification into, respectively, a relational database, an ontology
and mappings.

2.3. Representing tabular data files as a relational database

The process of transforming a set of tabular files into a
relational database is devised so that the tabular files are faithfully
represented in the database, mapping tabular files and their fields
one-to-one to relational tables and columns. This is designed on
purpose for the use case study in order to make the relational
database an exact as possible representation of the tabular source
data. However, it is of course possible to exploit this step to
introduce more sophisticated transformations of the data.

The specification maps that are used in the transformation step
from tabular to relational data are entity maps, identifier maps,
attribute maps and relation maps. An entity map specifies a table
name (E1), a label describing the table (E2) and a reference to one
identifier map (E3).6 An identifier map specifies a primary key, and
refers to a list of attribute maps (I2). An attribute map specifies
a column, and contains a column name (A1), an SQL datatype for
the column, a Boolean value indicating if a value for the column
is optional or not (A3), and a label describing the column (A4). A
relation map specifies a foreign key, and does this with a source
column (in a table) (R1 + R2) and a target column (R3 + R4).
Filenames of tabular files, without their file extension, are used as
names for the corresponding tables, and similarly, field names are
used as column names. Additionally, all field names in a tabular file
are assumed to refer to attribute maps, hence indirectly specifying

a list of attribute maps of the entity map that the tabular file is
specified by.

This information is now straight-forwardly used to specify the
transformation of a set of tabular data files into a database schema
as follows:
 Each entity map results in a table, with the specified table name
and the label set as comment on the table. The identifier map
specifies the primary key of the table, and the attribute maps
specify the columns of the table.

 A primary key is constructed from an identifier map by taking
all the columns of the attribute maps that the identifier map
refers to as primary key columns.

 Columns are built from the attribute maps that an entity map
refers to by setting the column name and SQL datatype as
indicated by the map, adding the label as a comment to the
column, and declaring the column as NULL or NOT

according to whether or not the value for the column is set as
optional or not.

 Each relation map results in a foreign key that requires that
values in the specified source column references values in the
specified target column.

The formal description of how a database schema is constructed
from the specification and the tabular source data files are given by
the two database rules in Appendix A.2. Database Rule 1 produces
tables with columns and primary key, while Database Rule 2
constructs foreign keys.

Consider Fig. 6 which contains a sample of the CSV file
field_operator_hst.csv with all eight tabular field names
in first row, and data in the consecutive rows, here only showing
one row concerning the field operator history of the Troll field.7
The structure and metadata of the tabular file is represented in the
specification, as is partly illustrated in Fig. 5. The resulting database
table definition of table field_operator_hst is given in Fig. 6,
using syntax accepted by a MySQL database. Note that the table is
defined with the same columns as the fields found in the tabular

6 The parenthesis refer to the fields listed in Fig. 4. See the figures caption for
information.

7 See, e.g., https://en.wikipedia.org/wiki/Troll_gas_field.

(a) Entity map.

(b) Identifier map.

(c) Attribute maps. In the column type, D, O and OP indicate a datatype property, object property and object property pattern,
respectively. In the column null, 1 is true and 0 is false.

(d) Relation maps.

(e) Subtype maps.

Fig. 5. An example of an OBDA engineering specification, using a simplified excerpt of the specification developed for the use case described in Section 3. For the sake of
brevity, only a sample of the available specification map fields are shown here. Each map instance is numbered for easy identification and reference in examples; this is not
part of the specification.

Fig. 6. Excerpt of field_operator_hst.csv: Troll operator history data.

file in Fig. 6 (see lines 29 in Fig. 7), the label set in the entity map is
added as comment to the table (line 11) and that the four columns
set by the identifier map in Fig. 5(b) constitute the primary key
(line 10). Moreover, the datatypes, NOT NULL flags and comments
for the columns are correctly set according to the attribute maps in
Fig. 5(c), and the relation maps in Fig. 5(d) amount to foreign keys,
as seen in lines 13 and 14.

This database table can be loaded with the data in the tabular
file, from which the structure of the table is defined, by using the
SQL script in Fig. 8. This script is built from the specification and
uses MySQL database syntax.

2.4. Specifying an ontology from a relational database

Different from the specification of the transformation of tabular
files to a relational database, where the purpose is to let the
database identically reflect the set of tabular files, the lifting of the
database to an ontology must allow for far greater customisation,
as the main purpose of this step is to increase the semantic value of
the data and to create a vocabulary for the OBDA interface suitable
for its intended users. This includes, on the simple end, giving
comprehensible names for the resulting classes and properties,
and converting database constraints to ontology axioms, but
also exploiting the expressiveness of the ontology language to

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 7. field_operator_hst table definition.

prefix and suffix used to produce new URIs (S4, S5). A subtype
map may also refer to a translation table (S6).

 A translation table specifies a one-to-one mapping between
values in the database (T1) and values that will,
in this
transformation step, occur in the ontology (T2). A translation
table is used for gaining detailed control of how database values
are translated to (parts of) URIs.

We will now explain how the specification maps are used to
transform a relational database into an ontology. In the following,
ontology axioms are serialised using the OWL 2 Functional-Style
Syntax [14].

Entity maps. Each entity map that is selected for this lifting step
results in a class declaration using the specified URI as identifier,
and adding annotations that reflect the labels, description and
provenance information. Ontology Rule 1 defines this translation
formally.8 Applying the rule to the entity map in line 2 of Fig. 5(a)
will result in the following axioms:

Declaration(Class(ndpv:Field))
AnnotationAssertion(rdfs : label ndpv:Field "Field")
AnnotationAssertion(rdfs : comment ndpv:Field

"Fields on the NCS")

Attribute maps. Similarly, attribute maps result
in property
declarations using the specified property type to control what type
to declare. Ontology Rule 2 defines the translation, and applying it
to the attribute map in line 1 of Fig. 5(c), will give the following
axioms:

Declaration(DataProperty(ndpv:name))
AnnotationAssertion(rdfs : label ndpv:name

"Field name")

Attribute maps that are flagged as mandatory are translated
to unqualified existential restrictions with Ontology Rule 4; each
class that refers to the attribute map is set as a subclass of the
restriction class expression. The attribute map in line 4 in Fig. 5(c)
contains an attribute map where the property type is a datatype
property and the mandatory flag is true. This map is referred to
by the entity map that produces the class npdv:FieldOperator
(line 2 of Fig. 5(a)). The result of applying Ontology Rule 4 to this

8 To keep the rules simple, they do not include representing provenance
information in the ontology. An example of how this is represented is found in Fig. 9.

Fig. 8. Script for loading the field_operator_hst table with data from the CSV
file field_operator_hst.csv.

represent the implicit semantics of the database explicitly in the
ontology.

All specification maps are used for this lifting step.

 Entity maps are used to map tables to classes. They contain a
class URI (E5), a list of references to attribute maps that will
be associated with the class (E4), a label (E2) and a textual
description or definition of the class (E6). Additionally, in order
to trace the source for the class mapping, the entity map will
also make use of the database table name (E1).

 Identifier maps specify ontology keys. They contain a reference
to a entity map (I1) and a list of references to attribute maps (I2)
which are used to select the properties will identity the given
class.

 Attribute maps transform database columns to properties and
restriction axioms. The fields that are used for this purpose
are those that specify a URI (A5), the property type (A6), the
RDF datatype (A10), if a column value is optional or not (A3),
a label (A4), a textual description or definition (A11), and data
about the originating database column (A1, A2). The type of
the property may be set to an object, datatype or annotation
property, and only for certain choices of the type will the other
fields be used, for instance, an RDF datatype is only relevant if
the type is a datatype property.

 Relation maps result in object properties and restriction axioms.
They contain a URI (R5) and references to source and target
entities (R1, R3).

 Subtype maps are used to transform the values in database
columns that contain type information and specialisations into
subclass axioms. The map contains fields that specify a URI (S3),
a reference to an entity map (S1), a column name of the table
that corresponds to the referred entity map (S2), and a possible

attribute map is the following axioms:9
SubClassOf(npdv : FieldOperator

DataSomeValuesFrom(
npdv : dateFieldOperatorFrom
rdfs : Literal))

Relation maps. Relation maps are used to transform foreign keys
into object properties and restriction axioms. Ontology Rule 5
translates relation maps to qualified existential restrictions. The
restriction is qualified with the class specified as the target entity
of the relation map, and setting the source entity class as subclass
of the restriction class.

The relation map in line 1 of Fig. 5(d) specifies:

 the source entity class as npdv:FieldOperator,
 the target entity class as npdv:Company, and
 the property URI as npdv:fieldOperator.
Applying Ontology Rule 5, this map results in the following
axioms:10
Declaration(ObjectProperty(npdv : fieldOperator))
SubClassOf(npdv : FieldOperator

ObjectSomeValuesFrom(
npdv : fieldOperator
npdv : Company))

Identifier maps. Identifier maps are translated to OWL keys with
Ontology Rule 3. A key axiom is declared for the class that the
identifier map refers to, if all the attributes needed to create the
key are also included in the map of the class to be identified.

Applying the rule to the identifier map in line 2 of Fig. 5(b),

creates the following axiom:
HasKey(npdv : Field () (npdv : idNPD))

No key is produced for the identifier map in line 1 of Fig. 5(b)
since not all the attribute maps specified for the identifier map
(fldNpdidField, cmpNpdidCompany, fldOperatorFrom,
fldOperatorTo) occur in the attribute map list (E4) of the entity that is to be identified by the key:
(dateSyncNPD, fldOperatorDateUpdated,
fldOperatorFrom, fldOperatorTo).
Subtype maps. Subtype maps produce in this lifting step subclass
axioms from values in designated database columns. This map
hence behaves differently than the other maps which use only
the database schema information to build ontology constructs.
A subtype map contains the necessary information to convert
database values into URL s which are declared as subclasses of the
class URI specified by the map. The database value is converted
to a URI by optionally prepending and appending it with the
given prefix and suffix, and also optionally translating the value
with a translation table. Translation tables will be explained more
thoroughly in next subsection. Ontology Rule 6 formally specifies
the translation of subtype maps.

Consider line 1 in Fig. 5(e); this map specifies that the values of
the fclKind of the table that the entity map facility_fixes

9 In description logic syntax this would naturally translate to the axiom
npdv:FieldOperator   npdv:dateFieldOperatorFrom

10 In description logic syntax this translates to:
npdv:FieldOperator   npdv:fieldOperator . npdv:Company.

represents shall be converted to URIs by prepending the prefix npdv:
and appended the string Facility. There is no specified translation table for this map, the default transformation set for the use
case is to convert the database values to UpperCamelCase strings.
Assume CONDEEP 4 SHAFTS and SINGLE WELL TEMPLATE are
the only values in the fclKind column. The Ontology Rule 6 will
then produce the following axioms from this subtype map:
SubClassOf(npdv : Condeep4ShaftsFacility
SubClassOf(npdv : SingleWellTemplateFacility

npdv : Facility)
npdv : Facility)

It is worth noting that, unlike the other specification maps that
use the information stored in an OBDA specification to produce
output, subclass maps use also the actual data contents. Hence, the
size of the output produced by subtype maps depends also on the
size of the database data contents and not only the schema.

Specification maps are not considered for the lifting step from
the database to an ontology if they lack required fields. Specifically,
entity maps, attribute maps, and relation maps are not applied if
they lack a URI. This is the method used for refraining to transform
certain database constructs to ontology constructs. In our running
example, this is seen in, for example, the attribute maps in lines 6
and 7 of Fig. 5(c), which both have unspecified d4 values. These
columns are chosen not to be mapped by their respective attribute
maps since they are covered by the relation maps in lines 1 and 2
of Fig. 5(d).

By keeping track of which classes and datatypes the generated
ontology properties relate, we can add domain and range axioms to
reflect this. In the current version of the OBDA specification we add
an axiom giving the domain of a property whenever the map that
produced the property is only referred to by one entity map, i.e., the
property uses only one class or datatype as its domain, similarly for
the ranges of properties. This can be easily changed to make axioms
that set the domain and range of properties to all (or some number
of) classes and datatypes that a property relates.

To provide a combined example of many of the abovementioned specification maps, Fig. 9 gives an excerpt of the
class npdv:FieldOperator as defined by the map specifications of field_operator_hst in Fig. 5, i.e., the entity map in
line 1 of Fig. 5(a) with referenced attribute maps, and the relation maps in Fig. 5(d); the result of generating ontology axioms for the columns dateSyncNPD and fldOperatorTo is
dropped as they follow the same pattern as for other maps.
We note that only the four columns listed in the last field of
the entity map: dateSyncNPD, fldOperatorDateUpdated,
fldOperatorFrom, fldOperatorTo are mapped, with the effect that the primary key of the table is not represented as an
OWL key, since some of the properties necessary to define the
key are missing. The foreign key column cmpNpdidCompany
is mapped to the object property npdv:fieldOperator and
to a qualified existential restriction axiom. The mandatory column fldOperatorFrom is mapped to the datatype property npdv:dataFieldOperatorFrom and to an essentially
unqualified existential restriction axiom. The object property
npdv:fieldOperator only relates the classes seen in this ex-
ample, so a domain and range for the property is set, respectively npdv:FieldOperator and npdv:Company. Similarly
for the datatype property npdv:dataFieldOperatorFrom,
the domain and range is set to npdv:FieldOperator and
xsd:dateTime. Lastly, as seen from the sql: prefixed proper-
ties, which are taken from a custom-built ontology, the constructed
ontology is annotated with the labels and, when available, the descriptions set in the specification. Additionally, as far as possible,
each ontology axioms is marked with the database origin of the

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 9. Excerpts of the npdv:FieldOperator class definition.

axiom, for instance naming as source a specific foreign key from
the database. We do this so that axioms can easily be traced back
to the database, which helps the design process of an OBDA engineering specification.

2.5. Designing mappings from the database to the ontology

Given that the schema of the relational database and the
resulting ontology are thoroughly described and related by the
OBDA engineering specification, the mapping between the two
representations is already implicitly defined. What remains is
to define how database values are translated to RDF resources,
which means specifying URIs and translating data values to the
correct format according to its RDF datatype. This is the task of
the mappings produced by this transformation step. A mapping
consists of two parts, a query over the relational database and a
description of how the results of this query is output as RDF triples.
To produce these mappings, all specification maps are used. The
relevant parts of the specification are those necessary for building
the required database queries and for specifying the RDF output.
This means essentially that all fields, except those that contain
metadata about the database schema and ontology, for instance,
database datatypes and textual descriptions, are relevant. We refer
to Fig. 4 for the selection of these fields, and point out that the
fields used exclusively to produce mappings are those which are
directly related to specifying RDF output: URI patterns (I3, A8) and
other fields that are used for designing proper RDF resources, value
translations (A7) and value language tags (A9).

Before we explain the output of the different maps, we explain
how URI patterns and translation tables work, as these are central
to the production of mappings. Consider the URI pattern in line 1
of Fig. 5(b). It specifies that URIs are to be built by replacing
the special placeholders in the pattern, like {$1}, with values
in the database that are collected from the columns that the
attribute maps of the identifier map refer to. The number in the
placeholder corresponds to the index of the attribute map in the
list. This is very similar to the string templates of the R2RML mapping
language. Translation tables offer fine-grained translations of
database values to RDF resources. This is feature is taken from
D2RQ s mapping language [21]. A two-column translation table

specifies a partial one-to-one mapping function between the two
domains. This feature is very useful for translating sets of values
that are otherwise difficult to automatically transform to suitable
URIs using generically defined translation functions, or for simple
translations that involve few values. An example of the latter from
our use case, is the need to translate different two-value value
domains, for instance, YES/NO, in different variations (Y/N) and
languages (Ja/Nei), and Turnarea included/Turnarea not
included, into the standard RDF Boolean values.

We now explain how the specification maps are used to

produce mappings.
Entity and identifier maps. Entity and identifier maps are used to
specify mappings that create ontology individuals from the rows
of database tables; the identifier map specifies the URI of the
individuals and the entity map its type. This is formally specified
by Mapping Rule 1.

The entity map and identifier map in lines 1 of Fig. 5(a) and 1 of
Fig. 5(b) result in the following mapping seen in Fig. 10(a) together
with the RDF output produced when the mapping is applied to the
data in Fig. 6. Note that the date have been transformed to the
correct datatype format.
Attribute maps. Attribute maps specify mappings that relate the
individuals that represent the rows of a table to their values
in different columns. An attribute behaves slightly differently
according to the specified property type of the map. If the property
type is a datatype property, then the column value may be typed
(with a datatype) or given a language tag. If the type is an object
property, then a URI pattern may be used to convert the database
value to a proper URI ; it is also possible to specify that the
database value already is a valid URI. In all cases (datatype, object
or annotation property), a translation table can be used to produce
correct output from the values in the database. Mapping Rule 2
specifies this transformation.

Applying this rule to the attribute map in line 4 in Fig. 5(c)
results in the first mapping seen in Fig. 10(b), which maps a
datatype property; we notice that the mappings set the correct
datatype for the object resources of the output triples. Mapping
no. 2 in the same figure represents a mapping of a object property
where the complete URI is stored in the database; this is the

(a) Entity and identifier mapping example.

(b) Attribute mapping examples.

(c) Relation mapping example.

(d) Subtype mapping example.

Fig. 10. Mapping examples from applying the specification rules to selected specification maps in Fig. 5. The RDF triples given in (a), (b) and (c) are the results of using the
mappings on the source data in Fig. 6.

mapping resulting from attribute map in line 12 of Fig. 5(c).
Mapping no. 3 illustrates the mapping resulting from translating
a object property pattern mapping, line 13 in Fig. 5(c). Here,
values from the database are combined with the specified URI
pattern to form URIs ; in this case also using external URIs,
https://ws.brreg.no/lod/data/$1.
Relation maps. Relation maps result in mappings that create triples
relating individuals created from the relation maps source and
target entity maps references. Mapping Rule 3 specifies this
transformation.

The relation map in line 2 of Fig. 5(d) will result in the
mapping shown in Fig. 10(c). Notice that the mapping requires
that both the sets of columns necessary to construct the URIs to be
related are collected, and furthermore that the two database tables

(that are already related through a foreign key) must be joined
appropriately.
Subtype maps. The outcome of subtype maps in this transformation
step is one mapping per subclass axiom that was generated by
the subtype map in the transformation step for constructing
an ontology from the database, i.e., by Ontology Rule 6. The
mapping results in a type relationship between the individual that
represents the row of the table that the subtype map is defined for
and the subclass created in the ontology for the same row. This is
formulated more precisely by Mapping Rule 4.

Using the same example as for the explanation of subtype maps
in previous section, assume CONDEEP 4 SHAFTS is a value in the
fclKind column found in the subtype map in line 1 in Fig. 5(e). A
mapping for this value is found in Fig. 10(d).

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 11. Materialised RDF of the data in Fig. 6.

Fig. 12. Excerpt of D2RQ mappings from database table field_operator_hst to npdv:FieldOperator and related properties.

Fig. 13. Excerpt of Ontop mappings from database table field_operator_hst to npdv:FieldOperator and related properties.

The materialised triples resulting from using all applicable
mappings on the data in Fig. 6 is given in Fig. 11.11 Figs. 12 and 13
contain the mappings, respectively in D2RQ s and Ontops mapping
languages, to the parts of the npdv:FieldOperator class in
Fig. 9.

2.6. Discussion

The OBDA specification language is capable of representing common relational database structures and translating these to ontology constructs, together with mappings for transforming data from
source the database to the ontology. As explained and exemplified in this section, the language allows for transformations with

11 Note that the RDF data is published in a linked data front-end according to bestpractice principles, meaning the data may be examined by visiting the URIs with a
web browser.

considerable customisation, many of which are basic, but nevertheless important, in order to produce OBDA systems of high qual-
ity. The clear strengths of the language are its design and compact
format that support the construction and maintenance of multiple
artefacts of an OBDA system. For instance, by limiting the specification of URI patterns to only identity maps and attribute maps, the
URIs generated by the mappings created by a specification may be
changed easily and consistently in selected and natural parts of the
specification. The shortcomings of the specification is its limited
support of corner cases, exemplified by the fact that is it not possible to easily specify mappings that require the joining of multiple
database tables, or datatype property values that should be produced by combining multiple database columns. In the current version of the specification language such descriptions must be added
manually after generating the OBDA artefact.

The specification language is designed to be very general
and does not commit to specific versions of schema, ontology
or mappings languages, nor to any given ontology engineering
methods or reasoning requirements. The current specification

implementation outputs the OBDA output to single standalone files.
If a specification should fit particular needs, the output generated
from an OBDA specification may be profiled by altering the rules for
translating specification instances to OBDA system components.

3. Use case study

In this section we present the use case study that has guided
the development of the OBDA engineering specification and show
the applicability of the specification in the development and design
of OBDA systems. We also share from our experience of designing
and installing multiple different OBDA systems starting only with
the data sources, which are on different formats and with little and
often poorly structured schema information.

We first present the datasets in our use case, and proceed
to explain how an OBDA engineering specification and additional
supporting ontology was developed for these datasets. Then we
present the specific tools that we used in our study, and how
they were used in the process of designing and installing the
different OBDA systems. We conclude the section by reporting on
the results from using the OBDA specification and tools, and a query
evaluation test we performed in order to examine the suitability of
the different OBDA architectures and tools.

3.1. The use case datasets

The datasets in our case study are the NPD FactPages, the Diskos
database and the NORLEX database. They all concern information
related to the exploration and production of petroleum on the
Norwegian Continental Shelf (NCS).
NPD FactPages. The NPD FactPages [43] contain data about the
petroleum activities on the NCS. This means data about the
companies that operate on the NCS, e.g., oil companies that operate
fields or own shares of production licences, and service companies
that perform seismic surveys; geographical data for different
physical installations, like platforms, pipelines, and process plants,
and the areas of fields and seismic surveys; wellbore data from
results of tests taken during drilling; transfers of shares of
production licences between companies; and production results
measured in volumes of petroleum. The dataset is kept updated
to reflect ongoing activity, e.g., the results of exploratory drilling,
locations of ongoing seismic surveys, and monthly production
figures for the NCS. Additionally, historical data, that is data about
shutdown platforms, completed surveys, old production data and
more, is also represented in the dataset, some of it dating back to
the early 1970s when oil production on the NCS started.

The NPD FactPages are made available as open data [44,45] in a
web application serving a set of HTML pages in a factsheet format.
Most of the background data for a fact page is also available for
download in bulk as tabular data in CSV, Excel and XML format.
The published dataset is updated daily, and changes to the dataset
schema occur at irregular intervals.

A typical use of the dataset is by engineers in oil companies
who want to find information related to an area of exploration they
are examining. The NPD FactPages, and its accompanying FactMap
application [46], can then be used to identify nearby wellbores
and see if there exist test results from these wellbores, or to find
other locations that have the same type of reservoir formation
that the current area contains, for instance, a promising sand type
formation, or simply to establish the ownership of a particular
production licence or oil field.
Diskos database. The Diskos database is the common national
data repository for seismic and navigational data, production
data and well data from the NCS
[47]. This amounts largely to
data from different types of seismic surveys, and different types

of measurements taken from wellbores, for example rock and
core sample tests, and fluid and pressure tests. Unlike the NPD
FactPages, where mostly metadata and aggregated data about this
type of information is kept, the Diskos database also contains the
actual data collected from the surveys and tests; additionally, its
metadata is often richer than for the similar NPD FactPages data.
The database contains parts of the NPD FactPages for reference to
related data on the NCS.

The Diskos database currently contains 500 TB of data, which
is kept in a high-volume data storage capable of handling large
amounts of traffic. The data is accessed by users using custom-built
software provided by the current operator of the Diskos database.
Using this tool we exported data from different views into 17 CSV
files, and additionally downloaded one Excel spreadsheet.

In large oil companies, Diskos is typically used by data managers
who are responsible for gathering all the relevant data for a project
working on the exploration or development in a particular area.
Needless to say, seismic data and well test data is an extremely
important asset for petroleum engineers as it is vital for analysing
results to discover new deposits of oil and gas.
Norwegian offshore stratigraphic Lexicon database. The Norwegian
Offshore Stratigraphic Lexicon (NORLEX) [48] database contains
regional lithostratigraphy and biostratigraphy12 for the North Sea,
Norwegian Sea, Barents Sea and Svalbard, following guidelines laid
out in the International Stratigraphic Guide [49]. The database is
created and maintained by a project led by the Geology Museum
at the University of Oslo, with scientific contributions from the
NPD, the Diskos group and major oil companies. The aim of the
project is to update outdated lithostratigraphic data previously
published by the NPD, amend this information with biostratigraphic
data, and compile the results into a relational database to be
published online and made accessible through an interactive user
interface [48]. The project website and online database interface is
available at http://nhm2.uio.no/norlex/.

The NORLEX database was made available as a MySQL database

dump.

3.2. Constructing an OBDA engineering specification

An OBDA engineering specification is an asset that describes the
construction of multiple artefacts of non-trivial complexity. The
process of creating a specification is therefore also complex. It is
possible to bootstrap a specification from the source data, but as
explained in the previous section, not all necessary data is available
from the source. We will now explain the steps we took to create
the specification.
Lifting tabular files to a database. From Fig. 2 it is clear that it is
possible to bootstrap information for creating database tables and
columns source tabular files. What is not possible to directly read
from the tabular files are primary keys and foreign keys. Addition-
ally, not all information required for specifying tables and columns
is available from the files; what is missing are labels and textual descriptions, and column datatypes and optional/mandatory
flags.

In our case, in order to let the specification bootstrapper
correctly read the tabular files, some files needed to be cleaned for

12 Stratigraphy is the description of all rock bodies forming the Earths crust
and their organisation into distinctive, useful, mappable units, based on their
inherent properties or attributes, in order to establish their distribution and
relationship in space and their succession in time, and to interpret geologic
history. Lithostratigraphic units are bodies of strata based on their lithology,
i.e., observable physical
texture, grain size, and
composition. Biostratigraphic units are bodies of strata that are defined or
characterised on the basis of their contained fossils [49].

features, such as colour,

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

irregularities. These where largely due to data values containing
linebreaks, hence destroying the structure of the file.

After bootstrapping the specification from the tabular files,
textual labels and description were added manually when this
information was available (in various formats) from the source
data provider. Datatypes and optional flags where set by manually
inspecting the data files. We experimented with automatic
procedures for setting datatypes and optional flags, but in our
experience this was difficult to automate, since the dataset
contains too many ill-shaped values that need to be cleaned in
order to fit the intended datatype, and available cleaning tools, like
OpenRefine [50], where not possible to run as a batch process. This
is a requirement in our setup, since loading data and processing
the specification is a task that need to be executed regularly. Our
cleaning procedure is therefore manually created, by specifying
a list of corrections of values and formats to those accepted by
the database. Primary and foreign keys were also set by manual
inspection, but here field names in many cases provided invaluable
help, for instance for the NPD FactPages dataset many keys are
prefixed with Npdid (NPD ID).

The added information was tested by iteratively loading the
generated SQL schema and tabular data into the newly constructed
database, and processing any error messages that the database
reported. Thus, the work associated with this transformation step
is very technical and focused on syntactical problems of making
the tabular data fit in a relational database.
Lifting the database to an ontology. As indicated by Fig. 2 the by
far most laborious part of creating the OBDA specification, is engineering the lifting of the database to the ontology. This is naturally
the step that to the largest degree concerns semantic modelling, so
knowledge of standard ontology engineering methods, cf. [51], are
relevant for this task.

The specification bootstrapper will create a direct mapping
specification of the database, with one entity map per table, one
identifier map per primary key, one attribute map per column and
one relation map per foreign key. The remaining manual tasks are:
 Deciding if there are any bootstrapped specification maps
(entity maps, attribute maps or relation maps) that should not
be mapped to the ontology.

 Deciding if any tables or columns should have multiple

 Specifying URIs for all specification maps.
We discuss these in turn below. Note that these choices are very
tightly connected to the production of mappings which we shall
explain next.

Cases where it is natural to exclude a specification map from
producing output to an ontology, is when the database is oddlyshaped (from the viewpoint of automatic ontology bootstrapping)
due to database normalisation or lack of normalisation. A case of
the first is when a multivalued attribute is represented with a
separate table; from the use case, field_description is a table
containing a description over limited number of events for each
field. Instead of representing such descriptions with a class, we
chose to relate these values by a limited set of properties directly
related to the field. In the other case, removing attributes from
entity maps can be necessary when the columns are duplicated
across multiple tables. In the use case datasets, columns from
tables that are referred to are often also included in other tables
even though the table is properly referenced using a primary key
value, for example the table field_operator_hst in Fig. 7
contains the field name (fldName) even though this is also a

specification maps.

 Selecting the attribute maps to map for each entity map.
 Deciding what type of property columns should be mapped to.
 Identifying columns that contain values that should be handled

by subtype maps.

column in the table field which field_operator_hst refers
to by a foreign key. This denormalisation is probably implemented
to make the database more user-friendly, but creates an erroneous
ontology by connecting a property to the wrong classes.

It is natural to map a table to multiple classes when the table represents more than one concept. From the use case, the
wellbore_exploration is an example. This table contains
information about exploration wellbores, including information
about the stratigraphic layers that the wellbore penetrates. Ad-
ditionally, the table includes information that is directly associated to the stratigraphic layer, but not the wellbore. In this
case we have separated the information about the stratigraphic
layer into separate classes, by specifying two entity maps for the
wellbore_exploration table, and partitioning the attribute
maps over these entity maps.

When specifying subtype maps, important considerations are
how to generate URIs from the values that represent subclasses. In
many cases using a translation table is a good choice, as it gives
detailed control of the translation, for instance when values are
to be translated to class URIs defined in existing ontologies. The
downside of using translation tables is that the translation table
needs to be kept updated to reflect the values in the corresponding
database column.

Since a URI can and must be specified for every specification
map where this is possible, it is easy to design your own ontology
vocabulary or use an existing vocabulary when lifting the database
to an ontology.
Specifying mappings from the database to the ontology. As with the
lifting of the tabular files to a database, this step is also technical
in character as it concerns the problem of fitting database values
into RDF format. The tasks that are necessary in this step are:

 Designing URI patterns.
 Setting correct datatypes or language tags.
 Possibly translating values with translation tables.

Designing a consistent set of URI patterns can increase the
semantic value of the resulting URIs. As a case in point, we specified
informative URI patterns which often were built by extending
existing URI patterns, e.g., three URIs for a wellbore, a wellbore core
and a wellbore core photo, are

npd:wellbore/5
npd:wellbore/5/core/6
npd:wellbore/5/core/6/photo/7607/2714-2718m

The structure of these URIs indicate a hierarchical relationship
between the instances they represent and may help a user better
understand the dataset. There are however issues with including
information in URIs ; for general discussions on how to design
proper URIs, see [52,53].

Specifying datatypes for datatype property values are usually
not needed, since the translation of the common SQL datatypes
to RDF datatypes is set by the translation scripts. However, for
less common datatypes this may be necessary. In the use case,
the datasets contained geometric values stored as a GEOMETRY
datatype in the database. These values were represented as RDF by
using datatypes from the GeoSPARQL ontology [54].

We used translation tables to translate database values to URIs
in order to represent geologic periods, which are named with
string such as CRETACEOUS, TRIASSIC, LATE JURASSIC in
the database, using the International Chronostratigraphic Chart
(2012) (ICS) ontology [55], which models the global time scale and
represents these eras as individual URIs.

3.3. Further ontology development

Although the ontology that is produced by the engineering
process is an significant improvement over a direct mapping
generated ontology, this technique cannot in general replace
manual ontology development. There will be information that is
difficult to extract from the database and also simpler to include
in the ontology using a regular ontology editor, rather than adding
such capabilities in the specification language.

In our case, we extended the ontology by manually adding axioms which capture information that was not available to the automatic process. This was largely done to adding atomic general superclasses to the generated classes, e.g., npdv:Facility was set
as the superclass of the generated classes npdv:FixedFacility
and npdv:MoveableFacility, and introducing a set of mutually disjoint top level classes like npdv:Agent, npdv:Area and
npdv:Point. A selection of classes and properties that model geographical data were mapped to the GeoSPARQL vocabulary and to
the ICS vocabulary.

The manually constructed ontologies were kept in separate files
that represent separate domains: geology, geometry, facilities (off-
and onshore constructions), organisational, petroleum production
and seismic surveys. In order to ensure a minimal quality of the
integration of the different ontologies, all classes where placed
under an appropriate class from the upper ontology Basic Formal
Ontology BFO [40]. Only for the NPD FactPages dataset did we do
a thorough manual ontology engineering effort. The ontologies
created for each data source were imported to a hub ontology
containing only import statements. The three data source hub
ontologies were imported to a central hub ontology called NCS.
The import relationships between the ontologies are illustrated in
Fig. 14.

3.4. Use case tools

As the primary OBDA tools in our case study we have used
Ontop [26] version 1.9, Fuseki [31] version 1.0.0, D2RQ [21] version
0.8.1, and Stardog [32] version 2.0. These are further described
below. The relational database we used in our study was MySQL
version 5.1.
Fuseki. Fuseki is a SPARQL server that is part of Apache Jena, a
free and open source Java framework for building Semantic Web
and Linked Data applications [31]. The standalone server version
(which is the version we used) is bundled with the RDF store TDB,
also a component of Jena. In addition, Jena supports storing data in
a relational database (SDB) or in-memory. Querying the data is done
using the query engine ARQ. Fuseki provides an implementation of
SPARQL 1.1 over HTTP, using ARQ.
Stardog. Stardog [56] is a commercial RDF database developed
by Clark & Parsia and performs reasoning in a very lazy and
late-binding fashion: it does not materialise inferences; but,
rather, reasoning is performed at query time according to a
given reasoning level. This allows for maximum flexibility while
maintaining excellent performance and scalability. [32]
D2RQ. D2RQ [57] was initially developed by the Freie Universitat
Berlin and is a system for accessing relational databases as virtual,
read-only RDF graphs [21]. The relationship between the relational
database and the RDF view is given by a set of mappings, similar
to those of Ontop. RDF triples are not stored in a triple store;
rather, D2RQ transforms incoming SPARQL queries to SQL queries over
the relational data. D2RQ does not provide ontological reasoning
capabilities.
Ontop. Ontop [58,59] is developed at the University of Bozen-
Bolzano. Query execution in Ontop is based on two rewriting steps,
as illustrated in Fig. 1(b); the user inputs a SPARQL query q, which

Fig. 14.
Import hierarchy of ontologies. Ontologies suffixed with -db are lifted
from the databases with the OBDA specification. BFO, ICS and GeoSPARQL are external
ontologies, while the others are manually constructed. NPD-BFO contains a mapping
from NPD to BFO using subclass axioms. sql contains the annotation properties used
by the ontology rule of the specification.

is rewritten to q0 using the ontology. q0 is then unfolded into the
SQL query qDB using the mappings. The latter query is passed to
the database. qDB is constructed such that the answer from the
database is already a serialisation of the triples in the answer, and
does not need any further processing before being passed back to
the user. This is a high level overview leaving out many details,
in particular, many features and optimisations to facilitate faster
query answering.
In summary, Fuseki and Stardog provide a physical triple store
which require that the source data is transformed into RDF,
while Ontop and D2RQ are based on a virtual triple store and
transform SPARQL queries into SQL queries which are evaluated
over a relational database. Stardog and Ontop support ontological
reasoning and both use rewrite techniques for answering queries.
Since Ontop relies on query rewriting for the transformation
process, reasoning support is restricted to the OWL 2 QL profile.
Stardog, on the other hand, supports all OWL 2 profiles, but
guarantees only complete answers for OWL 2 QL ontologies.

3.5. Technical setup of the use case study

The overall goal of our use case study is to install, design and
test multiple OBDA systems that efficiently accesses all the selected
data sources, and in the process developing and using the OBDA
engineering specification. In this section we will report from many
of the specific problems that we were faced with when lifting
and representing the source data in different formats: a relational
database, an ontology, and mappings, but also problems that were
due to the specific formats and tools used in the study.

As discussed in the introduction and illustrated in Fig. 1, two
important choices when designing an OBDA system is deciding
on the type of transformation, if the system shall materialise the
data as RDF (data transformation) or leave the data in the original
sources and translate queries over the ontology to queries over the
database (query transformation), and if the system shall support
ontological reasoning. In our case study we have installed instances
of all combinations for the two choices:
1. Data transformation and no reasoning. The main tools involved
were D2RQ for transforming the data, and Fuseki as triple store
and query processor.

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. 15. Schematic overview of the deployment process.

2. Data transformation and reasoning support. The main tools
were D2RQ for transforming the data, and Stardog as triple store
and reasoning enabled query processor.

3. Query transformation and no reasoning support. The main tools
were D2RQ for query transformation and MySQL as underlying
relational database.

4. Query transformation and reasoning support. The main tools
were Ontop for query transformation and query rewritingbased reasoning and MySQL as underlying relational database.
The development and deployment process for the OBDA
specification for and on these architectures as it was performed in
our use case study is illustrated in Fig. 15. These steps were:
1. Downloading data.
2. Bootstrapping the OBDA engineering specification.
3. Creating and loading the relational database.
4. Generating mappings.
5. Materialising data as RDF.
6. Developing the ontology.
7. Loading the systems with data, ontology and/or mappings.

After an initial download of data and the bootstrapping of an
OBDA specification, the specification was active in steps 1, 3, 4
and 6: Since the filenames of the CSV files are represented in the
specification, this information was used to script an automatic
download of the CSV data. The bootstrapping of the specification
was only performed once. Steps 3, 4 and 6 was implemented as
already explained in this section. Step 5 was performed by the
export capabilities of D2RQ, while in the last step the different
import features of the individual OBDA tools were used.

These seven steps were not performed in a single straight
sequence, but with longer and longer iterative development cycles
adding more information and features to the specification. These
cycles can be categorised in four:
1. Creating and loading tabular files into the database.
2. Designing RDF output.
3. Ensuring that the produced artefacts loaded into the system.
4. Further ontology development.

In this process the OBDA specification was key in ensuring that

all artefacts were synchronised, for instance:
 All URIs are synchronised in all representations.
 Values used to build individual URI are always taken from

primary keys.

 Other values that form part of URIs are indexed by the database

to make query answering more efficient.

 All URI patterns may be developed in a single specification.
 Mappings, which are very verbose and difficult to manually edit,

are automatically generated.

3.6. Key characteristics of the use case configuration

The input and output of the different conversion steps are
summarised in numbers in Tables 1 and 2. Table 1 shows in
numbers how the input data evolve into mappings and classes and
properties through the conversion process. A total of 87 CSV files,
one spreadsheet and one relational database was input as source
data to the process. This was converted into a relational database
with 107 tables and 1435 columns. To materialise the database as
RDF, a D2RQ map for each of the data sources was generated and
used to dump the datasets to RDF files containing respectively 2M,
5M and 22M triples using approximately 4.5 h, 2.5 h and 10 h for
the NPD FactPages, NORLEX database and the Diskos data.

The numbers in Table 1 reveal that the schema of the source
data is faithfully represented in the relational database, as seen
by the fact that the number of CSV files equals the number of
tables and CSV fields are almost the same as the number of SQL
columns. The added columns are auto increment-valued columns
that are created in the absence of a suitable primary key for some
tables. Additionally, the increase from number tables to number
of D2RQ class maps shows that more than one class per table
is created. There are fewer D2RQ columns, due to the effort of
removing duplicated unnormalised data in the mapping of the
database. Furthermore, that there are more RDF classes than D2RQ
class maps shows that some maps generate classes. The difference
between D2RQ property maps and RDF properties is explained by the
fact that many tables contain columns with the same name, and
also some columns with different names are mapped to the same
property, e.g., fldName (field name) and cmpLongName (company
name) are both mapped to npdv:name. Finally, the numbers for
Ontop maps versus D2RQ maps show that Ontop mappings are more
verbose than D2RQ s.

It is evident from the number of generated triples that the
NPD FactPages dataset is the smallest of the three data sources.
However, it is the most complex in terms of the number of classes
and properties, containing more than half of the total number of
classes and properties generated from all of the data. The Diskos
database is by far the largest dataset, giving almost 75% of the total
number of 29M triples.

Table 2 lists the numbers of axioms, classes, properties and
individuals and the expressivity of the ontologies developed in our
case study, using data as reported by Protege 4.3.13 The table is
grouped according to dataset. Central hub ontologies, i.e., npd,

13 http://protege.stanford.edu/.

Table 1
Conversion process results in numbers.

CSV files
SQL tables
D2RQ class maps
RDF generated classesb
OWL generated classesc
CSV fields (distinct)
SQL columns
D2RQ property maps
RDF generated propertiesb
OWL generated propertiesc
Ontop maps
RDF triples
Time to produce, hours
a 17 CSV files and one spreadsheet.
b Counted by querying a preliminary D2RQ SPARQL endpoint.
c Counted by querying the generated RDF/OWL ontology file.

2,002,556

NORLEX

5,389,793

Diskos
17 + 1a

21,924,435

Total

29,316,784

Table 2
Ontology metrics.

Ontology

sql
npd-db
npd-facility
npd-geology
npd-geometry
npd-org
npd-prod
npd-seismic
npd-bfo
npd
npda
npdb
norlex-db
norlex
norlexab
diskos-db
diskos
diskosab
ncs
ncsa
ncsb

Axioms

Logical axioms

Classes

Object properties

Datatype properties

Individuals

Expressivity

ALE (D)

ALF (D)
ALCHIF (D)
SHOIF (D)
ALE (D)
ALC(D)
ALC(D)
ALE (D)

ALE (D)

ALCHOIF (D)
SHOIF (D)

Metrics given with no ontology imports, unless noted otherwise.

a Only local imports.
b All imports.

norlex, diskos and ncs, which (indirectly) import all ontologies
in our study, are listed both with and without imports, and with
and without external imports. The norlex and diskos ontologies
do not import external ontologies.

The numbers in Table 2 show, as also seen in Table 1, that
the ontology for the NPD FactPages is the largest. Furthermore, we
see that the generated ontologies (db) contribute to the largest
numbers, disregarding the external imports, and that their expressivity is consistently ALE (D), as expected from our translation
of database constraints to ontological axioms. We notice from the
expressivity column that the handcrafted ontologies are not very
expressive; they all have the lowest expressivity reported by Pro-
tege, AL,14 except for: npd-geology, which contains a few disjointness axioms; npd-geology, which contains an inverse object

14 Although the expressivity reported by Protege can be an important and valuable
characteristic of an ontology, it is not fine-grained enough to positively identify if
an ontology fits a specific OWL profile. AL is used as the base language, and all other
languages Protege can recognise are extensions of this, hence it is merely an upper
bound, meaning that the ontology is expressed within that language.

property; norlex, which contains intersection axioms; and npd,
which contains one functional data property.

All data files, mappings and ontologies from the conversion procedure, except those concerned with the Diskos data,
which is not publicly open data, are published on the website
http://sws.ifi.uio.no/project/npd-v2/. This includes the NPD FactPages CSV files, the NORLEX database dump, the combined database
dump of the two datasets, RDF dumps, D2RQ maps, Ontop maps and
the developed ontology. The NPD FactPages and NORLEX data are also
published as linked open data at http://sws.ifi.uio.no/data/npd-v2
and http://sws.ifi.uio.no/data/norlex, respectively.

3.7. Query evaluation experiment

The OBDA systems in our case study were evaluated by querying
the systems with a series of SPARQL queries. In this section we
describe the concrete setup of this evaluation.
Queries. Our evaluation consists of a total of 42 SPARQL queries. 21
of these were constructed by translating questions provided by
domain expert users of the original datasets into SPARQL queries.
These queries are listed in B.1 together with the original domain

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Table 3
Query evaluation result I: Domain expert queries.

Query
Query 1

Query 2

Query 3

Query 4

Query 5

Query 6

Query 7

Query 8

Query 9

Query 10

Query 11

Query 12

Query 13

Query 14

Query 15

Query 16

Query 17

Query 18

Query 19

Query 20

Query 21

Fuseki

Stardog (SD)

D2RQ

Ontop


SD +QL

SD +RDFS

SD +EL

SD +RL

The number of results returned by the system for the given query (in bold), and below, the time in seconds.

a Error message: Error, the translation of the query generated 0 rules. This is not possible for any SELECT query (other queries are not supported by the translator). Probable cause:

query contains a subquery with a variable bound by the outer query.

b Timeout: System does not return within 5 min.

questions. The questions can be grouped into three categories:
Production Licences, Fields and Wellbores. All of the questions
concern in part the NPD FactPages, which is natural, since this data
source covers all of the category subjects, while the Diskos dataset
and NORLEX are mainly only concerned with the category Wellbores.
For this reason, only Wellbore queries make explicit references also
to other datasets than the NPD FactPages.

The other queries in our evaluation were designed to test and
compare the usability of the systems with respect to efficiency
and specific language support, e.g., support for various SPARQL
query constructs, and test how different types of reasoning
configurations and the use of ontological constructs can affect what
answers are returned by the systems. For this reason the queries
are mostly small and focused; we call these queries, listed in B.2,
unit test queries.

We recorded the time it took for the queries to return results,
and also counted the number of results returned by each tested
OBDA system for each of the queries. The SPARQL queries were often
constructed so that they answer the intended question in a more
general manner, usually returning more results, and we often use
DISTINCT when this is necessary to make sure that the number of
answers returned by different systems are comparable.

Setup. We issued the queries over HTTP and measured the time
for the systems to return results by the help of the statistics
programming language R [60] and the package RC url [61]; RC url an
interface to cURL [62], a well-known data transfer tool. The results
of the query evaluation experiment are listed in Tables 3 and 4;
Table 3 contains the results from the domain expert queries, and
Table 4 the unit test queries. For each query we give the number
of returned results and the time it took for all the different OBDA
configurations: Fuseki, Stardog without reasoning enabled, D2RQ,
Ontop, and Stardog with respectively OWL 2 QL, RDFS, OWL 2 EL, and
OWL 2 RL reasoning enabled. The timeout on each query was 5 min.
For each OBDA configuration, the query evaluation process is as
follows. First the order of the 42 queries is randomised, then each
query is run against the endpoint using RC url. This is repeated
25 times. The times listed are the median value of the 25 values,
and given in number of seconds. Ontop was set up using the Jetty
webserver packaged with the Sesame Workbench as published on
the Ontop website [26], configured to use a maximum of 8 GB
memory. D2RQ was deployed on a Tomcat server, given a total of
4 GB. Fuseki and Stardog were executed with their self provided
server scripts, but given respectively 4 and 6 GB of memory.
Disclaimer. In the case of D2RQ, the configuration of the system in
our evaluation is loaded with a map containing the URIPATTERN

Table 4
Query evaluation results II: Unit test queries.

Query
Query 22

Query 23

Query 24

Query 25

Query 26

Query 27

Query 28

Query 29

Query 30

Query 31

Query 32

Query 33

Query 34

Query 35

Query 36

Query 37

Query 38

Query 39

Query 40

Query 41

Query 42

Fuseki

458d

Stardog (SD)

458d

D2RQ


Ontop

SD+QL


428e

SD+RDFS


427e

SD+EL


431e

SD+RL


426e

The number of results returned by the system for the given query (in bold), and below, the time in seconds.

b Timeout: System does not return within 5 min.
c Error message: XML Parsing Error: not well-formed. Probable cause: query selects GEOMETRY datatype values.
d 36 of the results are different blank nodes.
e Examining the results reveals that result contains duplicates; there are 425 distinct results.

bug fix, as described in [63]. This means that the URIs produced by
the system are incorrect, and thus the number of results returned
may very well not be correct according to what a correct intended
mapping would give. However, we believe that in those cases
where the pattern of the URI is used only to return the results of the
correct format (and, e.g., not to match with resources generated
from multiple class maps), D2RQ will return the correct number
of results. We do not use a mapping that would return correct
URIs since initial testing revealed that this map severely hampered
query execution and left the system unresponsive for most queries
in our evaluation. For this reason we will not further evaluate the
query results returned by D2RQ, other than presenting the numbers
of answers returned and the time it took together with the other
systems in Tables 3 and 4.

The query evaluation experiment was performed in 2013. Since
then, Fuseki, Stardog and Ontop have released new versions which
may include more query language and reasoning support. We refer
the reader to the tools webpages for more information.

3.8. Query evaluation results

We now present the results from our query evaluation. We first
consider the results from the queries constructed from domain
expert questions, then the unit test queries.
Domain queries. As general remarks, we observe that for all these
queries, if the query returns any answers from a system, then
the number of results is the same as for the other systems that
return answers for the same query. This conformity of answers
might be surprising at firstwhy does reasoning not result in
added answers? However, on inspection of the SPARQL queries, it
is clear that the queries use only vocabulary that is generated
from the data sources and not vocabulary from the manually
constructed ontology. Since the manually constructed part of the
ontology consists mostly of adding superclasses to the generated
part, it is reasonable to assume that the manually added ontology
is a conservative extension [64] of the generated part, such that
reasoning over the whole ontology will not add any additional
query answers. Given that all systems deliver the same number of

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

results or no results, we focus our attention on the queries that do
not return results for some systems.

Of the 21 domain queries only 5 return results when issued to
Ontop. This is largely because the evaluated version of Ontop does
not support the full SPARQL 1.1 query language [65], but only most
parts of SPARQL 1.0. Specifically, aggregate functions like MIN, MAX,
SUM and COUNT from SPARQL 1.1 are not supported. Also, the wellknown workaround of using OPTIONAL and the bound function,
which are defined in SPARQL 1.0 and can be used to mimic some of
the aggregate functions, are not supported by Ontop. This affects
Query 2 (MAX), Query 4 (MIN), Query 6 (COUNT), Query 9 (SUM),
Query 10 (SUM) and Query 11 (SUM). The questions behind Query 2
and Query 4 require the use of MAX and MIN since they ask for the
latest and earliest time of specific events. Query 6 explicitly asks
for the number of wells in a licence, and answering the queries 9,
10 and 1115 requires adding production numbers, in both cases the
use of COUNT and SUM, respectively, is unavoidable.

Query 14 uses the SPARQL 1.1 constructs BIND and IF to find
the stratigraphic units a wellbore core sample overlaps using the
top and bottom depths of the units and the sample. The same
question is possible to represent with a slightly different SPARQL
query that does not use BIND and IF, essentially by rewriting
IF into a different Boolean test using logical-and and logical-or.
Hence, even though Ontop does not return any results from Query
14, it is possible to represent the question in the query language
fragment that Ontop does support.

Query 15 includes a regex function in a FILTER. This is
not supported by the evaluated version of Ontop. The regular
expression is used as a straight-forward way of selecting all
companies with a name starting with Statoil. The company
commonly known as simply Statoil has had several formal names
through history, due to mergers and restructuring, so selecting
all previous versions of the company is interesting for queries
about historic data. All variants of, e.g., Statoil, could of course
be selected by a query by explicitly giving all identifiers, but in
order to declaratively select all historic instances of a company
one would need to transitively follow a relation which relates the
old and new company in a so called change-of-name transfer. The
current ontology does not support such a query.

Queries 1 and 8 return the error message that the query
generated zero rules; we assume that this error is caused by the fact
that these queries contain a subquery which use a variable bound
by the outer query, since an error message given by earlier versions
of Ontop and Fuseki reported this as the cause of the malfunction.
As for Query 16, which asks for all fields with wellbores that are
drilled earlier than 2000, for which there is a core sample available,
and where the oldest penetrated age is Jurassic, we believe that
the reason for zero results returned is that the query contains an
individual, the geological era Jurassic. This individual exists only
in an external ontology packaged with the OWL ontology that is
loaded into Ontop, i.e., it does not exist at the data sources. The
functionality of also considering individuals given in the ontology,
i.e., materialised data, together with the data produced by the
mappings, i.e., the virtual ABox, is called hybrid Abox functionality
and was first added in the current version of Ontop, and it is
a feature that Ontop as the only virtual ABox rewriting system
supports.16 To test our hypothesis we created a slightly modified
version of the query, Query 17, which answers the same question
as above, but without the requirement that the oldest penetrated
age is Jurassic. Ontop returns 420 results for this query, the same
as of the other systems, which supports our assumption.

15 Note that Query 11 is a generalisation of Query 10.
16 http://ontop-obda.blogspot.it/2013/09/hybrid-rdf-graphs-or-hybrid-aboxes-
as.html

The domain question that produced Query 12 asks for the kinds
of facilities that are used for a field, e.g., oil producing facility. The
corresponding query contains rdfs:subClassOf, used to get all
subclasses of Facility, and hence requires the reasoner to infer facts
from both the TBox and the ABox in order to correctly answer
the query. TBox reasoning is generally outside the scope of OBDA
systems as the primary goal of OBDA is querying over large datasets.
The corresponding query contains a triple ?x rdfs:subClassOf
:Facility, used to get all subclasses of Facility, which requires
instantiating a variable with a class resource, and querying the type
hierarchy. Such queries that combine the type level and instance
level of the ontology are generally not supported by OBDA systems.
We see that the only systems that return results in our evaluation
setup is Fuseki and Stardog without reasoning, returning 1106
results. Ontop returns zero results, and Stardog with any reasoner
enabled does not return results within 5 min.

The last queries of the domain queries that do not return
results from one or more of the systems are 19, 21 and 20. Query
20 asks for all wellbores drilled before 2000 for which there exist
biostratigraphic samples from a cuttings sample, and the log curve
is stored online. This query selects data from potentially all data
sources, and uses vocabulary that is generated from all three
datasets; classes and relations for biostratigraphic samples are
defined from the NORLEX dataset, log curves come from the Diskos
dataset, and the NPD FactPages vocabulary is natural to use when
possible since it plays the role of an upper domain ontology in our
setupand it contains the entry year of wellbores. The query relies
on equality reasoning to return answers. A wellbore that occurs
in multiple datasets will not have the same URI, all data records
from the sources are put in the namespace constructed for each
dataset. To identity URIs which represent the same actual wellbore,
we use owl:sameAs whenever this information is available in the
datasets. Since the query requires reasoning to compute answers, it
is no surprise that Fuseki and Stardog without reasoning return no
results, and since the OWL 2 QL profile does not permit owl:sameAs
statements, this explains why Ontop and Stardog with QL reasoning
enabled do not return any results either. However, the OWL profiles
EL and RL do support owl:sameAs statements, but unfortunately
Stardog does not support this feature: Only explicit owl:sameAs
and owl:differentFrom data assertions will be taken into
account for query answering. [66] In order to construct a query
that returns results for the question, we need to explicitly encode
the equality relationship into the query, this is done in Query 21.
By interpreting the query in a literal manner, we see that it asks
for three different wellbores from each of the datasets, collecting
different information about them from each set, and requires that
the three wellbores are related by the owl:sameAs relation. Note
that the direction of the relation is not arbitrary; since Stardog and
Fuseki (in our evaluation setup) do not infer any new facts from
owl:sameAs statements, e.g., that it is a symmetric relation, the
query can only make use of facts that are explicit in the dataset.
For Query 21 all configurations of Stardog return 68 results, Fuseki
times out, and Ontop returns zero results.
Unit test queries. The unit test queries are designed to test specific
features of the query answering capabilities of the systems,
by using small and focused queries. However, since some test
functionality of one particular system might not be supported by
other systems, some of these tests give unexpected results, which
might well be due to a malformed query from the viewpoint of the
system rather then ill-behaviour by the system. We discuss them
in turn, although identifying the cause of all discrepancies between
query answers is outside the scope of this paper.

Query 22 selects all resources that are related by owl:sameAs.
As expected, Fuseki and Stardog return the same numbers of
results, 7441, confirming that Stardog does not perform any
equality reasoning. Ontop, however, returns 7312 results, which

is roughly the set of answers returned from the other systems, but
without the owl:sameAs relationship that exists in the ontology
files.

Query 23 effectively asks for wellbores that have the same name
in the NPD FactPages dataset and the NORLEX database, requiring
that nlxv:Wellbores are inferred as npdv:Wellbores using a
subclass axiom. All systems return the expected results.

Queries 24, 25 and 26 ask for geographical data. Running
queries 24 and 25 against Ontop gives a parsing error, which
we believe is caused by the mapping of the GEOMETRY datatype.
Ontop did not accept a mapping were the geographical data was
converted into text; the data is therefore probably given to the
endpoint in an unexpected format, which results in the error. By
removing the geographical values from the query, giving Query 26,
Ontop returned the expected answers, although without removing
duplicates.

With Query 27 and Query 28 we test the support of inverse
object properties. The property npdv:isGeometryOfFeature
is defined as the inverse property of geo:hasGeometry. Inverse
properties are included in the OWL profiles QL and RL and as
expected only Ontop and Stardog with QL or RL reasoning enabled
returned any answers. By formulating the query using the SPARQL
inverse path construction, we were able to return the same results
as for Query 27, but for all systems (excluding D2RQ). This shows
that it is possible in some cases to use query constructs as
substitutes for lack of reasoning support. There were less answers
returned by Ontop for these queries; we assume this is due to the
different problems related to the representation of geographical
data using Ontop. Query 29 and Query 30 show similar results;
the first uses alternate property paths and gives the same results
as the latter query, which uses the super property of the alternate
properties of the first.

The queries 31, 32 and 33; and 34, 35 and 36 are two groups
of queries that respectively ask for all subclasses of a given class,
all instances of subclasses of the same class, and all instances
of the given class. Their results indicate firstly, that queries that
require combined type level and instance level reasoning are not
supported by rewriting systems as Ontop. Secondly, combined
reasoning is computational expensive, and lastly, the simple fact
that reasoning can dramatically increase the number of answers
for a query.

Query 37 is one of the few of the queries in the evaluation that
returns different results of different profiles in Stardog. The query
asks for all discovery wellbores. Note that the time dramatically
increases for the configurations that return more answers.

The queries 38, 39 and 40 return respectively all instances
of owl:Class, rdf:Property, and of owl:ObjectProperty,
owl:DatatypeProperty and owl:AnnotationProperty. For
the first query, Stardog does not remove duplicate answers although DISTINCT is used in the query. Otherwise, the results are
as expected.

Query 41 asks for all the types of a particular wellbore
individual. We see that the different profiles give different answers.
However, using a more general query, Query 42, all triples
where the subject is this same wellbore individual, then Fuseki
and Stardog with any reasoning configuration return the same
answers, and with less types than in Query 41. Ontop returns a
few more results than the other systems. We believe the reason
for these results are that Query 42, due to its very general graph
pattern, leaves the type of the predicate variable [67, ch. 7.1.3]
unspecified. Stardog handles this by computing results from
explicit triples only, while Ontop performs regular reasoning.

4. Conclusion and future work

We observed that the transformation of existing real-world
datasets into high-quality semantic web data is a complex task that
requires the concerted execution of a variety of processing steps
using a range of different tools. Creation and maintenance of such
datasets in the face of changing data and requirements leads to
a significant engineering challenge, that requires appropriate tool
support.

The main contribution of our work is a proposed architecture
that ties together and drives all the required components for all
configurations used in the transformation and publication process.
This architecture hinges on a single declarative description or
specification of the whole setup, that is translated into an ontology,
mappings, and any other objects required to drive the tool chain. In
this way, firstly, any changes to the involved tools, data, schemata,
ontology, etc., are limited to single points of change (either in
the specification itself or in the translation rules), and secondly,
the specification itself becomes an engineering artefact that can
be subjected to version control, documentation, tool support, and
more.

We have performed a thorough case study where we transform
a number of relational (SQL) and tabular (CSV) datasets from
the petroleum industry domain into high-quality RDF data, using
a variety of approaches and tools. In particular, we included
approaches using materialised RDF triple stores, as well as virtual
stores where incoming SPARQL queries are translated to queries over
the data sources in their original form. We also included tools with
reasoning support for different fragments of the OWL language.

Using a collection of queries elicited directly from individuals
in the petroleum industry who work with the underlying data
on a daily basis, we could run a comparative evaluation of the
suitability of the different approaches for the chosen queries, using
the particular tools chosen, on that particular data. We reported
the results in terms of evaluation time and completeness of query
answering.

However, the results of that comparison are clearly ephemeral,
since tools evolve rapidly, requirements will be different for
different data or different users of the same data, etc. We therefore
stress the importance of conducting query evaluations using real
queries when deciding on what OBDA system architecture and tools
to deploy.

In future work, we will extend the specification language with
more features and develop and release open source tools that
implement the specification and that can leverage existing tools
in the bootstrapping process of the OBDA specification. We will also
adopt the R2RML standard for mappings as part of our architecture,
as it starts to be adopted by an increasing number of tools we use.
We also want to investigate the possibility of configuring visual
query interfaces (like, e.g., [68]) for the produced datasets from the
same declarative specifications.

We also intend to continue work on the petroleum domain
case study, refining the ontology and possibly integrating further
datasets. The results will play an important role for the industrial
use cases of the Optique project [69,70].

Acknowledgements

This research was partially funded by the Seventh Framework
Program (FP7) of the European Commission under Grant Agreement
318338, Optique17 and The Research Council of Norway through
the Semicolon II project.18 We thank the employees of Statoil who

17 http://www.optique-project.eu/.
18 http://www.semicolon.no/.

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

Fig. A.16. Schematic overview of the OBDA engineering specification. (This figure is a copy of Fig. 4.)

provided the domain queries. Thanks to the Norwegian Petroleum
Directorate (NPD) for making the data of the NPD FactPages available
as open data, to yvind Hammer at the Natural History Museum in
Oslo for giving us access to the NORLEX database dump, and to Atul
Solanki at Halliburton Landmark Software & Services for providing
access to the Diskos database. Also thanks to the anonymous
reviewers whose feedback greatly improved the presentation of
this article.

the second list contains the referential constraints between fields
in a specification.
Permissible values. Default values are used when no value is set for
a field. A field without value and for which no default value is set,
is an unspecified field.

 E1 (tbl) and A1 (col) must contain legal table and column names

respectively.

Appendix A. The OBDA engineering specification language

The OBDA engineering specification language is a declarative
language for describing a set of maps that specify the translation
of a common set of modelling structures into a relational database,
an ontology and mappings between the ontology and database.
These constructs are (1) entities, (2) identifiers, (3) attributes, (4)
relations, and (5) subtypes. How the maps of these constructs are
represented in the specification language is listed in Fig. A.16.19
When referring to a field, for readability, we include the short
name of the field, which is unique among the fields for a map. For
example, E1(tbl) refers to the table name field of the entity map.
The specification language description is functional in the sense
that we do not prescribe a specific serialisation for representing
specification instances.
In the use case that has driven the
development of the specification language, the OBDA use case
specification was represented as a set of CSV files, but other formats,
like RDF or XML, could have been used.

The syntactic requirements of the current version of the
specification language, and the rules for translating an OBDA
specification for a set of source data into a relational database, an
ontology, and mappings, are given below. For examples and an
illustration of use of the specification, we refer to Sections 2 and
3.

A.1. Syntactic requirements

The syntactic and structural requirements that a specification
instance must abide by are given as two lists. The first list presents
the permissible values for the different fields in a specification and

19 This table is a copy of Fig. 4 that we included here for easy reference and in order
to make this description of the specification language self-contained.

 E5 (URI), A5 (URI), R5 (URI), S3 (URI) must contain legal URIs [71],

or no value.

 E2 (lab), E6 (desc), A4 (lab), A11 (desc) can contain arbitrary

strings.

 I3 (patn) and A8 (patn) must contain a URI with special role

fillers, or no value.

 A6 (type) must contain one of the values D (DataProperty),
A (AnnotationProperty), O (ObjectProperty), or OP
(ObjectPropertyPattern), or no value. The default value is
D.

 A7 (tran) and S6 (tran) must refer to a transformation table, or

contain no value. The default value is the identity function.

 A9 (lang) must contain a legal RDF language tag [72], or no value.
 A10 (XSD) must contain a URI denoting an RDF datatype, or no

value.

 A2 (SQL) must contain a string denoting a SQL datatype, or no

value.

 A3 (null) must contain a Boolean value.
 S4 (pre) and S5 (suf) must contain strings that can form part of

a URI, or no value. The default value is the empty string.

Reference requirements:

 An entity map refers to an identifier map (E3 (id)) and a list of

attribute maps (E4 (attr...)).

 An identifier map refers to an entity map (I1 (ent)) and a list of

attribute maps (I2 (attr...)).

 An attribute map can refer to a transformation table (A7 (tran)).
 For a relation map, source and target entity maps, R1 (src) and
R3 (trg), refer each to some entity map; and source and target
columns, R2 (scol) and R4 (tcol), refer each to some column
name (A1 (col)).
 A subtype map refers to an entity map (S1 (ent)) and a column
name (S2 (col)  A1 (col)).

A.2. Rules

When referring to specification values, we use the following dot
notation. Let m be a map and V1 the short name of a field (found in
Fig. A.16) available for this map, then m.V1 denotes the value of this
field.20 If a value is a reference to a map, then we can chain the dot
notation, for instance, if m.V1 refers to an entity map, then m.V1.URI
denotes the OWL class URI set for this entity map. To indicate a set or
list of values, we use three dots: m.V2 .... We also allow ourselves
to chain lists; assuming that V2 is a list of references to other maps,
m.V2 ... V3 is the list of V3 -values set for each of the V2 maps of m.

A.2.1. Database rules

Database rules specify how an OBDA engineering specification is

translated into a relational database.

When defining the rules to construct a relational database from
the OBDA engineering specification, we use the following pragmatic
definition of a database schema.

A database schema is a set of tables and a set of foreign keys. A
table is specified with a table name, a primary key, an optional
table comment and a set of columns. A column is specified with
a column namewhich is unique for the table, an SQL datatype, a
mandatory flag, and an optional column comment. A primary key
is specified as a list of column names. A foreign key is specified
with a source table name, source column name, target table
name and target column name.
Database Rule 1 specifies how to construct tables with columns

and primary keys. Database Rule 2 constructs foreign keys.

Database Rule 1 (Tables). For each entity map e, create a table by
setting
 the table name to e.tbl, and
 the table comment to e.lab.
 The primary key is declared as the set of column names

e.id.attr...col.

 For every field name f in the tabular file to which the table name
corresponds, let a be the attribute map where the column name
a.col is equal to f and add a column to the table by setting
 the column name to a.col,
 the column datatype to a.SQL,
 the column mandatory flag to a.null, and
 the column comment to a.lab.

Note that Database Rule 1 uses not only the specification, but
reads column names out of the first row of the tabular files to be
represented in a database.

Database Rule 2 (Foreign Keys). For each relation map r, create a
foreign key by setting
 the source table to r.src.tbl,
 the source column to r.scol,
 the target table to r.trg.tbl, and
 the source column to r.tcol.

A.2.2. Ontology rules

Ontology rules specify how an OBDA engineering specification is
translated into a ontology axioms. In order to keep the rules simple
and free from too may exceptions, we assume that an axiom that
involves a specification field that is unspecified is not created.

20 For example, if e is the entity map in Fig. 5(a) line 1, then e.URI is the value
npdv:FieldOperator.

Ontology Rule 1 declares classes, Ontology Rule 2 declares
properties, and Ontology Rule 3 constructs keys. Ontology Rule
4 and Ontology Rule 5 construct axioms that reflect respectively
mandatory attributes and foreign keys. Ontology Rule 6 extracts
subclasses from the source data.
Ontology Rule 1 (Classes).
following axioms:
Declaration(Class(e.URI))
AnnotationAssertion(rdfs : label e.URI e.lab)
AnnotationAssertion(rdfs : comment e.URI e.desc)

For each entity map e, create the

Ontology Rule 2 (Properties). For each attribute map a, create the
following axioms:
Declaration(X (a.URI))
AnnotationAssertion(rdfs : label a.URI a.lab)
AnnotationAssertion(rdfs : comment a.URI a.desc)
where X has the following value:
 if a.type = ObjectProperty
or a.type = ObjectPropertyPattern ,
then X = ObjectProperty.
 if a.type = DataProperty ,
then X = DataProperty.
 if a.type = AnnotationProperty ,

then X = AnnotationProperty.

Ontology Rule 3 (Keys). For each identifier map i, if every a in
i.attr... occurs in i.ent.attr..., create the following axiom:
HasKey(i.ent.URI (ao . . . URI) (ad . . . URI))
where ao . . . is the list of attribute maps in i.attr...that specify an
object property, and ap . . . is the list of attribute maps in i.null that
specify a datatype property.

Ontology Rule 4 (Mandatory properties). For each entity map e, and
for each attribute map a in e.attr..., if a maps to an object property or
a datatype property, and a.null is true, create the following axiom:
SubClassOf(e.URI A(a.URI X ))
where A and X have the following values:
 if a maps to an object property, then A is ObjectSomeValuesFrom

 if a maps to a datatype property, then A is DataSomeValuesFrom

and X is owl:Thing,

and X is rdfs:Literal.

For each relation map r, create the

Ontology Rule 5 (Relation).
following axioms:
Declaration(ObjectProperty(r.URI))
SubClassOf(r.src.URI

ObjectSomeValuesFrom(r.URI

r.trg.URI))

Unlike the rules presented so far, the next rule also directly
handles values collected from the source data. We give some
preliminary definitions to address the translation of these values.
A translation table is a partial function that maps strings (of
source data) to strings (representing RDF resources). If the function
is not defined for an input string, the output is unspecifiedand
the affected axiom is hence not created.

Let URIsubtype (s, v) be a function that takes a subtype map s and
a string v as input, and outputs a string that is the concatenation of
s.pre, s.tran(v), s.suf. The function transforms the input value with
the translation table in s.tran, and prefixes and suffixes it with s.pre
and s.suf, respectively.

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

SELECT

s.col FROM

Ontology Rule 6 (Subclasses from Data). For each subtype map s, let
C be the set of answers from the following query over the source data:
For each c  C, add the following axioms:
Declaration(Class( URIsubtype (s c)))
SubClassOf( URIsubtype (s c) s.URI)

s.ent.tbl.

A.2.3. Mapping rules

Mapping rules specify how an OBDA engineering specification
is translated into mappings that relate the ontology to the source
data on relational format.

The mappings are described by two parts. The first is an SQL
query over the source database, the second an RDF triple pattern
in turtle serialisation which may contain role filler parts placed
in {curly braces} that refer to names in the SELECT clause of the
SQL query, using the same format as for a template-valued term map
in the R2RML specification [37].

To help build such patterns we introduce the function URI-
pat(pattern, cols...) that takes as input a URI pattern of the form
used in the OBDA specification, where role filler parts are indicated
with strings of the form {$1}, and a list of column names cols....
The function returns a URI pattern on the R2RML format by replacing each role filler in the input pattern consecutively with the input column names surrounded with curly braces; for example, URI
pat(http://example.org/{$1}/bbb/{$2}/ddd, aaa, ccc)
returns
http://example.org/{aaa}/bbb/{ccc}
/ddd. Let URIentpat be the function that returns the URI pattern for
an entity map: URI entpat(e) = URI pat(e.id.patn, e.id.attr...col).

The function SQL2XSD maps a standard set of SQL datatypes to XSD

datatypes, for instance SQL2XSD(INT)= xsd:integer.

Mapping Rule 1 specifies mappings to create individuals, while
Mapping Rule 2 and Mapping Rule 3 make mappings that create
relationships between individuals. Mapping Rule 4 results in
mappings that produce the membership statements to those
subclasses extracted from the source data by Ontology Rule 6.

Mapping Rule 1 (Individuals). For each entity map e, create the
following mapping:

DB : SELECT e.id.attr...col FROM e.tbl
RDF : <URIentpat(e)> rdf:type < e.URI>

Mapping Rule 2 (Relationships). For each entity map e, and for each
a in e.attr..., create the following mapping:

DB : SELECT e.id.attr...col, a.col FROM e.tbl
RDF : <URI entpat(e)>< a.URI> O
where O has the following value:
 if a maps to a datatype property and
 if a.XSD is specified, then O = {a.col}a.XSD;
 if SQL2XSD(a.SQL) is specified,
 if a.lang is specified, then O = {a.col} a.lang;
 if a maps to an object property and
 if a.type is ObjectPropertyPattern ,

then O = {a.col}SQL2XSD(a.SQL);

then O = <URI pat(a.patn, a.col)>;
 if a.type is ObjectProperty ,
then O = < {a.tran(a.col)} >;
 else O = {a.col}.
Mapping Rule 3 (Relations). For each relation map r, create the
following mapping:

DB : SELECT r.src.id.attr...col, r.trg.id.attr...col

FROM r.src.tbl, r.trg.tbl

RDF : <URIentpat(r.src)>< r.URI><URIentpat(r.trg)>

Mapping Rule 4 (Subclasses from data). For each subtype map s, let
C be the set of answers from the following query over the source data:
For each c  C, add the following mapping:

s.col FROM

s.ent.tbl.

SELECT

DB : SELECT s.ent.id.attr...col, s.col FROM s.ent.tbl
RDF : <URI entpat(s.ent)> rdf:type <URIsubtype(s, c)>

Appendix B. Queries

All queries used in the use case query experiment are listed
here, partitioned in two: domain expert queries, which are queries
derived from real questions from domain experts, and unit test
queries, which are aimed to complement the domain expert
queries for the testing of the OBDA systems. The domain expert
queries are also given with the original question.

B.1. Domain expert queries

Query 1. Who are the licensees of production licence X and how big is
their share?

npdv:name ?licence .

[] a npdv:ProductionLicenceLicensee ;
npdv:dateLicenseeValidFrom ?date ;
npdv:licenseeInterest ?interest ;
npdv:licenceLicensee [npdv:name ?licensee] ;
npdv:licenseeForLicence ?licenceURI .
{ SELECT ?licenceURI (MAX(?d) AS ?date)

1 SELECT DISTINCT ?licence ?licensee ?interest ?date
2 { ?licenceURI a npdv:ProductionLicence ;

14 } ORDER BY ?licence DESC(?interest)

npdv:dateLicenseeValidFrom ?d ;
npdv:licenseeForLicence ?licenceURI .

{ [] a npdv:ProductionLicenceLicensee ;

} GROUP BY ?licenceURI }

Query 2. Who is the operator of licence X?

npdv:name ?licence .

(?date AS ?operatorSince)

[] a npdv:ProductionLicenceOperator ;
npdv:dateOperatorValidFrom ?date ;
npdv:licenceOperatorCompany
[ npdv:name ?company ] ;

1 SELECT ?licence (?company AS ?operatingCompany)

3 { ?licenceURI a npdv:ProductionLicence ;

15 } ORDER BY ?licence

npdv:operatorForLicence ?licenceURI .
{ SELECT ?licenceURI (max(?d) AS ?date)
{ [] a npdv:ProductionLicenceOperator;

npdv:dateOperatorValidFrom ?d ;
npdv:operatorForLicence ?licenceURI

} GROUP BY ?licenceURI }

Query 3. When was the licence X granted and to when is it valid?

1 SELECT ?licence ?dateGranted ?dateValidTo
2 { [] a npdv:ProductionLicence ;

6 } ORDER BY ?licence

npdv:name ?licence ;
npdv:dateLicenceGranted ?dateGranted ;
npdv:dateLicenceValidTo ?dateValidTo .

Query 4. When was the first well drilled in licence X?

1 SELECT ?licence (MIN(?entry) AS ?firstEntry)
2 { [] npdv:explorationWellboreForLicence

npdv:dateWellboreEntry ?entry .

6 } GROUP BY ?licence ORDER BY ?licence

[ a npdv:ProductionLicence ;

npdv:name ?licence ] ;

Query 5. What companies have been owners in licence X?

[ a npdv:ProductionLicence ;

1 SELECT ?licence ?company ?licenseeFrom
2 { [] npdv:licenseeForLicence

7 } ORDER BY ?licence ASC(?licenseeFrom)

npdv:name ?licence ] ;

npdv:licenceLicensee [ npdv:name ?company ] ;
npdv:dateLicenseeValidFrom ?licenseeFrom .

Query 6. How many wells have been drilled in licence X?

npdv:name ?licence .

{ SELECT DISTINCT ?licenceURI ?well {

{ [] npdv:explorationWellboreForLicence

1 SELECT ?licence (COUNT(?well) AS ?noOfWells)
2 { ?licenceURI a npdv:ProductionLicence ;

12 } GROUP BY ?licence ORDER BY ?licence

{ [] npdv:developmentWellboreForLicence

npdv:belongsToWell ?well . } } }

npdv:belongsToWell ?well . }

?licenceURI ;

?licenceURI ;

Query 7. What is the estimated reserves for field X and how much oil,
gas and condensate does the reserves consist of?

npdv:reservesForField ?f .

1 SELECT DISTINCT ?field ?OE ?oil ?gas ?NGL ?con
2 { ?fr a npdv:FieldReserve ;

10 } ORDER BY ASC(?field)

?f a npdv:Field ; npdv:name ?field .
OPTIONAL{ ?fr npdv:remainingCondensate ?con }
OPTIONAL{ ?fr npdv:remainingGas ?gas }
OPTIONAL{ ?fr npdv:remainingNGL ?NGL }
OPTIONAL{ ?fr npdv:remainingOil ?oil }
OPTIONAL{ ?fr npdv:remainingOilEquivalents ?OE }

Query 8. What is/was the top production month for field X?

npdv:productionForField [npdv:name ?field] ;
npdv:productionYear ?y;
npdv:productionMonth ?m;
npdv:producedOilEquivalents ?maxOE .

1 SELECT ?field ?maxOE ?y ?m ?oil ?gas ?NGL ?con
2 { ?f a npdv:FieldMonthlyProduction ;

} GROUP BY ?field }

16 } ORDER BY DESC(?maxOE)

OPTIONAL { ?f npdv:producedCondensate ?con }
OPTIONAL { ?f npdv:producedGas ?gas }
OPTIONAL { ?f npdv:producedNGL ?NGL }
OPTIONAL { ?f npdv:producedOil ?oil }
{ SELECT ?field (MAX(?OE) AS ?maxOE)

npdv:producedOilEquivalents ?OE ;
npdv:productionForField [npdv:name ?field] .

{ [] a npdv:FieldMonthlyProduction ;

Query 9. What is the total production of field X?

(SUM(?OE) AS ?sumOE)
(SUM(?oil) AS ?sumOil)
(SUM(?gas) AS ?sumGas)
(SUM(?NGL) AS ?sumNGL)
(SUM(?con) AS ?sumCon)

1 SELECT ?field

7 { ?p a npdv:FieldYearlyProduction ;

14 } GROUP BY ?field ORDER BY DESC(?field)

npdv:productionForField [ npdv:name ?field ] .
OPTIONAL { ?p npdv:producedCondensate ?con }
OPTIONAL { ?p npdv:producedGas ?gas }
OPTIONAL { ?p npdv:producedNGL ?NGL }
OPTIONAL { ?p npdv:producedOil ?oil }
OPTIONAL { ?p npdv:producedOilEquivalents ?OE }

(SUM(?g) AS ?gas)
(SUM(?o) AS ?oil)

1 SELECT ?field

4 { [ npdv:productionYear "2010"^^xsd:int ;

npdv:productionMonth ?m ;
npdv:producedGas ?g ;
npdv:producedOil ?o ;
npdv:productionForField
[ rdf:type npdv:Field ;

npdv:name ?field ;
npdv:currentFieldOperator
<http://sws.ifi.uio.no/data/npd-v2/company

/17237817> ] ]

FILTER(?m >= "01"^^xsd:int &&

15 } GROUP BY ?field ORDER BY ?field

?m <= "06"^^xsd:int)

Query 11.

(SUM(?g) AS ?gas)
(SUM(?o) AS ?oil)

npdv:productionMonth ?m ;
npdv:producedGas ?g ;
npdv:producedOil ?o ;
npdv:productionForField
[ rdf:type npdv:Field ;

1 SELECT ?field ?operator

4 { [ npdv:productionYear "2010"^^xsd:int ;

15 } GROUP BY ?field ?operator ORDER BY ?field

npdv:name ?field ;
npdv:currentFieldOperator

FILTER(?m >= "01"^^xsd:int &&

[ npdv:name ?operator ] ] ]

?m <= "06"^^xsd:int)

Query 12. What kind of facility is used for field X?

1 SELECT ?field ?facility ?facilitytype
2 { [] a ?facilitytype ;

7 } ORDER BY ?field ?facility

npdv:name ?facility ;
npdv:belongsTo [ a npdv:Field ;

npdv:name ?field ] .

?facilitytype rdfs:subClassOf npdv:Facility .

Query 13. What are the wellbores with total core length greater than
30 m?

npdv:coresForWellbore

1 SELECT DISTINCT ?wellbore ?length
2 { [] a npdv:WellboreCoreSet ;

7 } ORDER BY ?wellbore

[ npdv:name ?wellbore ] ;

FILTER(?length > 30)

npdv:coresTotalLength ?length .

Query 14. What wellbores have cores in a specific set of stratigraphic
units?

[ npdv:name ?unit ] .

?wellboreURI npdv:name ?well .
?core a npdv:WellboreCore ;

npdv:wellboreStratumBottomDepth ?stratBottom ;
npdv:stratumForWellbore ?wellboreURI ;
npdv:inLithostratigraphicUnit

1 SELECT DISTINCT ?unit ?well
2 { [] npdv:wellboreStratumTopDepth ?stratTop ;

17 } ORDER BY ?unit ?well

npdv:coreForWellbore ?wellboreURI ;
npdv:coreIntervalTop ?coreTop ;
npdv:coreIntervalBottom ?coreBottom .

BIND(IF(?coreBottom < ?stratBottom,

?coreBottom, ?stratBottom) AS ?min)

BIND(IF(?coreTop > ?stratTop,

?coreTop, ?stratTop) AS ?max)

FILTER(?max < ?min)

Query 10. What was the total production of oil and gas in the period
JanJun 2010 for fields where Statoil was operator?

Query 15. List wellbores completed before 2008 where Statoil as
drilling operator sampled less than 50 m of core(s).

M.G. Skjveland et al. / Web Semantics: Science, Services and Agents on the World Wide Web 33 (2015) 112140

npdv:coreForWellbore

[ rdf:type npdv:Wellbore ;

npdv:name ?wellbore ;
npdv:wellboreCompletionYear ?year ;
npdv:drillingOperatorCompany
[ npdv:name ?company ] ] .

1 SELECT DISTINCT ?wellbore ?length ?company ?year
2 { ?wc rdf:type npdv:WellboreCore ;

14 } ORDER BY ?wellbore

FILTER(?year < 2008 && ?length > 50 &&
regex(?company, "STATOIL", "i"))

npdv:member ?wc ;
npdv:coresTotalLength ?length .

[] a npdv:WellboreCoreSet ;

Query 16. What fields have wellbores that are drilled earlier than
2000, for which there is a core sample available, and where the oldest
penetrated age is Jurassic?
1 SELECT DISTINCT *
2 { ?wellbore

npdv:wellboreAgeTD ?ageTD ;
npdv:wellboreEntryYear ?year ;
npdv:explorationWellboreForField ?field .

[] npdv:coreForWellbore ?wellbore .
?ageTD skos:broaderTransitive

<http://resource.geosciml.org/classifier/ics/ischart

npdv:name ?fieldname .

?field a npdv:Field ;

/Jurassic> .

FILTER(?year < 2000) }

Query 17. What fields have wellbores that are drilled earlier than
2000, and for which there is a core sample available? This is a
slightly modified version of Query 16 used to test whether Ontop
returns answers when the use of its hybrid Abox functionality is
not necessary.
1 SELECT DISTINCT *
2 { ?wellbore

npdv:wellboreAgeTD ?ageTD ;
npdv:wellboreEntryYear ?year ;
npdv:explorationWellboreForField ?field .

[] npdv:coreForWellbore ?wellbore .
FILTER(?year < 2000) }

npdv:name ?fieldname .

?field a npdv:Field ;

npdv:currentFieldOperator ?operator ;
npdv:name ?field .
?w1 a npdv:Wellbore ;

Query 19. List the fields and wellbores with their operator for which
there exists a electromagnetic log curve.
1 SELECT DISTINCT ?wellbore ?field ?operator
2 { ?f a npdv:Field ;

npdv:name ?wellbore ;
npdv:explorationWellboreForField ?f .

diskosv:wellbore ?w2 ;
diskosv:curveClass "ELECTROMAG" .

?c a diskosv:WellLogCurve ;

diskosv:wellboreName ?n2 .

?w2 a diskosv:Wellbore ;

?w2 owl:sameAs ?w1 }

Query 20. List wellbores drilled before 2000 for which there exist
biostratigraphic samples from a cuttings sample, and the log curve is
stored online.
1 SELECT DISTINCT ?wellbore
2 { ?wellbore a npdv:Wellbore ;

?curve a diskosv:WellLogCurve ;
diskosv:wellbore ?wellbore ;
diskosv:curveStorageStatus "ONLINE" .

nlxv:fromWellbore ?wellbore ;
nlxv:sampleType "CU" .

?sample a nlxv:BiostratigraphicSample ;

npdv:wellboreEntryYear ?year .

FILTER(?year < 2000) }

Query 21. Same as Query 20, but using with necessary owl:sameAs
patterns in order work properly.

npdv:wellboreEntryYear ?year .

?sample a nlxv:BiostratigraphicSample ;

nlxv:fromWellbore ?wellboreB ;
nlxv:sampleType "CU" .

1 SELECT DISTINCT ?wellboreA
2 { ?wellboreA a npdv:Wellbore ;

?wellboreB a nlxv:Wellbore .
?wellboreC a diskosv:Wellbore .
?wellboreB owl:sameAs ?wellboreA .
?wellboreC owl:sameAs ?wellboreA .
FILTER(?year < 2000) }

?curve a diskosv:WellLogCurve ;
diskosv:wellbore ?wellboreC ;
diskosv:curveStorageStatus "ONLINE" .

B.2. Unit test queries

Query 22.

1 SELECT DISTINCT * { ?x owl:sameAs ?y }

Query 23.

1 SELECT DISTINCT ?n
2 { ?x a npdv:Wellbore ; nlxv:wellboreName ?n .

?y a npdv:Wellbore ; npdv:name ?n }

Query 24.

1 SELECT DISTINCT ?feature ?geometry ?WKT
2 { ?geometry geos:asWKT ?WKT ;

npdv:isGeometryOfFeature ?feature }

Query 25.

1 SELECT DISTINCT ?feature ?geometry ?WKT
2 { ?geometry a geos:SpatialObject, npdv:Point ;

geos:asWKT ?WKT ;
npdv:isGeometryOfFeature ?feature }

Query 26.

1 SELECT DISTINCT ?feature ?geometry
2 { ?geometry a geos:SpatialObject, npdv:Point ;

npdv:isGeometryOfFeature ?feature }

Query 27.

1 SELECT DISTINCT ?feature ?geometry
2 { ?feature geos:hasGeometry ?geometry }

Query 28.

1 SELECT DISTINCT ?feature ?geometry
2 { ?feature ^npdv:isGeometryOfFeature ?geometry }

Query 29.

1 SELECT DISTINCT *
2 { ?s npdv:wellboreAgeHcLevel1
npdv:wellboreAgeHcLevel2

npdv:wellboreAgeHcLevel3 ?o }

Query 30.

1 SELECT DISTINCT * { ?s npdv:wellboreAgeHc ?o }

Query 31.

1 SELECT DISTINCT ?class
2 { ?class rdfs:subClassOf npdv:Wellbore }

Query 32.

1 SELECT DISTINCT ?class ?instance
2 { ?class rdfs:subClassOf npdv:Wellbore .

?instance rdf:type ?class }

Query 33.

1 SELECT DISTINCT ?instance
2 { ?instance rdf:type npdv:Wellbore }

Query 34.

1 SELECT DISTINCT ?facility
2 { [] a npdv:Facility ;

4 } ORDER BY ?facility

npdv:name ?facility

Query 35.

1 SELECT DISTINCT ?class
2 { ?class rdfs:subClassOf npdv:Facility .
3 } ORDER BY ?class

Query 36.

1 SELECT DISTINCT ?class ?facility
2 { ?class rdfs:subClassOf npdv:Facility .

5 } ORDER BY ?facility

npdv:name ?facility .

[] rdf:type ?class ;

Query 37.

1 SELECT DISTINCT ?wellbore
2 { ?wellbore a npdv:DiscoveryWellbore }

Query 38.

1 SELECT DISTINCT ?s { ?s a owl:Class }

Query 39.

1 SELECT DISTINCT ?s { ?s a rdf:Property }

Query 40.

1 SELECT DISTINCT ?s
2 { { ?s a owl:ObjectProperty } UNION

{ ?s a owl:DatatypeProperty } UNION
{ ?s a owl:AnnotationProperty } }

Query 41.

1 SELECT DISTINCT ?o
2 { <http://sws.ifi.uio.no/data/npd-v2/wellbore/1343> rdf:

type ?o }

Query 42.

1 SELECT DISTINCT ?p ?o
2 { <http://sws.ifi.uio.no/data/npd-v2/wellbore/1343> ?p ?

3 } ORDER by ?p
