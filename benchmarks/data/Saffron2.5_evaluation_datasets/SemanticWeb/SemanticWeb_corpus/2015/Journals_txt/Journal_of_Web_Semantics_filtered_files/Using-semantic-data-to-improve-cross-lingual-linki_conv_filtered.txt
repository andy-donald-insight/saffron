Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 6470

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Using semantic data to improve cross-lingual linking
of article clusters
Evgenia Belyaeva a,b,, Aljaz Kosmerlj a, Andrej Muhic a, Jan Rupnik a, Flavio Fuart a,

a Jozef Stefan Institute, Jamova cesta 39, 1000 Ljubljana, Slovenia
b JSI International Postgraduate School, Jamova cesta 39, 1000 Ljubljana, Slovenia

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 15 February 2015
Received in revised form
19 August 2015
Accepted 2 September 2015
Available online 10 September 2015

Keywords:
Semantic data
Natural language processing
Cross-linguality
Canonical correlation analysis

1. Introduction

This paper presents a system that uses semantic data to improve cross-lingual linking of news article
clusters. Two approaches are compared. The first based on two different Canonical Correlation Analysis
(CCA) feature vector definitions: MAX-CCA and SUM-CCA, whereas the second one has been developed
using a better-performed CCA approach in combination with Entity vectors. The aim of the comparison
was to determine whether taking into account the semantic aspect of news increases performance and
improves linking. Evaluations of the aforementioned techniques on a news corpus, both against Google
News and manual, revealed good performance of our system. The overall gain in precision and recall when
using entity vectors was significant.

 2015 Elsevier B.V. All rights reserved.

translations [5], similarity analysis [6], the use of dictionaries and
information extraction [7].

Automatic linking of clusters across languages is an important
task in news monitoring applications. It is based on the core idea of
article clusters in different languages sharing the same content to
one another. Linking connects related clusters across all language
pairs involved, i.e. clusters dealing with a common subject matter
involving the same named entities (actors, organizations and loca-
tions) and time-frame.

Traditionally, news monitoring systems were monolingual,
but needs from the user community, technological and scientific
progress lead to an increasing number of news monitoring services
to add multilingual and cross-lingual features to existing monolingual components [1]. The main goal of automatic cross-lingual
linking of clusters is to interconnect many related news items that
are reported by numerous news outlets in different languages. It
also enables knowledge from around the world to be monitored
and aggregated [2]. Some research studies have been done to improve the quality of cross-lingual linking of clusters through such
tasks as: classification [3], summarization [4], machine learning

 Corresponding author at: Jozef Stefan Institute, Jamova cesta 39, 1000 Ljubljana,
 Corresponding author.

Slovenia.

E-mail addresses: jenya.belyaeva@ijs.si (E. Belyaeva), flavio.fuart@ijs.si

(F. Fuart).

http://dx.doi.org/10.1016/j.websem.2015.09.001
1570-8268/ 2015 Elsevier B.V. All rights reserved.

Thus, in the news monitoring domain, the basic task is to create monolingual clusters and then, when language-pair resources
become available, cross-lingual features are gradually introduced.
This approach allows for a wide range of monolingual features,
like clustering, classification, entity extraction and sentiment de-
tection, to be introduced even when no resources for specific language pairs are available. This is probably the main reason why
in the news monitoring domain cross-linking of monolingual clusters is preferred over full cross-lingual clustering. For example, the
EMM system [1] gathers news in 20 languages with 190 implemented language pairs. Newsfeed [8] covers 35 languages, with 12
languages annotated with entities and 276 language pairs covered
with article cross-link information. In this work we implemented
cross-linking of clusters for three language pairs.

The approach described in this paper is a crucial component of
an extension of the iDiversiNews application [9], which will allow
users to explore related news stories in different languages and discover new aspects of a story. A similar approach is used in the Event
Registry application [10] for detection of events from multilingual
news article data.

In order to improve cross-lingual linking of clusters, we have
proposed a new semantic approach for English, German and Spanish languages based on Canonical Correlation Analysis (CCA) technique in combination with named entity vectors. Instead of the
typical approaches of machine translations and cross-lingual information retrieval, we take into consideration the semantic part

of news, i.e. named entities. The use of entities is explained by their
amount of very vital information that is contributed to the news.
They largely define the main topic of an article [11] and appear to
play an important role when studying cross-linguality and multilingual information retrieval since research shows 30% of contentimportant words in a journalistic text are proper nouns [12]. We
also show that the presented approach outperformed the results
obtained by simply using two different CCA feature vectors.

To our knowledge, there is no existing database of curated and
reliable data that can be used as the golden standard for testing
cross-lingual linking of news clusters, thus we have performed
manual evaluation, which we consider an important contribution,
since human judgment is necessary to evaluate the performance of
any system.

This paper is organized as follows: Section 2 introduces the used
data sources, algorithms and named entity recognition system. In
Section 3 we describe our corpus. Evaluation results are detailed
in Section 4. Section 5 concludes the paper and announces future
work.

2. Semantic analysis and clustering

The news articles we used were aggregated by the JSI Newsfeed1a clean, continuous, real-time aggregated stream of semantically enriched articles from more than 1900 RSS-enabled sites
across the world in all major languages with around 300 000 articles per day [8]. For our purposes the articles were processed by
a linguistic and semantic analysis pipeline [2], which provides the
semantic annotations used to support cross-linking.

The semantic annotation tool developed as a part of the XLike
project consists of three main approaches for multi-lingual an-
notation: named entity recognition based on finding corresponding Wikipedia pages in the target language of previously detected
named entities; Wikipedia Miner Wikifiera simple approach based
on Wikipedia articles that tries to detect similar phrases and links
in any document of the same language as Wikipedia articles; crosslingual semantic analysis that links no longer detected word phrases
to corresponding Wikipedia pages, but articles by topic or con-
cepts, as described in [13,14]. The main aim of the cross-lingual
semantic annotation is to find links between entities mentioned in
articles in the source language with their corresponding entities in
the target language [15,16].

2.1. CCA-like approach for document comparison

There are many possible approaches to cross-lingual similarity computation in the literature: translation based [17],
Google Translate,2 probabilistic topic models based [18,19], approaches related to classification [20] and factorization based approaches [21,22] which also cover our proposed approach. There
are several aspects of the methods to consider when implementing a cross-lingual similarity component: scalability of training to
large corpora, similarity computation speed and cost, as well as the
ease of implementation and ease of use (the number of parameters
to tune).

Two closely related approaches, [23] and [24], also rely on
Wikipedia to compute document similarity. The first focuses on
comparing short informal texts (tweets) is less applicable in our
case, since we focus on news texts, which are longer and more for-
mal. The second approach (CL-ESA) could be seen as a baseline,
where no subspace computation is needed and each aligned pair

1 http://newsfeed.ijs.si/.
2 https://translate.google.com/.

represents a latent dimension. CL-ESA is easier to interpret, but
performs worse than CL-LSI according to the authors, where our
method can be seen as enhancing CL-LSI.

Using the vector space model we can represent documents
written in a given language as vectors in a vector space [25],
whose dimension is the number of terms (typically words, word
n-grams, character n-grams, etc.). When dealing with more than
one language, this results in several vector spaces with varying
dimensionalities, members of which we need to compare.

The goal of the Canonical Correlation Analysis (CCA) based approach is to find a set of mappings from language-specific vector
spaces into a common semantic vector space, in which standard
machine learning tools apply. A good set of mappings should preserve the document similarities within each language, and work
well across languages: a document and its translation should be
similar when mapped to the common vector space. When given
training data, CCA [26] is a method that can be applied to determining such linear mappings between two languages by finding
aligned subspaces in each vector space that maximize a measure
of linear dependence. Cross-Lingual Latent Semantic Indexing (CL-
LSI) [22] is an alternative approach. Our approach is related to both.
The mappings can be learned if we have access to a large
comparable corpus, in which the training data consists of tuples
of documents in different languages that cover similar topics. For
this purpose we used the Wikipedia [27]a multilingual collection
of interlinked articles, that are comparable but not necessarily
translations of each other. Most of our analysis is based on low rank
decompositions of cross-covariance matrices, their estimation
based on the aligned corpus matrices.

This problem has some specific features that render standard
approaches intractable: the numbers of dimensions and samples
are high, which lead to overfitting issues with CCA. Another aspect,
specific to the training dataset, is the fact that the majority of
alignments includes English documentsthis means that term
cross-covariance matrices between English and other languages
can be well estimated more reliably.

The above issues are overcome by pre-mapping the data, using a
method very similar to CL-LSI, and then refining the mappings in a
lower dimensional setting. The refinement is based on analyzing
only cross-covariance matrices related to the English language,
which enables us to solve effectively a generalization of CCA to
more than two languages (sum of squares of correlations, REF) as
a lower-dimensional eigenvalue problem. For details refer to [28].

2.2. Clustering algorithm

Our clustering algorithm is an adaptation and extension of the
Incremental Clustering of News Reports algorithm described by
Azzopardi et al. [29]. The basic algorithm performs well when
evaluated against the Google News corpus, thus deemed to be
tailored to cluster high-volume time-dependent text streams,
which is suitable for our needs.

Azzopardi et al. create a BoW TFIDF vector for each news
article and calculate cosine similarity against centroids of already
existing clusters. The new article is added to the first cluster
that matches above a predefined similarity threshold (typically
between 0.3 and 0.4). If there is no match, then a new cluster is
formed.

We have tested and adapted the basic algorithm in order to
process reliably the Newsfeed data flow, which is considerably
higher compared to test performed with the original algorithm.
Additionally, it is necessary to take into account that the news
stream may contain material of a lower quality, obtained from a
moderated list of sources. From our point of view Azzopardi et al.
fail to provide a satisfactory explanation of cluster aging, so we

E. Belyaeva et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 6470

Table 1
Example: news items and cluster CCA-MAX and CCA-SUM feature vectors.

Table 2
Numbers of news items used in the evaluation.

Article ID
E1

E2

CCA-spa
S1, 0.2
S2, 0.8
S2, 0.2
S3, 0.5

MAX-CCA
S1, 0.2
S2, 0.8
S3, 0.5

SUM-CCA
S1, 0.2
S2, 1.0 = 0.8 + 0.2
S3, 0.5

have determined an optimal time interval for keeping the clusters
alive.

Even with no extension, an evaluation would be essential,
implementation was tested against the

because the original
English news corpus.

Our extension of the original algorithm is explained through
the list below, where each feature can be tuned with a set of
parameters:
similarity thresholds The last level defines the similarity threshold for clusters, i.e. what is the minimum similarity for an
item to be added to a cluster. Other levels are used for a
rough classification of clusters for performance reasons.
Typically we use a hierarchy of two thresholds: 0.005 and
0.4.

stop words threshold Word document-frequency tables were
created for long-term time periods. This threshold
defines the percentage of top words used as stop words.
Typically 0.0002.

word limit Only the initial N words of a document are used by the
algorithm. This parameter defines the number of words
used. The parameter can also be set to all.

smallest cluster size Smaller clusters are discarded.
cluster expiry in hours If there are no new news items in the
defined period then the cluster is deemed as final, i.e. no
more updates are possible.

initial buffer The first clustering run is performed after enough

items have been collected.

2.3. Cross-linking clusters

Consequently,

in order to identify clusters describing the
same news item in different languages, two CCA feature vector
definitions, MAX-CCA and SUM-CCA, are used. In both cases, for
each cluster a set of feature vectors is created, one per language.
Vector components are newsfeed article unique identifiers from
CCA fields of all news items in the cluster.

For example, suppose an English cluster contains news items
E1 and E2 with their corresponding CCA similarities for Spanish
articles (S1, S2, S3) as listed in Table 1. The corresponding feature
vector is given in the third column. Due to the large number of
articles feature vectors are very large and sparse.

Additionally,

for each cluster a feature vector of unique
identifiers (i.e. English Wikipedia links) of annotated entities
is constructed. It consists of entities that appear in at least
20% of the articles. The values in the feature vectors are
percentages. For example,
if Robin Williams appears in 30%
of articles, the vector component for the unique identifier
http://en.wikipedia.org/wiki/Robin_Williams would be 30.

It is important to note that our system cross-links clusters over a
time window of 24 h. Cosine similarity is used to compare their CCA
vectors for a specific language pair and Entity vectors. We manually
evaluate all links that have at least one of the three vectors over
the threshold of 0.2 and discard all other pairs. Keeping a large
number of candidates for manual evaluation allows us to get a
rough estimate for recall as well.

It would have been possible to combine the entity information
with the output of the CCA-like similarity measure into a single

Timespan
Entire period

28/05/2014

Dataset
Newsfeed
Google

Newsfeed
Google

Deu
70 245
8 648

11 477
1 919

Eng
275 349
44 028

56 350
9 696

Spa
49 497
6 961

9 213
1 538

Total
395 091
59 637

77 040
13 153

Table 3
Clustering parameters used for manual evaluation.

Parameter
Thresholds
Smallest cluster size
Cluster expiry (h)
initial buffer size (h)

Deu
(0, 0.4)

Eng
(0, 0.4)

Spa
(0, 0.4)

metric to directly cluster the articles across all languages. However,
clustering articles in their respective languages first allows us
to detect the level of reporting for each language separately,
which is important as the volume of reporting across languages
is unbalanced as shown in Table 2. This is especially important
for small volume languages we process with our system, like
Slovenian or Catalan.

3. Corpus and news items statistics

The main goal was to determine whether named entities had
any influence on the overall quality of cross-lingual cluster linking
results. In order to measure this effect, we first describe our
data and calculate automatic and manual evaluation of clusters to
ensure that the clusters and its parameters used are good enough to
be used further for our main evaluation, i.e. the manual evaluation
of cross-lingual linksthe main contribution of this paper.

3.1. News items

To explore the cross-lingual clusters, using several techniques,
English, German and Spanish news items published over the
period from 19/05/2014 to 01/06/2014 have been processed. In
order to evaluate the performance of our clustering algorithm
against the commercial, but publicly available news navigation
and analysis application Google News stories, we have focused
on the period between 28/05/2014 and 30/05/2014, since there
existed a satisfactory number of Google News references for the
above-mentioned period. We selected Wednesday, 28/05/2014 for
the manual evaluation of clusters and cross-links. The numbers of
news items used are listed in Table 2.

3.2. Evaluation against Google News stories

For our evaluation against Google News stories, we explored
the whole period from 19/05/2014 to 01/06/2014, despite the
fact that Google data was available for a shorter time range. We
performed 51 clustering runs with different sets of parameters.
For each run and language, standard metrics are used: Speed of
execution, Recall, precision, and F1 measure, along with some
additional indicators.

3.3. Manual evaluation

For the second evaluation executed manually, we have selected
empirically the most suitable set of parameters for clustering per
language. Parameters were set as listed in Table 3.

After the clustering was performed, all clusters that started on
28/05/2014 were provided to an expert, who evaluated the quality
of each cluster separately, giving the number of correct/wrong
articles in the cluster. This allows the overall clustering precision
to be calculated by weighting the precision for each cluster by its

Table 4
F1 measure for different sets of parameters, per language.

Lang

Deu

Eng

Spa

Ex:
WL:
Thresholds
(0.1, 0.05)
(0.2)
(0.2, 0.05)
(0.3)
(0.3, 0.05)
(0.4)
(0.4, 0.05)
(0.1, 0.05)
(0.2)
(0.2, 0.05)
(0.3)
(0.3, 0.05)
(0.4)
(0.4, 0.05)
(0.1, 0.05)
(0.2)
(0.2, 0.05)
(0.3)
(0.3, 0.05)
(0.4)
(0.4, 0.05)

4. Results

size, using the following equations:
Pc = TP

Poverall =

cC

TP + FP


|c|  Pc
|c|

cC

In this section, results obtained using the approach described in

Section 3 are presented.

4.1. Clusteringagainst Google News stories

3.4. Evaluation of cross-links

In the next evaluation step, we checked the multilingual news
story links in the following two phases. First, we analyzed which
feature vector between MAX-CCA and SUM-CCA performs better.
We compared the results and used the better performed approach
for our next step in the evaluation, where we aim at checking
whether taking into account the semantic aspect of news would
increase the overall performance of the system.

The expert was given a list of inter-cluster links. We provided
the following data for each cluster: language, title, link to the
cluster web page and its size. The evaluator then scored each
relation with the following scores: 1correct link, 2partially
correct link and 3wrong link. Partially correct cross-linking
implies links, in one or another language, that may mention the
main topic of the story, but have an additional information or a
different angle of the coverage.

We define two modes of evaluation: strict and weak, which also
reflects possible use-case scenarios of the system. Strict evaluation
means that a link is correct only when annotated with 1. This
would reflect a use case, in which the story topic, information
provided and angle of coverage match in both languages. Weak
evaluation means that a link is correct when marked with 1 or 2.
In that case, it is enough if the clusters are about the same topic.
Any additional information or different angles of coverage give us
additional insight, thus most users may even prefer weak links.
In addition to the manual cross-linking evaluation, we also want
to find the optimal threshold values and combination of weights
for CCA/Entity vectors. Thus, the result of evaluation would be used
to find an optimal combination of parameters W and similarity
threshold T.

We calculated precision, recall (from the selected dataset) and
F0.5 measure (F-measure giving more importance to precision
rather than recall) for each language. Parameters were calculated
for threshold settings between 0.2 and 0.6 (step 0.05).

By performing clustering runs with different parameter values
we found an optimal set of parameters that would take into account clustering quality and the speed of execution. We used stop
word threshold value 0.0002, smallest cluster size of 1 and initial
buffer size of 1000 items. Table 4 shows the F1 measure values obtained for different values of other parameters, namely thresholds,
word limit (WL) and cluster expiry in hours (Ex).

For the same set of parameters, we compute the clustering
speed, aggregated across all languages. Speed measurements are
listed in Table 5.

Except for an outlier (marked with *), all other measurements
form a consistent picture. The optimal similarity cutoff is in the
range 0.20.3 and increasing the cutoff slightly increases clustering
speed. Tests show that speed increases considerably (20%50%) by
introducing the two-level threshold hierarchy, whereas F1 measure is not significantly affected by this parameter. Number of
words, WL, has almost no effect on F1 score, but the lower value improves clustering speed. As for expiry interval, Ex, clustering gets
slower as the interval is increased (more than 2x for 4 h compared
to 48 h), but the optimal value depends on the language. Observing
the results, we concluded that for small languages we get almost
no clusters unless we increase the interval to 2448 h, while the
quality of English clusters drops for intervals longer than 812 h.
Measurements show that English gets optimal values in the
range 46 h, Spanish 48 h and German along the whole scale, but
with slightly better results in the interval 1248 h. Optimal threshold values are (0.3, 0.05) and for word limit 200 words. The expiry
interval is language-dependent, for English and Spanish at 6 h and
German 24 h.

4.2. Manual evaluation: clustering

For manual analysis we used a dataset of news items and
clusters from 28/05/2014. Exact numbers of news items and
clusters over languages is listed in Table 6.

E. Belyaeva et al. / Web Semantics: Science, Services and Agents on the World Wide Web 35 (2015) 6470

Table 5
Clustering speed [item/second].

Ex:
WL:
Thresholds
(0.1, 0.05)
(0.2)
(0.2, 0.05)
(0.3)
(0.3, 0.05)
(0.4)
(0.4, 0.05)

Table 6
Number of news items and clusters per
language. Clusters starting on 28/5/2014.
Clusters

News items
2 685
10 404
2 583
15 672

Language
Deu
Eng
Spa
Total

Table 7
Number of cross-links for language pairs.

Language pair
engdeu
spadeu
spaeng
Total

Links
7 477
6 621
6 003
20 101

Table 8
Number of links over threshold (0.2).

Language pair
engdeu
spadeu
spaeng

SUM-CCA

MAX-CCA

ENTITY

Table 9
Annotated set of links.

Language pair
engdeu
spadeu
spaeng

1match

2partial

3wrong link

N/A

We manually evaluated the performance of German, English
and Spanish clusters for the same date and obtained the
following precision per language: German0.93%, English0.95%
and Spanish0.97%. Although we did not attempt to evaluate
recall, the evaluator has estimated that there were almost no
clusters that could be merged into bigger clusters. We believe that
the opposite conclusion would point to low recall. However, the
evaluator did not have access to discarded clustersthose with less
than 5 articles for English and 3 articles for other languages. The
evaluator for all three languages estimated the margin of error,
while annotating, was around 5%, thus we estimate that for all
languages around 90% of precision was obtained. We also conclude
that there is no significant difference among clustering quality
across the languages.

4.3. Manual evaluation: cross-linking

Manual evaluation of cross-links was performed on the same
dataset of clusters from 28/05/2014. In total, we detected 20 101
links, spread across languages as shown in Table 7. Each of the
methods produced a different subset of these links. Exact numbers
of links over methods are listed in Table 8.

The evaluator classified the 20101 cross-links as described in

Section 3.4. The obtained classifications are listed in Table 9.

Table 10
F0.5 maximum values across all thresholds.

Language pair

engdeu
spadeu
spaeng

Strict evaluation Weak evaluation

Table 11
Optimal threshold values for the F.05 measure.
Annotation scores 1 and 2, CCA-MAX feature vector.

Threshold

engdeu

spadeu

spaeng

Table 12
F0.5 maximum values across all thresholds for MAX-CCA without
and with named entity features.

Language pair

engdeu
spadeu
spaeng

Strict evaluation
MAX+
ENTITY

Weak evaluation
MAX+
ENTITY

We have also compared the two methods, SUM-CCA and MAX-
CCA, by their F0.5 score. Table 10 shows the best scores across
all tested thresholds, which shows that MAX-CCA consistently
outperforms SUM-CCA in strict and weak evaluation.

Consequently, we were able to estimate for each language pair
the optimal threshold (Table 11) for the MAX-CCA vector, using the
weak annotation as our criteria because we believe even weak links
may give additional clues to news consumers.

For the best F0.5 measure we set the threshold somewhere
between 0.30 and 0.45. In real-world applications, we would opt
for higher recall values if our user group would consist of news
analysts and higher precision values for the public.

4.4. Manual evaluation: cross-linking with entities

Since proper nouns play an important part in journalistic works,
we had identified named entities as a potential indicator of good
cross-lingual linking performance. In this subsection, we test if
adding this semantic information (i.e. named entities) will improve
the performance of our system.

We are trying to optimize values for W (entity score weight)
and T (total score threshold) for a given dataset. Again, we will analyze F0.5 scores and precision in order to get a good estimate. The
evaluation of MAX-CCA in combination with Entity feature vectors

Table 13
F0.5 scores for engdeu cross-links, weak evaluation, MAX-CCA + ENTITY.

Table 14
F0.5 scores for spadeu cross-links, weak evaluation, MAX-CCA + ENTITY.

Table 15
F0.5 scores for spaeng cross-links, weak evaluation, MAX-CCA + ENTITY.

turned to perform much better then by using just CCA. In particu-
lar, a significant gain in quality can be observed in Table 12.

The optimal weight, W, and threshold, T were determined by
exploring the space of their values for each language pair as shown
in Tables 1315, where the weight of the entity similarity increases from left to right, taking into account only entities in the
last column, where W = 1.0. For brevity we omit rows at T 
{0.25, 0.35, 0.45, 0.55}. The values in bold are maxima over all val-
ues.
For evaluated language pairs an optimal combination would be

to use T in the range [0.20.4] and W [0.30.4].

5. Conclusion and future work

A new system has been developed to improve the linking of
cross-lingual clusters, based on the use of the semantic part of
news, i.e. entities. Several techniques were assessed, but the novel
approach used proved that the use of named entities (which articles have in common across languages) is a good source that can
support linking of multilingual clusters. The evaluation has shown
that the results are good, the use of semantic features improved
the scores by almost 50%. Thus, using named entities is a promising strategy for improving cross-lingual linking of clusters.

Future work will include taking into account the distribution of
the named entities in the clusters, using different named entities
such as date expressions, as well as experimenting and adapting
our system to different language pairs.

Acknowledgment

This work was funded by the European Union through project

XLike (FP7-ICT-2011-288342).
