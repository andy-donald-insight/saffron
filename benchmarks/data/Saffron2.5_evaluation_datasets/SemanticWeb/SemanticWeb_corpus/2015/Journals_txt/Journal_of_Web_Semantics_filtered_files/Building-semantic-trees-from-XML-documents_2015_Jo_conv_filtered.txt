Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Building semantic trees from XML documents
Joe Tekli a,, Nathalie Charbel b, Richard Chbeir b

a Electrical and Computer Engineering Department, Lebanese American University, 36 Byblos, Lebanon
b LIUPPA Lab., University of Pau and Adour Countries, 64600 Anglet, France

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 June 2015
Received in revised form
16 January 2016
Accepted 5 March 2016
Available online 14 March 2016

Keywords:
XML and Semi-structured data
Word sense disambiguation
Semantic-aware processing
Semantic ambiguity
Context representation
Knowledge bases

The distributed nature of the Web, as a decentralized system exchanging information between heterogeneous sources, has underlined the need to manage interoperability, i.e., the ability to automatically
interpret information in Web documents exchanged between different sources, necessary for efficient
information management and search applications. In this context, XML was introduced as a data representation standard that simplifies the tasks of interoperation and integration among heterogeneous data
sources, allowing to represent data in (semi-) structured documents consisting of hierarchically nested elements and atomic attributes. However, while XML was shown most effective in exchanging data, i.e., in
syntactic interoperability, it has been proven limited when it comes to handling semantics, i.e., semantic interoperability, since it only specifies the syntactic and structural properties of the data without any
further semantic meaning. As a result, XML semantic-aware processing has become a motivating challenge in Web data management, requiring dedicated semantic analysis and disambiguation methods to
assign well-defined meaning to XML elements and attributes. In this context, most existing approaches:
(i) ignore the problem of identifying ambiguous XML elements/nodes, (ii) only partially consider their
structural relationships/context, (iii) use syntactic information in processing XML data regardless of
the semantics involved, and (iv) are static in adopting fixed disambiguation constraints thus limiting
user involvement. In this paper, we provide a new XML Semantic Disambiguation Framework titled
XSDFdesigned to address each of the above limitations, taking as input: an XML document, and then
producing as output a semantically augmented XML tree made of unambiguous semantic concepts extracted from a reference machine-readable semantic network. XSDF consists of four main modules for:
(i) linguistic pre-processing of simple/compound XML node labels and values, (ii) selecting ambiguous
XML nodes as targets for disambiguation, (iii) representing target nodes as special sphere neighborhood
vectors including all XML structural relationships within a (user-chosen) range, and (iv) running context
vectors through a hybrid disambiguation process, combining two approaches: concept-basedand contextbased disambiguation, allowing the user to tune disambiguation parameters following her needs. Conducted experiments demonstrate the effectiveness and efficiency of our approach in comparison with
alternative methods. We also discuss some practical applications of our method, ranging over semanticaware query rewriting, semantic document clustering and classification, Mobile and Web services search
and discovery, as well as blog analysis and event detection in social networks and tweets.

 2016 Elsevier B.V. All rights reserved.

1. Introduction

Over the past decade, publishing and processing data in XML
has become increasingly attractive for organizations that want

 Work supported in part by the National Council for Scientific Research (CNRS),
 Corresponding author.
Lebanon, project: NCSR00695_01/09/15, and by LAU grant: SOERC1415T004.

E-mail addresses: joe.tekli@lau.edu.lb (J. Tekli), nathalie.charbel@univ-pau.fr

(N. Charbel), richard.chbeir@univ-pau.fr (R. Chbeir).

http://dx.doi.org/10.1016/j.websem.2016.03.002
1570-8268/ 2016 Elsevier B.V. All rights reserved.

to easily inter-operate and provide their information in a well-
defined, semi-structured, extensible, and machine-readable format to improve the quality of their Web-based information
retrieval and data management applications [1]. Most approaches
in this context use syntactic information in processing XML data,
while ignoring the semantics involved [2]. In fact, even when the
huge amount of raw information available to organizations exists
in natural language form and needs to be automatically processed,
it can be first distilled into a more structured form in which individual entities (pieces of data) are accessible. Here, information
extraction and wrapping technologies can be used: extracting and

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

WSD has been widely studied for flat textual data [19,18], yet, the
disambiguation of structured XML data remains largely untouched.
The few existing approaches to XML semantic-aware analysis (Sec-
tion 2) have been extended from traditional flat text WSD, and thus
show several limitations, motivating this work:
 Motivation 1: They completely ignore the problem of semantic
ambiguity, which in turn leads to questions of disambiguation
exhaustivity versus selectivity. To our knowledge, none of the
existing XML-based approaches addresses the problem of selecting target nodes to be processed for disambiguation, which
should inherently be the most ambiguous nodes in the XML document (similarly to selecting the most ambiguous words as targets in flat text WSD [20,21]). Rather, they perform semantic
disambiguation on all nodes of the XML document which is exhaustively time consuming and sometimes needless (e.g., no
need to disambiguate unambiguous nodes). Here, the main difficulty resides in identifying/selecting ambiguous nodes (words)
which need to be processed for disambiguation, since there is
no formal method for evaluating XML node (word) ambiguity.
 Motivation 2: They only partially consider the structural re-
lationships/context of XML nodes (e.g., solely focusing on
parent-node relationships [22], or ancestor-descendent relationships [2]). For instance, in Fig. 1, processing XML node cast
for disambiguation: considering (exclusively) its parent node
label (i.e., picture), its root node path labels (i.e., films and
picture), or its node sub-tree labels (i.e., star), remains insufficient for effective disambiguation.

 Motivation 3: They make use of syntactic processing techniques such as the bag-of-words paradigm [3,22] (commonly
used with flat text) in representing XML data as a plain set of
words/nodes, thus neglecting XML structural and/or semantic
features as well as compound node labels.

 Motivation 4: They are mostly static in adopting a fixed context
size (e.g., parent node [22], or root path [2]) or using preselected
semantic similarity measures (e.g., edge-based measure [23], or
gloss-based measure [2]), such that the users ability in tuning
disambiguation parameters to allow system adaptability (fol-
lowing her needs) is minimal.

The main goal of our study is to provide an effective method to XML
semantic analysis and disambiguation, overcoming the limitations
mentioned above. We aim to transform traditional syntactic XML
trees into semantic XML trees (or graphs, when hyperlinks come to
play), i.e., XML trees made of concept nodes with explicit semantic meanings. Each concept will represent a unique lexical sense,
assigned to one or more XML element/attribute labels and/or data
values in the XML document, following the latters structural con-
text. To do so, we introduce a novel XML Semantic Disambiguation
Framework titled XSDF, a fully automated solution to semantically
augment XML documents using a machine-readable semantic network (e.g., WordNet [16], Rogets thesaurus [24], Yago [25], an
adaptation of FOAF [12],1 etc.), identifying the semantic definitions
and relationships among concepts in the underlying XML structure.
Different from existing approaches, XSDF consists of four main
modules for: (i) linguistic pre-processing of XML node labels and
values to handle compound words (neglected in most existing so-
lutions), (ii) selecting ambiguous XML nodes as primary targets for
disambiguation using a dedicated ambiguity degree measure (un-
addressed in existing solutions), (iii) representing target nodes as

1 A semantic network structure can be automatically generated based on the
FOAF social network, where person profiles represent concepts, and links between
profiles represent concept relationships. Such a semantic network can be used
for person recognition and/or identification (as a special case of named entity
recognition or disambiguation [18]).

Fig. 1. Sample documents with different structures and tagging, yet describing the
same information.

structuring selected data from documents to make them easier to
handle in enterprise applications, where the preferred output is
largely XML (e.g., handling XML-based summaries of news head-
lines, legal decisions, research articles, medical reports, editorials,
book reviews, etc.) [3]. However, attaining a higher degree of automated data processing capability and human-machine cooperation requires yet another breakthrough: extracting and processing
the semantic features of XML data, whose impact has been highlighted in various XML-based applications, ranging over: semanticaware query rewriting and expansion [4,5] (expanding keyword
queries by including semantically related terms from XML documents to obtain relevant results), XML document classification and
clustering [3,6] (grouping together documents based on their semantic similarities, rather than performing syntactic-only process-
ing), XML schema matching and integration [7,8] (considering the
semantic meanings and relationships between schema elements
and data-types), and more recently Web and mobile services dis-
covery, recommendation, and composition [911]. (searching and
mapping together semantically similar WSDL/SOAP descriptions
when processing Web services, and semantic-aware mapping of
XHTML/free-text descriptions when dealing with RESTful and/or
mobile services), as well as XML-based semantic blog analysis and
event detection in social networks and tweets [1214]. Here, a
major challenge remains unresolved: XML semantic disambigua-
tion, i.e., how to resolve the semantic ambiguities and identify the
meanings of terms in XML documents [15], which is central to improve the performance of XML-based applications. The problem is
made harder with the volume and diversity of XML data on the
Web.

Usually, heterogeneous XML data sources exhibit different ways
to annotate similar (or identical) data, where the same real world
entity could be described in XML using different structures and/or
tagging, depending on the data source at hand (as shown in Fig. 1,
where two different XML documents describe the same Hitchcock
movie). The core problem here is lexical ambiguity: a term (e.g., an
XML element/attribute tag name or data value) may have multiple
meanings (polysemy), it may be implied by other related terms
(metonymy), and/or several terms can have the same meaning
(synonymy) [15]. For instance (according to a general purpose
knowledge base such as WordNet [16]), the term Kelly in XML
document 1 of Fig. 1 may refer to Emmet Kelly: the circus clown,
Grace Kelly: Princess of Monaco, or Gene Kelly: the dancer. However,
looking at its context in the document, a human user can tell that
Kelly here refers to Grace Kelly. Yet while seemingly obvious for
humans, such semantic ambiguities remain extremely complex to
resolve with automated processes.

In this context, word sense disambiguation (WSD), i.e., the computational identification of the meaning of words in context [18],
could be central to automatically resolve the semantic ambiguities
and identify the meanings of XML element/attribute tag names and
data values, in order to effectively process XML documents. While

special sphere neighborhood vectors considering a comprehensive
XML structure context including all XML structural relationships
within a (user-chosen) range (in contrast with partial context representations using the bag-of-words paradigm), and (iv) running
sphere neighborhood vectors through a hybrid disambiguation pro-
cess, combining two approaches: concept-based and context-based
disambiguation, allowing the user to tune disambiguation parameters following her needs (in contrast with static methods). We have
implemented XSDF to test and evaluate our approach. Experimental results reflect our approachs effectiveness in comparison with
existing solutions.

The overall architecture of XSDF has been introduced in [26].
This paper adds: (i) an extended presentation of XSDFs mathematical model for ambiguity degree and disambiguation com-
putations, with corresponding proofs, (ii) an adapted gloss-based
semantic similarity measure, expanded and normalized to be used
in concept-based disambiguation, (iii) dedicated algorithms to perform concept-based and context-based semantic disambiguation,
along with detailed complexity analysis, (iv) a detailed presentation of experimental results, (v) an extended discussion of the state
of the art solutions, as well as (vi) a discussion of the main applications scenarios which can benefit, in one way or another, from
XML semantic analysis and disambiguation.

The remainder of this paper is organized as follows. Section 2
reviews the background and related works. Section 3 develops our
XML disambiguation framework. Section 4 presents experimental
results. Section 5 discusses potential applications, before concluding the paper in Section 6 with current directions.

2. Background and related works

First, Section 2.1 briefly describes traditional word sense
disambiguation developed for flat text. Then, Section 2.2 covers
XML (semi-structured) semantic disambiguation methods.

2.1. Word sense disambiguation

WSD underlines the process of computationally identifying the
senses (meanings) of words in context, to discover the authors
intended meaning [19]. The general WSD task consists of the following main elements: (i) selecting words for disambiguation, (ii)
identifying and representing word contexts, (iii) using reference
knowledge sources, (iv) associating senses with words, and (v)
evaluating semantic similarity between senses.

2.1.1. Selecting words for disambiguation

There are two possible methods to select target words for dis-
ambiguation: (i) all-words, or (ii) lexical-sample. In all-words WSD,
e.g., [27,21], the system is expected to disambiguate all words in a
(flat) textual document Although considered as a complete and exhaustive disambiguation approach, yet it remains extremely timeconsuming and labor intensive. In addition, the high (time and
processing) costs might not match performance expectations after
all [18]. In lexical-sample WSD, e.g., [20,21], specific target words
are selected for disambiguation (usually one word per sentence).
These words are often the most ambiguous, and are usually chosen using supervised learning methods trained to recognize salient
words in sentences [18]. Experimental results reported in [18]
show high disambiguation accuracy using the lexical-sample ap-
proach, in comparison with the all-words approach. However,
a major difficulty in adopting the lexical-sample approach is in
selecting ambiguous (target) words, due to the lack of formal
methods to quantify semantic ambiguity, given that current supervised learning approaches are time-consuming including a training
phase requiring training data which are not always available.

2.1.2. Identifying and representing context

Once words have been selected for disambiguation, their contexts have to be identified, since sense disambiguation relies on
the notion of context, such as words that appear together in the
same context usually have related meanings [28]. The context of a
word in traditional flat textual data usually consists of the set of
terms in the words vicinity, i.e., terms occurring to the left and
right of the considered word, within a certain predefined window
size [28]. Other features can also be used to describe context, such
as information resulting from linguistic pre-processing including
part-of-speech tags (e.g., verb, subject, etc.), grammatical relations,
etc. [18]. Once the context has been identified, it has to be effectively represented to perform disambiguation computations. Here,
the traditional bag-of-words paradigm is broadly adopted with flat
textual data [19,18], where the context is processed as a set of
terms surrounding the word to disambiguate. A vector representation considering the number of occurrences of words in context can
also be used [18]. More structured context representations have
been investigated in [29,30], using co-occurrence graphs. Yet, they
require substantial additional processing than the bag-of-words
model.

2.1.3. Using reference knowledge sources

In addition to the contexts of target words, external knowledge is essential to perform WSD, providing reference data which
are needed to associate senses with words. In this context, WSD
methods can be distinguished as corpus-based or knowledge-based,
depending on the kind of external knowledge sources they rely
on. The corpus-based approach, e.g., [31,32,4], is data-driven, as it
involves information about words previously disambiguated, and
requires supervised learning from sense-tagged corpora (e.g., SemCor [33], where each word/expression is associated an explicit semantic meaning) in order to enable predictions for new words.
Knowledge-based methods, e.g., [103,18,3], are knowledge-driven,
as they handle a structured sense inventory and/or a repository of
information about words that can be automatically exploited to
distinguish their meanings in the text. Machine-readable knowledge bases (dictionaries, thesauri, and/or lexical ontologies, such as
WordNet [16], Rogets thesaurus [24], ODP [1], etc.) provide readymade sources of information about word senses to be exploited
in knowledge-based WSD. While corpus-basedmethods have been
popular in recent years, e.g., [31,32,4], they are generally data hungry and require extensive training, huge textual corpora, and/or a
considerable amount of manual effort to produce a relevant senseannotated corpus, which are not always available and/or feasible in
practice. Therefore, knowledge-based methods have been receiving more attention lately, e.g., [103,18,3], and include most solutions targeting XML data.

2.1.4. Associating senses with words

The final step in WSD is to associate senses with words,
taking into account the target words contexts as well as reference
external knowledge about word senses. This is usually viewed as a
word-sense classification task. In this regard, WSD approaches can
be roughly categorized as supervised or unsupervised. On one hand,
supervised methods, e.g., [34,18,35], involve the use of machinelearning techniques, using samples (a human expert manually
annotates examples of a word with the intended sense in context)
provided as training data for a learning algorithm that induces
rules to be used for assigning meanings to other occurrences of
the word. External knowledge (mainly corpus-based) is used and
combined with the human experts own knowledge of word senses
when manually tagging the training examples. While effective,
yet supervised methods include a learning phase which is highly
time-consuming, and requires a reliable training set with a wide
coverage which is not always available.

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Gloss-based methods [45,46] evaluate word overlap between
the glosses of concepts being compared, a gloss underlining the
textual definition describing a concept (e.g., the gloss of the 1st
sense of word Actor in WordNet is A theatrical performer, cf.
Fig. 1). A central gloss-based measure (extended in our study, cf.
Section 3.5.1) is proposed by Banerjee and Pedersen in [46]:
SimGloss(c1, c2, SN) = (gloss(c1)  gloss(rel(c1)))
 (gloss(c2)  gloss(rel(c2)))

(3)
where gloss(ci) is the bag of words in the textual definition of
concept (word sense) ci, and rel(ci) is the set of concepts related
to ci through a semantic relation in SN.

Discussion: It has been shown that gloss-based measures evalu-
ate, not only semantic similarity, but also semantic relatedness [36],
which is a more general notion: including similarity as well as any
kind of functional relationship between terms (e.g., penguin and
Antarctica are not similar, but they are semantically related due
to their natural habitat connection), namely antonymy (e.g., hot
and cold are dissimilar: having opposite meanings, yet they are
semantically related), which makes gloss-based measures specifically effective in WSD [18]: matching not only similar concepts,
but also semantic related ones.2

Note that unsupervised/knowledge-based WSD has been largely
investigated recently (including most methods targeting XML
data), in comparison with supervised and corpus-based meth-
ods, which usually require extensive training and large test corpora [18], and thus do not seem practical for the Web. The reader
can refer to [19,18] for reviews on traditional WSD.

2.2. XML semantic disambiguation

Few approaches have been developed for semantic disambiguation of XML and semi-structured data. The main challenges reside
in the notion of XML (structural) contextualization and how it is
processed, as described below.

2.2.1. XML context identification

While the context of a word in traditional flat textual data
consists of the set of terms in the words vicinity [28], yet there is
no unified definition of the context of a node in an XML document
tree. Various approaches have been investigated: (i) parent node,
(ii) root path, (iii) sub-tree, and (iv) versatile structural context.

2.2.1.1. Parent node context. The authors in [49,22] consider the
context of an XML data element to be efficiently determined by
its parent element, and thus process a parent node and its children data elements as one unified (canonical) entity. The approach
is based on the observation that an XML data node (i.e., ele-
ment/attribute value) constitutes by itself a semantically meaningless entity. They introduce the notion of canonical tree as a
structure grouping together a leaf (data) node with its parent node,
which is deemed as the simplest semantically meaningful structural entity. The authors utilize dedicated context-driven search
heuristics (e.g., structure pruning, identifying immediate relatives,
etc.) to determine the relationships between the different canonical trees. These are exploited to assign semantic node labels using
a manually built ontology [50], generalizing/specializing node concepts following ontology labels and the nodes structural positions
in the XML tree hierarchy.

2.2.1.2. Root path context.
In [3,2], the authors extend the notion of
XML node context to include the whole XML root path, i.e., the path

2 The reader can refer to [39,47,48] for comprehensive reviews and evaluations
of semantic similarity measures.

Fig. 2. Extract of the WordNet semantic network. Numbers next to concepts
represent concept frequencies (based on the Brown corpus [17]). Sentences next
to concepts represent concept glosses.

On the other hand, unsupervised methods, e.g., [23,36,2], are
usually fully automated and do not require any human intervention or training phase. Most recent (and XML-related) approaches,
e.g., [37,36,2], make use of a machine-readable knowledge base
(e.g., such as WordNet [16]) represented and processed as a semantic network made of a set of concepts representing word senses,
and a set of links connecting the concepts, representing semantic relations (synonymy, hyponymy, etc., [16,38], cf. Fig. 2). Given
a target word to be disambiguated, WSD consists in identifying
the semantic concept (word sense), in the reference semantic network that best matches the semantic concepts (word senses) of
terms appearing in the context of the target word. Semantic concept matching is usually performed using a measure of semantic
similarity between concepts in the reference semantic network,
also known as knowledge-based semantic similarity [39,40].

2.1.5. Semantic similarity measures in a semantic network

Several methods have been proposed to determine semantic
similarity between concepts (and consequently related terms) in
a knowledge base (semantic network). These can be organized
in three main categories [39]: edge-based, node-based, and glossbased [39]. Edge-based methods [41,42] estimate similarity as the
shortest path (in edges, weights, or number of nodes) between the
concepts being compared. A central edge-based measure (adopted
in our study) is proposed by Wu and Palmer in [42]:
SimEdge(c1, c2, SN) =
where c1 and c2 designate two semantic concepts (word senses),
SN designates the reference semantic network, N1 and N2 are
respectively the lengths of the paths separating concepts c1 and
c2 from their lowest common ancestor concept c0 in SN, and N0 is
the length of the path separating concept c0 from the root of SN.

N1 + N2 + 2N0

 [0, 1]

2N0

(1)

Node-based approaches [43,44] estimate similarity as the
maximum amount of information content that concepts share in
common, based on the statistical distribution of concept (term)
occurrences in a text corpus (e.g., the Brown corpus [17]). A central
node-based measure (adopted in our study) is proposed by Lin
in [43]:
SimNode(c1, c2, SN) =
having p(ci) = Freq(ci)
where c0, c1, and c2 are the same as in formula (1), p(ci) denotes the
occurrence probability of concept ci designating the normalized
frequency of occurrence of ci in a reference corpus such as the
Brown text corpus [17] (adopted in our study), and N designates
the size (total number of words) in the reference corpus.

log p(c1) + log p(c2)

 [0, 1]

2 log p(c0)

(2)

consisting of the sequence of nodes connecting a given node with
the root of the XML document (or document collection). They perform per-path sense disambiguation, comparing every node label
in each path with all possible senses of node labels occurring in the
same path. Each XML path is transformed into a weighted graph,
with nodes underlining the senses of each path element, and edges
connecting node senses following path direction and node sense
semantic similarities (Section 2.1.5). The authors utilize an existing gloss-based WordNet similarity measure [46] and introduce an
edge-based measure (similar to [42]) in comparing label semantic
senses to compute graph edge weights. Then, selecting the appropriate sense for a given node label consists in identifying the set of
connected node senses, in the corresponding weighted graph, such
as the sum of the weights over their edges is maximum.

2.2.1.3. Sub-tree context. Different from the notions of parent
context and path context, the authors in [51] consider the set of
XML nodes contained in the sub-tree rooted at a given element
node, i.e., the set of labels corresponding to the node at hand
and all its subordinates, to describe the nodes XML context. The
authors apply a similar paradigm to identify to contexts of all
possible node label senses in WordNet. As a result, both the target
XML node (to be disambiguated), and each of its possible node
label senses in the WordNet taxonomy are represented as sets
of lexical words/expressions. Consequently, XML node label sense
disambiguation is performed by comparing the XML node context
set to each of the candidate sense context sets, using a classic
similarity measure between bags-of-words (the authors utilize
Cosine, taking into account TF-IDF word frequencies in each of
the set representations). The target XML node is finally mapped
to the semantic sense such that their context sets have the highest
similarity.

2.2.1.4. Versatile structural context.
In [23,37], the authors combine the notions of parent context and descendent (sub-tree) context in disambiguating generic structured data (e.g., XML, web
directories, and ontologies). The authors consider that a nodes
context definition depends on the nature of the data and the application domain at hand. They propose various edge-weighting measures (namely a Gaussian decay function, cf. formula (4)) to identify
crossable edges, such as nodes reachable from a given node through
any crossable edge (following a user-specified direction, e.g., ances-
tor, descendent, or both) belong to the target nodes context:

weight(nc , n) = 2  e d2


+ 1 


(4)

where nc is a context node, n is the target node, and d is the distance (in number of edges) separating nc from n in the XML tree. An
extension of the traditional bag-of-words model is introduced to
consider edge weights in representing the XML context. Hence,
structure disambiguation is undertaken by comparing the target
node label with each candidate sense (semantic concept) corresponding to the labels in the target nodes context, taking into
account corresponding XML edge weights. The authors assign confidence scores to each semantic sense following its order of appearance in the corresponding WordNet term definition. They
consequently utilize an edge-based semantic similarity measure [52], exploiting the hypernymy/hyponymy relations (exclud-
ing remaining relations such as meronymy and holonymy, etc.), to
identify the semantic sense best matching the target node label.

2.2.2. XML context representation and processing

Another concern in XML-based WSD is how to effectively
process the context of an XML node (once it has been identified)
considering the structural positions of XML data. Most existing

WSD methods  developed for flat textual data (Section 2.1)
and/or XML-based data [3,2,49,22]adopt the bag-of-words model
where the context is processed as a set of words surrounding
the term/label (node) to be disambiguated. Hence, all context
nodes are treated the same, despite their structural positions in
the XML tree. One approach in [23,37], identified as relational
information model, extends the bag-or-words paradigm toward a
vector-based representation with confidence scores combining:
(i) distance weights separating the context and target nodes,
and (ii) semantic weights highlighting the importance of each
sense candidate. The authors utilize a specially tailored distance
Gaussian decay function (cf. formula (4)) estimating edge weights
such that the closer a node (following a user-specified direction),
the more it influences the target nodes disambiguation [23,37].
The distance decay function is not only utilized in identifying
the context of a target node, but also produces weight scores
which are assigned to each context node in the context vector
representation, highlighting the context nodes impact on the
target nodes disambiguation process [23].

2.2.3. Associating senses with XML nodes

Once the contexts of XML nodes have been determined,
they can be handled in different ways. Two approaches, both
unsupervised and knowledge-based, have been adopted in this
context, which we identify as: (i) concept-based and (ii) context-
based. On one hand, the concept-based approach adopted in [3,
2] consists in evaluating the semantic similarity between target
node senses (concepts) and those of its context nodes, using
measures of semantic similarity between concepts in a semantic
network. The target node label is matched with the candidate sense
corresponding to the candidate combination having the maximum
score. On the other hand, the context-based approach introduced
in [51] consists in building a context vector for the target node
in the XML document tree, and context vectors for each target
node sense (concept) in the semantic network (SN). Then, the XML
context set is compared with each of the SN context sets, using a
typical set comparison measure (e.g., Jaccard similarity). After, the
target node label is matched with the candidate sense having the
SN context set with maximum score w.r.t. the XML target node
context set. A hybrid method in [23,37] combines variants of the
two preceding approaches to disambiguate generic structured data
(including XML). Yet, the authors do not compare their solution
with existing XML disambiguation methods.

Wrapping up: we identify four major limitations motivating
our work (which were highlighted in Section 1): most existing
methods (i) completely ignore the problem of semantic ambiguity,
(ii) only partially consider the structural relationships/context
of XML nodes (e.g., parent-node [22] or ancestor-descendent
relations [2]), (ii) neglect XML structural/semantic features by
using syntactic processing techniques such as the bag-of-words
paradigm [3,22], and (iv) are static in choosing a fixed context
(e.g., parent node [22], or root path [2]) or preselected semantic
similarity measures, thus minimizing user involvement.

3. XML disambiguation framework

To address all motivations above and provide a more complete
and dynamic XML disambiguation approach, we introduce XSDF
(XML Semantic Disambiguation Framework) as an unsupervised
and knowledge-based solution to resolve semantic ambiguities
in XML documents. XSDFs overall architecture is depicted in
Fig. 3. It is made of four modules: (i) linguistic pre-processing,
(ii) nodes selection for disambiguation, (iii) context definition
and representation, and (iv) XML semantic disambiguation. The
system receives as input: (i) an XML document tree, (ii) a
semantic network (noted SN), and (iii) user parameters (to tune

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Definition 2 (XML Document Tree). It is a rooted ordered labeled
tree where nodes represent XML elements/attributes, labeled using
element/attribute tag names. Element nodes are ordered following
their order of appearance in the XML document. Attribute nodes
appear as children of their containing element nodes, sorted4 by
attribute name, and appearing before all sub-elements [55,56].
Element/attribute text values are stemmed and decomposed into
tokens (using our linguistic pre-processing component), mapping
each token to a leaf node labeled with the respective token,
appearing as a child of its container element/attribute node, and
ordered following their appearance order in the element/attribute
text value (Fig. 4a).


Fig. 3. Overall XSDF architecture.

Fig. 4. Sample input (syntactic) XML and output (semantic) XML trees.

the disambiguation process following her needs), and produces as
output a semantic XML tree.

We develop XSDFs modules in the following, starting with the

XML and semantic data models adopted in our study.

3.1. XML and semantic data models

XML documents represent hierarchically structured information and can be modeled as rooted ordered labeled trees (Fig. 4a and
b), based on the Document Object Model (DOM) [53].

Definition 1 (Rooted Ordered Labeled Tree). It is a rooted tree in
which the nodes are labeled and ordered. We denote by T[i] the
ith node of T in preorder traversal, T[i].l its label, T[i].d its depth
(in number of edges), and T[i].f its out-degree (i.e., the nodes fan-
out). R(T ) = [0] designates the root node of tree T.3


3 Tree and rooted ordered labeled tree are used interchangeably hereafter.

Note that element/attribute values can be disregarded (struc-
ture-only) or considered (structure-and-content)
in the XML
document tree following the application scenario at hand. Here,
we believe integrating XML structure and content is beneficiary
in resolving ambiguities in both element/attribute tag names
(structure) and data values (content). For instance, in Fig. 1a,
considering data values Kelly and Stewart would be beneficial
to disambiguate tag label cast. The same applies the other way:
cast can help disambiguate Kelly and Stewart.

Also, we formally define a semantic network, as the semantic

(knowledge base) data model adopted in our study.5

Definition 3 (Semantic Network). It is made of concepts representing groups of words/expressions designating word senses, and
links connecting the concepts designating semantic relationships.
It can be represented as SN = (C, L, G, E, R, f , g, h):
 C: set of nodes representing concepts in SN (e.g., synsets as in

WordNet [16]),

 L: set of words (expressions) describing concept labels and

synonyms,
 G: set of glosses describing concept definitions,
 E: set of edges connecting concept nodes, E  C  C,
 R: set of semantic relationship names describing edge labels,
i.e., R = {Is-A, Has-A, Part-Of, Has-Part, etc.}. Note that synonymous words/expressions are integrated in the concepts themselves (as concept labels), and thus the relationship (edge label)
synonym does not exist in R.
 f : function designating the labels, sets of synonyms, and glosses
associated to concept nodes, f : C  (L, Ln, G) where n designates the number of synonyms per concept,
 g: function designating the labels of edges, g: E  R,
 h: function associating to concepts their occurrence frequencies (cf. Fig. 2) statistically quantified from a given text corpus
(e.g., the Brown corpus [17]).6
Note that c  C denotes a concept with c.l its label, c.syn its set of
synonymous labels, c.gloss its gloss, and c.freq its frequency. Simi-
larly, e  E ( SN) designates an edge linking a pair of concepts,
with e.rel its edge label (semantic relationship name).


After disambiguating target XML node labels in the input XML
document tree, the latter are replaced with the identifiers of
corresponding semantic concepts from the reference semantic
network, thus producing an output XML tree augmented with
semantic meaning. Formally:

4 While the order of attributes (unlike elements) is irrelevant in XML [54], yet we
adopt an ordered tree model to simplify processing [55,56].
5 Knowledge base & semantic network are used interchangeably hereafter.
6 Concept frequencies allow to estimate the information content that concepts
share in common, used in node-based semantic similarity (Section 2.1.5).

It

Definition 4 (Semantic XML Tree).
is an augmented XML
document treein which the labels of nodes which have been
targeted for disambiguation are replaced by semantic concepts
(identifiers) extracted from a reference semantic network, whereas
non-target XML nodes remain untouched, such that the original
XML tree structure remains intact (Fig. 4b).


3.2. Linguistic pre-processing

Linguistic pre-processing consists of three main phases: (i)
tokenization, (ii) stop word removal, and (iii) stemming, applied
on each of the input XML documents element/attribute tag names
and text values, to produce corresponding XML tree node labels.
Here, we consider three possible inputs:
 Element/attribute tag names consisting of individual words,
 Element/attribute tag names consisting of compound words,
usually made of two individual terms (t1 and t2)7 separated by
special delimiters (namely the underscore character, e.g., Di-
rected_By), or the use of upper/lower case to distinguish the
individual terms (e.g., FirstName),

 Element/attribute text values consisting of sequences of words

separated by the space character.

Considering the first case, no significant pre-processing is required,
except for stemming (when the word is not found in the reference
semantic network). In the second case (i.e., compound words,
usually disregarded in existing methods), if t1 and t2 match a
single concept in the semantic network (i.e., a synset in WordNet,
e.g., first name), they are considered as a single token. Otherwise,
they are considered as two separate terms, and are processed for
stop word removal and stemming. Yet, they are kept within a
single XML node (x) as its label (x.l) in order to be treated together
afterward, i.e., one sense will be finally associated to x.l, which is
formed by the best combination of t1 and t2s senses (in contrast
with studies in [23,37,51] which process token senses separately
as distinct labels). As for the third case, we apply traditional
tokenization, i.e., the text value sentence is broken up into a set
of word tokens ti, processed for stop word removal and stemming,
and then represented each as an individual node (xi) labeled with
the corresponding token (xi.l = ti), and appearing as a child of the
containing element/attribute node.

3.3. Node selection for disambiguation

Given an input XML tree, the first step is to select target
nodes to disambiguate, which (we naturally assume) are the most
ambiguous nodes in the document tree. Thus, we aim to provide
a mathematical definition to quantify an XML node ambiguity
degree which can be used to select target nodes for disambiguation
(answering Motivation 1). To do so, we start by clarifying our
intuitions about XML node ambiguity:
 Assumption 0: An XML node whose label has only one possible
sense is considered to be unambiguous,
its semantic
ambiguity is minimal.
 Assumption 1: The semantic ambiguity of an XML node is
related to the number of senses of the nodes label: (i) the more
senses it has, the more ambiguous the node is, (ii) the fewer
senses it has, the less ambiguous the node is.
 Assumption 2: The semantic ambiguity of an XML node is
related to its distance to the root node of the document tree:
(i) the closer it is to the root, the more ambiguous it is, (ii) the
farther it is from the root, the less ambiguous it is.

i.e.,

Fig. 5. Sample XML document trees.

Assumption 2 follows the nature of XML and semi-structured data,
where nodes closer to the root of the document tend to be more
descriptive of the whole document, i.e., having a broader meaning,
than information further down the XML hierarchy [57,56]. In other
words, as one descends in the XML tree hierarchy, information
becomes increasingly specific, consisting of finer details [58], and
thus tends to be less ambiguous.
 Assumption 3: The semantic ambiguity of an XML node is
related to its number of children nodes having distinct labels:
(i) the lesser the number of distinct children labels, the more
ambiguous the node is, (ii) the more the number of distinct
children labels, the less ambiguous the node is.

Assumption 3 is highlighted in the sample XML trees in Fig. 5.
One can clearly identify the meaning of root node label Picture
(i.e., motion picture) in Fig. 5a, by simply looking at the nodes
distinct children labels. Yet the meaning of Picture remains
ambiguous in the XML tree of Fig. 5b (having several children nodes
but with identical labels). Hence, we believe that distinct children
node labels can provide more hints about the meaning of a given
XML node, making it less ambiguous.

While our goal is to quantify XML semantic ambiguity, yet this
can be done in many alternative ways that would be consistent
with the above assumptions. Hence, we first provide some basic
definitions that introduce a set of ambiguity degree factors mapping
to the above assumptions, which we will then utilize to build our
integrated ambiguity degree measure.

Definition 5 (XML Node Polysemy Factor). The polysemy ambiguity
degree factor of an XML node x in tree T, noted AmbPolysemy,
increases when the number of senses of x.l is high in the reference
semantic network SN, or else it decreases:
senses(x.l)  1
AmbPolysemy(x.l, SN) =

 [0, 1]

(5)

Max(senses(SN))  1

where Max(senses(SN)) is the maximum number of senses of a
word/expression in SN (e.g., in WordNet 3.0 [16], Max(senses(SN))
=33, for the word head).


Lemma 1. The XML node polysemy factor AmbPolysemy in Definition 5 varies in accordance with Assumptions 0 and 1.


Proof of Lemma 1. Given formula (5), AmbPolysemy varies as fol-
lows:
 The minimum value AmbPolysemy = 0 is obtained when x.l = 1,
i.e., l has only one sense (e.g., first name in WordNet), i.e., x is
without ambiguity: it always refers to the same meaning.
 The maximum value AmbPolysemy = 1 is obtained when x.l =
Max(senses(SN)).
 When senses(x.l) increases/decreases, AmbPolysemy follows accordingly such that AmbPolysemy  [0, 1].

7 Having more than two terms per XML tag name is unlikely in practice [51]. This
is also true with WordNet concept names, usually made of 1-to-2 terms.

Definition 6 (XML Node Depth Factor). The depth ambiguity degree
factor of an XML node x in tree T, noted AmbDepth, increases when

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

the distance in number of edges between x and R(T ) is low, or else
it decreases such that:
AmbDepth(x, T ) = 1 
where Max(depth(T)) is the maximum depth in T.

Max(depth(T ))

 [0, 1]

x.d

(6)


Lemma 2. The XML node depth factor AmbDepth in Definition 6 varies
in accordance with Assumption 2.


Proof of Lemma 2. Given formula (6), AmbDepth varies as follows:
 The maximum value AmbDepth = 1 is obtained when x.d = 0,
i.e., when x = R(T ).
 The minimum value AmbDepth = 0 is obtained when x.d =
Max(depth(T )), i.e., when x is one of the farthest nodes from
R(T ): one of the deepest (most specific) leaf nodes in Ts
hierarchy.
 When x.d increases/decreases, AmbDepth follows inversely such
that AmbDepth  [0, 1].

Definition 7 (XML Node Density Factor). The density ambiguity
degree factor of an XML node x in tree T, noted AmbDensity, increases
when the number of children nodes of x having distinct labels,
designated as x.f , is low, or else it decreases such that:
AmbDensity(x, T ) = 1 

 [0, 1]

(7)

x.f

Max(fan-out(T ))

where Max(fan-out(T )) is the maximum number of children nodes
with distinct labels in T. We identify this factor as the node density
factor to distinguish it from traditional node fan-out: number of
children nodes (regardless of label, Definition 1).


Lemma 3. The XML node density factor AmbDensity
tion 7 varies in accordance with Assumption 2.

in Defini-


Proof of Lemma 3. Given formula (7), AmbDensity varies as fol-
lows:
 The maximum value AmbDensity = 1 is obtained when x.f = 0,
i.e., when x is a leaf node and does not have any children nodes
(to provide hints concerning xs meaning).
 The maximum value AmbDensity = 0 is obtained when x.f =
Max(fan-out(T)), i.e., when x has the largest number of children
nodes with distinct labels in T. In other words, it has the highest
possible number of hints about its meaning in T.
 When x.f increases/decreases, AmbDensity follows inversely such
that AmbDensity  [0, 1].

From the above ambiguity factor definitions, we can provide
an integrated definition for XML ambiguity degree specifically
tailored to satisfy all the stated assumptions:

Definition 8 (XML Node Ambiguity Degree). Given an XML tree T,
a node x  T, and a reference semantic network SN, we define
the ambiguity degree of x, Amb_Deg(x, T, SN), as the ratio between
AmbPolysemy(x.l, SN) on one hand, and the sum of 1-AmbDepth(x, T )
and 1-AmbDensity(x, T ) on the other hand:
Amb_Deg(x, T , SN)

wDepth  (1  AmbDepth(x, T )) + wDensity  (1  AmbDensity(x, T )) + 1
 [0, 1]

wPolysemy  AmbPolysemy(x.l, SN)

(8)

where wPolysemy, wDepth, wDensity  [0, 1] are independent weight
parameters allowing the user to fine-tune the contributions of
polysemy, depth, and density factors respectively.


In fact, we chose to define Amb_Deg as a ratio (rather than
a typical weighted sum) with AmbPolysemy as the only numerator
factor: in order to guarantee that Amb_Deg is minimal (=0)
when the AmbPolysemy is minimal (=0), regardless of other factors
which were allocated to the denominator. The latter AmbDepth and
AmbDensity factors were combined (within the denominator) using
weighted sum aggregation, with their values reversed (1-AmbDepth
and 1-AmbDensity) to counter their usage as denominator factors:
varying proportionally with Amb-Deg. As a result:

Lemma 4. The ambiguity degree measure Amb_Deg in Definition 8 varies in accordance with Assumptions 03.


Proof of Lemma 4. Given formula (8), Amb_Deg varies as follows:
 The minimum value Amb_Deg = 0 is obtained when the
label of node x has only one possible sense,
i.e., when
AmbPolysem(x.l, SN) = 0, regardless of its depth and density
factors, and regardless of parameter weights (Assumption 0 and
Lemma 1).
 The maximum value Amb_Deg = 1 is obtained when: (i) x.l
has the maximum number of senses in SN (e.g., 33 senses in
WordNet [16]), (ii) x is the root node of T, x.d = 0, and (iii) x has
the minimum number of children nodes with distinct labels in
T , x.f = 0, regardless of wDepth and wDensity parameter values
(Assumptions 13, and Lemmas 13).
 The value of Amb_Deg increases with: (i) the increase in x.ls
polysemy in SN, (ii) the decrease in xs depth, and (iii) the
decrease in xs density in the XML document tree. Inversely,
the value of Amb_Deg decreases with: (i) the decrease in x.ls
polysemy, (ii) the increase in xs depth, and (iii) the increase in
xs density (Assumptions 13, and Lemmas 13).

Special case: When the label of nodex consists of a compound
word made of tokens t1 and t2, we compute Amb_Deg(x) as the
average of the ambiguity degrees of t1 and t2.

Amb_Deg is computed for all nodes in the input XML tree.
Then, only the most ambiguous nodes are selected as targets
for disambiguation following an ambiguity threshold ThreshAmb
automatically estimated or set by the user, i.e., nodes having
Amb_Deg(x, T, SN)  ThreshAmb, whereas remaining nodes are left
untouched. Note that the user can disregard the ambiguity degree
measure: (i) by setting wPolysemy = 0 so that all nodes end up having
Amb_Deg = 0 regardless of polysemy, depth, and density factors,
or (ii) by setting ThreshAmb = 0 so that all nodes are selected for
disambiguation regardless of their ambiguity degrees.
Note that the fine-tuning of parameters is an optimization
problem such that parameters should be chosen to maximize
disambiguation quality (through some cost function such as f-
value, cf. Section 4). This can be solved using a number of
known techniques that apply linear programming and/or machine
learning in order to identify the best weights for a given problem
class, e.g., [5961]. Providing such a capability, in addition to
manual tuning, would enable the user to start from a sensible
choice of values (e.g., identical weight parameters to consider all
ambiguity features equally, i.e., wPolysemy = wDepth = wFan-out = 1,
with a minimal threshold ThreshAmb = 0 to consider all results
initially) and then optimize and adapt the disambiguation process
following the scenario and optimization (cost) function at hand.
We do not further address the fine-tuning of parameters here since
it is out of the scope of this paper (to be addressed in a future study).

Fig. 6. Sample XML context (ring and) sphere neighborhoods.

3.4. Context definition and representation

3.4.1. Sphere neighborhood

For each target node selected from the previous phase, node
contexts have to be defined and processed for disambiguation.
While current approaches only partly consider the semi-structured
nature of XML in defining disambiguation contexts (Motivation
2), we introduce the sphere neighborhood context model, inspired
from the sphere-search paradigm in XML IR [62],8 taking into
account the whole structural surrounding of an XML target node
(including its ancestors, descendants, and siblings) in defining its
context. We define the notion of context ring as the set of nodes
situated at a specific distance from the target node. An XML sphere
encompasses all rings included at distances less than or equal to
the size (radius) of the sphere.

Definition 9 (Context Ring). Given an XML tree T and a target node
x  T, we define an XML context ring with center x and radius
d as the set of nodes located at distance d from x with respect to
the XML structure containment relationship, i.e., Rd (x) = {xi 
T|Dist(x, xi) = d}. Rd (x) is more generally noted Rd,rel (x) when a
different relationship rel (other than XML structure containment)
is considered in building the ring (e.g., using hyperlinks, or
semantic relationships when available).
The distance between two XML nodes in an XML tree, Dist(xi, xj),
is evaluated as the number of edges separating the nodes. For
instance, in tree T of Fig. 6a, the distance between nodes T[2]
and T[6] of labels cast and Kelly respectively is equal to 2.
Hence, the XML ring R1(T[2]) centered around node T[2] (cast)
at distance 1 consists of nodes: T[1] (Picture), T[3] (star) and
T[5] (star).


(x)  dj  d}.

Definition 10 (Context Sphere). Given an XML tree T, a target node
x  T, and a set of XML context rings Rdj
(x)  T, we define an
XML context sphere with center x and radius d as the set of nodes
in the rings centered around x at distances less than or equal to d,
i.e., Sd(x) = {xi  T | xi  Rdj
In Fig. 6b, the XML sphere S2(T[2]) centered around node T[2]
of label cast with radius 2 consists of: ring R1(T[2]) of radius 1
comprising nodes T[1] (picture), T[3] (star) and T[5] (star),
and ring R2(T[2]) of radius 2 comprising nodes T[0] (Films), T[4]
(Stewart), T[6] (Kelly), and T[7] (Plot). The size (radius) of
the XML sphere context is tuned following user preferences and/or
the nature of the XML data at hand (e.g., XML trees might contain
specialized and domain-specific data, and thus would only require
small contexts to achieve good disambiguation, whereas more

8 While comparable with the concept of XML sphere exploited in [62] the latter
consists of an XML retrieval paradigm for computing TF-IDF scores to rank XML
query answers, which is orthogonally different, in its use and objectives, from our
disambiguation proposal.

Fig. 7. Sample context (ring and) sphere neighborhoods of concept Movie based on
the SN extract in Fig. 2.

generic XML data might require larger contexts to better describe
the intended meaning of node labels and values, cf. experiments in
Section 4).

While the notion of context sphere with XML seems limited
to that of a disk in 2D space (cf. Fig. 6), nonetheless, our sphere
neighborhood model
is general and can be straightforwardly
extended to consider different kinds of rings having different
tree node distance functions (including edge weights, density, or
direction, etc. [63,64]), and different kinds of node relationships
(including hyperlinks, or semantic relationships between nodes
when available). For instance, it can be applied to describe the
contexts of concepts in a semantic network SN (which we adopt
in our context-based disambiguation algorithm, cf. Section 3.5.2),
where Rd,rel (c) designates the context ring centered around
concept c and containing all concepts at distance d from c w.r.t.
semantic relationship rel. The context sphere Sd(c) would thus
contain all rings at distance d from c defined w.r.t. the different
kinds of semantic relationships considered in defining its context
rings (cf. Fig. 7).

Note that in this study, we restrict ourselves to the most intuitive notion of distance in building our ring and sphere contexts,
where distance is evaluated as the number of XML structural containment edges (semantic relationship edges when applied on a
SN), and report the investigation of more sophisticated XML (se-
mantic network) distance functions to a dedicated study.

3.4.2. Context vector representation

Having identified the context of a given XML target node, we
need to evaluate the impact of each of the corresponding context nodes in performing semantic disambiguation (in contrast
with existing methods using the bag-of-words paradigm where
context is processed as a set of words/nodes disregarding XML
structure: Motivation 3). Here, we introduce a relational information solution based on the general vector space model in information retrieval [65] (in comparison with the specific decay
function used in [23,37]), designed to consider the structural
proximity/relationships among XML nodes in computing disambiguation scores following our sphere neighborhood model. Our
mathematical formulation follows two basic assumptions:
 Assumption 4: XML context nodes closer to the target node
should better influence the latters disambiguation, whereas
those farther away from the target node should have a smaller
impact on the disambiguation process.

This is based on the structured nature of XML, such as nodes closer
together in the XML hierarchy are typically more related than more
separated ones.
 Assumption 5: Nodes with identical labels, occurring multiple
times in the context of a target node, should better influence the
latters disambiguation in contrast with nodes with identical
labels occurring a lesser number of times.

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

This is based on the notion of context in WSD, where words
occurring multiple times in the context of a target word have a
higher impact on the targets meaning.9 Therefore, we represent
the context of a target XML node x as a weighed vector,
whose dimensions correspond to all distinct node labels in its
sphere neighborhood context, weighted following their structural
distances from the target node.
Definition 11 (XML Context Vector). Given a target node x  XML
tree T, and its sphere neighborhood Sd (x)  T, the corresponding

Vd(x) is defined in a space whose dimensions
context vector
represent, each, a single node label lr  Sd (x), such as 1 <
r < n where n is the number of distinct node labels in Sd (x). The
coordinate of a context vector
(lr ),
stands for the weight of label lr in sphere Sd (x).
Definition 12 (XML Node Label Weight). The weight w
(lr ) of

Vd(x)
Vd(x) corresponding to the sphere
node label lr in context vector
neighborhood Sd (x) of target node x and radius d, consists of the
structural frequency of nodes xi  Sd(x) having label xi.l = lr.
It is composed of an occurrence frequency factor Freq (lr , Sd(x))
(based on Assumption 5) defined using a structural proximity
factor Struct (xi, Sd(x)) (based on Assumption 4). Formally, given
|Sd(x)| the cardinality (in number of nodes) of Sd(x), Freq (lr , Sd(x))
underlines the total number of occurrences of nodes xi  Sd (x)
having label xi.l = lr, weighted w.r.t. structural proximity:
Freq (lr , Sd(x))


Vd(x) on dimension lr, w
Vd(x)


Struct (xi, Sd(x))  1

xi Sd(x) /
xi.l = lr

|Sd(x)| + 1

d + 1

(9)

Struct (xi, Sd(x)) underlines the structural proximity between each
context node xi  Sd (x) having xi.l = lr, and the target (sphere
center) node x, formally:
Struct (xi, Sd(x)) = 1  Dist(x, xi)
d + 1
(10)
The denominator in Struct (xi, Sd(x)) is incremented by 1 (i.e., d+1)
to allow context nodes occurring in the farthest ring of the sphere

context Sd(x), i.e., the ring Rd(x) of radius d, to have a non-null
Vd(x), and thus a non-null impact on the disambiguation
weight in
of target node x.

d + 1

Consequently, the combined weight factor is defined as the

, 1

normalized occurrence frequency factor:

 [0, 1] (11)

(lr ) = Freq (lr , Sd(x))

= 2  Freq (lr , Sd(x))


|Sd(x)| + 1
following formula (9).

Vd(x)
MaxFreq
where MaxFreq = |Sd(x)|+1
For instance, given the XML tree in Fig. 6, Fig. 8 shows context
vectors of sphere neighborhoods S1(T[2]) and S2(T[2]) centered
around node T[2] of label Cast. Considering
(4)+1 = 0.4 given that: (i) Cast is

the label of S1(T[2])s target (center) node T[2], i.e., Struct
V1(Cast)
(T[2], S1(T[2])) = 1, (ii) T[2] is the only node occurrence of
label cast in S1(T[2]), i.e., Freq (Cast, S1(T[2])) = 1, and (iii)
|S1(T[2])| = 4.

(Cast) = 2(1)


V1(T[2]):

(Star) = 2(0.5+0.5)

Fig. 8. Sample sphere context vectors based on the sphere neighborhoods in Fig. 6.
(4)+1 = 0.4 given that: (i) S1(T[2])

contains two nodes T[3] and T[5] of label Star at distance =1
V1(Cast)
2 =
from the target node T[2], i.e., Struct (T[3], S1(T[2])) = 1 1
0.5, (ii) T[3] and T[5] are the only node occurrences of label
Star inS1(T[2]), i.e., Freq (Star, S1(T[2])) = 0.5 + 0.5 = 1,
and (iii) |S1(T[2])| = 4.

Similarly for remaining context vector weight computations.
Here, one can realize that label weights in Fig. 8 increase as
nodes occur closer to the target node (Assumption 4), and as
the number of node label occurrences increases in the sphere
(Star) =
context (Assumption 5, e.g., in
2  w
(Picture)since node label Star occurs twice in

V2(T[2])). For-
S1(T[2]) while Picture occurs once; similarly in
mally:
Lemma 5. The context vector weight measure w
Vd(x)
tion 11 varies in accordance with Assumptions 5 and 6 .


V1(T[2]), w 

(lr ) in Defini-

V1(Cast)

V1(Cast)


(lr ) varies as:

Proof of Lemma 5. Given formula (11), w 
Vd(x)

(lr ) increases with Freq (lr , Sd(x)), which increases with
Vd(x)
Struct (xi, Sd(x)):
 The value of Struct (xi, Sd(x)) is inversely proportional to
the distance between the target node x and context node xi
(following Assumption 1).
 The minimum value Struct (xi, Sd(x)) = 1
d+1 is reached
when xi  Rd(x) where Rd(x) is the outer-most ring in
sphere Sd(x).
 The maximum value Struct (xi, Sd(x)) = 1 is reached when
processing the target node itself, i.e., xi = x.
 The value of Freq (lr , Sd(x)) is proportional to the number of
occurrences of nodes having the same label xi.l = lr (following
Assumption 2).
 The minimum value Freq (lr , Sd(x)) = 1
d+1 is reached when
there is only one context node having label lr and occurring
on the outer-most ring Rd(x) of the sphere neighborhood,
more formally:
 xi  Sd(x)/(xi.l = lr ) (xi  Rd(x)) ( xj  Sd(x)/xj. =
lr ).
 The maximum value of Freq (lr , Sd(x)) = |Sd(x)|+1
is reached
when all context nodes have the same label lr and appear on
the inner-most ring R1(x) of the sphere neighborhood, more
formally:
 xi  Sd(x)/(xi.l = lr ) (xi  R1(x)). Here, Freq (lr , Sd(x)) =
2  (|Sd(x)|  1) = |Sd(x)|+1
1 + 1

Consequently:
(lr ) = 0 is obtained when no nodes
 The minimum value w
Vd(x)
(lr ) = 1 is obtained when maxi-
 The maximum value w
Vd(x)
mum frequency is obtained, since the weight score is normalized using maximum frequency, i.e., |Sd(x)|+1

of label lr occur in the sphere context of target node x.

9 This is also similar to the notion of term frequency (or TF) in information
retrieval, where the importance of a given term t in describing a document D
increases with the frequent occurrence of t in D [15,65].

In short, context nodes are weighted based on their labels
occurrences as well as the sizes (radiuses) of the sphere contexts

to which they correspond, varying their weights and thus their
impact on the target nodes disambiguation accordingly.

3.5. XML semantic disambiguation

Once the contexts of XML nodes have been determined, we
process each target node and its context nodes for semantic
disambiguation. Here, we propose to combine two strategies:
the concept-based approach and the context-based approach. The
former is based on semantic concept comparison between target
node senses (concepts) and those of its sphere neighborhood
context nodes, whereas the latter is based on context vector
comparison between the target nodes sphere context vector in the
XML tree and context vectors corresponding to each of its senses in
the reference semantic network. The user will be able to combine
and fine-tune both approaches (Motivation 4).

3.5.1. Concept-based semantic disambiguation

It consists in comparing the target node with its context
nodes, using a combination of semantic similarity measures
(edge-based, node-based, and gloss-based, cf. Section 2.1) in order
to compare corresponding semantic concepts in the reference
semantic network. Then, the target node sense with the maximum
similarity (relatedness) score, w.r.t. context node senses, is chosen
as the proper target node sense. To do so, we propose an extension
of context-based WSD techniques (cf. Section 3.5) where we:
 Build upon the sphere neighborhood context model, to consider
XML structural proximity in evaluating the semantic meanings
of context nodes (in comparison with the traditional bag-of-
wordscontext model),

 Allow an extensible combination of several semantic similarity
measures,
in order to capture semantic relatedness from
different perspectives (in comparison with most existing
methods which exploit pre-selected measures).

Definition 13 (Concept-Based Semantic Score). Given a target node
x  XML tree T and its sphere neighborhood Sd (x)  T, and given
sp as one possible sense for x.l in a semantic network SN, we define
Concept_ Score(sp, Sd(x), SN) to quantify the semantic impact of
sp as the potential candidate for the intended sense (meaning) of
x.l within context Sd(x) in T w.r.t. SN, computed as the average of
the weighted maximum similarities between sp and context node
senses:
Concept_Score(sp, Sd(x), SN)

Sim(sp, si

j, SN)  w

Vd(x)

(xi.l)

|Sd(x)|

(12)
j designates the jth sense of context node xi.l  Sd (x),
j, SN)designates the semantic similarity measure

 [0, 1]

xi  Sd(x)

Max
j  xi.l
s i

where si
and Sim(sp, si
between senses sp and si

j w.r.t. SN.


Definition 14 (Semantic Similarity Measure). It quantifies the semantic similarity (relatedness10) between two concepts (i.e., word
senses) c1 and c2 in a reference semantic network SN, computed as the weighted sum of several semantic similarity mea-
sures.11Formally:

10 Semantic relatedness is more general than similarity: dissimilar concepts may
be semantically related by lexical relationships such as meronymy (carwheel),
antonymy (darklight), or any kind of functional relationship (e.g., pencilpaper,
rainflood). Most semantic similarity measures typically capture relatedness rather
than just similarity, which is required in WSD [39].
11 Here, we use three typical semantic similarity measures, yet any other semantic
similarity measure can be used, or combined with the latter.

(13)

Sim(c1, c2, SN) = wEdge  SimEdge(c1, c2, SN)

+ wNode  SimNode(c1, c2, SN)
+ wGloss  SimGloss(c1, c2, SN)  [0, 1]

where:
 wEdge + wNode + wGloss = 1 and(wEdge, wNode,wGloss)  0
 SimEdge is a typical edge-based measure from [42], cf. formula (1)
 SimNode is a typical node-based measure from [43], cf. formula

(2)

 SimGloss is a typical gloss-based measure from [46], cf. formula

(3), expanded and normalized following Definition 15.


Definition 15 (Expanded/Normalized Gloss Similarity Measure).
Given concepts c1 and c2 in a semantic network SN, we define an
expanded version of the basic gloss-based measure from [46] (cf.
formula (3)) to include (in addition to the concepts original gloss
gloss(ci) and the glosses if its surrounding concepts in SN connected
using different semantic relations gloss(rel(ci)) the concepts set of
synonymous words/expressions syns(ci) within its extended gloss,
thus enriching the latter with more semantically related terms.
Formally:
SimGloss(c1, c2, SN) = ExtGloss(c1)  ExtGloss(c1)
= (gloss(c1)  gloss(rel(c1))  syns(c1))
 (gloss(c2)  gloss(rel(c2)  syns(c1)))  [0, 1]
where the set overlapping (intersection scoring) mechanism, noted
 (which was originally designed to assign a score of n2 to an
n-word overlap sequence, thus producing non-normalized scores
following [46]12) is normalized as follows:
ExtGloss(c1)  ExtGloss(c2)
n2

Max(|ExtGloss(c1)|, |ExtGloss(c1)|)2

 [0, 1]


(14)

(15)

having m the number of sequences of n consecutive overlapping
words between the two extended glosses of c1 and c2, where m
and n  [1, Max(| ExtGloss (c1)|,|ExtGloss(c2)|)].
Lemma 6. The gloss similarity measure in Definition 15 produces
normalized similarity scores  [0, 1].


Proof of Lemma 6. Given formulas (14) and (15), SimGloss varies as
follows:
 The minimum value SimGloss = 0 is obtained when the
two extended glosses of concepts c1 andc2 being compared
are completely different, i.e., the number of n-word overlap
sequences between the latter is = 0.
 The maximum value SimGloss = 1 is obtained when c1 andc2
have the exact same extended glosses, i.e., having one single n-
word overlap sequence (m = 1) where n = | ExtGloss(c1)| = |
ExtGloss (c2)|.
 The value of SimGloss increases with the increase of the number
(m) and size (n) of n-word overlap sequences, and decreases
otherwise.

12 The authors in [46] consider that a phrasal n-word overlap (i.e., overlap of a
sequence of n consecutive words) is a much rarer occurrence than a single word
overlap, and thus assign an n-word overlap the score of n2, which is greater than
the sum of the scores assigned to those n individual word overlaps which is equal
to n (=12 + 12 +  n times).

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Fig. 9. Concept-based XML Semantic Disambiguation algorithm.

for

concept-based XML

Algorithm XSDConcept

Semantic
Disambiguation, following Definition 12, is provided in Fig. 9. It
accepts as input a target XML node x and a user specified sphere
neighborhood radius d, along with the source XML tree T and a
reference semantic network SN. After generating x sphere neigh-

Vd(x) (line 1), each of the target
borhood Sd(x) and context vector
label senses sp in SN is compared with the senses si
j of each of the
context node labels xi.  (lines 39) weighted using corresponding sphere neighborhood vector weights w
(xi.l) (line 10). The
Vd(x)
target node sense with the maximum combined similarity score is
then chosen as the output concept c which best describes the sense
(meaning) of the target nodes label x.  w.r.t. SN (lines 1213).

Special case: If the target node label x.l is a compound word
consisting of two tokens t1 and t2 for which no single match
was found in the reference semantic network SN (Section 3.2),
an average score for each possible combination of senses (sp, sq)
corresponding to each of the individual token senses (sp for token
t1, and sq for t2) is computed to identify the sense combination
which is most suitable for the compound node label:
Concept_Score((sp, sq), Sd(x), SN)

Sim((sp, sp), si

j, SN)  w

(xi.l)

Vd(x)

Max
j  xi.l
s i

|Sd(x)|

xi  Sd(x)
 [0, 1]

where Sim((sp, sp), si

j, SN) = Sim(sp,si

j, SN)+Sim(sq,si

j, SN)

Note that a compound context node label xi.l which tokens ti

2 do not match any single concept in SN, is processed similarly

and ti
to a compound target node label.

(16)

 [0, 1].

3.5.2. Context-based semantic disambiguation

It consists in comparing the target node sphere neighborhood
in the XML tree with each of its possible sense (concept) sphere
neighborhoods in the reference semantic network. To do so, we
adopt the same notions of sphere neighborhood and context
vector (Definitions 811) defined for XML nodes in an XML
tree, to build the sphere neighborhood and context vector of a
semantic concept in the semantic network. The only difference

Fig. 10. Context-based XML Semantic Disambiguation algorithm.

here is that sphere rings in the semantic network are built
using the different kinds of semantic relationships connecting
semantic concepts (e.g., hypernyms, hyponyms, meronyms, etc., cf.
Definition 3 and Fig. 7), in contrast with sphere rings in an XML tree
which are built using XML structural containment relationships
(Definition 2). Here, given a reference semantic network SN, a
semantic concept c  SN, and a radius d, we designate by Rd,rel(c),

Vd (c) the ring, sphere, and context vector of radius
Sd (c), and
dcorresponding to concept c in SN respectively considering the
different kinds of semantic relationships rel connecting c within
SN. Note that linguistic pre-processing (cf. Section 3.2) can be
applied to concept labels (when needed13) before building context
vectors and computing vector weights. Formally:

Definition 16 (Context-Based Semantic Score). Given a target node
x  XML tree T, its sphere neighborhood Sd (x)  T and context

Vd(x), and given sp as one possible sense for x. in a reference
vector
semantic network SN, with its sphere neighborhood Sd (sp)  SN

Vd (sp), we define Context_Score(sp, Sd(x), d, SN)
and context vector
to quantify the semantic impact of sp as the potential candidate
designating the intended sense (meaning) of x.l within context
Sd(x) in T w.r.t. SN, computed using a vector similarity measure
between
Context_Score(sp, Sd(x), d, SN) = cos(


Vd (sp))  [0, 1]


Vd(x) and


Vd (sp):


Vd(x),

(17)

Semantic

where cos designates the cosine vector similarity measure.14

Algorithm XSDContext

for

context-based XML

Disambiguation, following Definition 16, is provided in Fig. 10.
XSDContext accepts as input a target XML node x and user
specified sphere neighborhood radiuses d and d, along with the

source XML tree and a reference semantic network SN. After
generating x sphere neighborhood Sd(x) and context vector
Vd(x)

in T (line 1), the algorithm generates the sphere neighborhood
Vd (sp) for each of the target label senses
Sd (sp) and context vector

13 This depends on the semantic network (not required with WordNet).
14 We adopt cosine since it is widely used in IR [65]. Yet, other vector similarity
measures can be used, e.g., Jaccard, Pearson corr. coeff., etc.

sp in SN (lines 35). It then compares the target node context vector


Vd (sp) using
Vd(x) with each of the label sense context vectors
(cosine) vector comparison (line 6). Consequently, the target node
sense with the maximum vector similarity score is chosen as the
output concept c which best describes the sense (meaning) of the
target nodes label x.l w.r.t. SN (lines 89).
Note that XML and semantic network sphere neighborhood
sizes d and d are not strictly tied (and can be chosen differently
by the user) since the semantic networks structure can be
quite different from the XML document structure (producing
smaller/larger contexts accordingly).

Special case: If the target node label x.l is a compound word
consisting of tokens t1 and t2 for which no single match was
found in the reference semantic network SN, an integrated score
for each possible combination of senses (sp, sq) corresponding to
the individual token senses (sp for token t1, and sq for token t2) is
computed. Here, the sphere neighborhoods and context vectors of
individual senses sp and sq are combined together to represent the
context sphere of the combination of senses (sp, sq) in SN:
Concept_Score((sp, sq), Sd(x), SN)

Vd (sp, sq))  [0, 1]
= cos(
where
compound sphere neighborhood Sd (sp, sq) = Sd (sp)  Sd (sq).


Vd(x),
(18)

Vd (sp, sq) is a compound context vector generated based on

3.5.3. Combined semantic disambiguation

While concept-based and context-based disambiguation can be
applied separately as described in the above sections, yet we allow
the user to combine and fine-tune both approaches (answering
Motivation 4), producing a combined score as the weighted sum
of concept-based and context-based scores:
Combined_Score(sp, Sd(x), SN)
= wConcept  Concept_Score(sp, Sd(x), SN)
+ wContext  Context_Score(sp, Sd(x), SN)  [0, 1]
where wConcept + wContext = 1 and (wConcept , wContext )  0.
andXSDContext and is thus omitted here for clarity.

Algorithm XSDCombined is an integration of algorithms XSDConcept

(19)

3.6. Complexity analysis

 (|Sd(x)|  |senses(xi.l)|) + |Sd(x)| +Sd(sp)))where|T| des-

The overall time complexity of our XML disambiguation approach following XSDF simplifies to: O(|T| + (|X|  |senses(x.l)|
ignates the cardinality (in number of nodes) of the XML tree, |X|
the total number of target XML nodes selected for disambiguation,
x a target node, | senses (x.l)| the total number of possible senses
(concepts) of the target nodes label x.l in the reference semantic
network SN, (e.g., WordNet), |Sd(x)| the cardinality of the sphere
neighborhood context of x in the XML tree, xi  Sd(x) a context
node, |Senses(xi.l)| the total number of possible senses of context
node label xi.l, sp  senses(x.l) a possible sense for target node
label x.l, and |Sd(sp)| the cardinality of the sphere neighborhood
of target label sense sp in SN. Complexity is evaluated as the sum
of the complexities of the four main phases of XSDF, and mainly
amounts to the complexities of our concept-base and context-based
disambiguation algorithms:

15 We organized documents based on ambiguity first, and structure second. This
explains why Group 2 has approximately the same Struct_Deg as Group 3 (whose
value is slightly smaller than that of Group 2): since Group 2 has a higher Amb_Deg
than Group 3. We adopted this organization since ambiguity is the foremost feature
targeted in this study, which also allowed us to improve and facilitate the discussion
of experimental results later.

 The complexity of the three initial phases of XSDF: (i)linguistic
pre-processing (tokenization, stop word removal and stem-
ming), (ii) node selection for disambiguation (ambiguity degree computation), and (iii) context definition and representation
(sphere neighborhood and context vector generation) respectively amounts to: O(|T| n |l|) +O(|T|) +O(|T|), which simplifies to O(|T|).
 The complexity of our XML semantic disambiguation phase
simplifies to:
O (|X|  |senses(x.l)|  (|Sd(x)|  |senses(xi.l)|)

+|Sd(x)| +Sd(sp) .

 The complexity of algorithm XSDConcept (cf. Table 1) amounts
to O(|senses(x.l)|n  |Sd(x)|  | senses (xi.l)|n  (|SN| 
depth(SN)) + O(|gloss|2)), where: (i) n designates the total
number of tokens within the target (and context) node label
(usually n = 1, i.e., the label consists of a single word, and
sometimes n = 2 when the label consists of a compound word
made of two terms16 without a single match in the SN), (ii)
O(|SN|  depth(SN)) underlines the complexity of the edgebased and node-based semantic similarity measures adopted
in our study; and (iii) O(|gloss|2) is the time complexity of our
gloss-based measure where gloss designates our extended gloss
(including synonyms). Yet, given that SN designates a fixed
reference throughout the disambiguation process (i.e., |SN|,
depth(SN), and |gloss| remain unchanged), and given that the
number of tokens per XML node label is usually n = 1,
XSDConcepts complexity simplifies to O(|senses(x.l)||Sd(x)||
senses (xi.l)|).
 The complexity of algorithm XSDContext (Fig. 10) amounts to
O|senses(x.l)|n  (|Sd(x)| + n  |Sd(sp)|) where: (i) O(|Sd(x)| 
|Sd(sp)|) designates the complexity of computing the cosine


Vd(x) and
Vd(sp), and (ii)
measure between context vectors

n|Sd(sp)| designates the size of vector
Vd(sp) extended to cover
the sphere neighborhoods of n context node label tokens when
context node labels consist of compound words. Yet, given that
the number of tokens per XML node label is usually n = 1,
XSDContexts complexity simplifies to O(|senses(x.l)| (|Sd(x)|)+
|Sd(sp)|).
 The complexity of algorithm XSDCombined consists of the sum
of the complexities of XSDConcept and XSDContext and thus
simplifies to: O(|senses(x.l)|  |Sd(x)|  |senses(xi.l)|) +
+ |Sd(x)| +Sd(sp)) .
O(|senses(x.l)|  (|Sd(x)| + |Sd(sp)|)) = O(|senses(x.l)| 
(|Sd(x)|  |senses(xi.l)|)
semantic disambiguation phase becomes :
 When applied to all target nodes |X|, the complexity of our XML
 (|Sd(x)|  |senses(xi.l)|) + |Sd(x)| +Sd(sp)) = O
x X O(|senses(x.l)|
+Sd(sp))).
(|X|  |senses(x.l)|  (|Sd(x)|  |senses(xi.l)|) + (|Sd(x)|

Space complexity is similar to time complexity and simplifies

(in worst case) to the same time complexity factor.

3.7. Parallel versus incremental disambiguation

On one hand, XSDFs architecture can be straightforwardly
implemented on a parallel (multi-core and/or multi-thread) processing platform, since XML nodes targeted for disambiguation can be processed independent, and thus simultaneously

16 Recall that having more than two terms per XML tag name is unlikely [51].

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Table 1
XML test documents organized based on average node ambiguity and structure. 15

Ambiguity +
Ambiguity 

Structure +
Group 1
Amb_Deg = 0.1127 & Struct_Deg = 0.6803
Group 3
Amb_Deg = 0.0625 & Struct_Deg = 0.612

+Sd(sp))) time.

(in parallel). With parallel disambiguation, complexity simplifies to O(|senses(x.l)|  (|Sd(x)|  |senses(xi.l)|) + (|Sd(x)|
On the other hand, XSDF can also be straightforwardly extended
toward incremental disambiguation, where: (i) nodes targeted for
disambiguation are processed one at a time, ii) ordered following their ambiguity degree, starting from the least (or most) ambiguous nodes, such that (iii) the senses of initially disambiguated
nodes are utilized in processing subsequent nodes. While time
complexity remains unaffected here (i.e., time is linear w.r.t. the
number of target nodes, cf. Section 3.6), yet, incremental disambiguation might affect disambiguation quality: previously disambiguated nodes (senses) might affect the result quality (senses
selected) for later disambiguated ones.

In this study, we adopt the most basic XSDF design: (i) nonparallel with (ii) no incremental processing, and defer the evaluation of parallel and incremental disambiguation architectures to
a dedicated upcoming study.

4. Experimental evaluation

We have developed a prototype titled XSDF 17 to test and
compare our approach with its most recent alternatives. We have
evaluated three criteria: (i) semantic ambiguity, (ii) disambiguation
quality, and (iii) time performance.

4.1. Experimental test data

We used a collection of 80 test documents gathered from
several data sources having different properties (cf. Table 4), which
we describe and organize based on two features: (i) node ambiguity,
and (ii) node structure (cf. Table 1). The former feature highlights
the average amount of ambiguity of XML nodes in the XML tree,
estimated using our ambiguity degree measure, Amb_Deg  [0, 1].
The latter feature describes the average amount of structural
richness of XML nodes, in terms of node depth, fan-out, and density
in the XML tree, estimated as the sum of normalized node depth
(1-AmbDepth), fan-out, and density (1-AmbDensity) factors, averaged
over all nodes in the XML tree, formally:

Struct_Deg(x, T ) = wDepth  x.d

+ wFan-out  x.f

Max(depth(T ))

+ wDensity  x.f

Max(fan-out(T ))
 [0, 1]

Max(fan-out(T ))

(20)
where wDepth+ wFan-out + wDensity = 1 and (wDepth, wFan-out ,wDensity)
 0.
High node depth, fan-out, and/or density here indicate a highly
structured XML tree, whereas low node depth, fan-out, and/or
density indicate a poorly structured tree (relatively flat document).
In our experiments, we set equal weights wDepth = wFan-out =
wDensity = 1/3 when measuring Struct_Deg (Table 1). As mentioned
previously, we do not address the issue of assigning weights, which

17 Available online at http://sigappfr.acm.org/Projects/XSDF/.

Structure 
Group 2
Amb_Deg = 0.1378&Struct_Deg = 0.6621
Group 4
Amb_Deg = 0.0447&Struct_Deg = 0.5515

could be performed using optimization techniques (e.g., linear
programming and/or machine learning [5961]) to help fine-tune
input parameters and obtain optimal results (Section 3.3). Such a
study would require a thorough analysis of the relative effect of
each parameter on disambiguation quality, which we report to a
dedicated study.

4.2. XML ambiguity degree correlation

We compared XML node ambiguity ratings produced by human
users with those produced by our system (i.e., via our ambiguity degree measure, Amb_Deg, cf. Section 3.3), using Pearsons correlation
coefficient, pcc = XY /(X  Y ) where: x and y designate user and
system generated ambiguity degree ratings respectively, X and Y
denote the standard deviations of x and y respectively, and XY denotes the covariance between the x and y variables. The values of
pcc  [1, 1] such that: 1 designates that one of the variables
is a decreasing function of the other variable (i.e., values deemed
ambiguous by human users are deemed unambiguous by the sys-
tem, and vice versa), 1 designates that one of the variables is an
increasing function of the other variable (i.e., values are deemed
ambiguous/unambiguous by human users and the system alike),
and 0 means that the variables are not correlated. Five test subjects
(two master students and three doctoral students, who were not
part of the system development team) were involved in the exper-
iment. For each (of the 80) test document(s) in our datasets, manual ambiguity ratings (in the form of integers  [0, 4], i.e., [min,
max] ambiguity) where acquired from each tester for 12 to 13
randomly pre-selected nodes,18 i.e., a total of 1000 nodes (dur-
ing an average 10 h rating time per tester). We first quantified
cross-(human) tester agreement, by computing pair-wise correlation scores among testers for each of the rated datasets (cf.
Table 2, Figs. 1113). One can realize that average cross-tester
correlation levels are relatively low when rating datasets with high
ambiguity (cf. Groups 1 and 2 in Table 2) and increase when rating datasets with low ambiguity (cf. Groups 3 and 4 in Table 2).
On one hand, we realized that testers tend to agree less when rating ambiguous documents: certain testers provide high ambiguity
scores for some XML node labels (considering them to be highly
ambiguous) whereas other testers provide low ambiguity scores
when rating the same node labels (considering them to be less am-
biguous), hence yielding low tester correlation (cf. scores of Groups
1 and 2 in Table 2). One can also realize this discrepancy in human ratings among node labels of the same dataset/document:
nodes of higher average ambiguity show higher discrepancy in individual tester ratings (e.g., labels line in Fig. 11a and header

18 The human testers were provided: (i) the source XML documents where
the nodes targeted for ambiguity rating were highlighted, and (ii) a document
with instructions on how to rate, including a table containing all target nodes
to which they should assign their ratings. Testers were instructed to check
each nodes context in the XML document while providing ratings, without
however informing them of the nature/size/span of the context. This was done
on purpose since we wanted our human testers to rely each on her/his own
basic understanding/intuition about the context of a node (rather than follow
any predefined mathematical concept) in providing her/his ratings. Sample test
documents and ratings are provided online.

Table 2
Correlation between human tester ratings.

Group 1
Group 2

Group 3

Group 4

Dataset 1
Dataset 2
Dataset 3
Dataset 4
Dataset 5
Dataset 6
Dataset 7
Dataset 8
Dataset 9
Dataset 10

Testers


Avg.

in Fig. 12a where tester ratings are significantly different) in comparison with labels of lower average ambiguity (e.g., labels scene
description Fig. 11a and item type in Fig. 12a where tester ratings are almost identical). This was expected since it seemed
normal that different human testers perceive the meanings of ambiguous nodes/documents differently. On the other hand, testers
tend to agree more when rating documents which are less ambigu-
ous: most testers provide similar scores for the same XML node
labels (usually agreeing when labels are of low ambiguity, and
sometimes agreeing when labels are of high ambiguity, cf. Fig. 13a),
hence yielding high tester correlation (cf. scores of Groups 3 and 4
in Table 2). On the whole, considering all inter-tester correlations
over all datasets highlights an overall average correlation score of
0.411, which seemed a reasonably high inter-tester correlation
given the diversity of the documents being rated (hence considering the obtained average tester ratings as a reasonably acceptable
reference for empirical evaluation). Subsequently, we produced
average human tester ratings per target XML node label in each
dataset, which then were correlated with system ratings, computed with variations of Amb_Degs parameters to stress the impact of its factors (AmbPolysemy, AmbDepth, and AmbDensity): (i) Test #1
considers all three factors equally (wPolysemy = wDepth = wDensity =
1), (ii) Test #2 focuses on the polysemy factor (wPolysemy = 1 while
wDepth = wDensity = 0), (iii) Test #3 focuses on the depth factor
(wDepth = 1 while wPolysemy = 0.2 and wDensity = 0), (iv) Test #4
focuses on the density factor (wDensity = 1, wPolysemy = 0.2 and
wDepth = 0).
Results are compiled in Table 3. Detailed manual and system
ambiguity ratings concerning the three extreme correlations
scores (maximum, closest to null, and minimum scores, highlighted
in bold in Table 3) are shown in Figs. 1113.

Results in Table 3 and Figs. 1113 show that the highest
correlation scores are obtained when evaluating nodes of Dataset
1 in Group 1 (high ambiguity and rich structure), with a maximum
corr = 0.439 with Test #4 (using the density factor of Amb_Deg).
The lowest scores with all four tests are obtained with Dataset 9
in Group 4 (low ambiguity and poor structure), with a minimum
corr = 0.456 with Test #4. Close to null scores are obtained with
Dataset 2 of Group 2 (high ambiguity and poor structure), Dataset
3 of Group 3 (low ambiguity and rich structure), and Dataset 8 of
Group 4 (low ambiguity and poor structure), such that the closest
to null score corr = 0.017 is obtained with Dataset 2 of Group 2
in Test #1 (combining all factors of Amb_Deg). The above results
highlight several observations.

(1) When highly ambiguous and highly structured XML nodes
are involved (e.g., Group 1), XML ambiguity seems to be perceived

19 Node labels shown in the graphs of Figs. 1113 are snapshots of those in
the corresponding datasets, statistically sampled to represent correlation score
variation between extremes (maximum, closest to null, and minimum scores)
describing the node label distribution in each dataset.

Fig. 11. Manual and system generated average ambiguity degrees highlighting
maximum correlation with documents of Dataset 1 of Group 1. The x axis represents
sample node labels (tag names/values) statistically selected to describe Dataset 1.19

and evaluated similarly by human users and our system, obtaining
maximum positive correlation between human and Amb_Deg
scores (cf. visualized sample in, cf. Fig. 11b).

(2) When less ambiguous and/or poorly structured XML
nodes are involved, ambiguity seems to be evaluated differently
by users and our system, attaining: negative or close to null
correlation when low ambiguity and/or poorly structured XML
nodes are evaluated: (i) negative correlation (opposite ambiguity
scores) when low ambiguity and poorly structured XML nodes
are evaluated (e.g., Group 4, cf. visualized sample in Fig. 13b), (ii)

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Fig. 12. Manual and system generated average ambiguity degrees highlighting
closest to zero (null) correlation with Dataset 2 of Group 2.

Table 3
Correlation between human ratings and system generated ambiguity degrees.

Test #1
All factors

Test #2
Polysemy

Group 1
Group 2

Group 3

Group 4

Dataset 1
Dataset 2
Dataset 3
Dataset 4
Dataset 5
Dataset 6
Dataset 7
Dataset 8
Dataset 9
Dataset 10


Test #3
Depth


Test #4
Density


close to null correlation (broadly related or unrelated ambiguity
scores) with either low ambiguity and/or poorly structured nodes
(e.g., Groups 24), and (iii) varying correlation (ranging from
positive to negative scores) when less ambiguous nodes are
evaluated (e.g., Dataset 4 in Group 3 yields a positive correlation
score, whereas Datasets 3 and 5 yield close to null and negative
scores).

These contradictory and/or unrelated correlation scores are
mainly due to the intuitive understanding of semantic meaning
by humans, in contrast with the intricate processing done by our

Fig. 13. Manual and system generated average ambiguity degrees highlighting
minimum correlation with Dataset 9 of Group 4 (Fig. 14).

Fig. 14. Sample XML document from Dataset 9 of Group 4, with corresponding
grammar.

automated system. For instance, regarding documents of Dataset
9 of Group 4 (conforming to the personnel.dtd grammar of the
Niagara XML document collection, cf. Fig. 14), the meaning of
child node label state under node label address was obvious
for our human testers (providing an ambiguity score of 0/4). Yet,
the interpretation of the meaning of state is not so obvious
for a machine, especially using a rich lexical dictionary such as
WordNet where word state has 8 different meanings. Here, a
label considered relatively unambiguous from the users point of
view was assigned a higher ambiguity score by the system based
on the expressiveness of the lexical reference.

Table 4
Characteristics of test documents.

Groups

Datasets Source dataset

Grammar

N# of
docs

Avg. N# of nodes
per doc

Group 1

Group 2

Group 3

Group 4

Shakespeare collection

Shakespeare.dtd

Amazon product files

Amazon_product.dtd 10

SIGMOD Record
IMDB database
Niagara collection

W3Schools
W3Schools
W3Schools
Niagara collection
Niagara collection

ProceedingsPage.dtd
Movies.dtd
Bib.dtd

cd_catalog.dtd
Food_menu.dtd
Plant_catalog.dtd
Personnel.dtd
Club.dtd

16.

19.

Shakespeare collection: Available at http://metalab.unc.edu/bosak/xml/eg/shaks200.zip.
Amazon product files: Available at simply-amazon.com/content/XML.html.
SIGMOD Record: Available at http://www.acm.org/sigmod/xml.
IMDB database: Data extracted from http://www.imdb.com/ using a wrapper generator.
Niagara collection: Available at http://www.cs.wisc.edu/niagara/.
W3Schools: Available from http://www.w3schools.com.

Node
depth

Node
density

Node
Fan-out

Node
label
polysemy
(N# of
senses)
Avg. Max. Avg. Max. Avg. Max. Avg. Max.
7.052 30

0.604 20

8.407 72

4.615 16

4.384 13

3.937 10

3.454 15

4.533 10

0.539 13

(3) Concerning Amb_Degs parameter weight variations (for
wPolysemy, wDepth, and wDensity) with tests 24, all three parameters
seem to have comparable impacts on ambiguity evaluation. This
can be clearly seen in Table 3 where similar correlation scores are
obtained for each dataset when running Tests #24.

Note that certain parameters naturally have more impact in
certain cases, based on the nature of the XML data involved. For
instance, XML files with uneven distributions of depth, such as
those in Dataset 2 of Group 2 (Amazon products file), containing
nodes of depth 0 or 1 and others of depth 9, produce the highest
correlation score with Test #3 (0.243) which evaluates the depth
factor of Amb_Deg.

Likewise, XML files with uneven distributions of density, such
as those in Dataset 1 of Group 1 (Shakespeare plays), in which
there are nodes with 0 or 1 distinct children and others with 6
distinct children, produce the highest correlation score with Test
#4 (0.439) which evaluates the density factor of Amb_Deg. Note that
when XML documents do not contain serious disparities in XML
structure (i.e., depth and density factors are almost homogeneous
across all nodes), the polysemy factor would naturally have the
highest impact in evaluating ambiguity, such as with Dataset 4 of
Group 3 (IMDB movies file) where maximum correlation is reached
with Test #2 evaluating the polysemy factor of Amb_Deg.

Note that evaluating XML node ambiguity is not addressed
in existing methods (they do not select target nodes, but rather
disambiguate all of them, which can be complex and needless).

4.3. XML semantic disambiguation quality

In addition to evaluating our XML ambiguity degree measure,
we ran a series of experiments to evaluate the effectiveness
of our XML disambiguation approach. We used the same test
datasets described previously. Target XML nodes were first subject

Fig. 15. Average f-value scores considering different features and configurations of
our approach.20

to manual disambiguation (12-to-13 nodes were randomly preselected per document yielding a total of 1000 target nodes,
allowing the same human testers to manually annotate each node
by choosing appropriate senses from WordNet, which required on
average 22 h per tester) followed by automatic disambiguation.
We then compared user and system generated senses to compute
precision, recall and f-value scores.

20 When applying the context-based (XSDContext) and combined (XSDCombined)
disambiguation algorithms, XML and semantic network sphere neighborhood
radiuses d and d were tied such that d = d, since our objective here was to evaluate
the effect of increasing/decreasing context size (mainly in the XML document),
regardless of the nature of the context itself.

4.3.1. Testing with different configurations

We first tested the effectiveness of our approach considering

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

its different features and possible configurations, considering:
(i) the properties of XML data (w.r.t. ambiguity and structure),
(ii) context size (sphere neighborhood radius), and (iii) the
disambiguation process used (concept-based, context-based, and
the combined approaches). Note that when applying the conceptbased (XSDConcept) and the combined (XSDCombined) disambiguation
algorithms, semantic similarity measures were considered with
identical parameter weights (wEdge = wNode = wGloss = 1/3 =
0.3334). Note that in this study, we do not address the issue of
assigning semantic similarity weights, which could help the user
fine-tune the latter algorithms to obtain optimal results. Such a
study would require a dedicated analysis of the relative effect of
each individual measure on concept-based disambiguation, which
is out of the scope of this paper. In the following, we show
average f-value results in Fig. 15 (precision and recall levels follow
similar patterns and are omitted for clarity). Several interesting
observations can be made here.
(1) Considering XML data properties, our approach produced
consistent f-value levels  [0.55, 0.69] over all the tested
configurations. The highest levels were reached with XML nodes
of Dataset 1 of Group 1 having high ambiguity and rich structure,
which resonates with the node ambiguity results discussed in
the previous section: highly ambiguous and structurally rich XML
nodes seem to be most effectively processed by our approach, in
comparison with less ambiguous/structurally poor nodes.
(2) Considering context size, optimal f-value levels are obtained
with the smallest sphere neighborhood radius d = 1 with Group
1 (high ambiguity and rich structure XML nodes), whereas optimal levels are obtained with larger contexts having d = 3 with
Groups 24 (low ambiguity and/or poor structure). This is expected
since increasing context size with highly ambiguous/structure rich
XML would increase the chances of including noise (e.g., unre-
lated/heterogeneous XML nodes) in the disambiguation context
and thus disrupt the process. Yet, increasing context size with less
ambiguous/poorly structured XML could actually help in creating a
large-enough and/or rich-enough context to perform effective dis-
ambiguation.

In both cases, the above results emphasize the need for a flexible
approach (such as ours), allowing the user to fine-tune context size
based on the nature and properties of the data in order to optimize
disambiguation results.

(3) Considering the disambiguation process, one can realize
that the concept-based approach (XSDConcept) generally produces
higher f-value levels in comparison with the context-based approach (XSDContext), the latter appearing to be more sensitive to
context size (especially with Groups 24). This is expected since
algorithm XSDContext primarily depends on the notion of context
and context nodes, in both the XML document and semantic net-
work, and thus increasing/decreasing context size would disturb
its effectiveness. The effect of context size here could be aggravated
when using a rich semantic network (such as WordNet) where a
small increase in sphere neighborhood radius could include a huge
number of concepts (synsets) in the semantic network context
vector, thus adding considerable noise. Yet, algorithm XSDConcept
seems less sensitive to varying context sizes since it largely focuses
on individual context nodes and their possible senses: (i) even with
a small number of context nodes (small context size), the number
of combination of possible senses would be enough to provide good
quality disambiguation, and (ii) increasing the number of context
nodes (i.e., by increasing context size) increases the number of
possible sense combinations, which does not necessarily (or dras-
tically) reduce quality since the best sense combination (i.e., the
right sense) can still be identified with reduced noise effect.

Results also show that the combined approach, using XSDCombined,
seems to provide a good compromise, emphasizing (once more)
the usefulness of having a tunable approach allowing the user to
adapt the process following her needs.

4.3.2. Comparative study

In order to further evaluate our approach, we have conducted
a comparative study to assess its effectiveness w.r.t. related XML
disambiguation methods. A qualitative comparison is shown in
Table 6. In short, XSDF: (i) considers both XML structure (tag
names) and content (element/attribute values), (ii) identifies and
selects ambiguous XML nodes as targets for disambiguation, (iii)
considers sphere neighborhood as comprehensive XML context
model including all XML structural relationships within a certain radius from the target node, (iv) allows the user to choose
context size (radius) following her needs, (v) represents sphere
neighborhoods as context vectors following a relational information model considering structural proximity between XML nodes,
(vi) introduces a hybrid approach combining two disambiguation
algorithms (XSDConcept and XSDContext), allowing the user to finetune disambiguation parameters following her needs. On the other
hand, existing approaches: (i) only consider XML structure (disre-
garding element/attribute values) [23,37,2,51], (ii) do not address
the issue of automatically selecting target nodes for disambiguation [23,37,2,51], (iii) consider XML contexts with partial coverage
of XML data such as with parent node [49,22], sub-tree [51] or root
path models [2], (iv) are static in that context size is pre-defined
[2,49,22,51] and cannot be adapted by the user, (v) represent contexts as sets of nodes using the bag-of-words paradigm [2,51],
disregarding structural proximity among nodes, (vi) use static disambiguation algorithms which cannot be easily tuned by the user
[23,37,2,51].

We have also experimentally compared our methods effectiveness with two of its most recent alternatives, i.e., RPD (Root Path
Disambiguation) [2], and VSD (Versatile Structure Disambiguation)
[23].21 We ran a battery of experiments on the same experimental
data groups (described in Section 4.1), considering the same target
XML nodes (considered in Section 4.3.1). Here, we provide a compiled presentation considering optimal input parameters for our
approach (i.e., context size d = 1 when processing Groups 1 and 4
using the concept-based disambiguation process, d = 1 when processing Group 2 using the combined approach, and d = 3 when
processing Group 3 using the combined approach),22 as well as optimal input parameters for each of the alternative disambiguation
approaches, as indicated in their corresponding studies. Results in
Fig. 16 show that our method yields precision, recall, and f-value levels higher than those achieved by its predecessors, with almost all
test cases except for two: VSD produces better results with Group 3
(Fig. 16c), and RPD produces better results with Group 4 (Fig. 16d).
Here, one can realize that XML nodes in Groups 3 and 4 are less
ambiguous and poorly structured in comparison with the first two
test groups (cf. Table 1). As a result, choosing a reduced context
made of root path nodes (with VSD) or select/reachable context
nodes (with RPD) has proven to be less noisy (including less context
nodes) in these cases, in comparison with the more comprehensive
context models used with our approach.

One can also realize that our method produces highest precision,
recall, and f-value levels with Group 1 (high ambiguity and rich
structure XML nodes), with average 44% improvement over RDP
and VSD (Fig. 16a), in comparison with average 15% with Group
2, and almost 0% improvements with Groups 3 and 4.23 This

21 RPD and VSP were developed within standalone disambiguation approaches
(including the whole pipeline from linguistic pre-processing to sense mapping)
which we could compare with our approach. In contrast, we could not compare
our method with the parent node context [49] and sub-tree context [51] approaches
since they were not developed as standalone disambiguation processes, but were
integrated within specific applications (i.e., XML semantic search and document
classification respectively).
22 Manually identified from repeated tests with different parameter values.
23 We compute the average improvement (or deterioration) of XSDF over both RPD
and VSD, for each of the test groups.

Table 5
Precision, recall, and f-value results averaged over all groups of XML documents
(Fig. 16), comparing our approach with its alternatives.

RPD [2]
VSD [23]

Precision

Recall

f-value

Fig. 16. Average precision, recall and f-value scores comparing our approach with
RPD [2] and VSD [23].

Fig. 18. Timing results regarding our context-based approach (when executing
algorithm XSDContext, cf. Fig. 10).

Fig. 17. Timing results regarding our concept-based approach (when executing
algorithm XSDConcept, cf. Fig. 9).

concurs with our results of the previous section: our method is
more effective when dealing with highly ambiguous nodes within
a rich XML structure, in comparison with less ambiguous/poorly
structured XML. Overall precision, recall, and f-value results in
Table 5 averaged over all four groups of XML test data, confirm our
methods improvement in comparison with RPD and VSD.

4.4. Performance evaluation

In addition to testing the effectiveness of our approach in
identifying correct mappings, we evaluated its efficiency in terms
of execution time.

Fig. 19.
approach [2].

Comparing time results with existing root path disambiguation

Results highlight the linear complexities of both our conceptbased approach (using algorithm XSDConcept) i.e., O(|senses(x.l)| 
|Sd(x)|  | senses (xi.l)|) (Fig. 17) and our context-based approach
(using algorithm XSDContext), i.e., O(|senses(x.l)|  (|Sd(x)|) +
|Sd(sp)|) (Fig. 18). Time is also linear w.r.t. the number of target
nodes being disambiguated, designated as |X|.
We have also compared the time complexity of our approach,
its most recent
using different configurations, with one of
predecessors. Results in Fig. 19 show closely correlated and
even reduced time results (depending on the configuration used,

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

Table 6
Comparing our method with existing approaches.

Approaches

RPD [2]
VSD [23]
XSDF (our
approach)

Considers
linguistic
pre-
processing

Considers
tag tok-
enization
(compound
terms)

Addresses
XML node
ambiguity

Integrates
an inclusive

structure
context

Flexible
w.r.t.
context size

Adopts
relational
information
approach


Combines
the results
of various
semantic
similarity
measures


Straightforward
mathematical
functions

Disambiguates
XML structure
and content


e.g., smaller context radiuses in XML document and/or in the
reference semantic network). This means that our approach was
able to produce improved disambiguation quality while preserving
(and sometimes reducing) execution time levels in comparison
with its alternatives.24

A future goal is to extend our algorithms designs and im-
plementations, using XML-based parallel processing techniques,
namely micro- and macro-level processing architectures (using
bit-level, data-level, and/or instruction-level parallelism) [66],
aiming to further increase the processing speed of our disambiguation process. A snapshot of the experimental study (along with
sample test documents and user ratings) is available online.25

5. Applications

In this section, we discuss some of the main application scenarios which can benefit, in one way or another, from XML semantic
analysis and disambiguation. Most of these applications are built
around methods for XML structure and semantic similarity evalua-
tion, e.g., [67,68,58], i.e., comparing the structural positions of XML
element/attribute nodes in the XML document tree while considering the semantic similarities between node labels and/or values. In
this context, developing semantic-aware applications usually requires three main steps:
a. XML semantic disambiguation: an initial pre-processing step to

identify the intended meanings of node labels and/or values.

b. XML similarity evaluation: comparing semantically augmented
XML trees w.r.t. the meanings of node labels/values identified
in the initial step.

c. Semantic-aware processing: an application specific step, where
semantic-aware processing is undertaken based on XML
semantic similarity evaluation.

Accordingly, in this section, we present an overview of such
applications which we gradually look at from different angles,
starting from (i) the layer of abstraction at which XML similarity
is evaluated, and then describing high-end application domains
covering: (iii) Information Retrieval, (iv) Web and Mobile Services,
and (v) the (Social) Semantic Web.

5.1. XML similarity at different abstraction layers

XML semantic similarity evaluation can take place at three
different abstraction levels: (i) the data layer (i.e., document/
document comparison), (i) the type layer (i.e., document/grammar
comparison), and (ii) in-between the data and type layers [69],
each underlining its own battery of semantic-aware applications.

24 Note that we did not compare execution time with the Versatile Structure
Disambiguation approach [37] since we were unable to acquire the system
implementation from the authors. We used the authors online version of the
prototype, which is relatively slow due to network access, and thus could not use it
to evaluate processing time.
25 http://sigappfr.acm.org/Projects/XSDF/.

5.1.1. Similarity at the XML data layer

Performing XML document/document comparison, is relevant
in a variety of applications (cf. reviews in) [67,58], such as data
versioning, monitoring, and temporal querying: a user may want to
view or access a version of a certain document (e.g., an XHTML Web
site, a Web Service SOAP description, an RSS feed, etc.) which was
available during a certain period of time, or may want to view the
results of a continuous query, or monitor the evolution of a certain
document in time. Such tasks can be implemented using semanticaware tree edit distance similarity measures which produce, along
with the similarity score, an edit script consisting of a set of edit
operations describing semantic changes to the disambiguated data
(e.g., inserting/deleting/updating semantically related nodes, to
transform one XML document tree into another) [58]. Another
application is document clustering, i.e., grouping XML documents
together, based on their structural and semantic similarities,
which can improve data storage indexing [70] and thus positively
affect the data retrieval process [67]. Also, clustering is central
is information extraction, wrapping, and summarization, allowing
to automatically identify the sets of semantically similar XML
elements to be extracted from documents to be reformulated
(e.g., substituting disambiguated node labels with semantically
related ones), restructured, or summarized, making them easier
to process in enterprise applications (e.g., adapting/simplifying the
content of a Web page, blog, RSS feed, or Web Service description
for non-experts) [71,3].

5.1.2. Similarity at the XML type layer

Performing XML grammar/grammar comparison, is also useful for many tasks (cf. reviews in) [7,72], namely data integra-
tion, which consists in: (i) comparing (matching) grammars to
identify semantically related elements [8], and then (ii) merging the matched elements within a unified grammar or semantic view [73]. Here, a disambiguation step is necessary to
capture the meaning of grammar elements prior to performing
grammar matching. Data integration allows the user to efficiently
access and acquire more complete information (e.g., accessing similar Websites, blogs, or RSS feeds concurrently) [74]. It is also
essential in performing data warehousing,26 where XML information is transformed from different data-sources complying with
different grammars into data conforming with grammars defined
in the data warehouse [75]. Other applications include message
translation in Business-to-Business (B2B) integration [76]: reconciling the semantics of XML message grammars used by trading
partners in order to translate in-coming and out-going messages
accordingly, which is essential in E-commerce and B2B applications [77]; and XML data maintenance and schema evolution: detecting the structural and semantic differences/updates between
different versions of a given grammar to revalidate corresponding
XML documents [78].

26 A warehouse is a decision support database that is extracted from a set of data
sources (e.g., different databases describing related data).

5.1.3. Similarity between XML data and type layers

Performing XML document/grammar comparison, can also
benefit from XML disambiguation applied on the documents and
grammars being compared, highlighting various applications (cf.
review in) [68]. One such application is XML document classification,
i.e., categorizing XML documents gathered from the Web against a
set of grammars declared in an XML repository. Here, evaluating
semantic similarity between incoming disambiguated documents
on one hand, and reference disambiguated grammars on the other
hand (e.g., defined in the repository), allows the identification
of entities that are conceptually close, but not syntactically
identical, which is common in handling heterogeneous XML
sources, particularly on the Web where users have different
backgrounds and no precise definitions about the matter of
discourse [57]. Evaluating semantic similarity between documents
and grammars can also be exploited in XML document retrieval
via structural queries [79]: a query being represented as an XML
grammar with additional constraints on content; as well as in the
selective dissemination of information:user profiles being expressed
as grammars against which the incoming XML document stream is
matched [80].

5.2. Information retrieval

Information Retrieval (IR) is one of the leading application
domains requiring sophisticated semantic-aware and similaritybased processing where systems aim at providing the most relevant (similar) documents w.r.t. a user information need expressed
as a search query. In this context, a wide range of techniques extending traditional IR systems to handle XML IR have been designed (cf. reviews in) [81,58]. In brief, XML IR systems accept
as input: (i) a user query: expressed as an XML document [82],
an XML fragment [83], an XML structured query (e.g., XPath or
XQuery [84]), or as a set of keywords [85], and (ii) an indexed
XML document repository [86], and produce as output: a ranked
list of XML elements (and their sub-trees)27 selected form the
repository, and ordered following their relevance (similarity) w.r.t.
the user query [22]. Hence, the quality of an XML IR engine depends on two key issues: (i) how documents and queries are represented (indexed), and (ii) how these representations are compared
(matched) to produce relevant results. Here, most solutions in the
literature have explored syntactic XML indexing paradigms (based
on node positions, paths, or structural summaries) integrated in
dedicated inverted indexing structures, e.g., [86,87].

Yet, as XML data on the Web became increasingly available
and diverse, element/attribute labels and values became noisier,
such that syntactic indexing techniques could not keep pace [112].
As a result, non-expert users have been increasingly faced with
what is described as the vocabulary problem [89]: query keywords
chosen by users are often different from those used by the authors
of the relevant documents, lowering the systems precision and
recall rates. This highlights the need for XML disambiguation,
where both the XML query and documents can be processed and
represented using semantic concepts, instead of (or in addition
to) syntactic keywords and element names/values (e.g., typical
XML indexing techniques can be used, except that element
names/values would be replaced with semantic concepts) [90]
then, query/document matching can be performed in the semantic
concept space, instead of performing syntactic keyword/node label
matching, thus extending XML IR toward semantic XML IR, or socalled concept-based XML IR [88]. Preliminary studies on semantic

XML IR have shown that representing documents and queries using
semantic concepts usually results in a retrieval model that is more
effective and less dependent on the specific terms/node labels
used, significantly increasing search precision and recall rates [91].
Note that semantic XML IR, and ontological (RDF/OWL28) IR, is
presently a hot research topic [81,91].

5.3. Web and mobile services

Another interesting application area which requires XML
semantic disambiguation is the matching, search, and composition
of Web Services (WS). A WS comes down to a self-contained,
modular application that can be described, published, and invoked
over the Internet, and executed on the remote system where it is
hosted [92]. WS rely on two standard XML schemata: (i) WSDL
(Web Service Description Language) [93] allowing the definition
of XML grammar structures to support the machine-readable
description of a services interface and the operations it supports,
and (ii) SOAP (Simple Object Access Protocol) [94] for XMLbased communications and message exchange among WS end-
points. RESTful services have been recently promoted as a simpler
alternative to SOAP and WSDL-based WS [95]: communicating
over HTTP using HTTP request methods (e.g., Get, Post, Put, etc.,
instead of exchanging SOAP messages), and using XHTML or free
test to describe the services (instead of WSDL) [96].

Hence, when searching for WS (or RESTful services) achieving specific functions, XML (or XHTML/keyword) based service
requests can be issued, to which are then matched and ranked
service WSDL (or XHTML/keyword) descriptions, thus identifying
those services answering the desired requests. Here, matching and
ranking service descriptions requires effective XML semantic analysis and disambiguation techniques, due to service author/user
heterogeneity (same as the vocabulary problem in XML IR, cf.
Section 5.2). The same applies for services discovery, recom-
mendation, and composition: searching and mapping together
semantically similar WSDL/SOAP descriptions when processing
WS, and performing semantic-aware mapping of XHTML/keyword
descriptions when dealing with RESTful and/or mobile services
[10,11]. XML similarity and differential encoding can also be utilized to reduce processing costs and improve the performance
of SOAP communications [97]: comparing new SOAP messages
with message patterns or WSDL grammars (at the message
sender/receiver side), processing and transmitting only those parts
of the messages which are different (cf. review in) [66].

5.4. Semantic web and social semantic web

Above all, the Semantic Web vision [98] benefits from most of
the above-mentioned applications, as it inherently requires XML
disambiguation to deal with the semantics of Web documents (en-
coded in XML-based format), to improve interoperability between
systems, ontologies, and users. Typically, XML disambiguation can
be utilized in ontology learning, to build domain taxonomies [99]
and enrich/update large-scale semantic networks [100], based on
user input data streams (e.g., Web pages, blogs, image annotations,
etc.) encoded in XML. In this context, technologies such as RDF (Re-
source Description Framework) [101]), and OWL (Web Ontology
Language) [102] can be used to construct such ontologies.

RDF enables the definition of statements specifying relationships between instances of data in the form of < subject , predicate,
object > triples, which designate that a resource (i.e., the sub-
ject) has a property (i.e., the predicate) whose value is a resource

27 Selecting a whole XML document as a potential answer comes down to selecting
its root node (along with the corresponding sub-tree).

28 Refer to Section 5.4.

J. Tekli et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 124

or a literal (i.e., the object). OWL is built on top of RDF and adds
additional expressiveness which depends on the type of Description Logic (DL) language applied (OWL allows different levels of
semantic expressiveness, ranging from OWL-Lite, to OWL-DL and
OWL-Full [104]). As a result, RDF and OWL highlight the concept
of Linked Data: the seamless connection of pieces of information
and knowledge on the Semantic Web [105], where a given resource
(i.e., subject) can be associated with new properties (i.e., objects)
via new relationships (i.e., predicates), and where additional statements (i.e., triples) can be easily added to describe resources and
properties [104]. In this context, XML disambiguation is essential:
to extract the semantic information form XML data so that it can
be utilized or integrated with semantic annotations from: (i) reference ontologies, (ii) previously annotated (disambiguated) XML
documents, or (iii) user generated annotations (e.g., social tagging).
Practical examples include integrating hotel and airline reserva-
tions, order processing, and insurance renewal with social networking information [106]. Also, augmenting Web data (in XML)
with semantic annotations (i.e., triples) provides a way of blending
traditional information with Linked Data and Semantic Web constructs [107].

An emerging trend in this context is the integration of user information (e.g., user annotations, hash-tags, search queries, and
selected search results), i.e., so-called social semantics [91], to semantically augment Web (XML-based) data. This underlines the
concept of the Social Semantic Web [71,91], a Web in which social interactions lead to the creation of collective and collaborative
knowledge representations such as Wikipedia, Yahoo Answers,
and Flikr, providing semantic information based on human contributions and paving the way for various new applications ranging over: (i) blog classification, e.g., introducing simple and effective
methods to semantically classify blogs [108], (ii) social semantic
network analysis, e.g., disambiguating entities in social networks,
and identifying semantic relationships between users [13], and (iii)
socio-semantic information retrieval, e.g., considering user information to improve/adapt Web data indexing, query formulation, result ranking, and result presentation techniques [109] (cf. reviews
in [110,91]).

6. Conclusion

This paper introduces a novel XML Semantic Disambiguation
Framework titled XSDF, to semantically annotate XML documents with the help of machine-readable lexical knowledge base
(e.g., WordNet), which is a central pre-requisite to various applications ranging over semantic-aware query rewriting [4,5],
XML document classification and clustering [3,6], XML schema
matching [7,8], Web and mobile services discovery, recommen-
dation, and composition [911], and blog analysis and event
detection in social networks [12,13]. XSDF covers the whole
disambiguation pipeline from: (i) linguistic pre-processing of XML
node labels to handle compound words (neglected in most existing solutions), to (ii) selecting ambiguous nodes for disambiguation using a dedicated ambiguity degree measure (unaddressed in
most solutions), (iii) representing target node contexts as comprehensive and flexible (user chosen) sphere neighborhood vectors (in
contrast with partial and fixed context representations, e.g., parent
node or sub-tree context), and (iv) running a hybrid disambiguation process, combining two (user chosen) methods: concept-based
and context-based (in contrast with static methods). Experimental
results reflect our approachs effectiveness in selecting ambiguous
XML nodes and identifying node label senses w.r.t. user judgments
of semantic ambiguity. Comparative theoretical and experimental analyses highlight our approachs effectiveness in comparison
with existing methods. Time analysis underlines the linear impact

of context size and polysemy (number of senses) among other fac-
tors, on disambiguation time.

As continuing work, we are currently investigating different
XML tree node distance functions (including edge weights, den-
sity, direction, and the semantic similarity between parent/child
nodes, etc. [63,64]), to define more sophisticated neighborhood
contexts. Fine-tuning user parameters using dedicated optimization techniques [5961] is another work in progress. We are
also investigating the issue of semantic indexing [111], aiming
to put forward a dedicated XML indexing approach (based on
structural indexing [112,113], or extended vector-space representations [83,82]), in order to allow effective and efficient XML
semantic search and retrieval. We are also investigating the
use of additional/alternative lexical knowledge sources such as
Google [114], Wikipedia [115], and FOAF [12] to acquire a wider
word sense coverage, and thus explore our approach in practical applications, namely wiki document clustering [116], semantic blog analysis, and event detection in heterogeneous and collective social data [117]. In the near future, we aim to explore nontraditional processor architectures, including XML-based microand macro-level parallel processing solutions [66], to increase the
processing speed of our disambiguation process. Comparing parallel and incremental disambiguation strategies would naturally fol-
low.
