Semantic Web 0 (2014) 117
IOS Press

WYSIWYM  Integrated Visualization,
Exploration and Authoring of Semantically
Enriched Un-structured Content

Editor(s): Roberto Garcia, Universitat de Lleida, Spain; Heiko Paulheim, University of Mannheim, Germany; Paola Di Maio, Universal
Interfaces Research Lab, ISTCS, Edinburgh, UK
Solicited review(s): Heiko Hornung, University of Campinas, Brazil; Haofen Wang, Shanghai Jiao Tong University, China; one anonymous
reviewer

Ali Khalili a, Soren Auer b
a AKSW, Universitat Leipzig, Augustusplatz 10, 04109 Leipzig, Germany
khalili@informatik.uni-leipzig.de
b CS/EIS, Universitat Bonn, Romerstrae 164, 53117 Bonn, Germany
auer@cs.uni-bonn.de

Abstract. The Semantic Web and Linked Data gained traction in the last years. However, the majority of information still is
contained in unstructured documents. This can also not be expected to change, since text, images and videos are the natural
way how humans interact with information. Semantic structuring on the other hand enables the (semi-)automatic integration,
repurposing, rearrangement of information. NLP technologies and formalisms for the integrated representation of unstructured
and semantic content (such as RDFa and Microdata) aim at bridging this semantic gap. However, in order for humans to truly
benefit from this integration, we need ways to author, visualize and explore unstructured and semantically enriched content in
an integrated manner. In this paper, we present the WYSIWYM (What You See is What You Mean) concept, which addresses
this issue and formalizes the binding between semantic representation models and UI elements for authoring, visualizing and
exploration. With RDFaCE, Pharmer and conTEXT we present and evaluate three complementary showcases implementing the
WYSIWYM concept for different application domains.

Keywords: Visualization, Authoring, Exploration, Semantic Web, WYSIWYM, WYSIWYG, Visual Mapping

1. Introduction

The Semantic Web and Linked Data movements
with the aim of creating, publishing and interconnecting machine readable information have gained traction
in the last years. However, the majority of information still is contained in and exchanged using unstructured documents, such as Web pages, text documents,
images and videos. This can also not be expected to
change, since text, images and videos are the natural
way how humans interact with information. Semantic
structuring on the other hand provides a wide range
of advantages compared to unstructured information.

It facilitates a number of important aspects of information management:

 For search and retrieval enriching documents
with semantic representations helps to create
more efficient and effective search interfaces,
such as faceted search [33] or question answering [20].

 In information presentation semantically enriched
documents can be used to create more sophisticated ways of flexibly visualizing information,
such as by means of semantic overlays as described in [2].

1570-0844/14/$27.50 c 2014  IOS Press and the authors. All rights reserved

Khalili et al. / WYSIWYM

 For information integration semantically enriched
documents can be used to provide unified views
on heterogeneous data stored in different applications by creating composite applications such as
semantic mashups [1].

 To realize personalization, semantically enriched

documents provide customized and context-specific
information which better fits user needs and will
result in delivering customized applications such
as personalized semantic portals [29].

 For reusability and interoperability enriching
documents with semantic representations facilitates exchanging content between disparate systems and enables building applications such as
executable papers [24].

Natural Language Processing (NLP) technologies
(e.g. named entity recognition and relationship extrac-
tion) as well as formalisms for the integrated representation of unstructured and semantic content (such as
RDFa and Microdata) aim at bridging the semantic gap
between unstructured and semantic representation for-
malisms. However, in order for humans to truly benefit from this integration, we need ways to author, visualize and explore unstructured and semantically enriched content in an integrated manner.

In this paper, we present an approach inspired by the
WYSIWYM metaphor (What You See Is What You
Mean), which addresses the issue of an integrated vi-
sualization, exploration and authoring of semantically
enriched un-structured content. Our WYSIWYM concept formalizes the binding between semantic representation models and UI elements for authoring, visualizing and exploration. We analyse popular tree, graph
and hyper-graph based semantic representation models and elicit a list of semantic representation elements,
such as entities, various relationships and attributes.
We provide a comprehensive survey of common UI
elements for authoring, visualizing and exploration,
which can be configured and bound to individual semantic representation elements. Our WYSIWYM concept also comprises cross-cutting helper components,
which can be employed within a concrete WYSIWYM
interface for the purpose of automation, annotation,
recommendation, personalization etc.

With RDFaCE, Pharmer and conTEXT we present
and evaluate three complementary showcases implementing the WYSIWYM concept for different do-
mains. RDFaCE is a domain agnostic editor for text
content with embedded semantic in the form of RDFa

or Microdata. Pharmer is a WYSIWYM interface for
the authoring of semantic prescriptions and thus targeting the medical domain. conTEXT is a Linked-Data
based lightweight text anlaytics platform supporting
different views for semantic analytics. Our evaluation
of these tools with end-users (in case of RDFaCE and
conTEXT) and domain experts (in case of Pharmer)
shows, that WYSIWYM interfaces provide good us-
ability, while retaining benefits of a truly semantic rep-
resentation.

The contributions of this work are in particular:
1. A formalization of the WYSIWYM concept
based on definitions for the WYSIWYM model,
binding and concrete interfaces.

2. A survey of semantic representation elements of
tree, graph and hyper-graph knowledge representation formalisms as well as UI elements for au-
thoring, visualization and exploration of such el-
ements.

3. Three complementary use cases, which evaluate
different, concrete WYSIWYM interfaces in a
generic as well as domain specific context.

The WYSIWYM formalization can be used as a basis for implementations; allows to evaluate and classify
existing user interfaces in a defined way; provides a
terminology for software engineers, user interface and
domain experts to communicate efficiently and effec-
tively. We aim to contribute with this work to making Semantic Web applications more user friendly and
ultimately to create an ecosystem of flexible UI com-
ponents, which can be reused, repurposed and choreographed to accommodate the UI needs of dynamically evolving information structures.

The remainder of this article is structured as follows:
In Section 2, we describe the background of our work
and discuss the related work. Section 3 describes the
fundamental WYSIWYM concept proposed in the pa-
per. Subsections of Section 3 present the different components of the WYSIWYM model. In Section 4, we introduce three implemented WYSIWYM interfaces together with their evaluation results. Finally, Section 5
concludes with an outlook on future work.

2. Related Work

WYSIWYG. The term WYSIWYG as an acronym for
What-You-See-Is-What-You-Get is used in computing
to describe a system in which content (text and graph-

ics) displayed on-screen during editing appears in a
form closely corresponding to its appearance when
printed or displayed as a finished product. The first
usage of the term goes back to 1974 in the print industry to express the idea that what the user sees on
the screen is what the user gets on the printer. Xerox PARCs Bravo was the first WYSIWYG editorformatter [23]. It was designed by Butler Lampson and
Charles Simonyi who had started working on these
concepts around 1970 while at Berkeley. Later on by
the emergence of Web and HTML technology, the
WYSIWYG concept was also utilized in Web-based
text editors. The aim was to reduce the effort required
by users to express the formatting directly as valid
HTML markup. In a WYSIWYG editor users can edit
content in a view which matches the final appearance of published content with respect to fonts, head-
ings, layout, lists, tables, images and structure. Because using a WYSIWYG editor may not require any
HTML knowledge, they are often easier for an average computer user to get started with. The first programs for building Web pages with a WYSIWYG interface were Netscape Gold, Claris HomePage, and
Adobe PageMill.

WYSIWYG text authoring is meanwhile ubiquitous
on the Web and part of most content creation and
management workflows. It is part of content management cystems (CMS), weblogs, wikis, fora, product
data management systems and online shops, just to
mention a few. However, the WYSIWYG model has
been criticized, primarily for the verbosity, poor support of semantics and low quality of the generated
code and there have been voices advocating a change
towards a WYSIWYM (What-You-See-Is-What-You-
Mean) model [32,30].
WYSIWYM. The first use of the WYSIWYM term
occurred in 1995 aiming to capture the separation of
presentation and content when writing a document.
The LyX editor1 was the first WYSIWYM word processor for structure-based content authoring. Instead
of focusing on the format or presentation of the doc-
ument, a WYSIWYM editor preserves the intended
meaning of each element. For example, page head-
ers, sections, paragraphs, etc. are labeled as such in
the editing program, and displayed appropriately in the
browser. Another usage of the WYSIWYM term was
by Power et al. [28] in 1998 as a solution for Symbolic Authoring. In symbolic authoring the author gen-

1http://www.lyx.org/

erates language-neutral symbolic" representations of
the content of a document, from which documents in
each target language are generated automatically, using Natural Language Generation technology. In this
What-You-See-Is-What-You-Meant approach, the language generator was used to drive the user interface
(UI) with support of localization and multilinguality.
Using the WYSIWYM natural language generation
approach, the system generates a feed-back text for the
user that is based on a semantic representation. This
representation can be edited directly by the user by manipulating the feedback text.

The WYSIWYM term as defined and used in this
paper targets the novel aspect of integrated visualiza-
tion, exploration and authoring of unstructured and semantic content. The rationale of our WYSIWYM concept is to enrich the existing WYSIWYG presentational view of the content with UI components revealing the semantics embedded in the content and enable
the exploration and authoring of semantic content. Instead of separating presentation, content and meaning,
our WYSIWYM approach aims to integrate these aspects to facilitate the process of Semantic Content Au-
thoring. Two Yous in our WYSIWYM concept refer to the end user (with no or limited knowledge of
Semantic Web ) who is viewing an unstructured content which is semantically enriched by himself. The
Mean refers to the metadata or semantics which is
encoded in the unstructured content viewed by user.
There are already some approaches (i.e. visual mapping techniques), which go into the direction of integrated visualization and authoring of structured con-
tent.
Visual Mapping Techniques. Visual mapping techniques are knowledge representation techniques that
graphically represent knowledge structures. Most of
them have been developed as paper-based techniques
for brainstorming, learning facilitation, outlining or to
elicit knowledge structures. According to their basic
topology, most of them can be related to the following
fundamentally different primary approaches [5,31]:

 Mind-Maps. Mind-maps are created by drawing
one central topic in the middle together with labeled branches and sub-branches emerging from
it. Instead of distinct nodes and links, mind-maps
only have labeled branches. A mind-map is a connected directed acyclic graph with hierarchy as
its only type of relation. Outlines are a similar
technique to show hierarchical relationships using tree structure. Mind-maps and outlines are not

Khalili et al. / WYSIWYM

suitable for relational structures because they are
constrained to the hierarchical model.

 Concept Maps. Concept maps consist of labeled
nodes and labeled edges linking all nodes to a
connected directed graph. The basic node and link
structure of a connected directed labeled graph
also forms the basis of many other modeling approaches like Entity-Relationship (ER) diagrams
and Semantic Networks. These forms have the
same basic structure as concept maps but with
more formal types of nodes and links.

 Spatial Hypertext. A spatial hypertext is a set
of text nodes that are not explicitly connected
but implicitly related through their spatial layout,
e.g., through closeness and adjacency  similar
to a pin-board. Spatial hypertext can show fuzzily
related items. To fuzzily relate two items in a spatial hypertext schema, they are simply placed near
to each other, but possibly not quite as near as to a
third object. This allows for so-called constructive ambiguity and is an intuitive way to deal
with vague relations and orders. Spatial Hypertext abandons the concept of explicitly interrelating objects. Instead, it uses spatial positioning as
the basic structure.

Binding data to UI elements. There are already many
approaches and tools which address the binding between data and UI elements for visualizing and exploring structured content. Dadzie and Rowe [3] present
the most exhaustive and comprehensive survey to date
of these approaches. For example, Fresnel [27] is a display vocabulary for core RDF concepts. Fresnels two
foundational concepts are lenses and formats. Lenses
define which properties of an RDF resource, or group
of related resources, are displayed and how those properties are ordered. Formats determine how resources
and properties are rendered and provide hooks to existing styling languages such as CSS.
Parallax, Tabulator, Explorator, Rhizomer, Sgvizler,
Fenfire, RDF-Gravity, IsaViz and i-Disc for Topic
Maps are examples of tools available for visualizing
and exploring structured data. In these tools the binding between semantics and UI elements is mostly performed implicitly, which limits their versatility. How-
ever, an explicit binding as advocated by our WYSIWYM model can be potentially added to some of these
tools.

In contrast to the structured content, there are many
approaches and tools which allow binding semantic
data to UI elements within semantically enriched un-

Fig. 1. Schematic view of the WYSIWYM model.

structured content (cf. our comprehensive literature
study [11]). As an example, Dido [10] is a datainteractive document which lets end users author semantic content mixed with unstructured content in a
web-page. Dido inherits data exploration capabilities
from the underlying Exhibit2 framework. Loomp as
a prove-of-concept for the One Click Annotation [6]
strategy is another example in this context. Loomp is
a WYSIWYG web editor for enriching content with
RDFa annotations. It employs a partial mapping between UI elements and data to hide the complexity of
creating semantic data.

3. WYSIWYM Concept

In this section we introduce the fundamental WYSIWYM concept and formalize key elements of the con-
cept. Formalizing the WYSIWYM concept has a number of advantages: First, the formalization can be used
as a basis for design and implementation of novel applications for authoring, visualization, and exploration
of semantic content (cf. Section 4). The formalization
serves the purpose of providing a terminology for software engineers and UI designers to communicate efficiently and effectively. It provides insights into and
an understanding of the requirements as well as corresponding UI solutions for proper design and implementation of semantic content management applica-
tions. Secondly, it allows to evaluate and classify existing user interfaces according to the conceptual model
in a defined way. This will highlight the gaps in existing applications dealing with semantically enriched
documents and will help to optimize them based on the
defined requirements.

2http://simile-widgets.org/exhibit/

Semantic Representation Data ModelsVisualizationTechniquesExplorationTechniquesAuthoringTechniquesHelper componentsBindingsConfigsConfigsConfigsWYSIWYM InterfaceKhalili et al. / WYSIWYM

user interface technique ui (ui  V  X  T ) and c is
a configuration c  Cui.

Figure 4 gives an overview on all data model
(columns) and UI elements (rows) and how they can
be bound together using a certain configuration (cells).
The shades of gray in a certain cell indicate the suitability of a certain binding between a particular UI and
data model element.

For example, having tree-based semantic representation model, framing and segmentation UI techniques
can be used as external augmentation to visualize the
items in the text. It is also possible to use text formatting techniques as inline augmentation for highlighitng
the items but since they might interfere with the current text format, we assume a partial binding for them.
A possible configuration for this example binding is to
set different border and text colors to distinguish different item types.

Once a selection of data models and UI elements
was made and both are bound to each other encoding a
certain configuration in a binding, we attain a concrete
instantiation of our WYSIWYM model called WYSIWYM interface.
Definition 3 (WYSIWYM interface). An instantiation
of the WYSIWYM model I called WYSIWYM interface
now is a hextuple (DI , VI , XI , TI , HI , bI ), where:

 DI is a selection of semantic representation data

 VI is a selection of visualization techniques

 XI is a selection of exploration techniques

Figure 1 provides a schematic overview of the
WYSIWYM concept. The rationale is that elements of
a knowledge representation formalism (or data model)
are connected to suitable UI elements for visualiza-
tion, exploration and authoring. Formalizing this conceptual model results in three core definitions (1) for
the abstract WYSIWYM model, (2) bindings between
UI and representation elements as well as (3) a concrete instantiation of the abstract WYSIWYM model,
which we call a WYSIWYM interface.
Definition 1 (WYSIWYM model). The WYSIWYM
model can be formally defined as a quintuple (D, V, X, T, H)
where:

 D is a set of semantic representation data models,
where each Di  D has an associated set of data
model elements EDi;

 V is a set of tuples (v, Cv), where v is a visualization technique and Cv a set of possible configurations for the visualization technique v;

 X is a set of tuples (x, Cx), where x is an exploration technique and Cx a set of possible configurations for the exploration technique x;

 T is a set of tuples (t, Ct), where t is an authoring
technique and Ct a set of possible configurations
for the authoring technique t;

 H is a set of helper components.
Semantic representation data models are techniques
to define the meaning of data within the context of
its interrelationships with other data (cf. Section 3.1).
Tree, Graph and Hypergraph are examples of commonly used data models. Visualization techniques include UI techniques for highlighting, associating and
detail viewing of semantic entities (cf. Section 3.2).
Exploration techniques include UI techniques for efficient browsing and navigating semantic data (cf.
Section 3.3). Authoring techniques include UI techniques for adding and editing semantic entities and
their relations (cf. Section 3.4). Helper components
are cross-cutting aspects to enhance and customize the
user/application requirements of a WYSIWYM interface (cf. Section 3.6).

The WYSIWYM model represents an abstract concept from which concrete interfaces can be derived
by means of bindings between semantic representation
model elements and configurations of particular UI el-
ements.
Definition 2 (Binding). A binding b is a function which
maps each element of a semantic representation model
e (e  EDi) to a set of tuples (ui, c), where ui is a

models (DI  D);
(VI  V );
(XI  X);
(TI  T );
(HI  H);

 TI is a selection of authoring techniques

 HI is a selection of helper components

 bI is a binding which binds a particular occurrence of a data model element to a visualization,
exploration and/or authoring technique.

Note, that we limit the definition to one binding,
which means that only one semantic representation
model is supported in a particular WYSIWYM interface at a time. It could be also possible to support several semantic representation models (e.g. RDFa and
Microdata) at the same time. However, this can be confusing to the user, which is why we deliberately excluded this case in our definition. In the remainder
of this sections we discuss the different parts of the
WYSIWYM concept in more detail.

Khalili et al. / WYSIWYM

 E3: Item-subitem relationships  e.g. Magnoliidae

referring to subitem magnolias.
 E4: Item property value  e.g.

the synonym

flowering plant for the item Magnoliidae.
 E5: Related items  e.g. the sibling item Eudicots

to Magnoliidae.

Tree-based data can be serialized as Microdata or

Microformats.

Graph-based. This semantic representation model
adds more expressiveness compared to simple treebased formalisms. The most prominent representative
is the RDF data model, which can be seen as a set of
triples consisting of subject, predicate, object, where
each component can be a URI, the object can be a
literal and subject as well as object can be a blank
node. The most distinguishing features of RDF from
a simple tree-based model are: the distinction of entities in classes and instances as well as the possibility
to express arbitrary relationships between entities. The
graph-based model is suited for representing combinatorial schemes such as concept maps. Graph-based
models are used in a very broad range of domains,
for example, in the FOAF (Friend of a Friend) for describing people, their interests and interconnections in
a social network, in MusicBrainz to publish information about music albums, in the medical domain (e.g.
DrugBank, Diseasome, ChEMBL, SIDER) to describe
the relations between diseases, drugs and genes, or
generically in the SIOC (Semantically-Interlinked Online Communities) vocabulary. Elements of RDF as a
typical graph-based data model are:

 E1: Instances  e.g. Warfarin as a drug.
 E2: Classes  e.g. anticoagulants drug

for Warfarin.

 E3: Relationships between entities (instances
or classes)  e.g. the interaction between
Aspirin as an antiplatelet drug and Warfarin
which will increase the risk of bleeding.

 E4: Literal property values  e.g. the halflife for

the Amoxicillin.
 E41: Value  e.g. 61.3 minutes.
 E42: Language tag  e.g. en.
 E43: Datatype  e.g. xsd:float.

RDF-based data can be serialized in various for-

mats, such as RDFa, RDF/XML, JSON-LD or Turtle/N3/N-
Triples.

Fig. 2. Comparison of existing visual mapping techniques in terms
of semantic expressiveness and complexity of visual mapping.

3.1. Semantic Representation Models

Semantic representation models are conceptual data
models to express the meaning of information thereby
enabling representation and interchange of knowledge.
Based on their expressiveness, we can roughly divide
popular semantic representation models into the three
categories tree-based, graph-based and hypergraphbased (cf. Figure 2). Each semantic representation
model comprises a number of representation elements,
such as various types of entities and relationships.
For visualization, exploration and authoring it is of
paramount importance to bind the most suitable UI elements to respective representation elements. In the sequel we briefly discuss the three different types of representation models.
Tree-based. This is the simplest semantic representation model, where semantics is encoded in a treelike structure. It is suited for representing taxonomic
knowledge, such as thesauri, classification schemes,
subject heading lists, concept hierarchies or mind-
maps. It is used extensively in biology and life sci-
ences, for example, in the APG III system (Angiosperm
Phylogeny Group III system) of flowering plant classi-
fication, as part of the Dimensions of the XBRL (eXten-
sible Business Reporting Language) or generically in
the SKOS (Simple Knowledge Organization System).
Elements of tree-based semantic representation usually include:

 E1: Item  e.g. Magnoliidae, the item repre-

senting all flowering plants.

 E2: Item type  e.g. biological term for

Magnoliidae.

Semantic expressiveness Complexity of visual mapping low high high low Mind-maps Concept maps Spatial hypertext Tree-based Graph-based Hypergraph-based Microdata Microformat Topic Maps        XTM        LTM        CTM        AsTMa RDF   RDF/XML   Turtle/N3/N-Triples   RDFa   JSON-LD  Khalili et al. / WYSIWYM

Hypergraph-based. A hypergraph is a generalization
of a graph in which an edge can connect any number of vertices. Since hypergraph-based models allow n-ary relationships between arbitrary number of
nodes, they provide a higher level of expressiveness
compared to tree-based and graph-based models. The
most prominent representative is the Topic Maps data
model developed as an ISO/IEC standard which consists of topics, associations and occurrences. The semantic expressivity of Topic Maps is, in many ways,
equivalent to that of RDF, but the major differences are
that Topic Maps (i) provide a higher level of semantic abstraction (providing a template of topics, associations and occurrences, while RDF only provides a template of two arguments linked by one relationship) and
(hence) (ii) allow n-ary relationships (hypergraphs) between any number of nodes, while RDF is limited to
triplets. The hypergraph-based model is suited for representing complex schemes such as spatial hypertext.
Hypergraph-based models are used for a variety of ap-
plications. Amongst them are musicDNA3 as an index
of musicians, composers, performers, bands, artists,
producers, their music, and the events that link them
together, TM4L (Topic Maps for e-Learning), clinical decision support systems and enterprise information integration. Elements of Topic Maps as a typical
hypergraph-based data model are:

 E1: Topic name  e.g. University of Leipzig.
 E2: Topic type  e.g. organization for Uni-

versity of Leipzig.

 E3: Topic associations  e.g. member of a
project which has other organization partners.
 E4: Topic role in association e.g. coordinator.
 E5: Topic occurrences  e.g. address.

 E51: value  e.g. Augustusplatz 10,
 E52: datatype  e.g. text.

04109 Leipzig.

Topic Maps-based data can be serialized as an
XML-based syntax called XTM (XML Topic Map),
LTM (Linear Topic Map Notation), CTM (Compact
Topic Maps Notation) and AsTMa (Asymptotic Topic
Map Notation).

3.2. Visualization

The primary objectives of visualization are to present,
transform, and convert semantic data into a visual rep-

3http://www.musicdna.info/

resentation, so that, humans can read, query and edit
them efficiently. We divide existing techniques for visualization of knowledge encoded in text, images and
videos into the three categories Highlighting, Associating and Detail view. Highlighting includes UI techniques which are used to distinguish or highlight a part
of an object (i.e. text, image or video) from the whole
object. Associating deals with techniques that visualize the relation between some parts of an object. Detail
view includes techniques which reveal detailed information about a part of an object. For each of the above
categories, the related UI techniques are as follows:
- Highlighting.

 V1: Framing and Segmentation (borders, overlays and backgrounds). This technique can be applied to text, images and videos, we enclose a semantic entity in a coloured border, background or
overlay. Different border styles (colours, width,
types), background styles (colours, patterns) or
overlay styles (when applied to images and videos)
can be used to distinguish different types of semantic entities (cf. Figure 3 no. 1, 2). The technique is already employed in social networking
websites such as Google Plus and Facebook to
tag people within images.

 V2: Text formatting (color, font, size, margin,
etc.). In this technique different text styles such
as font family, style, weight, size, colour, shad-
ows, margin and other text decoration techniques
are used to distinguish semantic entities within a
text (cf. Figure 3 no. 6). The problem with this
technique is that in an HTML document, the applied semantic styles might overlap with existing
styles in the document and thereby add ambiguity
to recognizing semantic entities.

 V3: Image color effects. This technique is similar to text formatting but applied to images and
videos. Different
image color effects such as
brightness/contrast, shadows, glows, bevel/emboss
are used to highlight semantic entities within an
image (cf. Figure 3 no. 7). This technique suffers from the problem that the applied effects
might overlap with the existing effects in the image thereby making it hard to distinguish the semantic entities.

 V4: Marking (icons appended to text or image). In
this technique, which can be applied to text, images and videos, we append an icon as a marker
to the part of object which includes the semantic
entity (cf. Figure 3 no. 9). The most popular use

Khalili et al. / WYSIWYM

Fig. 3. Screenshots of user interface techniques for visualization and exploration: 1-framing using borders, 2-framing using backgrounds, 3-video
subtitle, 4-line connectors and arrow connectors, 5-bar layouts, 6-text formatting, 7-image color effects, framing and line connectors, 8-expand-
able callout, 9-marking with icons, 10-tooltip callout, 11-faceting

of this technique is currently within maps to indicate specific points of interest. Different types of
icons can be used to distinguish different types of
semantic or correlated entities.

 V5: Bleeping. A bleep is a single short highpitched signal in videos. Bleeping can be used to
highlight semantic entities within a video. Different type of bleep signals can be defined to distinguish different types of semantic entities.

 V6: Speech (in videos). In this technique a video
is augmented by some speech indicating the semantic entities and their types within the video.

- Associating.

 V7: Line connectors. Using line connectors is the
simplest way to visualize the relation between semantic entities in text, images and videos (cf. Figure 3 no. 4). If the value of a property is available in the text, line connectors can also reflect
the item property values. Problematic is that normal line connectors can not express the direction
of a relation.

 V8: Arrow connectors. Arrow connectors are extended line connectors with arrows to express the
direction of a relation in a directed graph.

Besides the line and arrow connectors techniques
which explicitly visualize the association between en-
tities, implicit techniques defined as Gestalt principles [9] can be used for modeling association. These
techniques are psychological assumptions that impose
structure for human visual perception. Principles such
as proximity, similarity, continuity, closure, symmetry,
figure/ground and common fate can be used to affect
our perception of whether and how the objects are organized into groups. Discussing these principles are
out of the scope of this paper.
- Detail view.

 V9: Callouts. A callout is a string of text connected by a line, arrow, or similar graphic to a part
of text, image or video giving information about
that part. It is used in conjunction with a cursor,
usually a pointer. The user hovers the pointer over
an item, without clicking it, and a callout appears
(cf. Figure 3 no. 10). Callouts come in different

styles and templates such as infotips, tooltips, hint
and popups. Different sort of metadata can be embedded in a callout to indicate the type of semantic entities, property values and relationships. Another variant of callouts is the status bar which
displays metadata in a bar appended to the text,
image or video container. A problem with dynamic callouts is that they do not appear on mobile devices (by hover), since there is no cursor.
 V10: Video subtitles. Subtitles are textual versions
of the dialog or commentary in videos. They are
usually displayed at the bottom of the screen and
are employed for written translation of a dialog
in a foreign language. Video subtitles can be used
to reflect detailed semantics embedded in a video
scene when watching the video. A problem with
subtitles is efficiently scaling the text size and relating text to semantic entities when several semantic entities exist in a scene.

3.3. Exploration

To increase the effectiveness of visualizations, users
need to be capable to dynamically navigate and explore the visual representation of the semantic data.
The dynamic exploration of semantic data will result
in faster and easier comprehension of the targeted con-
tent. Techniques for exploration of semantics encoded
in text, images and videos include:

 X1: Zooming. In a zoomable UI, users can change
the scale of the viewed area in order to see more
detail or less. The zooming elements and techniques vary on different applications. Zooming in
a semantic entity can reveal further details such
as property value or entity type. Zooming out can
be employed to reveal the relations between semantic entities in a text, image or video. Supporting rich dynamics by configuring different visual
representations for semantic objects at different
sizes is a requirement for a zoomable UI. The
iMapping approach[5] which is implemented in
the semantic desktop is an example of the zooming technique.

 X2: Faceting. Faceted browsing is a technique
for accessing information organized according to
a faceted classification system, allowing users to
explore a collection of information by applying
multiple filters (cf. Figure 3 no. 11). Defining
facets for each component of the predefined semantic models enable users to browse the under-

lying knowledge space by iteratively narrowing
the scope of their quest in a predetermined order.
One of the main problems with faceted browsers
is the increased number of choices presented to
the user at each step of the exploration [4].

 X3: On-demand highlighting. Unlike the highlighting approach discussed in the visualization
methods, on-demand highlighting is used to navigate the semantic entities encoded in text in a
dynamic manner. One technique to realize ondemand highlighting is Bar layout. In the bar lay-
out, each semantic entity within the text is indicated by a vertical bar in the left or right margin (cf. Figure 3 no. 5). The colour of the bar reflects the type of the entity. The bars are ordered
by length and order in the text. Nested bars can be
used to show the hierarchies of entities. Semantic entities in the text are highlighted by a mouseover the corresponding bar. This approach is employed in Loomp [21].

 X4: Expanding & Drilling down. Expandable
callouts are interactive and dynamic callouts
which enable users to explore the semantic data
associated to a predefined semantic entity (cf.
Figure 3 no. 8). Drilling down in a callout enables users to move from summary information
to detailed data by focusing in on entities. This
technique is employed in OntosFeeder [16].

3.4. Authoring

Semantic authoring aims to add more meaning to
digitally published documents. If users do not only
publish the content, but at the same time describe what
it is they are publishing, then they have to adopt a
structured approach to authoring. A semantic authoring UI is a human accessible interface with capabilities for writing and modifying semantically enriched
documents. The following techniques can be used for
authoring of semantics encoded in text, images and
videos:

 T1: Form editing. In form editing, a user employs
existing form elements such as input/check/radio
boxes, drop-down menu, slider, spinner, buttons,
date/color picker etc. for content authoring.

 T2: Inline edit. Inline editing is the process of
editing items directly in the view by performing
simple clicks, rather than selecting items and then
navigating to an edit form and submitting changes
from there.

Khalili et al. / WYSIWYM

 T3: Drawing. Drawing as part of informal user
interfaces [19], provides a natural human input
to annotate an object by augmenting the object with human-understandable sketches. For in-
stance, users can draw a frame around semantic
entities, draw a line between related entities etc.
Special shapes can be drawn to indicate different
entity types or entity roles in a relation.

 T4: Drag and drop. Drag and drop is a pointing
device gesture in which the user selects a virtual
object by grabbing it and dragging it to a different
location or onto another virtual object. In general,
it can be used to invoke many kinds of actions, or
create various types of associations between two
abstract objects.

 T5: Context menu. A context menu (also called
contextual, shortcut, or pop-up menu) is a menu
that appears upon user interaction, such as a right
button mouse click. A context menu offers a limited set of choices that are available in the current
state, or context.

 T6: (Floating) Ribbon editing. A ribbon is a command bar that organizes functions into a series of
tabs or toolbars at the top of the editable content.
Ribbon tabs/toolbars are composed of groups,
which are a labeled set of closely related com-
mands. A floating ribbon is a ribbon that appears
when user rolls the mouse over a target area.
A floating ribbon increases usability by bringing
edit functions as close as possible to the users
point of focus. The Aloha WYSIWYG editor4 is an
example of floating ribbon based content author-
ing.

 T7: Voice commands. Voice commands permit the
users hands and eyes to be busy with another
task, which is particularly valuable when users
are in motion or outside. Users tend to prefer
speech for functions like describing objects, sets
and subsets of objects [26]. By adding special signals to input voice, users can author semantic content from the scratch.

 T8: (Multi-touch) gestures. A gesture is a form
of non-verbal communication in which visible
bodily actions communicate particular messages.
Technically, different methods can be used for
detecting and identifying gestures. Movement-
sensor-based and camera-based approaches are
two commonly used methods for the recognition

4http://aloha-editor.org

of in-air gestures [22]. Multi-touch gestures are
another type of gestures which are defined to interact with multi-touch devices such as modern
smartphones and tablets. Users can use gestures
to determine semantic entities, their types and relationship among them. The main problem with
gestures is their high level of abstraction which
makes it hard to assert concrete property values.
Special gestures can be defined to author semantic entities in text, images and videos.

3.5. Bindings

Figure 4 surveys possible bindings between the user

interface and semantic representation elements.

The bindings were derived based on the following

methodology:

1. We first analyzed existing semantic representation models and extracted the corresponding elements for each semantic model.

2. We performed an extensive literature study regarding existing approaches for visual mapping
as well as approaches addressing the binding between data and UI elements. If the approach was
explicitly mentioning the binding composed of
UI elements and semantic model elements, we
added the binding to our mapping table.

3. We analyzed existing tools and applications
which were implicitly addressing the binding between data and UI elements.

4. Finally, we followed a predictive approach. We
investigated additional UI elements which are
listed in existing HCI glossaries and carefully analyzed their potential to be connected to a semantic model element.

Although we deem the bindings to be fairly complete,
new UI elements might be developed or additional data
models (or variations of the ones considered) might ap-
pear, in this case the bindings can be easily extended.
Partial binding indicates the situation when a UI
technique does not completely cover a semantic model
element but still can be used in particular cases. For
example, different text colors can be used to highlight
predefined item types in text but since the colors might
interfere with the current colors in the text (in case
of HTML document), we assign this binding as partial binding. Another example are the line connectors
used to represent the relation between items in a tree or
graph-based model. In this case, on the contrary to arrow connectors, since we cannot determine the source

and destination of the line, we are unable to model directional relations completely, thereby, a partial binding is assigned.

The asterisks in Figure 4, indicate the cases when
the metadata value is explicitly available in the text
and the user just needs to provide the connection (e.g.
imagine that we have Berlin and Germany mentioned in the text and we want to assign the relation
isCapitalOf).

The following binding configurations (extracted
from the literature and current tools) are available and
referred to from the cells of Figure 4:

 Defining a special border or background style
(C1), text style (C2), image color effect (C4),
beep sound (C5), bar style (C6), sketch (C7),
draggable or droppable shape (C8), voice command (C9), gesture (C10) or a related icon (C3)
for each type.

 Progressive shading (C11) by defining continuous
shades within a specific color scheme to distinguish items in different levels of the hierarchy.

 Hierarchical bars (C12) by defining special styles

for nested bars.

 Grouping by similar border or background style
(C13), text style (C14), icons (C15) or image color
effects (C16).

For example, a user can define a set of preferred border colors to distinguish different item types (e.g. Per-
sons, Organizations or Locations) or to group related
items (e.g. all the cities in Germany).

3.6. Helper Components

In order to facilitate, enhance and customize the
WYSIWYM model, we utilize a set of helper compo-
nents, which implement cross-cutting aspects.

A helper component acts as an extension on top
of the core functionality of the WYSIWYM model.
The following components can be used to improve the
quality of a WYSIWYM UI depending on the requirements defined for a specific application domain:

 H1: Automation means the provision of facilities for automatic annotation of text, images
and videos to reduce the need for human work
and thereby facilitating the efficient annotation
of large item collections. For example, users can
employ existing NLP services (e.g. named entity recognition, relationship extraction) for automatic text annotation.

 H2: Real-time tagging is an extension of automa-
tion, which allows to create annotations proactively while the user is authoring a text, image or
video. This will significantly increase the annotation speed and users are not distracted since they
do not have to interrupt their current authoring
task.

 H3: Recommendation means providing users
with pre-filled form fields, suggestions (e.g. for
URIs, namespaces, properties), default values etc.
These facilities simplify the authoring process,
as they reduce the number of required user inter-
actions. Moreover, they help preventing incomplete or empty metadata. In order to leverage
other users annotations as recommendations, approaches like Paragraph Fingerprinting [8] can
be implemented.

 H4: Personalization and context-awareness describes the ability of the UI to be configured according to users contexts, background knowledge and preferences. Instead of being static, a
personalized UI dynamically tailors its visual-
ization, exploration and authoring functionalities
based on the user profile and context.

 H5: Collaboration and crowdsourcing enables
collaborative semantic authoring, where the authoring process can be shared among different authors at different locations. There are a
vast amounts of amateur and expert users which
are collaborating and contributing on the Social
Web. Crowdsourcing harnesses the power of such
crowds to significantly enhance and widen the
results of semantic content authoring and anno-
tation. Generic approaches for exploiting singleuser Web applications for shared editing [7] can
be employed in this context.

 H6: Accessibility means providing people with
disabilities and special needs with appropriate
UIs. The underlying semantic model in a WYSIWYM UI can allow alternatives or conditional
content
in different modalities to be selected
based on the type of the user disability and information need.

 H7: Multilinguality means supporting multiple
languages in a WYSIWYM UI when visualizing,
exploring or authoring the content.

Khalili et al. / WYSIWYM

Fig. 4. Possible bindings between user interface and semantic representation model elements.

* If value is available in the text/subtitle.No binding           Partial binding             Full bindingTree-based(e.g. Taxonomies)Graph-based(e.g. RDF)Hypergraph-based(e.g. Topic Maps)ItemItem typeItem-subitemItem property valueRelated ItemsInstanceClassRelationships between entitiesLiteral property valuesTopicTopic typeTopic associationsTopic role in associationTopic OccurrencesValueLanguage tagDatatypeValueDatatypeStructure encoded in: UI categoriesUI techniquesVisualizationtextHighlightingFraming and segmentation (borders, overlays,backgrounds)C1C11C13C1C1Text formatting (color, font, size etc.)C2C11C14C2C2Marking (appended icons)C3C15C3C3AssociatingLine connectors***Arrow connectors***DetailviewCallouts (infotips, tooltips, popups)imagesHighlightingFraming and segmentation (borders, overlays, backgrounds)C1C11C13C1C1Image color effectsC4C11C16C4C4Marking (appended icons)C3C15C3C3AssociatingLine connectorsArrow connectorsDetailviewCallouts (infotips, tooltips, popups)videosHighlightingFraming and segmentation (borders, overlays,backgrounds)C1C11C13C1C1Image color effectsC4C11C16C4C4Marking (appended icons)C3C15C3C3BleepingC5C5C5SpeechAssociatingLine connectors***Arrow connectors***DetailviewCallouts (infotips, tooltips, popups)SubtitleExplorationtextZoomingFacetingOn-demand highlightingC5C12C5C5Expanding & Drilling down imagesZoomingFacetingvideosFaceting (excerpts)Authoringtext, images, videosForm editingInline editDrawingC7C7C7C7Drag and dropC8C8C8C8Context menu(Floating)Ribbon editingVoice commandsC9C9C9C9(Multi-Touch)GesturesC10C10C10C10Khalili et al. / WYSIWYM

4. Implementation and Evaluation

In order to evaluate the WYSIWYM model, we implemented the three applications RDFaCE, Pharmer
and conTEXT, which we present in the sequel.

RDFaCE. RDFaCE (RDFa Content Editor) [13] is
a WYSIWYM interface for semantic content author-
ing. It is implemented on top of the TinyMCE rich
text editor. RDFaCE extends the existing WYSIWYG
user interfaces to facilitate semantic authoring within
popular CMSs, such as blogs, wikis and discussion
forums. The RDFaCE implementation (cf. Figure 5,
left) is open-source and available for download together with an explanatory video and online demo
at http://aksw.org/Projects/RDFaCE. RDFaCE as a WYSIWYM instantiation can be described
using the following hextuple:

 D: RDFa, Microdata5.
 V: Framing using borders (C: special border color
defined for each type), Callouts using dynamic
tooltips.

 E: Faceting based on the type of entities.
 T: Form editing, Context Menu, Ribbon editing.
 H: Automation, Recommendation.
 b: bindings defined in Figure 4.
RDFaCE comes with a special edition [12] customized for Schema.org vocabulary. In this ver-
sion, different color schemes are assigned to different
schemas defined in Schema.org. Users are able to create a subset of Schema.org schemas for their intended
domain and customize the colors for this subset. In this
version, nested forms are dynamically generated from
the selected schemas for authoring and editing of the
annotations.

In order to evaluate RDFaCE usability, we conducted an experiment with 16 participants of the ISSLOD 2011 summer school6. The user evaluation comprised the following steps: First, some basic information about semantic content authoring along with a
demo showcasing different RDFaCE features was presented to the participants as a 3 minutes video. Then,
participants were asked to use RDFaCE to annotate
three text snippets  a wiki article, a blog post and a
news article. For each text snippet, a timeslot of five

5Microdata support is implemented in RDFaCE-Lite available at

http://rdface.aksw.org/lite

6Summer

school on Linked Data: http://lod2.eu/

Article/ISSLOD2011

Usability Factor/Grade Poor Fair Neutral Good Excellent
0% 12.50% 31.25% 43.75% 12.50%
Fit for use
0% 12.50% 50% 31.25% 6.25%
Ease of learning
0% 56.25% 37.50% 6.25%
0%
Task efficiency
0% 37.50% 50% 12.50%
0%
Ease of remembering
Subjective satisfaction
0% 18.75% 50%
25% 6.25%
6.25% 18.75% 31.25% 37.50% 6.25%
Understandability

Table 1

Usability evaluation results for RDFaCE.

Fig. 6. Usability evaluation results for Pharmer (0: Strongly dis-
agree, 1: Disagree, 2: Neutral, 3: Agree, 4: Strongly agree).

minutes was available to use different features of RDFaCE for annotating occurrences of persons, locations
and organizations with suitable entity references. Sub-
sequently, a survey was presented to the participants
where they were asked questions about their experience while working with RDFaCE. Questions were targeting six factors of usability [17,25] namely Fit for
use, Ease of learning, Task efficiency, Ease of remem-
bering, Subjective satisfaction and Understandability.
Results of the survey are shown in Table 1. They indicate on average good to excellent usability for RD-
FaCE. A majority of the users deem RDFaCE being fit
for use and its functionality easy to remember. Also,
easy of learning and subjective satisfaction was well
rated by the participants. There was a slightly lower
(but still above average) assessment of task efficiency
and understandability, which we attribute to the short
time participants had for familiarizing themselves with
RDFaCE and the quite comprehensive functionality,
which includes automatic annotations, recommendations and various WYSIWYM UI elements.
Pharmer. Pharmer [15] is a WYSIWYM interface
for the authoring of semantically enriched electronic
prescriptions. It enables physicians to embed drugrelated metadata into e-prescriptions thereby reducing the medical errors occurring in the prescriptions
and increasing the awareness of the patients about

Khalili et al. / WYSIWYM

Fig. 5. Screenshots of our implemented WYSIWYM interfaces. Left: RDFaCE for semantic text authoring (T6 indicates the RDFaCE menu bar,
V1  the framing of named entities in the text, V9  a callout showing additional type information, T5  a context menu for revising annotations).
Right: Pharmer for authoring of semantic prescriptions (V1  highlighting of drugs through framing, V9  additional information about a drug
in a callout, T1/T2 combined form and inline editing of electronic prescriptions).

the prescribed drugs and drug consumption in gen-
eral. In contrast to database-oriented e-prescriptions,
semantic prescriptions are easily exchangeable among
other e-health systems without need to changing their
related infrastructure. The Pharmer implementation
(cf. Figure 5, right) is open-source and available for
download together with an explanatory video and online demo7 at http://code.google.com/p/
pharmer/. It is based on HTML5 contenteditable el-
ement. Pharmer as a WYSIWYM instantiation is defined using the following hextuple:

 D: RDFa.
 V: Framing using borders and background (C:
special background color defined for each type),
Callouts using dynamic popups.

 E: Faceting based on the type of entities.
 T: Form editing, Inline edit.
 H: Automation, Real-time tagging, Recommen-

dation.

 b: bindings defined in Figure 4.
In order to evaluate the usability of Pharmer, we performed a user study with 13 subjects. Subjects were 3
physicians, 4 pharmacist, 3 pharmaceutical researchers
and 3 students. We first showed them a 3-minute tutorial video of using different features of Pharmer then
asked each one to create a semantic prescription with
Pharmer. After finishing the task, we asked the participants to fill out a questionnaire. We used the System
Usability Scale (SUS) [18] as a standardized, simple,
ten item Likert scale-based questionnaire to grade the
usability of Pharmer. SUS yields a single number in the

7http://bitili.com/pharmer

range of 0 to 100 which represents a composite measure of the overall usability of the system. The results
of our survey (cf. Figure 6) showed a mean usability
score of 75 for Pharmer WYSIWYM interface which
indicates a good level of usability. Participants particularly liked the integration of functionality and the ease
of learning and use. The confidence in using the system was slightly lower, which we again attribute to the
short learning phase and diverse functionality.

conTEXT.
conTEXT [14] is a WYSIWYM interface
which allows users to semantically analyze text corpora (such as blogs, RSS/Atom feeds, Facebook, G+,
Twitter) and provides novel ways for browsing and
visualizing the results. It helps non-programmer Web
users to use sophisticated NLP techniques for text analytics and to give feedback to the NLP services for improving their quality for named entity recognition. The
conTEXT implementation (cf. Figure 7) together with
an explanatory video and online demo is available at
http://context.aksw.org.

conTEXT as a WYSIWYM instantiation is defined

using the following hextuple:

 D: RDFa.
 V: Framing using borders and background (C:
special background color defined for each type),
Callouts using dynamic popups, Text margin format for hierarchies, Line collectors for entity re-
lations.

 E: Faceting based on the type of entities.
 T: Form editing, Inline edit.
 H: Recommendation.
 b: bindings defined in Figure 4.

Fig. 7. Screenshots of the conTEXT WYSIWYM interfaces (T2 indicates the inline editing UI, V1  the framing of named entities in the text, V2
 text margin formatting for visualizing hierarchy, V7  line connectors to show the relation between entities, V9  a callout showing additional
type information, X2  faceted browsing, H3  recommendation for NLP feedback).

10 questions pertaining to knowledge discovery in corpora of unstructured data. Similar to Pharmer evalua-
tion, we used the SUS questionnaire to grade the usability of conTEXT. The results of our survey (cf. Figure 8) showed a mean usability score of 82 for conTEXT WYSIWYM interface which indicates a good
level of usability. The responses to question 1 suggests
that our system is adequate for frequent use by users.
While a small fraction of the functionality is deemed
unnecessary by some users, the users deem the system easy to use. Only one user suggested that he/she
would need a technical person to use the system, while
all other users were fine without one. The modules of
the system in itself were deemed to be well integrated.
Overall, the output of the system seems to be easy to
understand while users even without training assume
themselves capable of using the system.

5. Conclusions

Bridging the gap between unstructured and semantic content is a crucial aspect for the ultimate success of semantic technologies. With the WYSIWYM
concept we presented in this article an approach for
integrated visualization, exploration and authoring of
unstructured and semantic content. The WYSIWYM
model binds elements of a knowledge representation
formalism (or data model) to a set of suitable UI elements for visualization, exploration and authoring.
Based on such a declarative binding mechanism, we

Fig. 8. Usability evaluation results for conTEXT (0: Strongly dis-
agree, 1: Disagree, 2: Neutral, 3: Agree, 4: Strongly agree).

Faceted browsing view in conTEXT clearly shows
the concept of integrated unstructured and structured
view in a WYSIWYM interface. Users see unstructured text enriched with highlighted entities and detail
description of the entities. In addition to that, different
facets (e.g. entity type tree) allow users to filter out the
text by their preferences. Users can also use inline editing within unstructured text to refine the annotations
and to send feedback to NLP services.

In order to evaluate the usability of conTEXT, we
performed a user study with 25 subjects (20 PhD
students having different backgrounds from computer
software to life sciences, 2 MSc students and 3 BSc
students with good command of English) on a set of

Khalili et al. / WYSIWYM

aim to increase the flexibility, reusability and development efficiency of semantics-rich user interfaces

We deem this work as a first step in a larger research
agenda aiming at improving the usability of semantic user interfaces, while retaining semantic richness
and expressivity. In future work we envision to adopt a
model-driven approach to enable automatic implementation of WYSIWYM interfaces by user-defined pref-
erences. This will help to reuse, re-purpose and choreograph WYSIWYM UI elements to accommodate the
needs of dynamically evolving information structures
and ubiquitous interfaces. We also aim to bootstrap an
ecosystem of WYSIWYM instances and UI elements
to support structure encoded in different modalities,
such as images and videos. Creating live and contextsensitive WYSIWYM interfaces which can be generated on-the-fly based on the ranking of available UI
elements is another promising research venue.

Acknowledgments

We would like to thank our colleagues from the
AKSW research group for their helpful comments during the development of the WYSIWYM model. This
work was partially supported by a grant from the European Unions 7th Framework Programme provided for
the project LOD2 (GA no. 257943).
