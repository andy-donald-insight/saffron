Semantic Web 1 (2012) 15
IOS Press

DBpedia  A Large-scale, Multilingual
Knowledge Base Extracted from Wikipedia

Editor(s): Name Surname, University, Country
Solicited review(s): Name Surname, University, Country
Open review(s): Name Surname, University, Country

Jens Lehmann a,, Robert Isele g, Max Jakob e, Anja Jentzsch d, Dimitris Kontokostas a,
Pablo N. Mendes f, Sebastian Hellmann a, Mohamed Morsey a, Patrick van Kleef c, S oren Auer a,
Christian Bizer b
a University of Leipzig, Institute of Computer Science, AKSW Group, Augustusplatz 10, D-04009 Leipzig, Germany
E-mail: {lastname}@informatik.uni-leipzig.de
b University of Mannheim, Research Group Data and Web Science, B6-26, D-68159 Mannheim
E-mail: chris@informatik.uni-mannheim.de
c OpenLink Software, 10 Burlington Mall Road, Suite 265, Burlington, MA 01803, U.S.A.
E-mail: pkleef@openlinksw.com
d Hasso-Plattner-Institute for IT-Systems Engineering, Prof.-Dr.- Helmert-Str. 2-3, D-14482 Potsdam, Germany
E-mail: mail@anjajentzsch.de
e Neofonie GmbH, Robert-Koch-Platz 4, D-10115 Berlin, Germany
E-mail: max.jakob@neofonie.de
f Kno.e.sis - Ohio Center of Excellence in Knowledge-enabled Computing, Wright State University, Dayton, USA.
E-Mail: pablo@knoesis.org
g Brox IT-Solutions GmbH, An der Breiten Wiese 9, D-30625 Hannover, Germany
E-Mail: mail@robertisele.com

Abstract. The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely
available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different
language editions of Wikipedia. The largest DBpedia knowledge base which is extracted from the English edition of Wikipedia
consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other
110 Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps
Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties.
The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the different Wikipedia editions to
be combined. The project publishes regular releases of all DBpedia knowledge bases for download and provides SPARQL query
access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases,
the project maintains a live knowledge base which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million
RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia
data. Several hundred data sets on the Web publish RDF links pointing to DBpedia themselves and thus make DBpedia one of
the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia
community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and
applications.

Keywords: Knowledge Extraction, Wikipedia, Multilingual Knowledge Bases, Linked Data, RDF

1570-0844/12/$27.50 c 2012  IOS Press and the authors. All rights reserved

1. Introduction

Lehmann et al. / DBpedia

Wikipedia is the 6th most popular website1, the most
widely used encyclopedia, and one of the finest examples of truly collaboratively created content. There are
official Wikipedia editions in 287 different languages
which range in size from a couple of hundred articles
up to 3.8 million articles (English edition)2. Besides of
free text, Wikipedia articles consist of different types of
structured data such as infoboxes, tables, lists, and categorization data. Wikipedia currently offers only freetext search capabilities to its users. Using Wikipedia
search, it is thus very difficult to find all rivers that flow
into the Rhine and are longer than 100 miles, or all
Italian composers that were born in the 18th century.

The DBpedia project builds a large-scale, multilingual knowledge base by extracting structured data from
Wikipedia editions in 111 languages. This knowledge
base can be used to answer expressive queries such as
the ones outlined above. Being multilingual and covering an wide range of topics, the DBpedia knowledge
base is also useful within further application domains
such as data integration, named entity recognition, topic
detection, and document ranking.

The DBpedia knowledge base is widely used as a
testbed in the research community and numerous appli-
cations, algorithms and tools have been built around or
applied to DBpedia. DBpedia is served as Linked Data
on the Web. Since it covers a wide variety of topics
and sets RDF links pointing into various external data
sources, many Linked Data publishers have decided
to set RDF links pointing to DBpedia from their data
sets. Thus, DBpedia has developed into a central interlinking hub in the Web of Linked Data and has been
a key factor for the success of the Linked Open Data
initiative.

The structure of the DBpedia knowledge base is
maintained by the DBpedia user community. Most
importantly, the community creates mappings from
Wikipedia information representation structures to the
DBpedia ontology. This ontology  which will be explained in detail in Section 3  unifies different template structures, both within single Wikipedia language
editions and across currently 27 different languages.

*Corresponding author. E-mail:

lehmannn@informatik.uni-

leipzig.de.

1http://www.alexa.com/topsites. Retrieved in Octo-

ber 2013.

2http://meta.wikimedia.org/wiki/List_of_

Wikipedias

The maintenance of different language editions of DBpedia is spread across a number of organisations. Each
organisation is responsible for the support of a certain
language. The local DBpedia chapters are coordinated
by the DBpedia Internationalisation Committee.

The aim of this system report is to provide a description of the DBpedia community project, including the
architecture of the DBpedia extraction framework, its
technical implementation, maintenance, internationali-
sation, usage statistics as well as presenting some popular DBpedia applications. This system report is a comprehensive update and extension of previous project descriptions in [2] and [6]. The main advances compared
to these articles are:

 The concept and implementation of the extraction
based on a community-curated DBpedia ontology.

 The wide internationalisation of DBpedia.
 A live synchronisation module which processes
updates in Wikipedia as well as the DBpedia ontology and allows third parties to keep their copies
of DBpedia up-to-date.

 A description of the maintenance of public DBpe-

dia services and statistics about their usage.

 An increased number of interlinked data sets
which can be used to further enrich the content of
DBpedia.

 The discussion and summary of novel third party

applications of DBpedia.

Overall, DBpedia has undergone 7 years of continuous evolution. Table 17 provides an overview of the
projects timeline.

The system report is structured as follows: In the next
section, we describe the DBpedia extraction framework,
which forms the technical core of DBpedia. This is
followed by an explanation of the community-curated
DBpedia ontology with a focus on multilingual sup-
port. In Section 4, we explicate how DBpedia is synchronised with Wikipedia with just very short delays
and how updates are propagated to DBpedia mirrors
employing the DBpedia Live system. Subsequently, we
give an overview of the external data sets that are interlinked from DBpedia or that set RDF links pointing
to DBpedia themselves (Section 5). In Section 6, we
provide statistics on the usage of DBpedia and describe
the maintenance of a large scale public data set. Within
Section 7, we briefly describe several use cases and
applications of DBpedia in a variety of different areas.
Finally, we report on related work in Section 8 and give
an outlook on the further development of DBpedia in
Section 9.

Fig. 1. Overview of DBpedia extraction framework.

2. Extraction Framework

Wikipedia articles consist mostly of free text, but
also comprise of various types of structured information
in the form of wiki markup. Such information includes
infobox templates, categorisation information, images,
geo-coordinates, links to external web pages, disambiguation pages, redirects between pages, and links
across different language editions of Wikipedia. The
DBpedia extraction framework extracts this structured
information from Wikipedia and turns it into a rich
knowledge base. In this section, we give an overview
of the DBpedia knowledge extraction framework.

2.1. General Architecture

Figure 1 shows an overview of the technical frame-
work. The DBpedia extraction is structured into four
phases:
Input: Wikipedia pages are read from an external
source. Pages can either be read from a Wikipedia
dump or directly fetched from a MediaWiki installation using the MediaWiki API.

Parsing: Each Wikipedia page is parsed by the wiki
parser. The wiki parser transforms the source code
of a Wikipedia page into an Abstract Syntax Tree.
Extraction: The Abstract Syntax Tree of each Wikipedia
page is forwarded to the extractors. DBpedia offers extractors for many different purposes, for in-
stance, to extract labels, abstracts or geographical
coordinates. Each extractor consumes an Abstract
Syntax Tree and yields a set of RDF statements.

Output: The collected RDF statements are written to
a sink. Different formats, such as N-Triples, are
supported.

2.2. Extractors

The DBpedia extraction framework employs various
extractors for translating different parts of Wikipedia
pages to RDF statements. A list of all available extractors is shown in Table 1. DBpedia extractors can be
divided into four categories:
Mapping-Based Infobox Extraction: The mappingbased infobox extraction uses manually written
mappings that relate infoboxes in Wikipedia to
terms in the DBpedia ontology. The mappings also
specify a datatype for each infobox property and
thus help the extraction framework to produce high
quality data. The mapping-based extraction will
be described in detail in Section 2.4.

Raw Infobox Extraction: The raw infobox extraction
provides a direct mapping from infoboxes in
Wikipedia to RDF. As the raw infobox extraction
does not rely on explicit extraction knowledge in
the form of mappings, the quality of the extracted
data is lower. The raw infobox data is useful if a
specific infobox has not been mapped yet and thus
is not available in the mapping-based extraction.
Feature Extraction: The feature extraction uses a
number of extractors that are specialized in extracting a single feature from an article, such as a
label or geographic coordinates.

Lehmann et al. / DBpedia

Statistical Extraction: Some NLP related extractors
aggregate data from all Wikipedia pages in order to
provide data that is based on statistical measures
of page links or word counts, as further described
in Section 2.6.

2.3. Raw Infobox Extraction

The type of Wikipedia content that is most valuable
for the DBpedia extraction are infoboxes. Infoboxes are
frequently used to list an articles most relevant facts
as a table of attribute-value pairs on the top right-hand
side of the Wikipedia page (for right-to-left languages
on the top left-hand side). Infoboxes that appear in a
Wikipedia article are based on a template that specifies
a list of attributes that can form the infobox. A wide
range of infobox templates are used in Wikipedia. Common examples are templates for infoboxes that describe
persons, organisations or automobiles. As Wikipedias
infobox template system has evolved over time, different communities of Wikipedia editors use different templates to describe the same type of things (e.g.
Infobox city japan, Infobox swiss town
and Infobox town de). In addition, different templates use different names for the same attribute
(e.g. birthplace and placeofbirth). As many
Wikipedia editors do not strictly follow the recommendations given on the page that describes a template,
attribute values are expressed using a wide range of
different formats and units of measurement. An excerpt
of an infobox that is based on a template for describing
automobiles is shown below:

= Ford GT40

{{Infobox automobile
| name
| manufacturer = [[Ford Advanced Vehicles]]
| production
| engine
(...)
}}

= 1964-1969
= 4181cc

In this infobox, the first line specifies the infobox
type and the subsequent lines specify various attributes
of the described entity.

An excerpt of the extracted data is as follows:

dbr:Ford_GT40

dbp:name "Ford GT40"@en;
dbp:manufacturer dbr:Ford_Advanced_Vehicles;
dbp:engine 4181;
dbp:production 1964;
(...).

This extraction output has weaknesses: The resource
is not associated to a class in the ontology and parsed

values are cleaned up and assigned a datatyper based
on heuristics. In particular, the raw infobox extractor
searches for values in the following order: dates, coor-
dinates, numbers, links and strings as default. Thus, the
datatype assignment for the same property in different
resources is non deterministic. The engine for example is extracted as a number but if another instance of
the template used cc4181 it would be extracted as
string. This behaviour makes querying for properties in
the dbp namespace inconsistent. Those problems can
be overcome by the mapping-based infobox extraction
presented in the next subsection.

2.4. Mapping-Based Infobox Extraction

In order to homogenize the description of information in the knowledge base, in 2010 a community effort was initiated to develop an ontology schema and
mappings from Wikipedia infobox properties to this
ontology. The alignment between Wikipedia infoboxes
and the ontology is performed via community-provided
mappings that help to normalize name variations in
properties and classes. Heterogeneity in the Wikipedia
infobox system, like using different infoboxes for the
same type of entity or using different property names for
the same property (cf. Section 2.3), can be alleviated in
this way. This significantly increases the quality of the
raw Wikipedia infobox data by typing resources, merging name variations and assigning specific datatypes to
the values.

This effort is realized using the DBpedia Mappings
Wiki3, a MediaWiki installation set up to enable users
to collaboratively create and edit mappings. These mappings are specified using the DBpedia Mapping Lan-
guage. The mapping language makes use of MediaWiki
templates that define DBpedia ontology classes and
properties as well as template/table to ontology map-
pings. A mapping assigns a type from the DBpedia ontology to the entities that are described by the corresponding infobox. In addition, attributes in the infobox
are mapped to properties in the DBpedia ontology. In
the following, we show a mapping that maps infoboxes
that use the Infobox automobile template to the
DBpedia ontology:

{{TemplateMapping
|mapToClass = Automobile
|mappings =

{{PropertyMapping
| templateProperty = name

3http://mappings.dbpedia.org

Table 1

Overview of the DBpedia extractors (cf. Table 16 for a complete list
of prefixes.).

Description

Example

image of a

lines of the

Extracts the first
Wikipedia article.
Extracts the categorization of the
article.
Extracts labels for categories.
Extracts information about which
concept is a category and how categories are related using the SKOS
Vocabulary.
Extracts disambiguation links.
Extracts links to external web
pages related to the concept.
Extracts geo-coordinates.
Extracts grammatical genders for
persons.
Extracts links to the official homepage of an instance.
Extracts the first
Wikipedia page.
Extracts all properties from all in-
foboxes.
Extracts interwiki links.
Extracts the article title as label.
Extracts information about surface
forms and their association with
concepts (only N-Quad format).
Extraction based on mappings of
Wikipedia infoboxes to the DBpedia ontology.
Extracts page ids of articles.
Extracts
Wikipedia articles.
Extracts information about persons represented using the PersonData template.
Extracts PND (Personennamen-
datei) data about a person.
Extracts redirect links between articles in Wikipedia.
Extracts the revision ID of the
Wikipedia article.
Extracts thematic concepts, the
centres of discussion for cate-
gories.
Extracts topic signatures.
Extracts links to corresponding articles in Wikipedia.

all

links

between

dbr:Berlin dbo:abstract "Berlin is the capital city of
(...)" .
dbr:Oliver Twist dc:subject dbr:Category:English novels .

dbr:Category:English novels rdfs:label "English novels" .
dbr:Category:World War II skos:broader
dbr:Category:Modern history .

dbr:Alien dbo:wikiPageDisambiguates dbr:Alien (film) .
dbr:Animal Farm dbo:wikiPageExternalLink
<http://books.google.com/?id=RBGmrDnBs8UC> .
dbr:Berlin georss:point "52.5006 13.3989" .
dbr:Abraham Lincoln foaf:gender "male" .

dbr:Alabama foaf:homepage <http://alabama.gov/> .

dbr:Berlin foaf:depiction <http://.../Overview Berlin.jpg> .

dbr:Animal Farm dbo:date "March 2010" .

dbr:Albedo dbo:wikiPageInterLanguageLink dbr-de:Albedo .
dbr:Berlin rdfs:label "Berlin" .
dbr:Pine sptl:lexicalization lx:pine tree ls:Pine pine tree .
lx:pine tree rdfs:label "pine tree" .
ls:Pine pine tree sptl:pUriGivenSf "0.941" .
dbr:Berlin dbo:country dbr:Germany .

dbr:Autism dbo:wikiPageID "25" .
dbr:Autism dbo:wikiPageWikiLink dbr:Human brain .

dbr:Andre Agassi foaf:birthDate "1970-04-29" .

dbr:William Shakespeare dbo:individualisedPnd "118613723" .

dbr:ArtificialLanguages dbo:wikiPageRedirects
dbr:Constructed language .
dbr:Autism <http://www.w3.org/ns/prov#wasDerivedFrom>
<http://en.wikipedia.org/wiki/Autism?oldid=495234324> .
dbr:Category:Music skos:subject dbr:Music .

dbr:Alkane sptl:topicSignature "carbon alkanes atoms" .
dbr:AnAmericanInParis foaf:isPrimaryTopicOf
<http://en.wikipedia.org/wiki/AnAmericanInParis> .

Name

abstract

article categories

category label
category hierarchy

disambiguation
external links

geo coordinates
grammatical gender

homepage

image

infobox

interlanguage
label
lexicalizations

mappings

page ID
page links

persondata

redirects

revision ID

thematic concept

topic signatures
wiki page

Lehmann et al. / DBpedia

| ontologyProperty = foaf:name }}
{{PropertyMapping
| templateProperty = manufacturer
| ontologyProperty = manufacturer }}
{{DateIntervalMapping
| templateProperty = production
| startDateOntologyProperty =

productionStartDate

| endDateOntologyProperty =

productionEndDate }}

{{IntermediateNodeMapping
| nodeClass = AutomobileEngine
| correspondingProperty = engine
| mappings =

{{PropertyMapping
| templateProperty = engine
| ontologyProperty = displacement
| unit = Volume }}
{{PropertyMapping
| templateProperty = engine
| ontologyProperty = powerOutput
| unit = Power }}

}}
(...)

}}

The RDF statements that are extracted from the previous infobox example are shown below. As we can see,
the production period is correctly split into a start year
and an end year and the engine is represented by a distinct RDF node. It is worth mentioning that all values
are canonicalized to basic units. For example, in the
engine mapping we state that engine is a Volume
and thus, the extractor converts 4181cc (cubic cen-
timeters) to cubic meters (0.004181). Additionally,
there can exist multiple mappings on the same property
that search for different datatypes or different units. For
example, a number with PS as a suffix for engine.

dbr:Ford_GT40

rdf:type
dbo:Automobile;
rdfs:label "Ford GT40"@en;
dbo:manufacturer

dbr:Ford_Advanced_Vehicles;

dbo:productionStartYear

"1964"xsd:gYear;

dbo:productionEndYear "1969"xsd:gYear;
dbo:engine [

rdf:type AutomobileEngine;
dbo:displacement "0.004181";

(...) .

The DBpedia Mapping Wiki is not only used to map
different templates within a single language edition
of Wikipedia to the DBpedia ontology, but is used to
map templates from all Wikipedia language editions
to the shared DBpedia ontology. Figure 2 shows how

the infobox properties author and 		  author in Greek  are both being mapped to the global
identifier dbo:author. That means, in turn, that information from all language versions of DBpedia can
be merged and DBpedias for smaller languages can be
augmented with knowledge from larger DBpedias such
as the English edition. Conversely, the larger DBpedia editions can benefit from more specialized knowledge from localized editions, such as data about smaller
towns which is often only present in the corresponding
language edition [43].

Besides hosting of the mappings and DBpedia ontology definition, the DBpedia Mappings Wiki offers
various tools which support users in their work:

 Mapping Syntax Validator The mapping syntax
validator checks for syntactic correctness and highlights inconsistencies such as missing property
definitions.

 Extraction Tester The extraction tester linked on
each mapping page tests a mapping against a set
of example Wikipedia pages. This gives direct
feedback about whether a mapping works and how
the resulting data is structured.

 Mapping Tool The DBpedia Mapping Tool is a
graphical user interface that supports users to create and edit mappings.

2.5. URI Schemes

For every Wikipedia article, the framework introduces a number of URIs to represent the concepts described on a particular page. Up to 2011, DBpedia published URIs only under the http://dbpedia.org
domain. The main namespaces were:

 http://dbpedia.org/resource/ (prefix
dbr) for representing article data. There is a one-
to-one mapping between a Wikipedia page and a
DBpedia resource based on the article title. For
example, for the Wikipedia article on Berlin4, DBpedia will produce the URI dbr:Berlin. Exceptions
in this rule appear when intermediate nodes are
extracted from the mapping-based infobox extractor as unique URIs (e.g., the engine mapping
example in Section 2.4).

 http://dbpedia.org/property/ (prefix
dbp) for representing properties extracted from
the raw infobox extraction (cf. Section 2.3), e.g.
dbp:population.

4http://en.wikipedia.org/wiki/Berlin

Fig. 2. Depiction of the mapping from the Greek and English Wikipedia templates about books to the same DBpedia Ontology class [24].

 http://dbpedia.org/ontology/ (prefix
dbo) for representing the DBpedia ontology (cf.
Section 2.4), e.g. dbo:populationTotal.

calized data sets, the same thing is identified with the
same URI from the generic language-agnostic namespace http://dbpedia.org/resource/.

Although data from other Wikipedia language editions were extracted, they used the same namespaces.
This was achieved by exploiting the Wikipedia interlanguage links5. For every page in a language other
than English, the page was extracted only if the page
contained an inter-language link to an English page. In
that case, using the English link, the data was extracted
under the English resource name (i.e. dbr:Berlin).

Recent DBpedia internationalisation developments
showed that this approach omitted valuable data [24].
Thus, starting from the DBpedia 3.7 release6, two types
of data sets were generated. The localized data sets contain all things that are described in a specific language.
Within the datasets, things are identified with language
specific URIs such as http://<lang>.dbpedia.
org/resource/ for article data and http://
<lang>.dbpedia.org/property/ for property data. In addition, we produce a canonicalized data
set for each language. The canonicalized data sets only
contain things for which a corresponding page in the
English edition of Wikipedia exists. Within all canoni-

5http://en.wikipedia.org/wiki/Help:

Interlanguage_links

6A list of all DBpedia releases is provided in Table 17

2.6. NLP Extraction

DBpedia provides a number of data sets which have
been created to support Natural Language Processing
(NLP) tasks [33]. Currently, four datasets are extracted:
topic signatures, grammatical gender, lexicalizations
and thematic concept. While the topic signatures and
the grammatical gender extractors primarily extract data
from the article text, the lexicalizations and thematic
concept extractors make use of the wiki markup.

DBpedia entities can be referred to using many different names and abbreviations. The Lexicalization data
set provides access to alternative names for entities
and concepts, associated with several scores estimating
the association strength between name and URI. These
scores distinguish more common names for specific
entities from rarely used ones and also show how ambiguous a name is with respect to all possible concepts
that it can mean.

The topic signatures data set enables the description
of DBpedia resources based on unstructured informa-
tion, as compared to the structured factual data provided
by the mapping-based and raw extractors. We build a
Vector Space Model (VSM) where each DBpedia resource is a point in a multidimensional space of words.

Lehmann et al. / DBpedia

Each DBpedia resource is represented by a vector, and
each word occurring in Wikipedia is a dimension of
this vector. Word scores are computed using the tf-idf
weight, with the intention to measure how strong is the
association between a word and a DBpedia resource.
Note that word stems are used in this context in order to
generalize over inflected words. We use the computed
weights to select the strongest related word stems for
each entity and build topic signatures [30].

There are two more Feature Extractors related to Natural Language Processing. The thematic concepts data
set relies on Wikipedias category system to capture the
idea of a theme, a subject that is discussed in its arti-
cles. Many of the categories in Wikipedia are linked to
an article that describes the main topic of that category.
We rely on this information to mark DBpedia entities
and concepts that are thematic, that is, they are the
centre of discussion for a category.

The grammatical gender data set uses a simple heuristic to decide on a grammatical gender for instances of
the class Person in DBpedia. While parsing an article
in the English Wikipedia, if there is a mapping from
an infobox in this article to the class dbo:Person,
we record the frequency of gender-specific pronouns in
their declined forms (Subject, Object, Possessive Ad-
jective, Possessive Pronoun and Reflexive)  i.e. he,
him, his, himself (masculine) and she, her, hers, herself
(feminine). Grammatical genders for DBpedia entities
are assigned based on the dominating gender in these
pronouns.

2.7. Summary of Other Recent Developments

In this section we summarize the improvements of
the DBpedia extraction framework since the publication of the previous DBpedia overview article [6] in
2009. One of the major changes on the implementation
level is that the extraction framework has been rewritten in Scala in 20107 to improve the efficiency of the
extractors by an order of magnitude compared to the
previous PHP based framework. The new more modular framework also allows to extract data from tables
in Wikipedia pages and supports extraction from multiple MediaWiki templates per page. Another significant
change was the creation and utilization of the DBpedia
Mappings Wiki as described earlier. Further significant
changes include the mentioned NLP extractors and the
introduction of URI schemes.

7Table 17 provides an overview of the projects evolution through

time.

In addition, there were several smaller improvements
and general maintenance: Overall, over the past four
years, the parsing of the MediaWiki markup improved
quite a lot which led to better overall coverage, for
example, concerning references and parser functions.
In addition, the collection of MediaWiki namespace
identifiers for many languages is now performed semiautomatically leading to a high accuracy of detection.
This concerns common title prefixes such as User, File,
Template, Help, Portal etc. in English that indicate
pages that do not contain encyclopedic content and
would produce noise in the data. They are important for
specific extractors as well, for instance, the category hierarchy data set is produced from pages of the Category
namespace. Furthermore, the output of the extraction
system now supports more formats and several compliance issues regarding URIs, IRIs, N-Triples and Turtle
were fixed.

The individual data extractors have been improved as
well in both number and quality in many areas. The abstract extraction was enhanced producing more accurate
plain text representations of the beginning of Wikipedia
article texts. More diverse and more specific datatypes
do exist (e.g. many currencies and XSD datatypes such
as xsd:gYearMonth, xsd:positiveInteger,
etc.) and for a number of classes and properties, specific
datatypes were added (e.g. inhabitants/km2 for the population density of populated places and m3/s for the
discharge of rivers). Many issues related to data parsers
were resolved and the quality of the owl:sameAs
data set for multiple language versions was increased
by an implementation that takes bijective relations into
account.

There are also further extractors, e.g. for Wikipedia
page IDs and revisions. Moreover, redirect and disambiguation extractors were introduced and improved. For
the redirect data, the transitive closure is computed
while taking care of catching cycles in the links. The
redirects also help regarding infobox coverage in the
mapping-based extraction by resolving alternative template names. Moreover, in the PHP framework, if an
infobox value pointed to a redirect, this redirection was
not properly resolved and thus resulted in RDF links
that led to URIs which did not contain any further in-
formation. Resolving redirects affected approximately
15% of all links, and hence increased the overall interconnectivity of resources in the DBpedia ontology.

Finally, a new heuristic to increase the connectiveness of DBpedia instances was introduced. If an infobox
contains a string value that is not linked to another
Wikipedia article, the extraction framework searches

Fig. 3. Snapshot of a part of the DBpedia ontology.

for hyperlinks in the same Wikipedia article that have
the same anchor text as the infobox value string. If such
a link exists, the target of that link is used to replace
the string value in the infobox. This method further
increases the number of object property assertions in
the DBpedia ontology.

Orthogonal to the previously mentioned improve-
ments, there have been various efforts to assess the quality of the DBpedia datasets. [26] developed a framework for estimating the quality of DBpedia and a sample of 75 resources were analysed. A more comprehensive effort was performed in [48] by providing a
distributed web-based interface [25] for quality assess-
ment. In this study, 17 data quality problem types were
analysed by 58 users covering 521 resources in DBpe-
dia.

3. DBpedia Ontology

The DBpedia ontology consists of 320 classes which
form a subsumption hierarchy and are described by
1,650 different properties. With a maximal depth of
5, the subsumption hierarchy is intentionally kept
rather shallow which fits use cases in which the ontology is visualized or navigated. Figure 3 depicts
a part of the DBpedia ontology, indicating the relations among the top ten classes of the DBpedia on-
tology, i.e. the classes with the highest number of
instances. The complete DBpedia ontology can be
browsed online at http://mappings.dbpedia.
org/server/ontology/classes/.

Fig. 4. Growth of the DBpedia ontology.

The DBpedia ontology is maintained and extended
by the community in the DBpedia Mappings Wiki. Figure 4 depicts the growth of the DBpedia ontology over
time. While the number of classes is not growing too
much due to the already good coverage of the initial
version of the ontology, the number of properties increases over time due to the collaboration on the DBpedia Mappings Wiki and the addition of more detailed
information to infoboxes by Wikipedia editors.

3.1. Mapping Statistics

As of April 2013, there exist mapping communities
for 27 languages, 23 of which are active. Figure 5 shows
statistics for the coverage of these mappings in DBpe-
dia. Figures (a) and (c) refer to the absolute number
of template and property mappings that are defined for
every DBpedia language edition. Figures (b) and (d) de-

CPPxsd:decimalCrdf:type owl:Classrdf:type owl:DatatypePropertyrdf:type owl:ObjectPropertyLegenddbo:PopulatedPlaceCdbo:AgentCdbo:PlaceCowl:ThingCdbo:SpeciesCdbo:SettlementCdbo:PersonCdbo:AthleteCdbo:EukaryoteCdbo:OrganisationCdbo:Workdbo:producerPdbo:writerPdbo:birthPlacePdbo:familyPPdbo:conservationStatusxsd:StringPxsd:datedbo:releaseDatePdbo:runtimexsd:doublePxsd:datedbo:birthDatePxsd:datedbo:deathDatePxsd:doubledbo:areaTotalPxsd:doubledbo:elevationPxsd:Stringdbo:utcOffsetPdbo:populationTotalxsd:integerPxsd:Stringdbo:areaCoderdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:subClassOfrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domainrdfs:domaindbo:subsequentWorkPdbo:locationPdbo:cantonPrdfs:subClassOfrdfs:subClassOflllllllDBpedia VersionNumber of ontology elements3.23.33.43.53.63.73.802004006008001,0001,2001,4001,6001,8002,000ClassesProperties10

Lehmann et al. / DBpedia

Fig. 5. Mapping coverage statistics for all mapping-enabled languages.

pict the percentage of the defined template and property
mappings compared to the total number of available
templates and properties for every Wikipedia language
edition. Figures (e) and (g) show the occurrences (in-
stances) that the defined template and property mappings have in Wikipedia. Finally, figures (f) and (h)
give the percentage of the mapped templates and properties occurences, compared to the total templates and
property occurences in a Wikipedia language edition.
It can be observed in the figure that the Portuguese
DBpedia language edition is the most complete regarding mapping coverage. Other language editions such
as Bulgarian, Dutch, English, Greek, Polish and Spanish have mapped templates covering more than 50%

of total template occurrences. In addition, almost all
languages have covered more than 20% of property oc-
currences, with Bulgarian and Portuguese reaching up
to 70%.

The mapping activity of the ontology enrichment
process along with the editing of the ten most active
mapping language communities is depicted in Figure 6.
It is interesting to notice that the high mapping activity peaks coincide with the DBpedia release dates.
For instance, the DBpedia 3.7 version was released on
September 2011 and the 2nd and 3rd quarter of that
year have a very high activity compared to the 4th quar-
ter. In the last two years (2012 and 2013), most of the
DBpedia mapping language communities have defined

Fig. 6. Mapping community activity for (a) ontology and (b) 10 most active language editions

report on the number of instances of popular classes
within the 20 DBpedia versions as well as the conceptual overlap between the languages.

Table 2 shows the overall number of things, ontology and raw-infobox properties, infobox statements and
type statements for the 20 languages. The column headings have the following meaning: LD = Localized data
sets (see Section 2.5); CD = Canonicalized data sets
(see Section 2.5); all = Overall number of instances in
the data set, including instances without infobox data;
with MD = Number of instances for which mappingbased infobox data exists; Raw Properties = Number
of different properties that are generated by the raw
infobox extractor; Mapping Properties = Number of
different properties that are generated by the mappingbased infobox extractor; Raw Statements = Number of
statements (facts) that are generated by the raw infobox
extractor; Mapping Statements = Number of statements
(facts) that are generated by the mapping-based infobox
extractor.

It is interesting to see that the English version of DBpedia describes about three times more instances than
the second and third largest language editions (French,
German). Comparing the first column of the table with
the second and third reveals which portion of the instances of a specific language correspond to instances
in the English version of DBpedia and which portion
of the instances is described by clean, mapping-based
infobox data. The difference between the number of
properties in the raw infobox data set and the cleaner
mapping-based infobox data set (columns 4 and 5) results on the one hand from multiple Wikipedia infobox
properties being mapped to a single ontology property.
On the other hand, it reflects the number of mappings
that have been so far created in the Mapping Wiki for a
specific language.

Table 3 reports the number of instances for a set of
popular classes from the third and forth hierarchy level

Fig. 7. English property mappings occurrence frequency (both axes
are in log scale)

their own chapters and have their own release dates.
Thus, recent mapping activity shows less fluctuation.

Finally, Figure 7 shows the English property mappings occurrence frequency. Both axes are in log scale
and represent the number of property mappings (x axis)
that have exactly y occurrences (y axis). The occurrence
frequency follows a long tail distribution. Thus, a low
number of property mappings have a high number of
occurrences and a high number of property mappings
have a low number of occurences.

3.2. Instance Data

The DBpedia 3.8 release contains localized versions
of DBpedia for 111 languages which have been extracted from the Wikipedia edition in the corresponding language. For 20 of these languages, we report in
this section the overall number of entities being described by the localized versions as well as the number of facts (i.e. statements) that have been extracted
from infoboxes describing these things. Afterwards, we

Lehmann et al. / DBpedia

Table 2

Basic statistics about Localized DBpedia Editions.

Inst. with MD CD
2,359,521

Raw Prop. CD Map. Prop. CD

Inst. LD all
3,769,926
1,243,771
1,197,334

Inst. CD all
3,769,926

en
de
fr
it
es
pl
ru
pt
ca
cs
hu
ko
tr
ar
eu
sl
bg
hr
el

Raw Statem. CD Map. Statem. CD
33,742,015
2,880,381
2,901,809
4,804,731
4,383,206
4,511,794
1,389,473
4,005,527
1,301,868

65,143,840
7,603,562
8,854,322
12,227,870
7,740,458
7,696,193
6,973,305
6,255,151
3,689,870
1,857,230
2,506,399
1,035,606
1,350,679

2,255,897
1,213,801

of the ontology within the canonicalized DBpedia data
sets for each language. The indented classes are subclasses of the superclasses set in bold. The zero values in the table indicate that no infoboxes have been
mapped to a specific ontology class within the corresponding language so far. Again, the English version of
DBpedia covers by far the most instances.

Table 4 shows, for the canonicalized, mapping-based
data set, how many instances are described in multiple languages. The Instances column contains the total
number of instances per class across all 20 languages,
the second column contains the number of instances
that are described only in a single language version, the
next column contains the number of instances that are
contained in two languages but not in three or more lan-
guages, etc. For example, 12,936 persons are described
in five languages but not in six or more languages. The
number 871,630 for the class Person means that all
20 language versions together describe 871,630 different persons. The number is higher than the number of
persons described in the canonicalized English infobox
data set (763,643) listed in Table 3, since there are
infoboxes in non-English articles describing a person
without a corresponding infobox in the English article
describing the same person. Summing up columns 2 to
10+ for the Person class, we see that 195,263 persons
are described in two or more languages. The large difference of this number compared to the total number of

871,630 persons is due to the much smaller size of the
localized DBpedia versions compared to the English
one (cf. Table 2).

3.3. Internationalisation Community

The introduction of the mapping-based infobox
extractor alongside live synchronisation approaches
in [20] allowed the international DBpedia community
to easily define infobox-to-ontology mappings. As a
result of this development, there are presently mappings
for 27 languages8. The DBpedia 3.7 release9 in September 2011 was the first DBpedia release to use the localized I18n (Internationalisation) DBpedia extraction
framework [24].

At the time of writing, DBpedia chapters for 14 languages have been founded: Basque, Czech, Dutch, En-
glish, French, German, Greek, Italian, Japanese, Ko-
rean, Polish, Portuguese, Russian and Spanish.10 Be-

8Arabic (ar), Bulgarian (bg), Bengali (bn), Catalan (ca), Czech
(cs), German (de), Greek (el), English (en), Spanish (es), Estonian
(et), Basque (eu), French (fr), Irish (ga), Hindi (hi), Croatian (hr),
Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean
(ko), Dutch (nl), Polish (pl), Portuguese (pt), Russian (ru), Slovene
(sl), Turkish (tr), Urdu (ur)

9http://blog.dbpedia.org/2011/09/11/
10Accessed on 25/09/2013: http://wiki.dbpedia.org/

Internationalization/Chapters

Table 3

Number of instances per class within 10 localized DBpedia versions.

en

it

pl

es

pt

fr

de

ru

ca

hu

Person

Athlete
Artist
Politician

Place

Popul.Place
Building
River

Organisation

Company
Educ.Inst.
Band

Work

Music.Work
Film
Software

Cross-language overlap: Number of instances that are described in multiple languages.

Table 4

Class
Person
Place
Organisation
Work

Instances

10+

sides providing mappings from infoboxes in the corresponding Wikipedia editions, DBpedia chapters organise a local community and provide hosting for data sets
and associated services.

While at the moment chapters are defined by ownership of the IP and server of the sub domain A record
(e.g. http://ko.dbpedia.org) given by DBpedia maintainers, the DBpedia internationalisation committee11 is manifesting its structure and each language
edition has a representative with a vote in elections. In
some cases (e.g. Greek12 and Dutch13) the existence of
a local DBpedia chapter has had a positive effect on the
creation of localized LOD clouds [24].

In the weeks leading to a new release, the DBpedia project organises a mapping sprint, where communities from each language work together to improve
mappings, increase coverage and detect bugs in the extraction process. The progress of the mapping effort

11http://wiki.dbpedia.org/

Internationalization

12http://el.dbpedia.org
13http://nl.dbpedia.org

is tracked through statistics on the number of mapped
templates and properties, as well as the number of
times these templates and properties occur in Wikipedia.
These statistics provide an estimate of the coverage of
each Wikipedia edition in terms of how many entities
will be typed and how many properties from those entities will be extracted. Therefore, they can be used
by each language edition to prioritize properties and
templates with higher impact on the coverage.

The mapping statistics have also been used as a way
to promote a healthy competition between language
editions. A sprint page was created with bar charts that
show how close each language is from achieving total coverage (as shown in Figure 5), and line charts
showing the progress over time highlighting when one
language is overtaking another in their race for higher
coverage. The mapping sprints have served as a great
motivator for the crowd-sourcing efforts, as it can be
noted from the increase in the number of mapping contributions in the weeks leading to a release.

Lehmann et al. / DBpedia

4. Live Synchronisation

Wikipedia articles are continuously revised at a very
high rate, e.g. the English Wikipedia, in June 2013,
has approximately 3.3 million edits per month which
is equal to 77 edits per minute14. This high change
frequency leads to DBpedia data quickly being out-
dated, which in turn leads to the need for a methodology
to keep DBpedia in synchronisation with Wikipedia.
As a consequence, the DBpedia Live system was de-
veloped, which works on a continuous stream of updates from Wikipedia and processes that stream on the
fly [20,36]. It allows extracted data to stay up-to-date
with a small delay of at most a few minutes. Since the
English Wikipedia is the largest among all Wikipedia
editions with respect to the number of articles and the
number of edits per month, it was the first language
DBpedia Live supported15. Meanwhile, DBpedia Live
for Dutch16 was developed.

4.1. DBpedia Live System Architecture

In order for live synchronisation to be possible, we
need access to the changes made in Wikipedia. The
Wikimedia foundation kindly provided us access to
their update stream using the OAI-PMH protocol [27].
This protocol allows a programme to pull page updates
in XML via HTTP. A Java component, serving as a
proxy, constantly retrieves new updates and feeds them
to the DBpedia Live framework. This proxy is necessary to decouple the stream from the framework to
simplify maintenance of the software. The live extraction workflow uses this update stream to extract new
knowledge upon relevant changes in Wikipedia articles.
The overall architecture of DBpedia Live is indicated
in Figure 8. The major components of the system are
as follows:

 Local Wikipedia Mirror: A local copy of a
Wikipedia language edition is installed which is
kept in real-time synchronisation with its live version using the OAI-PMH protocol. Keeping a local
Wikipedia mirror allows us to exceed any access
limits posed by Wikipedia.

 Mappings Wiki: The DBpedia Mappings Wiki,
described in Section 2.4, serves as secondary input

14http://stats.wikimedia.org/EN/SummaryEN.

htm

15http://live.dbpedia.org
16http://live.nl.dbpedia.org

Fig. 8. Overview of DBpedia Live extraction framework.

source. Changes of the mappings wiki are also
consumed via an OAI-PMH stream. Note that a
single mapping change can affect a high number
of DBpedia resources.

 DBpedia Live Extraction Manager: This is the
core component of the DBpedia Live extraction
architecture. The manager takes feeds of pages for
re-processing as input and applies all the enabled
extractors. After processing a page, the extracted
triples are a) inserted into a backend triple store
(in our case Virtuoso [10]), updating the old triples
and b) saved as changesets into a compressed N-
Triples file structure.

 Synchronisation Tool: This tool allows third parties to keep DBpedia Live mirrors up-to-date by
harvesting the produced changesets.

4.2. Features of DBpedia Live

The core components of the DBpedia Live Extraction

framework provide the following features:

 Mapping-Affected Pages: The update of all pages

that are affected by a mapping change.

 Unmodified Pages: The update of unmodified

pages at regular intervals.

 Changesets Publication: The publication of triple-

changesets.

 Synchronisation Tool: A synchronisation tool for

harvesting updates to DBpedia Live mirrors.

 Data Isolation: Separate data from different

sources.

Mapping-Affected Pages: Whenever an infobox mapping change occurs, all the Wikipedia pages that use
that infobox are reprocessed. Taking Figure 2 as an
example, if a new property mapping is introduced (i.e.
dbo:translator) or an existing (i.e. dbo:illustrator) is
updated or deleted, then all entities belonging to the
class dbo:Book are reprocessed. Thus, upon a mapping
change, we identify all the affected Wikipedia pages
and feed them for reprocessing.

Unmodified Pages: Extraction framework improvements or activation / deactivation of DBpedia extractors
might never be applied to rarely modified pages. To
overcome this problem, we obtain a list of the pages
which have not been processed over a period of time
(30 days in our case) and feed that list to the DBpedia
Live extraction framework for reprocessing. This feed
has a lower priority than the update or the mapping
affected pages feed and ensures that all articles reflect a
recent state of the output of the extraction framework.
Publication of Changesets: Whenever a Wikipedia
article is processed, we get two disjoint sets of triples. A
set for the added triples, and another set for the deleted
triples. We write those two sets into N-Triples files,
compress them, and publish the compressed files as
changesets. If another DBpedia Live mirror wants to
synchronise with the DBpedia Live endpoint, it can just
download those files, decompress and integrate them.
Synchronisation Tool: The synchronisation tool enables a DBpedia Live mirror to stay in synchronisation
with our live endpoint. It downloads the changeset files
sequentially, decompresses them and updates the target
SPARQL endpoint via insert and delete operations.
Data Isolation:
In order to keep the data isolated,
DBpedia Live keeps different sources of data in
different SPARQL graphs. Data from the article
update feeds are contained in the graph with the
URI http://live.dbpedia.org, static data (i.e.
links to the LOD cloud) are kept in http://static.
dbpedia.org and the DBpedia ontology is stored
in http://dbpedia.org/ontology. All data is
also accessible under the http://dbpedia.org
graph for combined queries. Next versions of DBpedia Live will also separate data from the raw infobox
extraction and mapping-based infobox extraction.

5. Interlinking

DBpedia is interlinked with numerous external data
sets following the Linked Data principles. In this sec-
tion, we give an overview of the number and types
of outgoing links that point from DBpedia into other
data sets, as well as the external data sets that set links
pointing to DBpedia resources.

5.1. Outgoing Links

third party data sets. The DBpedia project maintains
a link repository17 for which conventions for adding
linksets and linkset metadata are defined. The adherence to those guidelines is supervised by a linking com-
mittee. Linksets which are added to the repository are
used for the subsequent official DBpedia release as well
as for DBpedia Live. Table 5 lists the linksets created
by the DBpedia community as of April 2013. The first
column names the data set that is the target of the links.
The second and third column contain the predicate that
is used for linking as well as the overall number of links
that is set between DBpedia and the external data set.
The last column names the tool that was used to generate the links. The value S refers to Silk, L to LIMES,
C to custom script and a missing entry means that the
dataset is copied from the previous releases and not
regenerated.

An example for the usage of links is the combination of data about European Union project funding
(FTS) [32] and data about countries in DBpedia. The
query below compares funding per year (from FTS) and
country with the gross domestic product of a country
(from DBpedia)18 .
SELECT * { {
SELECT ?ftsyear ?ftscountry (SUM(?amount) AS

?funding)

?com rdf:type fts-o:Commitment .
?com fts-o:year ?year .
?year rdfs:label ?ftsyear .
?com fts-o:benefit ?benefit .
?benefit fts-o:detailAmount ?amount .
?benefit fts-o:beneficiary ?beneficiary .
?beneficiary fts-o:country ?country .
?country owl:sameAs ?ftscountry .

} } {
SELECT ?dbpcountry ?gdpyear ?gdpnominal {

?dbpcountry rdf:type dbo:Country .
?dbpcountry dbp:gdpNominal ?gdpnominal .
?dbpcountry dbp:gdpNominalYear ?gdpyear .

} }
FILTER ((?ftsyear = str(?gdpyear)) &&
(?ftscountry = ?dbpcountry)) }

In addition to providing outgoing links on an
instance-level, DBpedia also sets links on schemalevel pointing from the DBpedia ontology to equivalent terms in other schemas. Links to other schemata
can be set by the community within the DBpedia Mappings Wiki by using owl:equivalentClass in
class templates and owl:equivalentProperty
in datatype or object property templates, respectively.
In particular, in 2011 Google, Microsoft, and Yahoo!
announced their collaboration on Schema.org, a col-

Similar to the DBpedia ontology, DBpedia also follows a community approach for adding links to other

17https://github.com/dbpedia/dbpedia-links
18Endpoint: http://fts.publicdata.eu/sparql

Results: http://bit.ly/1c2mIwQ.

Lehmann et al. / DBpedia

Table 5

Data sets linked from DBpedia

Data set
Amsterdam Museum
BBC Wildlife Finder
Book Mashup

Bricklink
CORDIS
Dailymed
DBLP Bibliography
DBTune
Diseasome
Drugbank

Eurostat (Linked Stats)
Eurostat (WBSG)
CIA World Factbook
flickr wrappr

Freebase

GeoNames
GeoSpecies

Project Gutenberg
Italian Public Schools
LinkedGeoData
LinkedMDB
MusicBrainz
New York Times
OpenCyc
OpenEI (Open Energy)
Revyu
Sider
TCMGeneDIT

US Census
WikiCompany
WordNet
YAGO2
Sum

Predicate
owl:sameAs
owl:sameAs
rdf:type
owl:sameAs
dc:publisher
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
dbp:hasPhoto-
Collection
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
rdf:type
owl:sameAs
owl:sameAs
dbp:wordnet type
rdf:type

Tool

Count

9 100

10 100

2 300
4 800
3 100

3 800 000

3 600 000
1 900
86 500
16 000

2 500
5 800
103 600
13 800
23 000
9 700
27 100

2 000

896 400
12 600
8 300
467 100
18 100 000
27 211 732

lection of vocabularies for marking up content on web
pages. The DBpedia 3.8 ontology contains 45 equivalent class and 31 equivalent property links pointing to
http://schema.org terms.

Table 6

Top 10 data sets in Sindice ordered by the number of links to DBpedia.

Data set
okaboo.com
tfri.gov.tw
naplesplus.us
fu-berlin.de
freebase.com
geonames.org
opencyc.org
geospecies.org
dbrec.net
faviki.com

Link Predicate Count

Link Count
2,407,121

5.2. Incoming Links

DBpedia is being linked to from a variety of data
sets. The overall number of links pointing to DBpedia
from other data sets is 39,007,478 according to the Data
Hub.19 However, those counts are entered by users and
may not always be valid and up-to-date.

In order to identify actually published and online
data sets that link to DBpedia, we used Sindice [39].
The Sindice project crawls RDF resources on the web
and indexes those resources. In Sindice, a data set is
defined by the second-level domain name of the entitys URI, e.g. all resources available at the domain
fu-berlin.de are considered to belong to the same
data set. A triple is considered to be a link if the data
set of subject and object are different. Furthermore, the
Sindice data we used for analysis only considers authoritative entities: The data set of a subject of a triple
must match the domain it was retrieved from, otherwise
it is not considered. Sindice computes a graph summary [9] over all resources they store. With the help
of the Sindice team, we examined this graph summary
to obtain all links pointing to DBpedia. As shown in
Table 7, Sindice knows about 248 data sets linking to
DBpedia. 70 of those data sets link to DBpedia via
owl:sameAs, but other link predicates are also very
common as evident in this table. In total, Sindice has
indexed 4 million links pointing to DBpedia. Table 6
lists the 10 data sets which set most links to DBpedia
along with the used link predicate and the number of
links.

It should be noted that the data in Sindice is not com-
plete, for instance it does not contain all data sets that

19See http://wiki.dbpedia.org/Interlinking for

details.

Sindice summary statistics for incoming links to DBpedia.

Table 7

Metric
Total links:
Total distinct data sets:
Total distinct predicates:

Value
3,960,212

Table 8

Top 10 datasets by incoming links in Sindice.

domain
purl.org
dbpedia.org
creativecommons.org
identi.ca
l3s.de
rkbexplorer.com
nytimes.com
w3.org
geospecies.org
livejournal.com

datasets

links
6,717,520
3,960,212
3,030,910
2,359,276
1,261,487
1,212,416
1,174,941

are catalogued by the DataHub20. However, it crawls for
RDFa snippets, converts microformats etc., which are
not captured by the DataHub. Despite the inaccuracy,
the relative comparison of different datasets can still
give us insights. Therefore, we analysed the link structure of all Sindice datasets using the Sindice cluster.
Table 8 shows the datasets with most incoming links.
Those are authorities in the network structure of the
web of data and DBpedia is currently ranked second in
terms of incoming links.

6. DBpedia Usage Statistics

DBpedia is served on the web in three forms: First,
it is provided in the form of downloadable data sets
where each data set contains the results of one of the
extractors listed in Table 1. Second, DBpedia is served
via a public SPARQL endpoint and, third, it provides
dereferencable URIs according to the Linked Data prin-
ciples. In this section, we explore some of the statistics
gathered during the hosting of DBpedia over the last
two of years.

6.1. Download Statistics for the DBpedia Data Sets

DBpedia covers more than 100 languages, but those
languages vary with respect to the download popular-

20http://datahub.io/

Fig. 9. The download count and download volume (in GB) of the
English language of DBpedia.

ity as well. The top five languages with respect to the
download volume are English, Chinese, German, Cata-
lan, and French respectively. The download count and
download volume of the English language is indicated
in Figure 9. To host the DBpedia dataset downloads,
a bandwidth of approximately 6 TB per month is currently needed.

Furthermore, DBpedia consists of several data sets
which vary with respect to their download popularity.
The download count and the download volume of each
data set during the year 2012 is depicted in Figure 10.
In those statistics we filtered out all IP addresses, which
requested a file more than 1000 times per month.21
Pagelinks are the most downloaded dataset, although
they are not semantically rich as they do not reveal
which type of links exists between two resources. Sup-
posedly, they are used for network analysis or providing
relevant links in user interfaces and downloaded more
often as they are not provided via the official SPARQL
endpoint.

6.2. Public Static DBpedia SPARQL Endpoint

The main public DBpedia SPARQL endpoint22 is
hosted using the Virtuoso Universal Server (Enterprise
Edition) version 6.4 software in a 4-nodes cluster con-
figuration. This cluster setup provides parallelization
of query execution, even when the cluster nodes are
on the same machine, as splitting a query over several
nodes allows better use of parallel threads on modern
multi-core CPUs on standard commodity hardware.

Virtuoso supports horizontal scale-out, either by redistributing the existing cluster nodes onto multiple ma-
chines, or by adding several separate clusters with a
round robin HTTP front-end. This allows the cluster
setup to grow in line with desired response times for
an RDF data set collection. As the size of the DBpedia

21The IP address was only filtered for that specific file and month

in those cases.

22http://dbpedia.org/sparql

0204060801001202012-Q12012-Q22012-Q32012-Q4Download Count (in Thousands)Download Volume (in TB)18

Lehmann et al. / DBpedia

Fig. 10. The download count and download volume (in GB) of the DBpedia data sets.

Hardware of the machines serving the public SPARQL endpoint.

Number of unique sites accessing DBpedia endpoints.

Table 9

Table 11

DBpedia
3.3 - 3.4
3.5 - 3.7

Configuration
AMD Opteron 8220 2.80Ghz, 4 Cores, 32GB
Intel Xeon E5520 2.27Ghz, 8 Cores, 48GB
Intel Xeon E5-2630 2.30GHz, 8 Cores, 64GB

data set increased and its use by the Linked Data community grew, the project migrated to increasingly pow-
erful, but still moderately priced, hardware as shown in
Table 9. The Virtuoso instance is configured to process
queries within a 1,200 second timeout window and a
maximum result set size of 50,000 rows. It provides
OFFSET and LIMIT support for paging alongside the
ability to produce partial results.

The log files used in the following analysis excluded

traffic generated by:

1. clients that have been temporarily rate limited

after a burst period,

2. clients that have been banned after misuse,
3. applications, spiders and other crawlers that are
blocked after frequently hitting the rate limit or
generally use too many resources.

Virtuoso supports HTTP Access Control Lists
(ACLs) which allow the administrator to rate limit certain IP addresses or whole IP ranges. A maximum number of requests per second (currently 15) as well as
a bandwidth limit per request (currently 10MB) are
enforced. If the client software can handle compres-
sion, replies are compressed to further save bandwidth.
Exception rules can be configured for multiple clients
hidden behind a NAT firewall (appearing as a single
IP address) or for temporary requests for higher rate

DBpedia

Avg/Day Median

Stdev Maximum

limits. When a client hits an ACL limit, the system
reports an appropriate HTTP status code23 like 509 and
quickly drops the connection. The system further uses
an iptables based firewall for permanent blocking of
clients identified by their IP addresses.

6.3. Public Static Endpoint Statistics

The statistics presented in this section were extracted
from reports generated by Webalizer v2.2124. Table 10
and Table 11 show various DBpedia SPARQL endpoint
usage statistics for the DBpedia 3.3 to 3.8 releases. Note
that the usage of all endpoints mentioned in Table 12 is
counted. The Avg/Day column represents the average
number of hits (resp. visits/sites) per day, followed by
the Median and Standard Deviation. The last column
shows the maximum number of hits (resp. visits/sites)
that was recorded on a single day for each data set
version. Visits (i.e. sessions of subsequent queries from
the same client) are determined by a floating 30 minute

23http://en.wikipedia.org/wiki/List_of_HTTP_

status_codes

24http://www.webalizer.org

010203040506070Download Count (in Thousands)Download Volume (in TB)Lehmann et al. / DBpedia

Table 10

Number of endpoint hits (left) and visits (right).

DBpedia

Avg/Day

1,212,549
1,122,612
1,328,355
2,085,399
2,910,410

Median

1,165,893
1,035,444
1,286,750
1,930,728
2,717,775

Stdev Maximum
1,319,539
2,371,657
2,908,720
2,495,031
8,873,473
7,678,490

1,057,398
1,085,640

DBpedia

Avg/Day Median

Stdev Maximum

Table 12

Hits per service to http://dbpedia.org in thousands.

Endpoint
/data
/ontology
/page
/property
/resource
/sparql
other
total

time window. All requests from behind a NAT firewall
are logged under the same external IP address and are
therefore counted towards the same visit if they occur
within the 30 minute interval.

Table 10 shows the increasing popularity of DBpedia.
There is a distinct dip in hits to the SPARQL endpoint in
DBpedia 3.5, which is partially due to more strict initial
limits for bot-related traffic which were later relaxed.
The sudden drop of visits between the 3.7 and the 3.8
data sets can be attributed to:

1. applications starting to use their own private

DBpedia endpoint

2. blocking of apps that were abusing the DBpedia

endpoint

3. uptake of the language specific DBpedia end-

points and DBpedia Live

6.4. Query Types and Trends

The DBpedia server is not only a SPARQL endpoint,
but also serves as a Linked Data Hub returning resources in a number of different formats. For each data
set we randomly selected 14 days worth of log files and
processed those in order to show the various services
called. Table 12 shows the number of hits to the various
endpoints.

The /resource endpoint uses the Accept: line in the
HTTP header sent by the client to return a HTTP sta-

Fig. 11. Traffic Linked Data versus SPARQL endpoint

tus code 30x to redirect the client to either the /page
(HTML based) or /data (formats like RDF/XML or Tur-
tle) equivalent of the article. Clients also frequently
mint their own URLs to either /page or /data version of
an articles directly, or download the raw data directly.
This explains why the count of /page and /data hits
in the table is larger than the number of hits on the
/resource endpoint. The /ontology and /property endpoints return meta information about the DBpedia on-
tology. While all of these endpoints themselves may
use SPARQL queries to generate various page content,
these requests are handled by the internal Virtuoso engine directly and do not show up as extra calls to the
/sparql endpoint in our analysis.

Figure 11 shows the percentages of traffic hits that
were generated by the main endpoints. As we can see,
the usage of the SPARQL endpoint has doubled from
about 22 percent in 2009 to about 44 percent in 2013.
However, this still means that 56 percent of traffic hits
are directed to the Linked Data service.

In Table 13, we focussed on the calls to the /sparql
endpoint and counted the number of statements per type.
As the log files only record the full SPARQL query on
a GET request, all the POST requests are counted as
unknown.

Finally, we analyzed each SPARQL query and

counted the use of keywords and constructs like:

Lehmann et al. / DBpedia

Table 13

Hits per statement type in thousands.

Statement
ask
construct
describe
select
unknown
total

Table 14

Trends in SPARQL select (rounded values in %).

Statement
distinct
filter
functions
geo
group
limit
optional
order
union

 DISTINCT
 FILTER
 FUNCTIONS like CONCAT, CONTAINS, ISIRI
 Use of GEO objects
 GROUP BY
 LIMIT / OFFSET
 OPTIONAL
 ORDER BY

For the GEO objects we counted the use of SPARQL
PREFIX geo: and wgs84*: declarations and usage in
property tags. Table 14 shows the use of various keywords as a percentage of the total select queries made
to the /sparql endpoint for the sample sets. In general,
we observed that queries became more complex over
time indicating an increasing maturity and higher expectations of the user base.

6.5. Statistics for DBpedia Live

Since its official release at the end of June 2011,
DBpedia Live attracted a steadily increasing number
of users. Furthermore, more users tend to use the synchronisation tool to synchronise their own DBpedia
Live mirrors. This leads to an increasing number of live
update requests, i.e. changeset downloads. Figure 12
indicates the number of daily SPARQL and synchroni-

sation requests sent to DBpedia Live endpoint in the
period between August 2012 and January 2013.

Fig. 12. Number of daily requests sent to the DBpedia Live for a)
SPARQL queries and b) synchronisation requests from August 2012
until January 2013

7. Use Cases and Applications

Due to DBpedias coverage of various domains as
well as its steady growth as a hub on the Web of Data,
the data sets provided by DBpedia can serve many pur-
poses. Such applications include improving search and
exploration of Wikipedia, data proliferation for applica-
tions, mashups as well as text analysis and annotation
tools.

7.1. Natural Language Processing

DBpedia can support many tasks in Natural Language Processing (NLP) [33]. For that purpose, DBpedia
includes a number of specialized data sets25. For in-
stance, the lexicalizations data set can be used to estimate the ambiguity of phrases, to help select unambiguous identifiers for ambiguous phrases, or to provide
alternative names for entities, just to mention a few ex-
amples. Topic signatures can be useful in tasks such as
query expansion or document summarization, and has
been successfully employed to classify ambiguously
described images as good depictions of DBpedia entities [13]. The thematic concepts data set of resources
can be used for creating a corpus from Wikipedia to be
used as training data for topic classifiers, among other

25http://wiki.dbpedia.org/Datasets/NLP

things (see below). The grammatical gender data set
can, for example, be used to add a gender feature in
co-reference resolution.

7.1.1. Annotation: Entity Disambiguation

An important use case for NLP is annotating texts
or other content with semantic information. Named
entity recognition and disambiguation  also known as
key phrase extraction and entity linking tasks  refers
to the task of finding real world entities in text and
linking them to unique identifiers. One of the main
challenges in this regard is ambiguity: an entity name,
or surface form, may be used in different contexts to
refer to different concepts. Many different methods
have been developed to resolve this ambiguity with
fairly high accuracy [22].

As DBpedia reflects a vast amount of structured real
world knowledge obtained from Wikipedia, DBpedia
URIs can be used as identifiers for the majority of domains in text annotation. Consequently, interlinking text
documents with Linked Data enables the Web of Data
to be used as background knowledge within documentoriented applications such as semantic search or faceted
browsing (cf. Section 7.3).

Many applications performing this task of annotating
text with entities in fact use DBpedia entities as targets.
For example, DBpedia Spotlight [34] is an open source
tool26 including a free web service that detects mentions of DBpedia resources in text. It uses the lexicalizations in conjunction with the topic signatures data
set as context model in order to be able to disambiguate
found mentions. The main advantage of this system is
its comprehensiveness and flexibility, allowing one to
configure it based on quality measures such as promi-
nence, contextual ambiguity, topical pertinence and disambiguation confidence, as well as the DBpedia on-
tology. The resources that should be annotated can be
specified by a list of resource types or by more complex
relationships within the knowledge base described as
SPARQL queries.

There are numerous other NLP APIs that link entities in text to DBpedia: AlchemyAPI27, Semantic API
from Ontos28, Open Calais29 and Zemanta30 among oth-
ers. Furthermore, the DBpedia ontology has been used
for training named entity recognition systems (without

26http://spotlight.dbpedia.org/
27http://www.alchemyapi.com/
28http://www.ontos.com/
29http://www.opencalais.com/
30http://www.zemanta.com/

disambiguation) in the context of the Apache Stanbol
project31.

A related project is ImageSnippets32, which is a system for annotating images. It uses DBpedia as one of
its main datasets for unambiguously identifying entities
depicted within an image.
Tag disambiguation Similar to linking entities in text
to DBpedia, user-generated tags attached to multimedia
content such as music, photos or videos can also be connected to the Linked Data hub. This has previously been
implemented by letting the user resolve ambiguities.
For example, Faviki33 suggests a set of DBpedia entities
coming from Zemantas API and lets the user choose
the desired one. Alternatively, similar disambiguation
techniques as mentioned above can be utilized to choose
entities from tags automatically [14]. The BBC34 [23]
employs DBpedia URIs for tagging their programmes.
Short clips and full episodes are tagged using two different tools while utilizing DBpedia to benefit from
global identifiers that can be easily integrated with other
knowledge bases.
7.1.2. Question Answering

DBpedia provides a wealth of human knowledge
across different domains and languages, which makes
it an excellent target for question answering and keyword search approaches. One of the most prominent
efforts in this area is the DeepQA project, which resulted in the IBM Watson system [12]. The Watson
system won a $1 million prize in Jeopardy and relies
on several data sets including DBpedia35. DBpedia is
also the primary target for several QA systems in the
Question Answering over Linked Data (QALD) workshop series36. Several QA systems, such as TBSL [44],
PowerAqua [31], FREyA [1] and QAKiS [7] have been
applied to DBpedia using the QALD benchmark ques-
tions. DBpedia is interesting as a test case for such
systems. Due to its large schema and data size as well
as its topic diversity, it provides significant scientific
challenges. In particular, it would be difficult to provide
capable QA systems for DBpedia based only on simple
patterns or via domain specific dictionaries, because of
its size and broad coverage. Therefore, a question an-

31http://stanbol.apache.org/
32http://www.imagesnippets.com
33http://www.faviki.com/
34http://bbc.co.uk
35http://www.aaai.org/Magazine/Watson/

watson.php

36http://greententacle.techfak.uni-

bielefeld.de/ cunger/qald/

Lehmann et al. / DBpedia

swering system, which is able to reliable answer questions over DBpedia correctly, could be seen as a truly
intelligent system. In the latest QALD series, question
answering benchmarks also exploit national DBpedia
chapters for multilingual question answering.

Similarly, the slot filling task in natural language
processing poses the challenge of finding values for a
given entity and property from mining text. This can
be viewed as question answering with static questions
but changing targets. DBpedia can be exploited for fact
validation or training data in this task, as was done by
the Watson team [5] and others [28].

7.2. Digital Libraries and Archives

In the case of libraries and archives, DBpedia could
offer a broad range of information on a broad range of
domains. In particular, DBpedia could provide:

 Context information for bibliographic and archive
records: Background information such as an authors demographics, a films homepage or an image could be used to enhance user interaction.

 Stable and curated identifiers for linking: DBpedia
is a hub of Linked Open Data. Thus, (re-)using
commonly used identifiers could ease integration
with other libraries or knowledge bases.

 A basis for a thesaurus for subject indexing: The
broad range of Wikipedia topics in addition to
the stable URIs could form the basis for a global
classification system.

Libraries have already invested both in Linked Data
and Wikipedia (and transitively to DBpedia) though
the realization of the Virtual International Authority
Files (VIAF) project.37 Recently, it was announced that
VIAF added a total of 250,000 reciprocal authority
links to Wikipedia.38 These links are already harvested
by DBpedia Live and will also be included in the next
static DBpedia release. This creates a huge opportunity
for libraries that use VIAF to get connected to DBpedia
and the LOD cloud in general.

7.3. Knowledge Exploration

Since DBpedia spans many domains and has a diverse schema, many knowledge exploration tools either
used DBpedia as a testbed or were specifically built

37http://viaf.org
38Accessed

on

12/02/2013:

http://www.oclc.org/

for DBpedia. We give a brief overview of tools and
structure them in categories:
Facet Based Browsers An award-winning39 facetbased browser used the Neofonie search engine to combine facts in DBpedia with full-text from Wikipedia in
order to compute hierarchical facets [15]. Another facet
based browser, which allows to create complex graph
structures of facets in a visually appealing interface and
filter them is gFacet [16]. A generic SPARQL based
facet explorer, which also uses a graph based visualisation of facets, is LODLive [8]. The OpenLink built-in
facet based browser40 is an interface, which enables
developers to explore DBpedia, compute aggregations
over facets and view the underlying SPARQL queries.
Search and Querying The DBpedia Query Builder41
allows developers to easily create simple SPARQL
queries, more specifically sets of triple patterns via intelligent autocompletion. The autocompletion functionality ensures that only URIs, which lead to solutions are
suggested to the user. The RelFinder [17] tool provides
an intuitive interface, which allows to explore the neighborhood and connections between resources specified
by the user. For instance, the user can view the shortest paths connecting certain persons in DBpedia. SemLens [18] allows to create statistical analysis queries
and correlations in RDF data and DBpedia in particular.
Spatial Applications DBpedia Mobile [4] is a location
aware client, which renders a map of nearby locations
from DBpedia, provides icons for schema classes and
supports more than 30 languages from various DBpedia
language editions. It can follow RDF links to other
data sets linked from DBpedia and supports powerful
SPARQL filters to restrict the viewed data.

7.4. Applications of the Extraction Framework:

Wiktionary Extraction

Wiktionary is one of the biggest collaboratively created lexical-semantic and linguistic resources, available in 171 languages (of which approximately 147 can
be considered active42). It contains information about
hundreds of spoken and even ancient languages. In the

39http://blog.dbpedia.org/2009/11/20/german-

government-proclaims-faceted-wikipedia-
search-one-of-the-365-best-ideas-in-germany/

40http://dbpedia.org/fct/
41http://querybuilder.dbpedia.org/
42http://s23.org/wikistats/wiktionaries_

research/news/2012/12-07a.html

html.php

case of the English Wiktionary there are nearly 3 million detailed descriptions of words covering several do-
mains43. Such descriptions provide, for a lexical word,
a hierarchical disambiguation to its language, part of
speech, sometimes etymologies, synonyms, hyponyms,
hyperonyms, example sentences, and most prominently
senses.

Due to its fast changing nature, together with the
fragmentation of the project into Wiktionary language
editions (WLE) with independent layout rules a, configurable mediator/wrapper approach is taken for its automated transformation into a structured knowledge base.
The workflow of this dedicated Wiktionary extractor
being part of the Wiktionary2RDF [19] project is as
follows: For every WLE to be transformed an XML
configuration file is provided as input. This configuration is used by the Wiktionary extractor, invoked by
the DBpedia extraction framework, to first generate a
schema reflecting the configured page structure (wrap-
per part). After this, these language specific schemas
are converted to a global schema (mediator part) and
later serialized to RDF.

To enable non-programmers (the community of
adopters and domain experts) to tailor and maintain
the WLE wrappers themselves, a simple XML dialect
was created to encode the page structure to be parsed
and declare triple patterns, that define how the resulting
RDF should be built. The described setup is run against
Wiktionary dumps. The resulting data set is open in
every aspect and hosted as Linked Data.44 Statistics are
shown in Table 15.

8. Related Work

8.1. Cross Domain Community Knowledge Bases

8.1.1. Wikidata

In March 2012, the Wikimedia Germany e.V. started
the development of Wikidata45. Wikidata is a free
knowledge base about the world that can be read and
edited by humans and machines alike. It provides data
in all languages of the Wikimedia projects, and allows
for central access to the data in a similar vein as Wikimedia Commons does for multimedia files. Things described in the Wikidata knowledge base are called items

and can have labels, descriptions and aliases in all lan-
guages. Wikidata does not aim at offering a single truth
about things, but providing statements given in a particular context. Rather than stating that Berlin has a
population of 3.5 million, Wikidata contains the statement about Berlins population being 3.5 million as of
2011 according to the German statistical office. Thus,
Wikidata can offer a variety of statements from different sources and dates. As there are potentially many
different statements for a given item and property, ranks
can be added to statements to define their status (pre-
ferred, normal or deprecated). The initial development
was divided in three phases:

 The first phase (interwiki links) created an entity
base for the Wikimedia projects. This provides
a better alternative to the previous interlanguage
link system.

 The second phase (infoboxes) gathered infoboxrelated data for a subset of the entities, with the
explicit goal of augmenting the infoboxes that are
currently widely used with data from Wikidata.

 The third phase (lists) will expand the set of properties beyond those related to infoboxes, and will
provide ways of exploiting this data within and
outside the Wikimedia projects.

At the time of writing of this article, the development
of the third phase is ongoing.

Wikidata already contains 11.95 million items and
348 properties that can be used to describe them. Since
March 2013 the Wikidata extension is live on all
Wikipedia language editions and thus their pages can
be linked to items in Wikidata and include data from
Wikidata.

Wikidata also offers a Linked Data interface46 as
well as regular RDF dumps of all its data. The planned
collaboration with Wikidata is outlined in Section 9.

8.1.2. Freebase

Freebase47 is a graph database, which also extracts
structured data from Wikipedia and makes it available
in RDF. Both DBpedia and Freebase link to each other
and provide identifiers based on those for Wikipedia
articles. They both provide dumps of the extracted data,
as well as APIs or endpoints to access the data and
allow their communities to influence the schema of the
data. There are, however, also major differences be-

43See http://en.wiktionary.org/wiki/semantic

for a simple example page

44http://wiktionary.dbpedia.org/
45http://wikidata.org/

46http://meta.wikimedia.org/wiki/Wikidata/

Development/LinkedDataInterface

47http://www.freebase.com/

Lehmann et al. / DBpedia

Table 15

Statistical comparison of extractions for different languages.

language
en
fr
ru
de

#words
2,142,237
4,657,817
1,080,156

#triples
28,593,364
35,032,121
12,813,437
5,618,508

#resources
11,804,039
20,462,349
5,994,560
2,966,867

#predicates

#senses

tween both projects. DBpedia focuses on being an RDF
representation of Wikipedia and serving as a hub on the
Web of Data, whereas Freebase uses several sources to
provide broad coverage. The store behind Freebase is
the GraphD [35] graph database, which allows to efficiently store metadata for each fact. This graph store is
append-only. Deleted triples are marked and the system
can easily revert to a previous version. This is neces-
sary, since Freebase data can be directly edited by users,
whereas information in DBpedia can only indirectly be
edited by modifying the content of Wikipedia or the
Mappings Wiki. From an organisational point of view,
Freebase is mainly run by Google, whereas DBpedia is
an open community project. In particular in focus areas
of Google and areas in which Freebase includes other
data sources, the Freebase database provides a higher
coverage than DBpedia.
8.1.3. YAGO

One of the projects that pursues similar goals
to DBpedia is YAGO48 [42]. YAGO is identical to
DBpedia in that each article in Wikipedia becomes an
entity in YAGO. Based on this, it uses the leaf categories in the Wikipedia category graph to infer type
information about an entity. One of its key features is to
link this type information to WordNet. WordNet synsets
are represented as classes and the extracted types of
entities may become subclasses of such a synset. In the
YAGO2 system [21], declarative extraction rules were
introduced, which can extract facts from different parts
of Wikipedia articles, e.g. infoboxes and categories, as
well as other sources. YAGO2 also supports spatial and
temporal dimensions for facts at the core of its system.
One of the main differences between DBpedia and
YAGO in general is that DBpedia tries to stay very
close to Wikipedia and provide an RDF version of its
content. YAGO focuses on extracting a smaller number
of relations compared to DBpedia to achieve very high
precision and consistent knowledge. The two knowledge bases offer different type systems: whereas the
DBpedia ontology is manually maintained, YAGO is

backed by WordNet and Wikipedia leaf categories.
Due to this, YAGO contains many more classes than
DBpedia. Another difference is that the integration of
attributes and objects in infoboxes is done via mappings
in DBpedia and, therefore, by the DBpedia community
itself, whereas this task is facilitated by expert-designed
declarative rules in YAGO2.

The two knowledge bases are connected, e.g. DBpedia
offers the YAGO type hierarchy as an alternative to
the DBpedia ontology and sameAs links are provided
in both directions. While the underlying systems are
very different, both projects share similar aims and
positively complement and influence each other.

8.2. Knowledge Extraction from Wikipedia

Since its official start in 2001, Wikipedia has always
been the target of automatic extraction of information
due to its easy availability, open license and encyclopedic knowledge. A large number of parsers, scraper
projects and publications exist. In this section, we restrict ourselves to approaches that are either notable, recent or pertinent to DBpedia. MediaWiki.org maintains
an up-to-date list of software projects49, who are able to
process wiki syntax, as well as a list of data extraction
extensions50 for MediaWiki.

JWPL (Java Wikipedia Library, [49]) is an open-
source, Java-based API that allows to access information provided by the Wikipedia API (redirects, cate-
gories, articles and link structure). JWPL contains a
MediaWiki Markup parser that can be used to further
analyze the contents of a Wikipedia page. Data is also
provided as XML dump and is incorporated in the lexical resource UBY51 for language tools.

Several different approaches to extract knowledge
from Wikipedia are presented in [37]. Given features

49http://www.mediawiki.org/wiki/Alternative_

parsers

50http://www.mediawiki.org/wiki/Extension_

Matrix/data_extraction

51http://www.ukp.tu-darmstadt.de/data/

48http://www.mpi-inf.mpg.de/yago-naga/yago/

lexical-resources/uby/

like anchor texts, interlanguage links, category links
and redirect pages are utilized e.g. for word-sense disambiguations or synonyms, translations, taxonomic relations and abbreviation or hypernym resolution, re-
spectively. Apart from this, link structures are used to
build the Wikipedia Thesaurus Web service52. Additional projects that exploit the mentioned features are
listed on the Special Interest Group on Wikipedia Mining (SIGWP) Web site53.

An earlier approach to improve the quality of the
infobox schemata and contents is described in [47].
The presented methodology encompasses a three step
process of preprocessing, classification and extraction.
During preprocessing refined target infobox schemata
are created applying statistical methods and training
sets are extracted based on real Wikipedia data. After
assigning a class and the corresponding target schema
(classification) the training sets are used to extract target infobox values from the documents text applying
machine learning algorithms.

The idea of using structured data from certain
markup structures was also applied to other user-driven
Web encyclopedias. In [38] the authors describe their effort building an integrated Chinese Linking Open Data
(CLOD) source based on the Chinese Wikipedia and
the two widely used and large encyclopedias Baidu
Baike54 and Hudong Baike55. Apart from utilizing MediaWiki and HTML Markup for the actual extraction,
the Wikipedia interlanguage links were used to link the
CLOD source to the English DBpedia.

A more generic approach to achieve a better crosslingual knowledge-linkage beyond the use of Wikipedia
interlanguage links is presented in [45]. Focusing on
wiki knowledge bases the authors introduce their solution based on structural properties like similar linkage
structures, the assignment to similar categories and similar interests of the authors of wiki documents in the
considered languages. Since this approach is language-
feature-agnostic it is not restricted to certain languages.
KnowItAll56 is a web scale knowledge extraction
effort, which is domain-independent, and uses generic
extraction rules, co-occurrence statistics and Naive
Bayes classification [11]. Cyc [29] is a large com-

52http://sigwp.org/en/index.php/Wikipedia_

Thesaurus

53http://sigwp.org/en/
54http://baike.baidu.com/
55http://www.hudong.com/
56http://www.cs.washington.edu/research/

knowitall/

mon sense knowledge base, which is now partially
released as OpenCyc and also available as an OWL
ontology. OpenCyc is linked to DBpedia, which provides an ontological embedding in its comprehensive
structures. WikiTaxonomy [40] is a large taxonomy derived from categories in Wikipedia by classifying categories as instances or classes and deriving a subsumption hierarchy. The KOG system [46] refines existing
Wikipedia infoboxes based on machine learning techniques using both SVMs and a more powerful jointinference approach expressed in Markov Logic Net-
works. KYLIN [47] is a system which autonomously
extracts structured data from Wikipedia and uses selfsupervised linking. Auer et al. [3] introduced an infobox extraction approach for Wikipedia, which later
became the DBpedia project.

9. Conclusions and Future Work

In this system report, we presented an overview on
recent advances of the DBpedia community project.
The technical innovations described in this article included in particular: (1) the extraction based on the
community-curated DBpedia ontology, (2) the live synchronisation of DBpedia with Wikipedia and DBpedia
mirrors through update propagation, and (3) the facilitation of the internationalisation of DBpedia. As a result,
we demonstrated that in the past four years DBpedia
matured and improved significantly in terms of cover-
age, usability, and data quality.

With DBpedia, we also aim to provide a proof-
of-concept and blueprint for the feasibility of largescale knowledge extraction from crowd-sourced content repositories. There are a large number of further
crowd-sourced content repositories and DBpedia already had an impact on their structured data publishing
and interlinking. Two examples are Wiktionary with
the Wiktionary extraction [19] meanwhile becoming
part of DBpedia and LinkedGeoData [41], which aims
to implement similar data extraction, publishing and
linking strategies for OpenStreetMaps.

In the future, we see in particular the following di-

rections for advancing the DBpedia project:

Multilingual data integration and fusion. An area,
which is still largely unexplored is the integration and
fusion between different DBpedia language editions.
Non-English DBpedia editions comprise a better and
different coverage of local culture. When we are able to
precisely identify equivalent, overlapping and complementary parts in different DBpedia language editions,

Lehmann et al. / DBpedia

we can reach significantly increased coverage. On the
other hand, comparing the values of a specific property between different language editions will help us
to spot extraction errors as well as wrong or outdated
information in Wikipedia.

Community-driven data quality improvement. In the
future, we also aim to engage a larger community of
DBpedia users in feedback loops, which help us to
identify data quality problems and corresponding deficiencies of the DBpedia extraction framework. By constantly monitoring the data quality and integrating improvements into the mappings to the DBpedia ontology
as well as fixes into the extraction framework, we aim to
demonstrate that the Wikipedia community is not only
capable of creating the largest encyclopedia, but also
the most comprehensive and structured knowledge base.
With the DBpedia quality evaluation campaign [48] we
were making a first step in this direction.

Inline extraction. Currently DBpedia extracts information primarily from templates. In the future, we
envision to also extract semantic information from
typed links. Typed links is a feature of Semantic Me-
diaWiki, which was backported and implemented as
a very lightweight extension for MediaWiki57. If this
extension is deployed at Wikipedia installations, this
opens up completely new possibilities for more finegrained and non-invasive knowledge representations
and extraction from Wikipedia.

Collaboration between Wikidata and DBpedia.
While DBpedia provides a comprehensive and current
view on entity descriptions extracted from Wikipedia,
Wikidata offers a variety of factual statements from
different sources and dates. One of the richest sources
of DBpedia are Wikipedia infoboxes, which are structured but at the same time heterogeneous and nonstandardized (thus making the extraction error prone in
certain cases). The aim of Wikidata is to populate infoboxes automatically from a centrally managed, highquality fact database. In this regard, both projects complement each other and there are several ongoing collaboration activities. In future versions, DBpedia will
include more raw data provided by Wikidata and add
services such as Linked Data/SPARQL endpoints, RDF
dumps, linking and ontology mapping for Wikidata.

Feedback for Wikipedia. A promising prospect is that
DBpedia can help to identify misrepresentations, errors
and inconsistencies in Wikipedia. In the future, we plan

57http://www.mediawiki.org/wiki/Extension:

LightweightRDFa

to provide more feedback to the Wikipedia community
about the quality of Wikipedia. This can, for instance,
be achieved in the form of sanity checks, which are
implemented as SPARQL queries on the DBpedia Live
endpoint, which identify data quality issues and are executed in certain intervals. For example, a query could
check that the birthday of a person must always be before the death day or spot outliers that differ significantly from the range of the majority of the other val-
ues. In case a Wikipedia editor makes a mistake or typo
when adding such information to a page, this could be
automatically identified and provided as feedback to
Wikipedians.

Integrate DBpedia and NLP. Despite recent advances
(cf. Section 7), there is still a huge potential for employing Linked Data background knowledge in various Natural Language Processing (NLP) tasks. One
very promising research avenue in this regard is to
employ DBpedia as structured background knowledge
for named entity recognition and disambiguation. Cur-
rently, most approaches use statistical information such
as co-occurrence for named entity disambiguation.
However, co-occurrence is not always easy to determine (depends on training data) and update (requires re-
computation). With DBpedia and in particular DBpedia
Live, we have comprehensive and evolving background
knowledge comprising information on the relationship
between a large number of real-world entities. Conse-
quently, we can employ this information for deciding
to what entity a certain surface form should be mapped.

Acknowledgment

We would like to thank and acknowledge the support
of the following people and organisations to DBpedia:

 all Wikipedia contributors
 all DBpedia Mappings Wiki contributors
 OpenLink Software for providing and maintaining
the server infrastructure for the main DBpedia
endpoint.

 Kingsley Idehen for SPARQL and Linked Data

hosting and community support

 Christopher Sahnwaldt for DBpedia development

and release management

 Claus Stadler for DBpedia development
 Paul Kreis for DBpedia development
 people who helped contributing data for certain

parts of the article:
 Instance Data Analysis: Volha Bryl (working at

University of Mannheim)

 Sindice analysis: Stphane Campinas, Szymon
Danielczyk, and Gabriela Vulcu (working at
DERI)
 Freebase: Shawn Simister, and Tom Morris

(working at Freebase)

This work was supported by grants from the European Unions 7th Framework Programme provided for
the projects LOD2 (GA no. 257943), GeoKnow (GA
no. 318159) and Dicode (GA no. 257184).

Appendix

Table 16

List of namespace prefixes.

Prefix

Namespace

dbo
dbp
dbr
dbr-de
dc
foaf
geo
georss
ls
lx
owl
rdf
rdfs
skos
sptl
xsd

http://dbpedia.org/ontology/
http://dbpedia.org/property/
http://dbpedia.org/resource/
http://de.dbpedia.org/resource/
http://purl.org/dc/elements/1.1/
http://xmlns.com/foaf/0.1/
http://www.w3.org/2003/01/geo/wgs84 pos#
http://www.georss.org/georss/
http://spotlight.dbpedia.org/scores/
http://spotlight.dbpedia.org/lexicalizations/
http://www.w3.org/2002/07/owl#
http://www.w3.org/1999/02/22-rdf-syntax-ns#
http://www.w3.org/2000/01/rdf-schema#
http://www.w3.org/2004/02/skos/core#
http://spotlight.dbpedia.org/vocab/
http://www.w3.org/2001/XMLSchema#
