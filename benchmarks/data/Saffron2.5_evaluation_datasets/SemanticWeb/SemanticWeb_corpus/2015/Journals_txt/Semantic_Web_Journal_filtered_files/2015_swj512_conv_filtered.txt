Semantic Web 0 (2014) 10
IOS Press

Link Traversal Querying for a Diverse
Web of Data

Editor(s): Soren Auer, University of Bonn, Germany
Solicited review(s): Saeedeh Shekarpour, AKSW, University of Leipzig, Germany; Gunter Ladwig, Karlsruhe Institute of Technology,
Germany; Luciano Serafini, Fondazione Bruno Kessler, Italy

Jurgen Umbrich a,, Aidan Hogan b, Axel Polleres a and Stefan Decker c
a Vienna University of Economics and Business; Welthandelsplatz 1; 1020 Vienna; Austria
E-mail: jueumb@gmail.com, axel.polleres@wu.ac.at
b Dept. of Computer Science; Universidad de Chile; Blanco Encalada 2120; Santiago; Chile
E-mail: ahogan@dcc.uchile.com
c INSIGHT @ NUI Galway; National University of Ireland, Galway; Ireland
E-mail: stefan.decker@deri.org

Abstract. Traditional approaches for querying the Web of Data often involve centralised warehouses that replicate remote data.
Conversely, Linked Data principles allow for answering queries live over the Web by dereferencing URIs to traverse remote data
sources at runtime. A number of authors have looked at answering SPARQL queries in such a manner; these link-traversal based
query execution (LTBQE) approaches for Linked Data offer up-to-date results and decentralised (i.e., client-side) execution,
but must operate over incomplete dereferenceable knowledge available in remote documents, thus affecting response times and
recall for query answers. In this paper, we study the recall and effectiveness of LTBQE, in practice, for the Web of Data.
Furthermore, to integrate data from diverse sources, we propose lightweight reasoning extensions to help find additional answers.
From the state-of-the-art which (1) considers only dereferenceable information and (2) follows rdfs:seeAlso links, we propose
extensions to consider (3) owl:sameAs links and reasoning, and (4) lightweight RDFS reasoning. We then estimate the recall of
link-traversal query techniques in practice: we analyse a large crawl of the Web of Data (the BTC11 dataset), looking at the
ratio of raw data contained in dereferenceable documents vs. the corpus as a whole and determining how much more raw data
our extensions make available for query answering. We then stress-test LTBQE (and our extensions) in real-world settings using
the FedBench and DBpedia SPARQL Benchmark frameworks, and propose a novel benchmark called QWalk based on random
walks through diverse data. We show that link-traversal query approaches often work well in uncontrolled environments for
simple queries, but need to retrieve an unfeasible number of sources for more complex queries. We also show that our reasoning
extensions increase recall at the cost of slower execution, often increasing the rate at which results return; conversely, we show
that reasoning aggravates performance issues for complex queries.

Keywords: Linked Data, SPARQL, RDFS, OWL, Semantic Web, RDF, Web of Data, Live Querying, Reasoning

1. Introduction

A rich collection of RDF data has been published on
the Web as Linked Data, by governments, academia,

*Corresponding author. E-mail: jueumb@gmail.com

industry, communities and individuals alike [39]. The
resulting collective of interlinked contributions from
a wide variety of Linked Data publishers has been
dubbed the Web of (Linked) Data: a novel corpus of
structured data distributed across the entire Web, described using the Semantic Web standards and made
available under the Linked Data principles.

1570-0844/14/$27.50  2014  IOS Press and the authors. All rights reserved

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

An open challenge is how to query this novel Web
of Data in an effective manner [6]. SPARQL [54]
the W3C standardised RDF query languageprovides
a powerful declarative means to formulate structured
queries over RDF data. However, processing SPARQL
queries over the Web of Data broaches many technical
challenges. Traditional centralised approaches cache
information from the Web of Data in local optimised
indexes, executing queries over the replicated content.
However, maintaining up-to-date coverage of a broad
selection of the Web of Data is exceptionally costly;
centralised caches of the Web of Data must settle for
either limited coverage and/or for indexing some out-
of-date information [43, 63, 66]. Users of centralised
query engines will thus often encounter stale or missing results with respect to the current Web of Data [66].

Conversely, per Linked Data principles, the URIs
that name resources (often) map through HTTP to the
physical location of structured data about them (called
dereferencing). The Web of Data itself can thus be
viewed as a scale-free, decentralised database, consisting of millions of Web documents indexing structured data [37, 39]. Furthermore, agents can traverse
and navigate the Web of Data through typed RDF
links to discover related information [39,  4.5].

This perspective suggests that queries can be answered directly over the Web of Data using HTTP,
where a number of live querying approaches have recently been proposed that take a (SPARQL) query,
find query-relevant sources on the Web of Data, and
dynamically retrieve data from these sources for runtime query processing. In one such approach, Hartig
et al. [34] followed the view of the Web of Data as a
global database and proposed to rely on dereferenceable URIs and RDF links for discovering sources relevant to answer a SPARQL query. Later work by Hartig [32] calls this idea Link Traversal Based Query
Execution (which we abbreviate to LTBQE).

However,

in the LTBQE approach, each HTTP
lookup can take seconds to yield content, many such
lookups may be required, and subsequent lookups to
the same remote server may need to be artificially delayed to ensure polite access (i.e., to avoid inadvertent denial-of-service attacks). In the live querying sce-
nario, remote sources are accessed while a user is waiting for answers to their queries: thus response times
are often (necessarily) much slower when compared
with centralised query engines operating over locally
replicated content. Thus, a core challenge for LTBQE

is to retrieve a minimal number of relevant sources
while maximising the number of answers returned. Re-
latedly, the success of LTBQE is premised on the assumption that query relevant data about a resource can
be found in its dereferenced document; as we show
later, this assumption only partially holds.

Herein, we look at the performance and recall of LTBQE in practice. We also propose that lightweight reasoning extensionsspecifically relating to owl:sameAs
and RDFS semanticscan help to squeeze additional answers from sources and to find additional
query-relevant sources on the diverse Web of Data.
We apply analysis over a large sample of the Web
of Data to get insights into what ratio of raw data is
available to LTBQE through dereferenceable principles vs. raw data in the entire corpus; we also analyse to what extent our proposed extensions make additional query-relevant data available. We then apply
LTBQE and our extensions to three different SPARQL
query benchmarks to stress-test how the approaches
work in real-world, uncontrolled settings. In general,
we find that LTBQE works best for simple queries that
require traversing a handful ( 100) sources. We
also show that our reasoning extensions often help to
find additional answers at the cost of increased query
time, but can run into trouble when accessing data
from domains such as DBpedia (which has a high fanout of owl:sameAs and schema level links) and can exacerbate performance issues with complex queries.

Paper contributions and structure. This paper extends previous work [64] where we first introduced
our reasoning extensions. Herein, we propose new
mechanisms for collecting schema data, test additional
benchmarks, and greatly extend discussion through-
out. This paper is then structured as follows:

 2 We first present some background work in the

area of Linked Data querying and reasoning.

 3 We present some formal preliminaries for RDF,

Linked Data, SPARQL, RDFS and OWL.

 4 We reintroduce the LTBQE approach using con-

crete HTTP-level methods.

 5 We introduce LiDaQ (Linked Data Query en-
gine): our implementation of LTBQE with novel
reasoning extensions and optimisations.

 6 We analyse a crawl of 7.4 m RDF/XML documents from the Web of Data (BTC11), looking
at issues of dereferenceability.

 7 We survey SPARQL benchmarks and how livequerying papers have evaluated their works. We
then propose a new benchmark called QWalk.

 8 We test LTBQE and our reasoning extensions for

three query benchmarks in native Web settings.

 9 We conclude with a summary of contributions

and remarks on future directions.

often index millions of documents, they require large
amounts of resources to run. In particular, maintaining
a local, optimal, up-to-date index with good coverage
of the Web of Data is a Sisyphean task.

Unlike these materialised approaches, the LTBQE
approach and our extensions do not require all content
to be indexed locally prior to query execution.

2. Background and Related Work

2.2. SPARQL Federation

Our work relates to research on querying over RDF
data; more precisely, we focus on executing SPARQL
queries over the Web of Data in a manner adhering to
the Linked Data principles. A comprehensive and recent overview of existing approaches to query Linked
Data was published by Hose et al. [42]. We similarly classify relevant query approaches into three categories (1) materialised systems and data warehouses,
(2) systems that federate SPARQL engines and (3)
live query approaches. Although our own work falls
into the third category, in order to provide a broader
background, in this section we also summarise developments in the first two categories. Additionally, we
briefly remark on hybrid proposals that combine different methods, as well as proposals for new languages
(aside from SPARQL) to query/navigate the Web of
Data. Finally, since our extensions for LTBQE further
relate to research on reasoning over Linked Data, we
also cover some background in this area.

2.1. Materialised Systems

Materialised query-engines use a crawler or other
data-acquisition methods to replicate (i.e., materalise)
remote Web of Data content into a local quad store
over which queries are executed. Such services include
FactForge [13]1 (which uses BigOWLIM [12]),
LOD Cache2 and Sindices Semantic Web Index [50]3 (which both use Virtuoso [22]).

The typical goals of such materialised engines are
to maintain broad and up-to-date coverage of the Web
of Data and to process SPARQL queries efficiently.
These objectives are (partially) met using distribution
techniques, replication, optimised indexes, compression techniques, data synchronisation strategies, and
so on [12, 22, 30, 50]. Still, given that such services

1http://factforge.net/sparql
2http://lod.openlinksw.com/sparql
3http://sparql.sindice.com/

Given the recent spread of independently operated
SPARQL endpoints on the Web of Data hosting various closed datasets with varying degrees of overlap4,
federated SPARQL querying is enjoying growing attention in the research community. The core idea is
to execute queries over a federation of endpoints: to
split an input query, send the relevant sub-queries to
individual (and possibly remote) endpoints in situ, and
subsequently merge and process the final result set.

A primary challenge for federated SPARQL engines
is to decide which parts of a query are best routed
to which endpoint. Many such engines rely on service descriptions or catalogues, which describe the
contents of remote endpoints. One of the earliest such
works (predating SPARQL by over three years) was by
Stuckenschmidt et al. [60], who proposed summarising the content of distributed RDF repositories using schema paths (non-empty property chains). More
recently, a variety of proposals have looked at using service descriptions or catalogues to enable routing in a federated scenario, including DARQ [55],
SemWIQ [46], SPLENDID [25], ANAPSID [1], etc.
Instead of relying (solely) on pre-computed service descriptions or catalogues, FedX [59] and SPLENDID
(also) probe SPARQL endpoints with ASK sub-queries
to test if they have relevant information.

New federation features were also introduced with
SPARQL 1.1: the SERVICE feature invokes remote endpoints and the VALUES feature can be used to ship
batches of intermediate bindings to an endpoint [28].
The SPARQL-DQP system [4] has investigated use of
these SPARQL 1.1 federated features.

However, in recent work, we analysed a broad range
of SPARQL endpoints available on the Web [5]. We
found that half of the endpoints listed in the public
datahub.io catalogue are no longer up, and that many
endpoints have reliability issues. This work thus calls

4http://www4.wiwiss.fu-berlin.de/lodcloud/state/

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

into question the current practicality of federation as a
solution for querying the open Web.

Like the LTBQE approach and our proposed exten-
sions, federated SPARQL engines may involve retrieving content from remote sources at runtime. However,
unlike federated SPARQL, LTBQE operates over raw
data sources on the level of HTTP, and does not require
the availability of SPARQL interfaces.

2.3. Live Linked Data Querying

Live querying approaches access raw data sources
at runtime to dynamically select, retrieve and build
a dataset over which SPARQL queries can be evalu-
ated. Ladwig & Tran [44] identify three categories of
such query evaluation approaches: (1) top-down, (2)
bottom-up, and (3) mixed strategy .

Top-down evaluation determines the query relevant
sources before the actual query execution using knowledge about the available sources stored in a sourceselection index. These source-selection indexes can
vary from simple inverted-index structures [47, 50], to
query-routing indexes [61], schema-level indexes [60],
and lightweight hash-based structures [65].

The bottom-up query evaluation strategy involves
discovering query-relevant sources on-the-fly during
the evaluation of queries. The LTBQE approach [17,
29,3335] we extend herein falls into this category (we
define LTBQE in Section 4). The unique challenges for
such an approach are (1) to find as many query-relevant
sources as possible to improve recall of answers; (2) to
conversely minimise the amount of sources accessed
to avoid traffic and slow query-response times; (3) to
optimise query execution in the absence of typical selectivity estimates, etc. [32,34]. In this paper, we focus
on the first two challenges.

As its name suggests, the third strategy, the mixed
approach, combines top-down and bottom-up tech-
niques. This strategy uses (in a top-down fashion)
some knowledge about sources to map query terms
or query sub-goals to sources which can contribute
answers, then discovering additional query relevant
sources using a bottom-up approach [44].

2.4. Hybrid and Navigational Query Engines

A mature Linked Data query service could require
a hybrid strategy that combines complementary ap-
proaches: for example to use a materialised approach
for static datasets, a federated approach for dynamic

datasets where a first-party SPARQL endpoint is avail-
able, or a live approach for dynamic datasets that have
no SPARQL endpoint [43]. A number of works have
investigated such combinations on a variety of levels
(see, e.g., [37, 45, 66]). In previous works we combined LTBQE with a materialised approach for getting
fresher SPARQL query answers from the Web than the
materialised indexes could offer but at a fraction of
the runtime of pure LTBQE [66]. We believe that live
query approaches, such as LTBQE, are most useful in
combination with other query paradigms.

Some authors have also questioned whether or
not SPARQL is the only language needed to query
the Web of Data [6]. There have also been a number of proposals to extend SPARQL with regular
expressions that capture navigational patterns5, including work by Alkhateeb et al. [2], Perez et al.s
nSPARQL language [53], and Fionda et al.s NautiLOD proposal [23]. Such work goes beyond pure
SPARQL querying, but perhaps touches upon some of
the broader potential of querying and consuming the
Web of Data in a declarative manner.

2.5. Reasoning over Web Data

In this paper, we propose lightweight reasoning extensions for LTBQE, which leverage (some of) the semantics of the RDFS and OWL standards to perform
inferencing and find additional answer and query relevant sources. We now cover some related works in the
area of RDFS/OWL reasoning over the Web of Data.

We investigate a terse profile of reasoning targeted
for Web data. Similarly, Glimm et al. [24] surveyed
the use of RDFS and OWL features in a large crawl
of the Web of Data (viz., BTC11), applying PageRank
over documents and summating the rank of all documents using each feature. They found that RDF(S)
features were the most prominently used. From OWL,
owl:sameAs occurred most frequently, though features
like owl:FunctionalProperty had higher ranks.

With respect to scalable rule-based reasoning over
RDFS (and OWL), a number of authors have proposed
separating schema data (aka. terminological data or
T-Box) from instance data (aka. assertional data or
A-Box) during inferencing [40, 67, 68]. The core assumptions are that the volume of schema data is much
smaller than instance data, that schema data are fre-

5SPARQL 1.1 includes a similar notion called property paths [28].

quently accessed during reasoning, that schema data
are more static than instance data, and that schemalevel inferences do not depend on instance data. Where
these assumptions hold, the schema data can be separated from the main body of data and compiled into
an optimised form in preparation for reasoning over
the bulk of instance data. We use similar techniques
herein when computing RDFS inferences: we consider
schema data separately from instance data.

Aside from scalability, the freedom of the Web
where anyone can say anything (almost) anywhere
raises concerns about the trustworthiness of data for
automated inferencing. On a schema level, for exam-
ple, various obscure documents on the Web of Data
make nonsensical definitions that would (naively) affect reasoning across all other documents [16]. Numerous authors have proposed mechanisms to incorporate notions of provenance for schema data into the
inferencing process. One such procedure, called authoritative reasoning, only considers the schema definitions for a class or property term that are given in
its respectively dereferenceable document [16, 18, 40].
We later use authoritative reasoning to avoid the unwanted effects of third-party schema contributions during RDFS reasoning. Delbru et al. [20] propose another solution called context-dependent reasoning (or
quarantined reasoning), where a closed scope is defined for each document being reasoned over, incorporating only the document itself and other documents it (recursively) imports or links, thus excluding the claims of arbitrary Web documents. We later
use a similar import mechanism to dynamically collect
schemata from the Web during RDFS reasoning.

Our extensions involving owl:sameAs semantics do
not directly involve schema data, but rather look at resolving coreferent resources in the corpus. Various authors have looked specifically at the use and the quality
of use of owl:sameAs on the Web of Data [21, 27, 41].
Halpin et al. [27] look at the semantics and quality of owl:sameAs links in Linked Data. Manually inspecting five hundred owl:sameAs relations sampled
from the Web of Data, they estimated an accuracy
for owl:sameAs linkswhere sameness could be confidently asserted for the sampled relationsat around
51% (21%), but found that judges often disagreed.
We later conducted a similar manual inspection of one
thousand owl:sameAs relations, where we asked a different questionis there any difference between these
two resources to confirm that they are not the same?
and where we estimated a respective (and much more

optimistic) precision of 97.2% [41]. We do not tackle
issues pertaining to the quality of owl:sameAs relations
in this particular work, but acknowledge this as an orthogonal challenge for Linked Data [27, 41].

Finally, we are not the first work to look at incorporating reasoning into SPARQL querying over the Web
of Data. Various materialised approaches for querying Linked Data have incorporated forward-chaining
rule-based reasoners to find additional answers, including the SAOR reasoner [40] for YARS2 (which we
use later), Sindices context-dependent reasoning [20],
Factforge [13], etc. In terms of reasoning for top-down
querying systems, Li and Heflin [47] also use reasoning techniques to find additional results through query
rewriting and bottom-up inferencing.

2.6. Novelty of Present Work

We briefly highlight our novelty. First and foremost,
we evaluate the bottom-up LTBQE approach using various benchmarksall in an uncontrolled, real-world
settingto see what kinds of practical expectations
one can have for answering queries directly over the
Web of Data. Second, we propose and evaluate reasoning extensions for LTBQE to find additional sources
on-the-fly and to generate further answers. To the best
of our knowledge, no other work has looked at evaluating live querying approaches over diverse sources live
on the Web, nor has any other work looked at the benefits of incorporating reasoning techniques into linktraversal querying techniques for the Web of Data.

3. Preliminaries

In this section, we cover some necessary preliminaries and notation relating to RDF ( 3.2), Linked Data
( 3.3), SPARQL ( 3.4) and RDFS & OWL ( 3.5).
Before we continue, however, we introduce a running
example used to explain later concepts.

3.1. Running Example

Figure 1 illustrates an RDF (sub-)graph taken from
five real-world interlinked documents on the Web of
Data.6 Prefixes for the abbreviated CURIE names
used in this section, and throughout the paper, are
available in Appendix C. The graph models informa-

6As last accessed on 2013-07-02.

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

Fig. 1. Snippets taken from five documents on the Web of Data. Individual documents are associated with individual background panes. The
URI of each document is attached to its pane with a shaded tab. The same resources appearing in different documents are joined using bridges.
Links from URIs to the documents they dereference to are denoted with dashed links. RDF triples are denoted following usual conventions
within their respective document.

Fig. 2. Snippet from the Friend Of A Friend (FOAF) Ontology: a schema document on the Web of Data. External terms have dashed lines.

tion about two real-world persons and a paper that
they coauthored together. One author is identified
by the URIs oh:olaf and dblpA:Olaf_Hartig and the
other by cb:chris and dblpA:Christian_Bizer. The URI
dblpP:HartigBF09 refers to the publication both authors
share. The five documents are as follows:
ohDoc:, cbDoc: refer to the personal FOAF profile doc-

uments that each author created for themselves;

dblpADoc:Olaf..., dblpADoc:Chris... refer to information
exported from the DBLP Computer Science Bibliography7 for each author, including a publication list;

dblpPDoc:HartigBF09 provides information about

the

co-authored paper exported from DBLP.

Each document is available as RDF/XML on the Web.
Dereferenceable relationships between resources and

documents are highlighted in Figure 1. Excluding
cbDoc: (which must be looked up directly), the other
four documents can be retrieved by dereferencing the
URI of their main resource; for example, dereferencing oh:olaf over HTTP returns the document ohDoc:
describing said resource.

In addition, Figure 2 illustrates a subset of RDFS
definitions in a schema document extracted from the
real-world FOAF ontology. Although left implicit, all
terms in the foaf: namespace (including the predicates
and values for rdf:type represented in Figure 1) dereference to this document. The relations between classes
and properties shown in this document are well defined
(using model-theoretic semantics) by the RDFS standard and can be used for automated inference [38].

3.2. RDF

7http://www.informatik.uni-trier.de/~ley/db/

In order to define our methods later, we must first

provide notation for RDF [38].

ohDoc:oh:olafOlaf Hartighttp://...cb:chriscb:chrishttp://...Chris BizerdblpA:Christian_BizerdblpP:HartigBF09dblpA:Olaf_HartigdblpA:Olaf_Hartigowl:sameAsfoaf:namefoaf:imgOlaf Hartigrdfs:labelowl:sameAsfoaf:namefoaf:depictiondblpP:HartigBF09foaf:knowsfoaf:makerdblpP:HartigBF09"2009"^^xsd:gYearfoaf:Agentrdf:typedcterms:issueddblpADoc:Olaf_HartigdblpADoc:Christian_BizercbDoc:rdfs:seeAlsocbDoc:dbpedia:Berlinfoaf:based_neardblpA:Olaf_HartigdblpA:Christian_Bizerfoaf:makerdblpA:Christian_Bizerfoaf:makerdblpPDoc:HartigBF09dereferencesdereferencesdereferencesfoaf:makerChristian Bizerfoaf:namefoafSpec:foaf:imgfoaf:depictionfoaf:Personfoaf:Agentfoaf:Imagefoaf:namegeo:SpatialThingrdfs:labelfoaf:based_nearrdfs:domainrdfs:rangerdfs:subPropertyOf rdfs:subPropertyOf rdfs:subClassOf rdfs:subClassOf rdfs:rangerdfs:rangeJ. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

Definition 1 (RDF Term, Triple and Graph).
The set of RDF terms consists of the set of URIs U,
the blank-nodes B and literals L. An RDF triple t :=
(s, p, o) is an element of the set G := UB  U 
UBL (where, e.g., UB is a shortcut for U B). Here
s is called subject, p predicate, and o object. An RDF
graph G  G is a finite set of RDF triples. We use the
functions subj(G), pred(G), obj(G), terms(G) to resp.
denote the set of all terms appearing in the subject,
predicate, object and all positions of triples in G.

3.3. Linked Data

The four Linked Data principles [10] are as follows:

LDP1: URIs are used to identify things
LDP2: URIs should be dereferenceable through HTTP
LDP3: Useful RDF content should be provided when

URIs are dereferenced

LDP4: Links should be offered within that content

We now provide some (time-invariant) Linked Data
notation that we use later for defining our methods.
This notation encapsulates the idea of the Web of Data
as a database, where dereferencing URIs acts as a
lookup that returns RDF data about those URIs.
Definition 2 (Data Source and Linked Dataset).
We define the http-download function get : U  2G
as the mapping from URIs to RDF graphs provided by
means of HTTP lookups which directly return status
code 200 OK and data in a suitable RDF format. We
define the set of (RDF) data sources S  U as the set
of URIs S := {s  U : get(s) 6= } (i.e., URIs that
return RDF content with 200 Okay). We define a Linked
Dataset as   get (i.e., a finite set of pairs (s, get(s))
such that s  S). The global RDF graph presented
by a Linked Dataset is denoted as

merge() := ]

(u,G)

where the operator ] denotes the RDF merge of RDF
graphs, which ensures that blank node labels in the input graphs are kept distinct in the merged graph [38].
Example 1. Taking Figure 1, e.g., get(ohDoc:) =
{(oh:olaf:, foaf:name, "Olaf Hartig"), . . .}, an RDF
graph containing the five triples in that document.
However, get(oh:olaf) =  since it does not return
a 200 Okay (redirects are supported in the next step).
Thus, ohDoc:  S whereas oh:olaf / S. If we denote Figure 1 as the Linked Dataset , we can say

that  = {ohDoc:, get(ohDoc:, . . .}, containing five

(URI, RDF-graph) pairs. Then, merge() is the set of
all 17 RDF triples shown in Figure 1.
Definition 3 (Dereferencing RDF).
A URI may redirect to another URI with a 30x response
code. We denote this function as redir : U  U, which
first strips the fragment identifier of a URI (if present)
and would then map a URI to its redirect target or to
itself in the case of failure (e.g., where no redirect ex-
ists). We denote the fixpoint of redir as redirs, denoting traversal of a number of redirects (a limit may be
imposed to avoid cycles). We then denote dereferencing by the composition deref := get  redirs, which
maps a URI to an RDF graph retrieved with status
code 200 OK after following redirects, or which maps
a URI to the empty set in the case of failure. We denote the set of dereferenceable URIs as D := {d 
U : deref(d) 6= }, where S  D and we place no
expectations on what deref(d) returns, other than returning some valid RDF. As a shortcut, we denote by
derefs : 2U  2U2G; U 7 {(redirs(u), deref(u)) |
u  U  D)} the mapping from a set of URIs to the
Linked Dataset it represents by dereferencing all URIs
(only including those in D which return some RDF).
In relation to the formal model of Hartig [33], we
favour concrete HTTP-level methods used for Linked
Data. He models the Web of Linked Data as a triple
W = (D, data, adoc), where our set S is equivalent
to his set D of document IDs, our function get(.) instantiates his (more general) function data(.) for mapping document IDs to RDF graphs, and our function
redirs(.) instantiates his function adoc(.) for partially
mapping URI names to document IDs.
Example 2. Taking Figure 1, oh:olaf redirects to
ohDoc:, denoted redir(oh:olaf) = ohDoc:. No further redirects are possible, and thus redirs(oh:olaf) =
ohDoc:. Dereferencing oh:olaf gives the RDF graph
in the document ohDoc:, where deref(oh:olaf) =

getredirs(oh:olaf) = get(ohDoc:). Instead taking

the URI cb:chris, redir(cb:chris) = cb:chris and
get(cb:chris) = ; this URI is not dereferenceable.
Thus we can say that oh:olaf  D and ohDoc:  D
whereas cb:chris / D.

3.4. SPARQL

We now introduce some concepts relating to the
query language SPARQL [52, 54]. We herein focus
on evaluating simple, conjunctive, basic graph pat-

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

terns (BGPs), where although supported by our im-
plementation, we do not formally consider more expressive parts of the SPARQL language, which
with the exception of non-monotonic features that assume a closed dataset like OPTIONAL in SPARQL and
MINUS/(NOT) EXISTS in SPARQL 1.1 can be layered on
top [52]. In addition, we consider URIs and not IRIs
for convenience with the RDF preliminaries.

Definition 4 (Variables, Triple Patterns & BGPs).
Let V be the set of variables ranging over UBL. A
triple pattern tp := (s, p, o) is an element of the set
Q := VULVUVUL.8 For simplicity, we do not
consider blank-nodes in triple patterns (they could be
roughly emulated by an injective mapping from B to
V). A finite (herein, non-empty) set of triple patterns
Q  Q is called a Basic Graph Pattern, or herein,
simply a query. We use vars(Q)  V to denote the
set of variables in Q. Finally, we may overload graph
notation for queries, where, e.g., terms(Q) returns all
elements of VUL in Q.

In this paper, we only look at evaluating queries representing BGPs. We denote the answers to a query Q
with respect to a Linked Dataset as [[Q]]. We now formally define this notion, where [[Q]] is a set of solutions generated by Q over the merge of graphs in .

Definition 5 (SPARQL solutions).
Call the partial function  : dom()  UL  UBL
a solution mapping with a domain dom()  V. A
solution mapping binds variables in dom() to UBL
and is the identify function for UL. Overloading no-
tation, let  : Q  G and  : 2Q  2G also resp.
denote a solution mapping from triple patterns to RDF
triples, and basic graph patterns to RDF graphs such
that (tp) := ((s), (p), (o)) and (Q) := {(tp) |
tp  Q}. We now define the set of SPARQL solutions
for a query Q over a (Linked) Dataset  as
[[Q]] :={ | (Q)merge()dom() = vars(Q)} .

For brevity, and unlike SPARQL, solutions are herein
given as sets (not multi-sets),
implying a default
DISTINCT semantics for queries, and we assume that
answers are given over the default graph consisting of
the merge of RDF graphs in the dataset.

Example 3. Taking  from Figure 1, let Q be:

SELECT ?maker ?issued WHERE {

dblpP:HartigBF09 foaf:maker ?maker ;

dcterms:issued ?issued . }

Then [[Q]] would be:
?maker

?issued

dblpA:Christian_Bizer

dblpA:Olaf_Hartig

"2009"^^xsd:gYear
"2009"^^xsd:gYear

3.5. RDFS and OWL

In preparation for defining our reasoning extensions
to LTBQE, we now give some preliminaries relating
to RDFS and OWL. Our RDFS rules are the subset of
the DF rules proposed by Munoz et al. [49] that deal
with instance data entailments (as opposed to schemalevel entailments).9 For supporting owl:sameAs, we use
a small subset of OWL 2 RL/RDF rules, given in Table 1, which constitute a partial axiomatisation of the
OWL RDF-Based Semantics relating to ground equal-
ity. Our selection of rules thus support a small subset
of standard RDFS/OWL semantics that we argue to be
important for answering queries over the Web of Data,
and our approach could be generalised to support, e.g.,
OWL 2 RL/RDF rules with some adaptations.
For a ruleset R and dataset , we denote by   R
the fixpoint of applying R against  such that applying
R over  R yields no new inferences. We now define
  R formally in terms of an immediate consequence
operator (reusing some SPARQL notation for brevity).
Definition 6 (Entailment Rules & Least Model).
An entailment rule is a pair r = (Body, Head)
(cf. Table 1) such that Body, Head  Q; and
vars(Head)  vars(Body). The immediate consequences of r for a Linked Dataset  are given as:
Tr() := {(Head) |   [[Body]]} \ merge() .
In other words, Tr() denotes the direct unique inferences from a single application of a rule r against
the merge of RDF data contained in . Let R denote
a finite set of entailment rules. The immediate consequences of R over  are given analogously as:

TR() :=S

rR Tr() .

8SPARQL allows literals in the subject position [54], though such

9We drop implicit typing [49] rules as we allow generalised RDF

patterns cannot match RDF triples as currently defined.

in intermediate inferences.

PRP-SPO1
PRP-DOM
PRP-RNG
CAX-SCO

EQ-SYM
EQ-TRANS
EQ-REP-S
EQ-REP-P
EQ-REP-O

Body
?p1 rdfs:subPropertyOf ?p2 . ?s ?p1 ?o .
?p rdfs:domain ?c . ?s ?p ?o .
?p rdfs:range ?c . ?s ?p ?o .
?c1 rdfs:subClassOf ?c2 . ?s a ?c1 .

Head
?s ?p2 ?o .
?p a ?c .
?o a ?c .
?s a ?c2 .

?x owl:sameAs ?y .
?x owl:sameAs ?y . ?y owl:sameAs ?z .
?s owl:sameAs ?s0 . ?s ?p ?o .
?p owl:sameAs ?p0 . ?s ?p ?o .
?o owl:sameAs ?o0 . ?s ?p ?o .

?y owl:sameAs ?x .
?x owl:sameAs ?z .
?s0 ?p ?o .
?s ?p0 ?o .
?s ?p ?o0 .

RDFS (DF subset) and owl:sameAs (OWL 2 RL/RDF subset) rules

Table 1

This is the union of a single application of all rules in
R over the data applied to the (raw) data in . Fur-
thermore, let   U denote a fresh URI which names
the graph GR of data inferred by R, and let GR0 = .
Now, for i  N, define:

:=  , GR

i )  GR

i+1 := TR(R

n = R

The least model of  with respect to R is R
n for the
least n such that R
n+1; at this stage the closure
is reached and nothing new can be inferred.10 Hence-
forth, we denote this least model with   R. Query
answers including entailments are given by [[Q]]R.

Example 4. Let R denote the set of rules in Table 1.
Also, consider  as the Linked Dataset comprising

ofohDoc:, get(ohDoc:) from Figure 1 and a second

named graph called foafSpec: with the following subset of triples from Figure 2:

foaf:img rdfs:domain foaf:Person ;

rdfs:range foaf:Image ;
rdfs:subPropertyOf foaf:depiction .

foaf:Person rdfs:subClassOf foaf:Agent .

These (real-world) triples can be retrieved by dereferencing a FOAF term; e.g., deref(foaf:img). Now, given
 and R, then GR0 = , GR1 = GR0 TR(R0 ) where, by
applying each rule in R over  once, TR(R0 ) contains
the following triples (abbreviating CURIEs slightly):

10Since our rules are a syntactic subset of Datalog, there is a

unique and finite least model (assuming finite inputs).

oh:olaf foaf:depiction <http. . .> .
oh:olaf a foaf:Person .
<http. . .> a foaf:Image .
dblpA:Olaf owl:sameAs oh:olaf .
dblpA:Olaf foaf:knows cb:chris .
...

#PRP-SPO1
#PRP-DOM
#PRP-RNG
#EQ-SYM
#EQ-REP-S

Subsequently, R1 =   {(, GR1 )}, where  is any
built-in URI used to identify the graph of inferences
and where GR1 contains the unique inferences thus far
(listed above). Thereafter, GR2 = GR1 TR(R1 ), where
TR(R1 ) contains:
oh:olaf a foaf:Agent .
dblpA:Olaf foaf:depiction <http. . .> .
dblpA:Olaf a foaf:Person .
dblpA:Olaf owl:sameAs dblpA:Olaf .
oh:olaf owl:sameAs oh:olaf .
...

#CAX-SCO
#EQ-REP-S
#EQ-REP-S
#EQ-REP-S
#EQ-REP-S

As before, R2 = {(, GR2 )}, where GR2 contains all
inferences collected thus far, and GR3 = GR2 TR(R2 ),
where TR(R2 ) contains:
dblpA:Olaf a foaf:Agent .

#CAX-SCO

This is then the closure since TR(R3 ) = ; nothing
new can be inferred, and so R3 = R4 . And thus we
can say that   R = R3 =   (, GR3 ).

4. Link Traversal Based Query Execution

Having covered some necessary preliminaries, in
the following section, we cover the Link Traversal
Based Query Execution (LTBQE) approach first proposed by Hartig et al. [33, 34] for executing SPARQL
queries over the Web of Data ( 4.1).

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

4.1. Overview of Baseline LTBQE

Given a SPARQL query, the core operation of LTBQE is to identify and retrieve a focused set of queryrelevant RDF documents from the Web of Data from
which answers can be extracted. The approach begins
by dereferencing URIs found in the query itself. The
documents that are returned are parsed, and triples
matching patterns of the query are processed; the URIs
in these triples are also dereferenced to look for further information, and so forth. The process is recursive
up to a fixpoint wherein no new query-relevant sources
are found. New answers for the query can be computed
on-the-fly as new sources arrive. We now formally define an idea of query-relevant documents in the context
of LTBQE. This is similar in principle to the generic
notion of reachability introduced previously [33, 35],
but relies here on concrete HTTP specific operations:
Definition 7 (Query Relevant Sources & Answers).
First let uris() := {u  U | v s.t. (v, u)  }
denote the set of URIs in a solution mapping . Given
a query Q and an intermediate dataset , we define
the function qrel, which extracts from  a set of URIs
that can (potentially) be dereferenced to find further
sources deemed relevant for Q:

qrel(Q, ) := [

First, the process extracts all raw query URIs: UQ =
{dblpP:HartigBF09, foaf:name, foaf:maker}. In the next
stage, the engine dereferences these URIs. Given that
redirs(dblpP:HartigBF09) = dblpPDoc:HartigBF09 &
redirs(foaf:maker) = redirs(foaf:made) = foafSpec:,
dereferencing UQ gives two unique named graphs, viz.:

dblpPDoc:HartigBF09, get(dblpPDoc:HartigBF09) and
foafSpec:, get(foafSpec:). These two named-graphs

comprise Q
mately contribute answers.)

0 . (In fact, only the former graph will ulti-

Second, LTBQE looks to extract additional query
relevant URIs by seeing if any query patterns are
matched in the current dataset. By reference to the
graph dblpPDoc:HartigBF09 in Figure 1, we see that for
the pattern dblpP:HartigBF09 foaf:maker ?author .,
the variable ?author is matched by two unique URIs,
namely dblpA:Christian_Bizer and dblpA:Olaf_Hartig,
which are added to qrel(Q, Q
0 ). Nothing else is
matched. Hence, these two URIs are dereferenced and
the results added to Q

0 to form Q
1 .

LTBQE repeats the above process until no new
sources are found. At the current stage, Q
1 now also
contains the two sources dblpADoc:Christian_Bizer and
dblpADoc:Olaf_Hartig needed to return:

uris()

?authorName

"Christian Bizer"

"Olaf Hartig"

tpQ

[[{tp}]]

To begin the recursive process of finding query-relevant
sources, LTBQE takes URIs in the querydenoted
with UQ := terms(Q)  Uas seeds, and builds
an initial dataset by dereferencing these URIs: Q
0 :=
derefs(UQ). Thereafter, for i  N, define:11

i+1 := derefsqrel(Q, Q

i )  Q

The set of LTBQE query relevant sources for Q is
given as the least n such that Q
n+1, denoted
simply Q. The set of LTBQE query answers for Q is
given as [[Q]]Q, or simply denoted bbQcc.
Example 5. Taking Figure 1, let Q be the following
query looking for the author-names of a given paper:

n = Q

SELECT ?authorName WHERE {

dblpP:HartigBF09 foaf:maker ?author .
?author foaf:name ?authorName . }

11In practice, URIs need only be dereferenced once; i.e., only
i1)UQ) need be dereferenced

i )\(qrel(Q, Q

URIs in qrel(Q, Q
at each stage.

Furthermore, no other query-relevant URIs are
found and so a fixpoint is reached and the process ter-
minates: bbQcc contains the above results.

4.2. Decidability and completeness

The decidability of LTBQEand indeed the decidability of the more general problem of evaluating a
SPARQL query over the Web of Datadepends on
how one scopes the sources of data considered for evaluation and which features of SPARQL are used.

If one considers an infinite Web of Dataaiming
for what Hartig calls Web completeness [33]then
the evaluation of a SPARQL query is not finitely computable in the general case, even if one considers
only the monotonic features of SPARQL [33]: there
are (countably) infinite documents that may contain
relevant data but that cannot be processed in finite
steps. If non-monotonic features of SPARQLsuch as
OPTIONAL, MINUS, etc.are used in the query, then evaluation is not even eventually computable by a nonterminating procedure [33] since, in the general case,
all documents need to be processed before a single

sound solution can be given, making the computation
of each individual solution infinitary.

If one rather considers a reachability condition
whereby, for example, only the query-relevant sources
in an LTBQE sense are considered in-scopefor similar reasons, Hartig [33] shows that queries are still
not finitely computable unless it can be proven that the
number of reachable sources is finite under the given
conditions. This does not hold for the set of query relevant sources given in Definition 7.
Example 6. The following query asks for a general
description of people known by oh:olaf:

SELECT ?s ?p ?o WHERE {

oh:olaf foaf:knows ?s .
?s ?p ?o . }

The initial query-relevant sources (per Definition 7)
are the documents dereferenced from oh:olaf and
foaf:maker. Thereafter, all triples in these documents
will match the open pattern, and thus all URIs in
these documents will be considered as potential queryrelevant links. This will continue recursively, crawling
the entire, potentially infinite Web of Data as reachable from the query URIs. The problem is not limited
to open patterns; take the following query:

SELECT ?o WHERE {

oh:olaf foaf:knows ?s .
?s foaf:knows ?o . }

This would end up crawling the connected Web of
FOAF documents, as are linked together by dereferenceable foaf:knows links.

Partly addressing this problem, Hartig et al. [34] defined an iterator-based execution model for LTBQE,
which rather approximates the answers provided by
Definition 7. This execution model defines an ordering of triple patterns in the query, similar to standard
nested-loop join evaluation. The most selective patterns (those expected to return the fewest bindings)
are executed first and initial bindings are propagated
to bindings further up the tree. Crucially, later triple
patterns are partially bound when looking for queryrelevant sources. Thus, taking the previous example,
the pattern ?s foaf:knows ?o . will never be used
to find query-relevant sources, but rather partiallybound patterns like cb:chris foaf:knows ?o . will be
used. As such, instead of retrieving all possible queryrelevant sources, the iterator-based execution model
uses interim results to apply a more focused traversal

of the Web of Data. This also makes the iterator-based
implementation order-dependent: results may vary depending on which patterns are executed first and thus
answers may be missed. However, it does solve the
problem of traversing too many sources when lowselectivity patterns are present in the query.

Relatedly, Harth and Speiser [29] also consider
an order-dependent version of LTBQE. Similar to
they remark that although the WebHartig [34],
completeness of SPARQL evaluation is useful as a theoretical notion, since the Web of Data cannot be ma-
terialised, more practical but weaker notions of completeness are also necessary. They propose another
two completeness conditions: seed-complete considers
answers from the set of documents that are within a
fixed-length traversal path from the seed URIs in the
query (the authors propose using the number of query
patterns to decide the maximum hops) and query-
reachable-complete considers a closed dataset of documents reachable through the LTBQE process under
some ordering of the query patterns. The authors then
demonstrate how Web-complete and query-reachable-
complete conditions coincide if only certain authoritative triplestriples containing URIs that dereference
to the container documentare considered.

We now give some practical examples as to why
LTBQE (be it order-dependent or order-independent)
cannot be Web-complete in the general case.

No dereferenceable query URIs: The LTBQE approach cannot return results in cases where the query
does not contain dereferenceable URIs. For example,
consider posing the following query against Figure 1:

SELECT * WHERE {

cb:chris ?p ?o . }

As previously explained, the URI cb:chris is not dereferenceable (deref(cb:chris) = ) so the query has no
place to start its traversal from.

Unconnected query-relevant documents: Relating to
reachability, results can be missed where documents
are connected by a join on literals, blank-nodes or
non-dereferenceable URIs. The following query illustrates such a case:

SELECT ?olaf ?name WHERE {
oh:olaf foaf:name ?name .
?olaf foaf:name ?name . }

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

Answers (other than oh:olaf) cannot be reached from
the starting URI oh:olaf because the relevant documents are connected by the literal "Olaf Hartig".
Dereferencing partial
In the general
case, the effectiveness of LTBQE is heavily dependent
on the amount of data returned by the deref(u) func-
tion. In an ideal case, dereferencing a URI u would return all triples mentioning u on the Web of Data. How-
ever, this is not always the case; for example:

information:

SELECT ?s WHERE {

?s owl:sameAs dblpA:Olaf_Hartig . }

This simple query cannot be answered since the triple
oh:olaf owl:sameAs dblpA:Olaf_Hartig . is not accessible by dereferencing dblpA:Olaf_Hartig. The assumption that all RDF available on the Web of Data
about a URI u can be collected by dereferencing u is
clearly idealised; hence, later in Section 6 we will empirically analyse how much the assumption holds in
practice, giving insights into the potential recall of LTBQE on an infrastructural level.

5. LiDaQ: Extending LTBQE with Reasoning

We now present the details of LiDaQ: our proposal
to extend the baseline LTBQE approach with components that leverage lightweight RDFS and owl:sameAs
reasoning in order to improve recall. We first describe
the extensions we propose ( 5.1), and then describe
our implementation of the system ( 5.2).

5.1. LTBQE Extensions

Partly addressing some of the shortcomings of the
LTBQE approach in terms of recall, Hartig et al. [34]
proposed extending the set of query relevant sources
to consider rdfs:seeAlso links, which sometimes overcomes the issue of URIs not being dereferenceable.
In the LiDaQ system, we include this extension and
further propose novel extensions that apply reasoning
over query-relevant sources to squeeze additional answers from these sources, which in turn may lead to
recursively finding additional query-relevant sources.
We now describe, formally define and provide motivating examples for each of the three extensions: following rdfs:seeAlso links, following owl:sameAs links
and applying equivalence inferencing, and collecting
schema information for applying RDFS reasoning.

5.1.1. Following rdfs:seeAlso links:

We first motivate the legacy rdfs:seeAlso extension

with a simple example.
Example 7. Consider executing the following simple query, asking for images of the friends of oh:olaf,
against the data in Figure 1 using baseline LTBQE:

SELECT ?f ?d WHERE {

oh:olaf foaf:knows ?f .
?f foaf:depiction ?d . }

LTBQE first dereferences the content of the query
URI oh:olaf, and then follows and dereferences all
URI bindings for the variable ?f, matching the second query pattern ?f foaf:depiction ?d . over the
retrieved content to find pictures. However, the query
processor needs to follow the rdfs:seeAlso link from
cb:chris (bound to ?f) to cbDoc: since the URI cb:chris
is not dereferenceable (recall that a dashed arrow in
Figure 1 denotes dereferenceability).

Hartig et al. [34] thus proposed to extend LTBQE to

consider rdfs:seeAlso links as follows.
Definition 8 (LTBQE Extension 1: rdfs:seeAlso).
Given a dataset , a set of URIs U and a predicate
URI p, first define:

link(, U, p) := {v  U | u  U s.t.

(u, p, v)  merge()}

which gives the target of links from URIs in U with
the property p in the data of . Next we extend the
qrel(, U) function from Definition 7 to allow for considering links through a predicate p as follows:

qrel(Q, , p) := qrel(Q, )  link, qrel(Q, ), p

Note that qrel(Q, )  qrel(Q, , p). The extension
to follow rdfs:seeAlso links then follows from Definition 7 by replacing qrel(, U) with the extended function qrel(, U, rdfs:seeAlso).

5.1.2. Following and reasoning over owl:sameAs links:
We now motivate the need for owl:sameAs traversal

and reasoning with an example:
Example 8. Consider the following query for Figure 1
asking for friends of oh:olaf that are also co-authors.

SELECT ?f WHERE {

oh:olaf foaf:knows ?f , foaf:maker ?p .
?f foaf:maker ?p }

Baseline LTBQE returns no answers: LTBQE requires
owl:sameAs support to return Chris as an answer (given
by the equivalences for oh:olaf/dblpA:Olaf_Hartig and
cb:chris/dblpA:Christian_Bizer).

We now formalise the details of our extension.

Definition 9 (LTBQE Extension 2: owl:sameAs). We
define an extension of LTBQE to consider owl:sameAs
links and inferences. As before, from Definition 7,
replace qrel(, U) with qrel(, U, owl:sameAs), which
follows owl:sameAs links. Next, let R= denote the set
of rules of the form EQ-* in Table 1. Finally, from
i  R=, such that the
Definition 7, replace Q
owl:sameAs inferences are applied at each step.

i with Q

5.1.3. Incorporating RDFS schemata and reasoning

Finally, we cover our novel extension for RDFS in-

ferencing, starting with a motivating example.
Example 9. Take the following query over Figure 1
asking for the images(s) depicting oh:olaf:

SELECT ?d WHERE {

oh:olaf foaf:depiction ?d . }

From Figure 2, we know that foaf:depiction is a subproperty of foaf:img, and we would thus hope to get
the answer <http://...> from ohDoc:. However, returning this answer requires two thing: (i) retrieving the
RDFS definitions of the FOAF vocabulary; and (ii)
performing reasoning using the first four rules in Table 1. In this case, finding the relevant schema information (the first step) is quite straightforward and can
be done dynamically since the relevant terms (foaf:img
and foaf:depiction) are within the same namespace
and are described by the same dereferenceable docu-
ment. However, consider instead:

SELECT ?d WHERE {

oh:olaf rdfs:label ?d . }

In this case, we know from the FOAF schema that
foaf:name is a sub-property of rdfs:label, and so "Olaf
Hartig" should be an answer. However, no FOAF vocabulary term is mentioned in the query, and so the
FOAF schema will not be in the query-relevant scope.

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

To overcome this, we can provide a static set of schema
information to the query engine as input, or we can
dereference property and class terms mentioned in the
query-relevant data to dynamically retrieve the relevant definitions at runtime.
Definition 10 (LTBQE Extension 3: RDFS). We define an extension of LTBQE to consider RDFS schema
data and a subset of RDFS inferences. Let R denote
rules {PRP-SPO1, PRP-DOM, PRP-RNG, CAX-SCO} in
Table 1 (other rules could be added as necessary).
i  i)  R,
From Definition 7, replace Q
such that inferences are applied at each step. We use
i to denote an auxiliary Linked Dataset containing
schema data at step i.

i with (Q

We are then left to define how i may be acquired,

where we provide three options (ac

).

1. A static corpus of schema data  can be provided

as input, such that a

i := .

2. The class and property terms used in Q

i can be
dereferenced. Letting preds() and o-type() de-
note, respectively, the set of all URIs appearing
as a predicate, and the set of all URIs appearing
as a value for rdf:type (class instance) in , we
can define b

i as:

i := derefspreds(Q

i )

i )  o-type(Q

3. Class and property terms can be dereferenced
and schema-level links followed. For a Linked
Dataset , let imports() denote all URIs appearing as the subject or object of a triple in
 with predicate rdfs:subPropertyOf, rdfs:domain,
rdfs:range or rdfs:subClassOf; or appearing as
the object of owl:imports. Extending b
i as above,
i, and thereafter, for j  N define:
let c

i,0 := b

i,j+1 := derefsimports(c

i,j)  c

i,j

such that links are recursively followed up to a
fixpoint: the least j such that c
i,j+1. We
define c

i as this fixpoint of recursive imports.

i,j = c

The second and third methods involve dynamically
collecting schemata at runtime. The third method of
schema-collection is potentially problematic in that it
recursively follows links, and may end up collecting
a large amount of schema documents (a behaviour we
encounter in evaluation later). However, where, for ex-
ample, class or property hierarchies are split across
multiple schema documents, this recursive process is
required to recreate the full hierarchy.

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

All three extensionsfollowing rdfs:seeAlso links,
following owl:sameAs links & applying owl:sameAs rea-
soning, retrieving RDFS data (using one of three ap-
proaches) & applying RDFS reasoningcan be combined in a straightforward manner. In fact, some answers may only be possible through the combination
of all extensions. We will later explore the effects of
combining all extensions in Section 8.

5.2. LiDaQ Implementation

The LiDaQ prototype (implemented in Java) draws
together a variety of techniques proposed in the literature [34, 36, 45] and has five main components, as depicted in Figure 3.

Query Processor: uses Jena ARQ to parse and process input SPARQL queries and format the output
results.12 We discuss query processing in further
detail below.

Source Selector: decides which query and solution
URIs should be dereferenced and which links
should be followed.

Source Lookup: an adapted version of the LDSpider crawling framework performs the live Linked
Data lookups required for LTBQE. LDSpider respects the robots.txt policy, blacklists typical
non-RDF URI patterns (e.g., .jpeg) and enforces
a half-second politeness delay between two consequential lookups for URIs hosted at the same
pay-level-domain. A per-domain queue is implemented from which a pool of threads polls on a
first-in-first-out basis.13

Local Repository: a custom implementation of an inmemory quad store (similar to [36]) is used to
cache the content of all query relevant data (in-
cluding inferences and schema data), as well as
indexing triple patterns from the query to match
against the data. Triple-pattern listeners match
cached data in a continuous fashion, feeding the
iterators.

Reasoner: the Java-based SAOR reasoner is used to
support rule-based reasoning extensions [16] and
executes inferencing over the local repository.

The query processing algorithm is based on a
nested-loop strategy. During LTBQE, retrieving certain sources may involve high latency. Thus rather

12http://jena.apache.org/documentation/query/
13http://code.google.com/p/ldspider/

Fig. 3. LTBQE architecture diagram.

than blocking while waiting for the result of a particular request, special non-blocking operators are required [34, 45], where we adopt a strategy analogous
to the symmetric-index-hash join [45]. When a dataaccess operator receives a lookup request, it: (i) registers the pattern in a hash-table, (ii) pulls all relevant data, currently found in the local repository, as
bindings into the hash-table, (iii) co-ordinates with the
source-selector to request remote accesses as neces-
sary, and (iv) registers a listener with the local repository that will push new relevant data into the operators hash-table as they arrive. Join operators use these
hash-tables to asynchronously concatenate compatible
tuples and return them to higher operators. By caching
intermediate bindings, this asynchronous model ensures that all relevant data are consumedin a bottom-
up, nested-loop senseas they arrive, no matter when
or from where they arrive [45].

We further investigate some practical optimisations
to minimise the number of query-relevant sources retrieved while maximising results. First, we avoid dereferencing URIs that do not appear in join positions. We
illustrate this with a simple example:
Example 10. Consider the following query issued
against the example graph of Figure 1, asking for
friends of oh:olaf that have some value defined for
foaf:based_near:

SELECT ?f ?fn ?b WHERE {

oh:olaf foaf:knows ?f .
?f foaf:name ?fn .
?f foaf:based_near ?b . }

Assuming the rdfs:seeAlso extension is enabled,
LTBQE will visit ohDoc: binding cb:chris for ?f; and
then visit cbDoc: binding "Chris Bizer" for ?fn and
dbpedia:Berlin for ?b. However, dereferencing the latter URI would be pointless: we do not need any infor-

ReasonerSource LookupTriplePatternOperatorCacheQuery ProcessorLocalRepositorydereferenced contentrequest dereferencing notifytriple patterns indexSource Selectionregistermatchscan &matchSPARQLQueryResultsINOUTinferredtriplesJ. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

mation about dbpedia:Berlin to answer the query. Our
optimisation thus proposes to avoid wasting lookups
by not dereferencing URIs bound to non-join variables
such as ?b.

By reducing the amount of sources and raw data
that are accessedand given that anyone in principle
can say anything, anywherewe may also reduce the
number of answers that are returned. Taking the previous example, for all we know, the document dereferenced through dbpedia:Berlin may contain people
based near Berlin that Olaf knows, which would help
to contribute other answers. However, we deem this to
be unlikely in the general case, and note that it goes
against the core LTBQE idea of using Linked Data
principles to find query-relevant sources.

Aside from this optimisation to avoid dereferencing
URIs bound to non-join variables, we note that URIs
in certain positions of a triple pattern may not be worth
dereferencing to look for matching information. For
example, given the pattern ?s foaf:knows ?o ., we
would not expect to find (m)any triples matching this
pattern in the document dereferenced by foaf:knows. In
the next section, we investigate precisely this matter
for different triple positions, and thereafter propose a
further variation on LiDaQs source selection to prune
remote lookups that are unlikely to contribute answers.

6. Empirical Study

LTBQE relies on the assumption that relevant data
are dereferenceable, which may not always hold in
practice. In this section, we analyse a large sample
of the Web of Data to see what ratio of information
is available in dereferenceable documents versus the
total information available in the entire sample. This
provides insights as to what percentage of raw data
is available to LTBQE versus, e.g., a materialised approach with a complete index over a large crawl of the
Web of Data. We can also test how much additional
raw information is made available by our extensions.

6.1. Empirical corpus

We take the dataset crawled for the Billion Triple
Challenge 2011 (BTC11) in mid-May 2011 as our
corpus. The dataset consists of 7.4 million RDF/XML
documents spanning 791 pay-level domains (data
providers). URIs extracted from all RDF triples positions (excluding common non-RDF/XML extensions

like .pdf, .jpg, .html, etc.) were considered for crawl-
ing. The resulting corpus contains 2.15 billion quadruples (1.97 billion unique triples) mentioning 538 million RDF terms, of which 52 million (10%) are liter-
als, 382 million (71%) are blank nodes, and 103 million (19%) are URIs. We denote the corpus as .
The core RDF data are serialised as N-Quads [19]: a
syntax that extends N-Triples with a fourth element,
used in this case to track which triple came from which
source (following our notion of a Linked Dataset).

Alongside the RDF data, all relevant HTTP informa-
tion, such as response codes, redirects, etc., are made
available. However, being an incomplete crawl, not all
URIs mentioned in the data were looked up. As such,
we only have knowledge of redir and deref functions
for 18.65 million URIs; all of these URIs are HTTP
and do not have non-RDF file-extensions. We denote
these URIs by U. Of the 18.65 million, 8.37 million
(45%) dereferenced to RDF; we denote these by D.
Again, this corpus is only a sample of the Web of
Data: we can only analyse the HTTP lookups and the
RDF data provided for the corpus. Indeed, a weakness of our analysis is that the BTC11 dataset only
considers dereferenceable RDF/XML documents and
not other syntaxes like RDFa or Turtle. Thus, our estimate of what ratios of relevant data are dereferenceable
should be considered as an upper bound since there are
many documents on the Web of Data that we do not
(or cannot [63]) know about.

6.2. Static Schema Data

For the purposes of this analysis, we extract a static
set of schema data for the RDFS reasoning. As argued
in [16], schema data on the Web is often noisy, where
third-party publishers redefine popular terms outside
of their namespace. Thus, we perform authoritative
reasoning, which conservatively discards certain thirdparty schema axioms (cf. [16]). In effect, our schema
data only includes triples of the following form:

PRP-SPO1 : (s, rdfs:subPropertyOf, o)  deref(s)
(s, rdfs:domain, o)  deref(s)
PRP-DOM :
(s, rdfs:range, o)  deref(s)
PRP-RNG :
(s, rdfs:subClassOf, o)  deref(s)
CAX-SCO :

We call these authoritative schema triples. Table 2
gives a breakdown of the counts of triples of this form
extracted from the dataset, and how many domains
(PLDs) they were sourced from: a total of 397 thousand triples were extracted from schema data provided
by 98 PLDs. We denote this dataset as .

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

Breakdown of authoritative schema triples extracted from the corpus

Table 2

Category

rdfs:subPropertyOf
rdfs:subClassOf
rdfs:domain
rdfs:range

total

Triples

PLDs

6.3. Recall for Baseline

We first measure the average dereferenceability
of information in our sample. Let data(u, G) give
the triples mentioning a URI u in a graph G, and,
let ddata(d) denote
for a dereferenceable URI d,
data(d, deref(d)): triples dereferenceable through d
mentioning d in some triple position. We then define
the sample dereferencing recall for d w.r.t. G as:

sdr(d, G) := ddata(d)
data(d, G)

Letting G := merge() denote the merge of our
corpus, we measure sdr(d, G), which gives the ratio of dereferenceable triples for d mentioning d vs.
unique triples mentioning d across the corpus. For
comparability, we do not dereference d live, but use
the HTTP-level information of the crawl to emulate
deref(.) at the time of the crawl. We denote by ddata
the average of ddata(d) for all d  D, and by sdr
the average of sdr(d, G) for all d  D.
We also measure analogues of ddata and sdr
where d must appear in specific triple positions: for
example, if LTBQE dereferences a URI in the predicate position of a triple pattern, we wish to know how
often relevant triplesi.e., triples with that URI as
predicateoccur in the dereferenced document, how
many, and what ratio compared with the whole corpus.
Table 3 presents the results, where for different

triples positions we present:

|U| : number of URIs in that position,
|D| : number of which are dereferenceable,
|D|
|U|
sdr : as above, with std. deviation ()
ddata : as above, with std. deviation ()

: ratio of dereferenceable URIs

The row TYPE-OBJECT only considers the object position of triples with the predicate rdf:type, and the

row OBJECT only considers object positions where the
predicate is not rdf:type.

A number of observations are directly relevant to
LTBQE. Given a HTTP URI (without a common non-
RDF/XML extension), we have a 45% success ratio to receive RDF/XML content regardless of the
triple position; for subjects, the percentage increases to
85%, etc. If such a URI dereferences to RDF, we receive on average (at most) 51% of all triples in which
it appears across the whole corpus. Given a triple pattern with a URI in the subject position, the dereferenceable ratio increases to 95%, such that LTBQE
would work well for (possibly partly bound) query patterns with a URI in the subject position. For objects of
non-type triples, the ratio drops to 44%. Further still,
LTBQE would perform very poorly for triple patterns
where it must rely on a URI in the predicate position
or a class URI in an object position: the documents
dereferenced from class and property terms rarely contain their respective extension, but instead often contain schema-level definitions.

Table 3 also features high standard-deviation val-
ues: these indicate that dereferenceability is often all
or nothing. In relative terms, predicate and typeobject deviations were the highest. Although most
such terms return little or no relevant information
e.g., dereferencing the predicate in a triple pattern
rarely yields triples where the dereferenced term appears as predicatewe observed a few predicates and
values for rdf:type return a great many relevant triples
in their dereferenced documents.14

6.4. Recall for Extensions

We now study how much additional data is made
available for query answering by the three LTBQE
extensions. Table 4 presents the average increase in
raw triples made available to LTBQE by considering
rdfs:seeAlso and owl:sameAs links, as well as knowledge materialised through owl:sameAs and RDFS rea-
soning. D+ indicates the subset of URIs in D that
have some relation to the extension, respectively: the
URI has rdfs:seeAlso link(s), has owl:sameAs link(s),
or has non-empty RDFS inferences. Also, ddata+ indicates the analogous ddata measure after the extension has been applied; i.e., after relevant links are followed and/or inferences applied.

14Many such examples for both classes and properties come from
the SUMO ontology: see, e.g., http://www.ontologyportal.
org/SUMO.owl#subsumingRelation for a large extension of ontology terms provided by the ontology itself.

Dereferenceability results for different triple positions

Table 3

Position

|U|

|D|

SUBJECT
PREDICATE
OBJECT
TYPE-OBJECT

1.87  107
9.55  106
4.77  104
9.73  106
2.13  105

8.37  106
8.09  106

4.50  106
2.11  104

|D|
|U|

sdr

ddata

avg.

0.00007 0.008

avg.
17.26 97.15
14.11 35.46
0.14 56.68
2.95 60.64
0.07 29.13

Additional raw data made available through LTBQE extensions

Table 4

Extension

|D+|

SEEALSO
SAMEAS

2.01  105
1.35  106
6.79  106

|D+|
|D|

ddata+
ddata

avg.

1.006 0.04

more detail, Figure 5 gives a breakdown for URIs from
individual domains, showing the number of URIs with
an information increase above the indicated threshold
due to owl:sameAs. The graph shows that, e.g., some
URIs from nytimes.com and freebase.com had an information increase of over 4000 (mostly due to DBpedia links); often the local descriptions were stubs
with few triples.

6.4.1. Benefit of rdfs:seeAlso extension

The percentage of dereferenceable URIs in D
with at least one rdfs:seeAlso link in their dereferenced document was 2% (about 201 thousand URIs).
Where such links exist, following them increases the
amount of unique triples (involving the original URI)
by a factor of 1.006 versus the unique triples in the
dereferenced document alone. We conclude that the
rdfs:seeAlso extension will only marginally affect the
recall increase of LTBQE in the general case.

6.4.2. Benefit of owl:sameAs extension

We measured the percentage of dereferenceable
URIs in D which have at least one owl:sameAs links in
their dereferenced document to be 16% for our sam-
ple. Where such links exist, following them and applying the EQ-* entailment rules over the resulting information increases the amount of unique triples (involv-
ing the original URI) by a factor of 2.5 vs. the unique
(explicit) triples in the dereferenced document alone.
The very high standard deviation of 36.23 shown in
Table 4 is explained by the plot in Figure 4 (log/log),
which shows the distribution of the ratio of increase
by considering dereferenceable owl:sameAs links and
inferences for individual entities: we again see that although the plurality of entities enjoy only a small increase in raw data (close to the x-axis), a few entities
enjoy a very large increase (farther from the x-axis). In

No

Relative information increase

ddata+(u)

ddata(u)

Fig. 4. Distribution of relative information increases by materialising
owl:sameAs information (log/log)

We conclude that owl:sameAs links are generally not
so common for dereferenceable URIs, but where avail-
able, following them and applying the entailment rules
generates significantly more (occasionally orders of
magnitude more) data for generating answers.

6.4.3. Benefit of RDFS extension

With respect to our authoritative static schema data
, we measured the percentage of dereferenceable
URIs in D whose dereferenced documents give nonempty entailments as 81%. Where such entailments
are non-empty, they increase the amount of unique
triples (involving the original URI) by a factor of 1.8
vs. the unique (explicit) triples in the dereferenced

No

Relative information increase

bbc.co.uk
dataincubator.org
dbpedialite.org
dbpedia.org
europa.eu

rectly over a diverse set of Web of Data sources. To
guide this evaluation, we first survey existing Linked
Data SPARQL benchmarks and look at how other
systems evaluate their approaches ( 7.1). We conclude that no benchmark offers a large and diverse
range of benchmark SPARQL queries and thus propose QWalk: a novel benchmark methodology tailored
for testing LTBQE-style query-answering approaches
over the broader Web of Data ( 7.2). Final evaluation
setup and results are presented later in Section 8.

7.1. Existing Linked Data SPARQL Benchmarks

Since we aim to run our evaluation over Linked
Data sources in situ, we ignore benchmarks designed
to run over synthetic datasets such as LUBM [26],
BSBM [15], or SP2Bench [57, 58]. To the best of our
knowledge, this leaves only two standard benchmarks:
FedBench [56] offers three data collections for testing Linked Data querying scenarios:

1. a Life Science Data Collection, which includes
datasets like KEGG, ChEBI, DrugBank and
DBPedia;

2. a synthetic dataset from the SP2Bench frame-

work [57, 58]; and

3. a general Linking Open Data Collection, which
includes datasets like DBpedia, GeoNames, Ja-
mendo, LinkedMDB, The New York Times and
Semantic Web Dog Food.

A query set is defined for each. The first query set
focuses on features of particular interest for federated query engines, such as the number of involved sources, (interim) query results size, and so
forth. The second query set consists of the original
SP2Bench queries. The third set provides 11 Linked
Data queries and is thus relevant to us.
DBSPB [48] (the DBpedia SPARQL Benchmark)
contains SPARQL queries distilled from real-world
DBpedia logs, consisting of 31.5 million queries issued by various users and agents over a four-month
time-frame in 2010. The raw set of queries is reduced
to a total of 35 thousand queries after less frequentlyoccuring query shapes were removed. These 35 thousand queries are clustered to generate 25 templates
that characterise the larger set. These templates can
be instantiated to create new queries from DBpedia
data. The core of the templates consist of Basic Graph
Pattern queries with 15 triple patterns, but may also
include various combinations of SPARQL query features (e.g., OPTIONALFILTERDISTINCT, UNIONFILTER,
and so forth).

ddata+(u)


ddata(u)
freebase.com
fu-berlin.de
geonames.org
ontologyportal.org
nytimes.com

Fig. 5. Binning of relative information increases by materialising
owl:sameAs information per domain (log/log)

document. We conclude that such reasoning often increases the amount of raw data available for LTBQE
query answering, and by a significant amount.

6.5. Discussion

Before looking at specific queries, in this section
we find that, in the general case, LTBQE works best
when a subject URI is provided in a query-pattern,
works adequately when only (non-class) object URIs
are provided, but works poorly when it must rely on
property URIs bound to the predicate position or class
URIs bound to the object position. Furthermore, we
see that rdfs:seeAlso links are not so common (found
in 2% of cases) and do not significantly extend the raw
data made available to LTBQE for query-answering.
Conversely, owl:sameAs links are a bit more common
(found in 16% of cases) and can increase the available
raw data significantly (2.5). Furthermore, RDFS reasoning often (81% of the time) increases the amount of
available raw data by a significant amount (1.8).

As discussed previously, we can use these results
to justify a variant of LTBQE that tries to minimise
wasted remote lookups: aside from skipping URIs
bound to non-join variables, and unless collecting
schema data dynamically, this variant skips dereferencing predicate URIs bound in triple patterns, or
URIs bound to the objects of triples patterns where the
predicate is bound to rdf:type, since we are unlikely
to find data matching those patterns in the respectively
dereferenced document (cf. Table 3).

7. Query Benchmarks

We wish to evaluate LiDaQ in a realistic, uncontrolled environment: answering SPARQL queries di-

A variety of Linked Data querying papers have also
defined once-off evaluation frameworks. The methods
used by a selection of papers are summarised in Table 5. We see that two approaches (FedX [59] and
SPLENDID [25]) evaluate their methods using Fed-
bench, but do not run their queries live. Most papers
simulate HTTP lookups or replicate SPARQL endpoints locally. We see that only two papers feature
evaluation of queries that are run live over HTTP,
both by Hartig [31,32], one involving six hand-crafted
queries over real-world sources [32], the other proposing a FOAF Letter application to keep track of social connections where five query templates are instantiated for 23 people, giving a total of 115 queries.

In summary, there is much diversity in how Linked
Data query proposals have been evaluated. Few benchmarks have been proposed for real-world Linked Data;
perhaps the most agreed upon is FedBench. Most evaluations involve either a handful of custom queries designed to run over a small number of sources (Fed-
Bench), or a large number of (semi-)automatically
generated queries tied to a specific domain (e.g.,
DBPSB) or vocabulary (e.g., FOAF Letter). Few engines run their queries live over remote resources, but
instead replicate raw content or endpoints within a
controlled environment. As such, no live Linked Data
query engine has been evaluated in an uncontrolled,
real-world setting for a large set of diverse queries: the
closest such evaluation is probably FOAF Letter [31],
but which only created queries over FOAF profiles.

7.2. QWalk: Random Walk Query Generation

Given the shortcomings of existing benchmarks, we
propose QWalk: a new benchmark framework that automatically builds a large set of queries that are answerable (e.g., by LTBQE) over a diverse set of realworld sources. The core idea is to take a large crawl
of the Web of Data (in this case, the BTC11 dataset)
and to conduct random walks of different shapes and
lengths through the corpus to generate Basic Graph
Patterns. The walk is guided to ensure that it crosses
documents through dereferenceable links: generated
queries should thus be answerable by LTBQE.

7.2.1. Query shapes

To inform the types of queries we generate, we
take observations from the work of Gallego et al. [7],
who analyse the SPARQL queries logs of the DBPedia
and Semantic Web Dog Food (SWDF) servers. They
found that most queries contain a single triple pattern

(66.41% in DBPedia, 97.25% in SWDF). The maximum number of patterns found was 15, but such complex queries occurred only rarely. The most common
forms of joins involved subjectsubject (5961%),
subjectobject (3236%) and objectobject (45%);
few joins involving predicate variables were found in
general. As such, most queries with multiple patterns
are star-shaped, with a few path shaped queries. Starshaped joins typically had a low fan-out, where 27%
of the DBpedia queries had a fan-out of three, and
3.7% had a fan-out of two; the bulk of the remaining queries were single-pattern with a trivial fan-out of
one, but went up to a maximum of nine. The lengths
of paths in the query were mostly one (98%) or two
(1.8%); very few longer paths were found.

Along similar lines, for our benchmark we generate
queries of elemental graph shapes as depicted in Figure 6, viz., edge, star and path queries. We now describe these query types in more detail.

Fig. 6. Visualisation of example query shapes (edge-s, edge-o, s
path-2, o-path-3, star-2-1); dotted nodes represent variables; solid
nodes represent URIs

Edge queries (edge-<s|o|so>) fetch all triples (edges)
for an entity. We generate three sub-types of
edge queries, asking for triples where a URI appears as the subject (edge-s); as the object (edge-
o); as the subject and object (edge-so). These
types of queries are very common in Linked Data
browsers or for dynamically serving dereferenceable Linked Data content. An example query for
edge-so would be:

SELECT DISTINCT ?p1 ?o ?s ?p2 WHERE {

<d> ?p1 ?o . ?s ?p2 <d> . }

Star queries (star-<s3|o3|s2-o1|s1-o2>) consist of three
acyclic triple patterns that share exactly one URI
(called the centre node). These queries are similar to edge queries but have only constant predi-

edge queriesstar queriespath queries20

J. Umbrich et al. / Link Traversal Querying for a Diverse Web of Data

Summary of evaluation setups in the Linked Data querying literature

Table 5
