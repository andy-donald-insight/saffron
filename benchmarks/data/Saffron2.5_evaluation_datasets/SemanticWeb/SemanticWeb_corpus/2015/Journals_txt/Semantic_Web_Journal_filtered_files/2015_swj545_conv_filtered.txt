Resource-Constrained Reasoning Using a 
Reasoner Composition Approach 

Editor(s): Christophe Gueret, Data Archiving and Networked Services (DANS), KNAW, Netherlands; Stephane Boyera, World Web 
Foundation, France; Mike Powell, IKM Emergent, USA; Martin Murillo, Data Connectivity Initiative - IEEE 
Solicited review(s): Christophe Gueret, Data Archiving and Networked Services (DANS), KNAW, Netherlands; Aidan Hogan, Universidad 
de Chile, Chile; Christophe Dupriez, DESTIN SSEB, Belgium 

Wei Tai a, *, John Keeney b, Declan OSullivan a 
a Knowledge and Data Engineering Group, School of Computer Science and Statistics, Trinity College Dublin, 
Dublin 2, Ireland 
b Network Management Lab, LM Ericsson, Athlone, Ireland 

Abstract:  To  increase  the  interoperability  and  accessibility  of  data  in  sensor-rich  systems,  there  has  been  a  recent 
proliferation  of  the  use  of  Semantic  Web  technologies  in  sensor-rich  systems.  Quite  a  range  of  such  applications  have 
emerged, such as hazard monitoring and rescue, context-aware computing, environmental monitoring, field studies, internet 
of things, and so on. These systems often assume a centralized paradigm for data processing, which does not always hold in 
reality especially when the systems are deployed in a hostile environment. At runtime, the infrastructure of systems deployed 
in  such  an  environment  is  also  prone to  interference  or  damage,  causing  part  of  the  infrastructure  to have  limited  network 
connection  or  even  to be  detached  from  the  rest.  A  solution  to  such  a  problem  would be  to  push  the  intelligence,  such  as 
semantic reasoning, down to the device layer. A  key enabler for such a solution is to run semantic reasoning on resourceconstrained  devices.  This  paper  shows  how  reasoner  composition  (i.e.  to  automatically  adjust  a  reasoning  approach  to 
preserve only a well-suited amount of reasoning for a given ontology) can achieve resource-efficient semantic reasoning. 
Two  novel  reasoner  composition  algorithms  are  introduced  and  implemented.  Evaluation  indicates  that  the  reasoner 
composition  algorithms  greatly  reduce  the  resources  required  for  OWL  reasoning,  potentially  facilitating  greater  semantic 
reasoning on sensor devices. 

Keywords: Reasoner Composition, Reasoning, OWL, Rule, Mobile, Resource Constrained, Semantic Web 

1. Introduction 

In  the  past  decade,  there  has  been  an  increasing 
interest  in  applying  Semantic  Web  technologies  to 
sensor-rich  systems  to  enhance  the  interoperability 
of  heterogeneous  data, 
to  bring  about  more 
intelligent  data  processing,  and  to  make  data  more 
easily accessible on the Web. Much of this research 
has been developed for a wide range of uses, such as 
hazard  monitoring  and  rescue  [15],  context-aware 
computing [27], environmental monitoring [4], field 
studies  [35],  internet  of  things  [18],  and  so  on.  A 
common  approach 
integrate  Semantic  Web 
technologies  into  Sensor  Network  is  to  use  a 
centralized  paradigm:  sensor  data  is  sent  over  the 
network  to  a  comparatively  powerful  central  server 
(either  a  local  gateway  [15]  or  implemented  as 
remote  services  on  the  Web  [4,  35,  27])  where  all 
semantic  processing 
the 
conversion of sensor data into RDF, semantic-based 
data  aggregation, 
semantic  query  answering, 
visualization,  and  so  on.  This  approach  has  been 
proven to work well for cases with reliable network 

takes  place,  such  as 

to 

connections  and  a  relatively  stable  environment,  on 
the other hand it  may not always be the best option 
for  some  harsher  use  cases  where  some  sensor-rich 
systems,  such  as  forest  fire  monitoring,  seafloor 
monitoring  or  space  exploration,  need 
to  be 
deployed.  Harsh  surroundings  often  mean  that  the 
deployment  of  an  onsite  powerful  server  with  fast 
and stable network connections could be very costly 
or  even  on  occasions  technically  impossible.  At 
runtime,  the  infrastructure  of  such  systems  is  also 
prone  to  interference  or  damage,  causing  a  limited 
network  connection  or  even  a  complete  detachment 
of  some  parts  of  the  infrastructure.  In  some  other 
cases,  to  cover  a  large  geographical  area  a  system 
might  comprise  of  hundreds  and  thousands  of 
sensors  of  various  types.  The  overhead  required  by 
such systems to relay raw un-processed data can be 
very high, and furthermore centralized semantic data 
processing could act as a bottleneck in such systems 
and incur significant overhead. 

Such  environments  often  require  a  sensor-rich 
system to be robust enough to retain part (or all)  of 
data  processing  on  the  networked  devices.  Thus  for 

semantic  uplifting  of 

semantic-enabled  sensor-rich  systems  that  are  of 
interest  in  this  work,  semantic  data processing  such 
sensor  data,  data 
as 
semantic-assisted 
aggregation, 
decisions/actuations  will  be 
required 
to  be 
performed on network edge devices such as sensors 
or  on  sensor  cluster  heads  [36].  In  this  way,  more 
meaningful aggregated data can be sent to  a central 
server,  which  on  the  one  hand  can  reduce  the 
amount  of  data  that  is  required  to  be  transmitted 
over  a  network  and  on  the  other  hand  can  achieve 
local  autonomy  to  some  extent  when  the  central 
server is out of reach. As a key enabling step for ondevice  semantic  data  processing,  it  is  essential  to 
push  semantic  reasoning  towards  the  edge  of  the 
system  where  various  resource-constrained  devices 
(for data gathering, actuation, and data aggregation) 
often  reside.  In  the  extreme  case  they  may  be  only 
sensors with very limited resources.  

to  be  directly  ported  on 

Existing  semantic  reasoners  are  too  resourceintensive 
resource-
constrained  devices  [3,  34,  38].  A  study  in  [10] 
shows  that  a  desktop  rule-entailment  reasoner  can 
take  from  tens  to  several  hundred  KBs  of  memory 
(depending  on  characteristics  of  the  ontology)  to 
reason each RDF triple loaded into  memory. Hence 
although technically portable to a small device with 
some  code-level  modifications,  a  desktop  semantic 
reasoner can still easily drain the resources of a very 
small  device  as  those  deployed  on  the  edge  of  the 
systems (e.g. Sun SPOT has 180MHz CPU, 512KB 
RAM, and 4MB Flash).  

In  this  work,  we  introduce  the  idea  of  reasoner 
composition  and  also  describe  how  reasoning 
composition  can  achieve  more  resource-efficient 
semantic reasoning for resource-constrained devices. 
The  rationale  behind  reasoner  composition  is  that 
the  different  expressiveness  of  ontologies  presents 
different  requirements  for  reasoning  capabilities.  A 
simple  example  would  be  that  reasoning  over  a 
simple  RDFS  ontology  only  needs  some  of  the 
RDFS  rules  whereas  reasoning  over  a  more 
expressive OWL ontology  would probably need the 
entire  OWL  2  RL  ruleset.  This  difference  of 
expressiveness of ontologies could then be exploited 
to  tailor  the  semantic  reasoning  process  such  that 
only  the  required  reasoning  abilities  are  loaded  and 
run.  

Different  semantic  reasoning  approaches  have 
been  developed  in  the  past  decades  and  they  are 
widely adopted by state of the art semantic reasoners. 
This  work focuses only on the composition of rule-

reasoners, 

in  particular 

the  semantic 

based 
rule-entailment 
reasoners, mainly due to the long standing history of 
using  forward-chaining  rule 
to  handle  domain 
semantics  in  sensors-rich  systems,  and  also  the 
potential  in  composing  a  rule-based  reasoner  [2]. 
Section  2.1  presents 
reasoning 
approaches and also a justification  for focusing this 
research  on  rule-based  reasoners.  In  our  research, 
two  novel  reasoner  composition  algorithms  have 
been devised to compose a rule-entailment reasoner 
from  two  complementary  perspectives:  A  selective 
rule  loading  algorithm  that  composes  a  selective 
ruleset  according  to  the  expressiveness  of  the 
ontology  to  be  reasoned  over,  and  a  two-phase 
RETE  algorithm 
the  RETE  rule 
matching  algorithm  to  operate  in  a  way  tailored  for 
the  particular  ontology  to  be  reasoned  over.  Both 
reasoner  composition  algorithms  were  implemented 
in a proof-of-concept resource-constrained  semantic 
reasoner 
results 
indicate 
composable  COROR  uses 
significantly  less  resources  than  a  non-composable 
reasoner, 
facilitating  semantic 
reasoning in resource-constrained environments.  

(called  COROR).  Experiment 
that 

thus  potentially 

that  adjusts 

The  paper  is  organized  as  follows.  Section  2 
presents  background  and  related  work.  Section  3 
elaborates  the  design  of  the 
two  composition 
algorithms.  Implementation  details  for  a  prototype 
composable  reasoner  (COROR)  are  provided  in 
section  4,  followed  by  Section  5  describing  an 
evaluation of the approach. Section 6 concludes with 
a  discussion  of  the  significance,  limitations  of  the 
work and directions for future work of the research. 

2. Background and Related Work 

This  section  first  presents  a  categorization  of 
semantic reasoning approaches and then justifies the 
choice  of  rule-entailment  reasoners  as  the  focus  of 
the  composition 
research.  This  categorization 
enables  the  composition  research  to  be  carried  out 
for  a  type  of  reasoner  rather  than  for  a  particular 
reasoner  implementation.  After  finding  that  ruleentailment  reasoners  are  the  most  suitable  type  for 
composition,  the  RETE  algorithm,  as  the  most 
widely  used  rule  matching  algorithm,  is 
then 
described.  Finally,  related  work 
is  discussed, 
including  resource-constrained  ontology  reasoners, 
query  optimizations 
rule-based 
ontology reasoning. 

scalable 

and 

Quite  different  semantic  reasoning  approaches 
have  been  developed  in  the  past  decades.  A  survey 
of  reasoning  approaches  was  carried  out  earlier  in 
this research over a set of 26 state of the art semantic 
reasoners.  The  survey  suggests  that  their  reasoning 
approaches  can  be  categorized  into  five  categories: 
Description  Logic 
reasoners, 
(forward-chaining) 
reasoners, 
resolution-based  reasoners,  reasoners  using  hybrid 
approaches, 
using 
miscellaneous approaches. 

tableau 
(DL) 
rule-entailment 

reasoners 

finally 

and 

the 

two 

DL  tableau  reasoners  convert  established OWL 
entailments  to  DL  satisfiability  checking  based  on 
semantic  connections  between 
[21]. 
Normally a DL tableau reasoner translates an OWL 
ontology  into  a  DL  knowledge  base,  and  then  does 
the satisfiability checking using well-established DL 
tableau algorithms. This basically tries to construct a 
non-contradictory model for the DL knowledge base 
using a set of consistency preserving transformation 
rules.  If  such  a  model  is  found,  then  the  DL 
to  be  satisfiable,  or 
knowledge  base 
otherwise  unsatisfiable.  State  of  the  art  reasoners 
falling  into  this  category  include  FaCT++  [45], 
Pellet [53], RacerPro [54], and HermiT [55]. 
compute 

total 
entailments  for  OWL  ontologies  based  on  a  set  of 
forward-chaining  production  rules  that  (partially) 
implement  an  OWL  semantic  profile.  These  rules 
are often termed semantic entailment rules. 

Rule-entailment 

reasoners 

is  said 

in 

(Car 

rules, 

entailment 

owl:subClassOf  Vehicle) 

Depending  on  the  way  OWL  semantics  are 
interpreted 
this  work 
distinguishes direct style entailment rules (DS rules) 
from  RDFS  style  entailment  rules  (RS  rules).  DS 
rules  are  often  dynamically 
transformed  from 
ontological  assertions  according  to  OWL  direct 
semantics  [60].  For  example,  a 
terminological 
assertion 
is 
transformed to a DS rule                    . O-
DEVICE [52] and DLEJena [29] use DS rules. The 
Description  Logic  Programs  (DLP)  perform  a 
similar  transformation  between  Description  Logic 
and Logic Programs [19]. RS rules are derived from 
RDF  compatible  OWL  semantic  conditions  [60]. 
Unlike  DS  rules  in  which  OWL  semantics  are 
implied,  OWL  constructs  are  explicitly  used  in  RS 
rules.  An  example  RS  rule  that  does  a  similar 
reasoning as the above example would look like: 

pD* rules [44] and OWL 2 RL entailment rules [17] 
follow the RDFS style. DS rules are mainly used for 
reasoning  assertional  data,  but  RS  rules  can  reason 
both  terminological  and  assertional  data.  Ruleentailment 
include 
BaseVISor 
[24], 
MiRE4OWL [26], the Jena forward chaining engine 
[12] and EYE1. 

reasoners  using  RS 
[28],  OWLIM 

[9],  Bossam 

rules 

reasoning  using 

logic  engine.  Therefore, 

At  the  heart  of  resolution-based  reasoners  is  a 
resolution-based 
the 
reasoning  process  for  resolution-based  reasoners 
first normally translates an OWL ontology into logic 
programs  consisting  of  sentences  written  in  the 
supported  logic  language,  such  as  Prolog  [40,  41], 
Datalog  [16],  or  first-order  TPTP  (Thousands  of 
Problems  for  Theorem  Provers)  language  [25].  It 
then  performs  ontology 

corresponding  logic  engine.  Both  RS  and  DS  rules 
are also used by resolution-based reasoners, in order 
to implement OWL semantics. However, resolutionbased 
rule-entailment 
reasoners,  in  that  here  rules  are  matched  in  a 
backward-chaining manner using a resolution engine 
rather 
forward-chaining  manner  using 
production engines. F-OWL [47], Surnia [20], Thea 
[40] and the Jena backward-chaining engine [12] are 
resolution-based 
rules. 
Resolution-based  reasoners  using  DS  rules  include 
KAON2  [56]  and  Bubo  [41].  Hoolet  [25]  translates 
an  ontology  into  first-order  TPTP  axioms  and 
performs  reasoning  using  the  Vampire  automated 
theorem prover. 

reasoners  differ 

using  RS 

reasoners 

than  a 

from 

Some  other  reasoners  have  been  devised  to 
provide  efficient  reasoning  for  a  particular  OWL 
profile or for some specific purpose or task, and they 
use a dedicated reasoning approach. These reasoners 
are  classified  as  miscellaneous  reasoners  in  our 
categorization. For example, CEL [30] and ELK [11] 
implement polynomial time classification algorithms 
designed  for  OWL  2  EL  [13].  One  item  worth 
noting  is  that  in  ELK  the  term  composition  in 
composition/decomposition 
is 
interpreted  differently  from  this  work:  it  means  to 
bring 
into  complex 
subsumption axioms. Other miscellaneous reasoners 
include Owlgres [39] and QuOnto [1] that use query 
rewriting  algorithms  to  efficiently  answer  queries 
over 
into  OWL  2  QL 
expressivity [13].The SPIN approach uses SPARQL 

together  atomic  concepts 

large  datasets  falling 

optimization 

1 http://eulersharp.sourceforge.net/ 

OWL 2 RL reasoning. 

reasoning approach, and (2) the suitability to operate 
in a resource-constrained environment.  

that 

to  exploit 

reasoning  approaches 

Some  reasoners  combine  two  or  more  of  the 
above 
the 
advantages  of  each  approach.  These  reasoners  are 
categorized  as  hybrid  reasoners.  Example  hybrid 
reasoners  are  Minerva  that  combines  DL  tableau 
approach  with  resolution-based  approach  [48], 
DLEJena that combines DL tableau approach with a 
rule-entailment  approach  [29],  Jena  that  combines 
rule-entailment  approach  with  a  resolution-based 
approach  [12],  and  Pellet 
incorporates  a 
forward-chaining  rule  engine  for  rule  handling. 
Reasoning  in  Stardog 2  is  performed  by  explicitly 
separating  the  terminological  part  of  the  ontology 
from  its  assertional  part.  The  terminological  part  is 
reasoned  using  Pellet,  and  the  assertional  reasoning 
is  performed  at  query-time  using  query-rewriting 
informed by the reasoned terminological part of the 
knowledge  base.  The  reasoning  approach  presented 
in  [51]  falls  into  this  category  as  well.  Rather  than 
using  a 
(comparatively)  slow  but  expressive 
reasoner  to  reason  over  the  ontology  with  an 
expressive DL language, this work extracts modules 
of the ontology each of which has an expressiveness 
falling  into  a  lesser  DL  language  that  can  be 
reasoned using less expressive but more efficient DL 
reasoners. An expressive reasoner is only used for a 
complete reasoning over the partial results generated 
by component reasoners.  

reasoner 

A  similar  categorization  of  semantic  reasoners 
can  also  be  found  in  [47],  which  defines  three 
categories: DL tableau reasoners, full FOL reasoners 
and  partial  FOL  reasoners.  This  categorization  has 
its  own  uses  but  it  was  not  fine-grained  enough  for 
our 
research.  Their 
categorization  was  based  on  underlying  logics 
implemented  by 
it  does  not 
distinguish  reasoners  using  different  reasoning 
algorithms.  Such  algorithmic  differences  are  quite 
important  for  reasoner  composition  research,  as  the 
different  reasoning  algorithms  can  support  different 
composition approaches.  

reasoners  and 

composition 

Choice of Rule-entailment reasoners 

The  choice  of  rule-entailment  reasoners  as  the 
target  of  our  composition  research  is  based  on 
consideration of: (1) the difficulty for a composition 
process  of  using  any  of  the  other  categories  of 

2 http://stardog.com/ 

the 

First, 

loose  coupling  characteristic  of 
entailment  rules  would  indicate  that  rule-entailment 
reasoners  is  a  good  candidate  for  a  composability 
process than the other reasoning approaches.  

The fact that RS rules often implement semantics 
in  a  fine-grained  way  and  that  each  RS  rule  can  be 
applied 
independently  without  other  RS  rules, 
indicates  that  RS  rules  could  be  more  natively 
composable,  based  on  the  amount  of  semantics  in 
the  target  ontology.  DS  rules  are  generated  by 
parsing  the  ontology  to  be  reasoned  over  according 
to  rule  templates.  Therefore  only  appropriate  but 
very specific rules are generated. However, DS rules 
are  mainly  used  for  reasoning  assertional  data,  and 
terminological 
relies  on  other 
reasoning  approaches  such  as  the  DL  tableau 
approach or RS rules, which makes it less appealing 
for reasoner composition research. 

reasoning  still 

impair 

transformation  rules  may 

Transformation  rules  are  used  in  tableau-based 
approaches  for  deciding  the  satisfiability  of  a  DL 
knowledge  base.  However,  as  transformation  rules 
describe  procedures  of  DL  tableau  calculi,  deselecting 
the 
completeness  of  the  original  DL  tableau  calculus 
[38].  Furthermore,  compared  to  the  plain  text 
formats of entailment rules, DL transformation rules 
are often hardcoded, further impeding their dynamic 
composition.  In  addition,  to  achieve  more  efficient 
practical DL reasoning by DL tableau calculi, a wide 
range  of  complex,  code-level  optimizations  are 
entwined  into  DL  tableau  implementations,  such  as 
different  backtracking  mechanisms,  loop  detection 
mechanisms,  and  model  caching  techniques  [7].  To 
comply  with  a  composition  algorithm, 
these 
optimizations  would  also  need  to  be  adjusted  at 
runtime,  which  would 
the 
possibility  of  dynamic  composition  of  DL  tableau 
reasoners. 

further  complicate 

to 

tasks, 

specific 

reasoning 

Reasoners 

that  belong 

the  miscellaneous 
category  are  designed  for  efficient  reasoning  for 
specific 
ontology 
expressivities,  or  specific  application  areas  rather 
than  for  general-purpose  OWL  reasoning.  Hence 
their  application-  or  domain-specific  hardwired 
algorithms  make  them  less  appealing  as  candidates 
for 
the 
composability of a hybrid reasoner relies on each of 
the individual component reasoning approaches used, 
where  composability  for  each  approach  has  already 
been discussed above. 

composability 

studies. 

In 

addition, 

discussion  above,  the  basis  for  our  composition 
research  was  narrowed  down  to  rule-entailment 
reasoners and resolution-based reasoners,  which are 
the two categories using entailment rules.  

The  suitability  of  applying  both 

reasoning 
approaches  to  resource-constrained  environments  is 
discussed next.  

to 

goal-directed 

In  addition  to  OWL  reasoning,  a  sensor-rich 
system  often  needs  to  react  to  events.  For  this 
purpose,  domain-specific  rules  are  often  required. 
Compared 
resolution-based 
reasoners,  rule-entailment  reasoners  use  production 
engines. They have the intrinsic capability to receive 
events  and  in  addition  to  trigger  a  corresponding 
non-semantic rule when a particular (combination of) 
event happens. This is also part of the reason for the 
popularity  and  success  in  applying  (non-reasoner) 
forward-chaining  production  rule  engines  to  small 
devices  [14,  42].  As  a  matter  of  fact,  some  ruleentailment reasoners have already been implemented 
for  resource-constrained  environments  [24,  26]. 
Such implementations further confirm the suitability 
of  our  choice  of  rule-entailment  reasoners  for  our 
work.  

As  discussed  above,  the  way  DS  rules  are 
generated  can  compose  an  entailment  ruleset  with 
well-suited semantics for the assertional data of an 
ontology.  Terminological  reasoning  and  domainspecific reasoning still rely on RS rules. Considering 
that  these  two  types  of  reasoning  are  important  in 
sensor-rich  systems,  we  chose  a  rule-entailment 
approach  using  RS  rules  to  be  the  target  of  our 
composition research. 

In  this  work,  our  research  is  not  limited  to  rule 
composition  but  also  the  composition  of  a  welloptimized  reasoning  algorithm.  RETE  is  a  fast 
pattern  matching  algorithm  for  forward-chaining 
production systems [5]. It forms  the underlying rule 
matching algorithm for  many general-purposed rule 
engines  such  as  Drools  and  JESS 3.  RETE  is  also 
used as the underlying reasoning algorithm for  most 
rule-entailment reasoners including the Jena forward 
engine,  BaseVISor,  Bossam,  MiRE4OWL  and  O-
DEVICE. Therefore in this work, the composability 
of RETE algorithm is studied. The remainder of this 
section  provides  readers  with  more  knowledge  on 
the RETE algorithm.  

Although  varied  between  implementations,  a 
RETE  algorithm  in  general  performs  rule  matching 

3 Drools: http://www.jboss.org/drools/ 
Jess: http://herzberg.ca.sandia.gov/ 

for  more  attributive 

termed  RETE  network. 
using  a  data  structure 
Normally a RETE network comprises two parts. The 
alpha  part  consists  of  inter-connected  alpha  nodes 
each  of  which  is  responsible  for  testing  if  a  fact 
matches  against  an  attribute  of  a  rule  condition. 
Facts  matched  to  one  alpha  node  are  passed  to  the 
next  one 
tests.  Facts 
successfully  matched  to  all  attributes  of  a  rule 
condition  is  said  to  match  to  the  condition  and  are 
cached in the alpha memory for the condition. A fact 
stored in alpha memory is often termed a token or a 
partial  instantiation  of  the  rule.  The  beta  part  of  a 
RETE network consists of networked beta nodes for 
finding full instantiations of a rule by joining tokens. 
Each  beta  node  performs  a  join  operation  between 
two  inputs  each  of  which  receives  tokens  from  an 
alpha  node  or  an  upper-level  beta  node.  Tokens 
received  from  either  input  are  also  cached  in  a 
separate  beta  memory.  Successfully  joined  tokens 
are  combined  into  a  new  (larger)  token  containing 
fields  from  both  input  tokens,  and  the  new  token  is 
then sent to a lower-level beta node for further joins. 
Normally a beta network for a rule has its first beta 
node  join  between  tokens  from  the  first  two  alpha 
memories  (they  keep  the  facts  matched  to  the  first 
two conditions of a rule). The following beta nodes 
join  between  the  previous  beta  node  and  the  next 
alpha  memory  in  the  condition  sequence.  Outputs 
from  the  last  beta  node  of  a  join  tree  are  then  full 
instantiations of a rule. Full instantiations are sent a 
RETE  terminal  node  where  actions  associated  with 
the  rules  are  executed,  also  known  as  rule  firing. 
Normally,  a  RETE  engine  does  rule  firing  in 
iterations: 
full 
instantiations  for  facts  in  the  current  working 
memory, schedule a proper rule firing sequence, fire 
rules  one  by  one  in  sequence,  insert  newly  inferred 
facts  into  the  working  memory,  and  start  another 
match-fire  iteration  based  on  the  updated  working 
memory.  This  match-fire  iteration  stops  when  an 
inference closure is reached for all rules, namely no 
more  new  facts  can  be  inferred.  In  this  work  each 
such  match-firing  iteration  is  called  a  RETE  cycle 
and  all  RETE  cycles  together  are  called  a  RETE 
reasoning. 

all  possible 

it  would 

find 

The  RETE  algorithm  used  in  this  research  is 
based on the Jena implementation and therefore  is a 
variation of the one described above. One difference 
is  that  the  alpha  network  of  the  RETE  algorithm  is 
an  array  of  alpha  nodes  rather  than  a  network.  In 
each  alpha  node  are  three  attributive  tests  each  of 
which  is  either  a  URI  test  or  a  variable  binding 

object  position  of  a  triple  pattern.  Although  not 
using  a  network  of  alpha  nodes,  the  term  alpha 
network  is  kept  in  this  work  to  conform  to  the 
RETE  naming  convention.  Another  difference  from 
the  original  RETE  approach  is  that  action(s)  of  an 
entailment  rule  is  the  insertion  of  triples  into 
working  memory  and  therefore  an  action  is  also 
represented as a triple pattern in this work.  

Before  the  RETE  algorithm  used  in  this  research 
is  formally  described,  notations  are  defined.  Let  U 
denote  the  set  of  URI  constants,  B  the  set  of  blank 
nodes,   the set of variables, L the set of literals, and 
               the  set  of 
terms.  Let      
                                             denote 
the  set  of  triples  and                          
                              the 
triple 
patterns.  Let  a  tuple                  denote  an 
entailment  rule  where                      is  a 
sequence  of  conditions  and                    a 
sequence  of  consequences.  The  notation  r.CDT.i  is 
used  to  identify  the  ith  condition  of  rule  r  and 
similarly r.ACT.i for the ith consequence of rule r.   
is  used  in  this  work  to  denote  a  set  of  entailment 
rules.  

set  of 

Now  we  define  match  operation,  join  operation, 
alpha  memory  and  beta  memory.  Given          
and          , then  

is a function executing over a pair of a triple tp and a 
triple pattern tpn. In short, it returns TRUE when tp 
instantiates tpn, and FALSE otherwise.  

Let                     denote 

the  working 
memory,  tokenize  a  function  turning  a  triple  into  a 
token,  an  alpha  node  for  a  particular  rule  condition 
tpn, written as     , is then defined as 

Based on alpha memories, the ith beta node of rule r, 
written as     

, is defined recursively as  

the 

idea  described 

where     represents  the  join  symbol.  The  above 
definition  captures 
the 
beginning  of  this  section:  a  beta  node  takes  two 
inputs,  performs  pair-wise  checks  for  consistent 
variable bindings for each input token, and generates 
a  new  token  for  each  pairs  with  consistent  variable 
bindings.  

in 

Now  RETE  cycle  and  RETE  reasoning  are 
defined. Let                  be an ontology, fire a 
function  that  generates  inferred  triples  for  each  full 
instantiation  found  in  each  rule,  a  RETE  reasoning 
                               is  defined  as  a 
sequence  of  tuples  and  each  tuple               in 
this sequence is a RETE cycle.  Ak, Bk and WMk are 
respectively the alpha network, the beta network and 
the  working  memory  in  the  kth  RETE  cycle.  They 
are defined as 

       |                      }  

The  operand     
 of  fire  represents  the  last 
beta  memory  for  rule  r  at  RETE  cycle  k-1.  When 
WMk-1=WMk,  the  RETE  algorithm  terminates.  Let 
       be  a  RETE  cycle.  A  RETE  reasoning   is 
then a chain of RETE cycles  

where WMl-1 = WMl.  

RETE Optimization 

In  addition  to  ruleset  composition,  this  research 
focuses  on  automatically  composing  an 
also 
appropriate  RETE  network 
the  particular 
ontology  to  be  reasoned.  Towards  this  goal,  RETE 
optimizations  are  discussed 
in  this  section  as 
background knowledge. 

for 

Caching tokens avoids the same match operations 
and  join  operations  being  performed  again  for  the 
same  tokens.  It  can  greatly  speed  up  the  rule 
matching  process.  However  the  downside  is  the 
potential  inflation  of  the  beta  network  when  join 
sequences  are 
inappropriately  arranged,  which 
sometimes leads to a vast amount of cached tokens. 
An  extreme  case  would  be  that  two  condition 
elements  have  no  common  variables,  leading  to 
Cartesian  production  joins  and  hence  to  a  drastic 

with  this  problem,  different  optimization  heuristics 
have been designed to re-order RETE join sequences, 
but  the  common  aim  is  to  perform  the  most 
discriminating joins first.  

Two  major  approaches  are  the  most  specific 
condition  first  heuristic  [22,  46,  23,  32]  and  the 
pre-evaluation  of  join  connectivity  heuristic  [50, 
22, 23, 32]. Briefly, the former heuristic is based on 
the  rule  of  thumb  that  the  more  specific  a  rule 
condition  is  the  more  likely  it  has  fewer  matched 
facts. Accordingly pushing a more specific condition 
towards the front of a join sequence is more likely to 
generate  fewer  tokens  in  beta  network  [46,  32].  In 
practice,  there  are  three  criteria  for  evaluating  the 
specificity  of  a  condition  [32]:  (1) 
the  more 
variables  a  condition  has  the  less  specific  it  is,  (2) 
the more complex property a condition has the more 
specific  it  is,  and  (3)  the  less  facts  in  the  alpha 
memory  for  a  condition  when  a  RETE  reasoning 
ends  the  more  specific  the  condition  is.  As  will  be 
discussed in section 3.2, this research opts to use the 
third criteria. The pre-evaluation of join connectivity 
heuristic deems that a condition should have at least 
one  common  variable  with  previous  conditions 
where  possible.  Therefore, 
join 
sequences to avoid Cartesian product joins as much 
as possible. Cartesian product joins, if any, are  then 
pushed to the end of the join sequence, where fewer 
input tokens are.  

re-orders 

it 

A  direct  application  of  RETE  optimization 
heuristics  considers  only  rules  and  therefore  often 
fails to construct a customized join sequence for the 
particular fact base to be reasoned over [22, 23]. The 
work in [22, 23] shows that by taking characteristics 
of the fact base (ontology in this work) into account 
a  more  optimal  RETE  network  can  be  constructed. 
However the approach proposed in [22, 23] involves 
the  computation  of  a  complicated  cost  model  on  a 
large  amount  of  enumerated  join  sequences,  which 
in  our  case  is  not  quite  suitable  given  the  very 
limited  computational  resources.  In  section  3.2  a 
composition  algorithm  that  composes  a  reasoner 
using 
is 
presented. 

two  RETE  optimizations 

the  above 

2.3 Related Work 

This  section  discusses  work  related 

this 
research  including  resource-constrained  ontology 
reasoners,  join  optimization,  and  large-scale  rulebased reasoning. 

to 

2.3.1 Resource-constrained Ontology Reasoners 

Previous  work  has  been  devoted  to  resourceconstrained  OWL  reasoners  [3,  24,  26,  37,  38]  and 
some  optimizations  are  incorporated  to  reduce 
resource  consumption,  especially 
the  memory 
the 
footprint.  This  section  describes  some  of 
resource-constrained 
and 
optimizations. 

reasoners 

Mire4OWL  [26]  is  designed  to  run  on  smart 
mobile  phones  for  context-aware  computing.  It 
implements  the  rule-entailment  approach  and  RS 
rules.  To  avoid  an  ever-growing  fact  base,  it  keeps 
only the latest context facts and makes extensive use 
of  indexing  to  speed  up  duplication  checking  [14, 
26].  OR  [3]  is  designed  to  help  realize  ambient 
intelligent  medical  devices.  It  uses  dynamically 
generated  DS  rules  for  reasoning  over  assertional 
data  and  uses  RS  rules  for  reasoning  over 
terminological  data.  A  simple  forward-chaining 
pattern  matching  algorithm  is  designed  for  rules 
matching. Experiments show that OR only needs a 
small amount of  memory and time to reason over a 
very small ontology (100  300 triples). Bossam [24] 
was originally designed to be a desktop reasoner but 
it  has  been  implemented  to  run  on  J2ME  CDC 
compatible  mobile  devices.  Bossam  follows  the 
rule-entailment approach and RS rules. Based on our 
review of related literature, there is no evidence that 
it 
resource 
consumption 
resource-constrained  devices. 
mTableaux [38] uses a DL tableau calculi optimized 
for  running  on  mobile  devices.  mTableaux  uses  a 
similar  approach  to  ruleset  composition  to  reduce 
resource consumption:  It customizes the application 
of transformation rules according to the ontology but 
with  a  compromise  on  reasoning  completeness. 
Pocket KRHyper [37] performs DL reasoning using 
a FOL theorem prover and hyper tableau calculi [8]. 
Two optimizations are employed to  reduce resource 
consumption,  including  (1)  use  different  DL/FOL 
transformation  schemes  for  different  parts  of  the 
knowledge  base  and  (2)  use  distinct  treatments  for 
interests  and  disinterests.  Both  optimizations 
compose the reasoner according to characteristics of 
therefore  could  be 
the  knowledge  base,  and 
considered 
reasoner  composition 
approach. 

optimizations 

follow  a 

reduce 

for 

uses 

to 

to 

approaches.  However, 

None  of  the  rule-entailment  reasoners  use  any 
composition 
both 
Mire4OWL  and  Bossam  adopt  the  rule-entailment 
reasoning approach  and RS rules, they  should  work 
well  with  the  reasoner  composition  approaches 

as 

reasoning  in  OR  also  uses  RS  rules  and  should 
allow  our  reasoner  composition  approaches  to  be 
applied. 

2.3.2 Query Optimizations 

The  RETE  algorithm  and  query  evaluation  share 
some common characteristics: (1) They both need to 
partition  records  (or  fact  base)  according 
to 
conditions  specified  in  queries  (or  rules);  (2)  They 
both  need  to  join  partitions  in  order  to  find 
consistent  variable  bindings.  These  common 
characteristics  show  potential  for  the  adoption  of 
query 
for  RETE 
optimization. 
join  sequence 
reordering is a major type of RETE optimization and 
also  a  key  part  of 
the  reasoner  composition 
algorithms  proposed  in  this  research,  this  section 
focuses  on  query  optimization  techniques  related  to 
ordering join sequences.  

approaches 
that 

  Considering 

optimization 

Work  in  [56]  shows  that  a  poorly  ordered  join 
sequence  can  give  rise  to  considerable  degradation 
of  query  performance.  Much  of 
the  research 
approach  this  issue  by  first  enumerating  query 
execution  plans  using  dynamic  programming  and 
then  finding  an  appropriate  one  according  to  cost 
models  [57].  A  similar  approach  is  also  used  in 
RETE  join  sequence  optimization  [22,  23]  but  it 
uses  a  full  a-priori  RETE  execution  for  gathering 
statistics  about  the  fact  base.  The  evaluation  of  a 
cost  model  is  performed  after  the  a-priori  RETE 
execution.  

for 

join 

sequence  ordering 

Cardinality estimation is an idea that is commonly 
used 
in  query 
optimization.  Histogram  approaches  are  commonly 
employed for such a purpose [59, 57]. However, [58] 
points  out  that  as  semantic  constraints  are  often 
imposed  upon  RDF  data,  a  single  bucket  histogram 
approach is often harnessed in a way that could lead 
to  a  mis-estimation  of  cardinality.  The  authors 
propose  to  use  a  characteristic  set  for  cardinality 
estimation  for  RDF  data.  A  characteristic  set  for  an 
entity  is  the  set  of  predicates  that  appears  in  the 
same  triples  in  the  data  set.  This  approach  is  based 
on the observation that an RDF entity can usually be 
identified  by  only  a  subset  of  their  emitting  edges 
(or properties). This approach computes the number 
of  distinct  entities  in  the  characteristic  set  and  for 
each entity the number of occurrences of each of its 
emitting  edge.  These  statistics  are  annotated  in  the 
characteristic  set,  and  based  on  these  statistics,  the 
cardinality for triple pattern(s) is then calculated.  

However  the  iterative  characteristic  of  the  RETE 
algorithm  makes  it  difficult  to  produce  an  optimal 
RETE  network  by  directly  applying  query 
optimization  approaches.  This  is  because  compared 
to  a  single-round  query  evaluation,  each  RETE 
iteration will infer new facts, altering the dataset and 
therefore  possibly  offsetting  already  applied  query 
optimizations.  Nevertheless,  as  will  be  discussed  in 
Section  3.3.2,  optimizing  only  the  first  RETE  cycle 
is  sufficient  in  our  scenarios  as  for  the  selected 
ontologies  (they  could  represent  the  characteristics 
of  the  ontologies  to  be  used  in  our  scenario)  the 
majority of  RETE match and RETE join operations 
are performed in the first RETE iteration.  

2.3.3 Scalable Rule-based Ontology Reasoning 

Much effort has been spent on scaling rule-based 
reasoning  to  very  large  input  by  minimising  the 
amount  of  data  stored 
in  memory.  Similar 
approaches could also be applicable where there is a 
low  availability  of  memory,  as  manifested  by 
resource-constrained devices. 

It is often deemed that the transitive, reflexive and 
symmetric  semantics  of  owl:sameAs  can  cause 
considerable  inflation  in  the  number  of  inferred 
triples  [9,  61].  One  approach  to  such  an  issue  is  to 
use  a  single  entity  as  a  representative  of  a  group  of 
identical  entities  [9].  The  research  in  [61]  uses  an 
identify-and-merge  approach  to  build  large  sameAs 
cliques in a (comparatively) small memory. It loads 
sameAs assertions from disk by batch, merges them 
as  partial  cliques  in  memory  and  then  appends  the 
partial  cliques  to  cliques  back  in  disk.  Considering 
the  small-sized  ontologies  that  could  be  used  in 
resource-constrained 
research 
concentrates on pure in-memory reasoning. 

devices, 

our 

is  however 

The  SOAR  reasoning  system  [63,  62]  has 
incorporated  a  range  of  optimizations  to  achieve 
scalable ontology reasoning. It uses partial-indexing 
to  store  and  index  only  terminological  data  in 
memory.  Assertional  data 
left  on 
secondary  storage.  DS  rules  are  used  in  SOAR  for 
calculating  ontology  entailments.  To  reduce  the 
number  of  rules,  rules  with  the  same  condition  set 
are  merged.  In  addition  rules  are  indexed  based  on 
different  forms  of  triple  patterns  to  enhance  the 
speed  for  looking  up  the  right  rules  for  a  triple. 
Optimizations  in  SOAR  system  are  not  quite 
applicable in our research. The RS rules (pD* rules) 
used in our research are static and no two pD* rules 
share  the  exact  same  set  of  conditions.  Hence  the 
rule merging approach is not applicable. In addition 

may  lead  to  overhead  offsetting  the  benefit  gained 
from indexing.  

impairing 

A less related topic is parallel OWL inference [64, 
61,  65].  The  work  in  [64]  proposes  an  approach  to 
partition  the  assertional  data  of  an  ontology  so  that 
all data partitions can be reasoned by RDFS rules in 
parallel  without 
the  soundness  and 
completeness.  The  work  in  [65]  describes  WebPIE, 
a  semantic  reasoner  using  MapReduce  for  large 
scale  rule-entailment  reasoning  [65].  It  incorporates 
several  optimizations 
reasoning, 
including  loading  schema  triples  in  memory,  data 
pre-processing  to  avoid  duplicates  and  ordering  the 
application  of  RDFS 
rules.  Whereas  parallel 
reasoning  is  not  directly  related  to  our  research  in 
that our target is to investigate the composability of 
a  reasoner  on  a  single  device,  research  work  on 
parallel reasoning still shows potential for resourceconstrained  reasoning  on  devices  with  multiple 
processors  or  distributed  over  multiple  networked 
devices.  

to  speed  up 

3. Rule Entailment Reasoner Composition 

the  design  of 

This  section  presents 

two 
composition  algorithms  that  tailor  rule-entailment 
reasoning according to the particular ontology to be 
reasoned  over,  so  as  to  enable  resource-efficient 
ontology reasoning on resource-constrained devices. 
The  pD*  semantics  [44]  is  used  in  this  work  to 
illustrate reasoner composition algorithms. The pD* 
semantics  extends  RDFS  with  some  OWL  features, 
including 
(in)equality,  property  characteristics, 
property  restrictions,  disjointWith,  and  hasValue. 
Semantics  of  these  OWL  features  are  partially 
supported  in  pD*.  Other  OWL  features  such  as 
cardinality,  Boolean 
class 
expressions,  and  oneOf  are  not  included.  Given  the 
low  resource  availability  on  resource-constrained 
devices,  the  target  ontology  should  not  be  too 
complicated  and  hence  pD*  semantics  should 
provide sufficient  semantic support. In addition, the 
pD*  semantics  is  represented  as  a  set  of  RS  rules 
(often  termed  pD*  rules).  Each  rule  implements  a 
small  part  of  the  semantics,  facilitating  ruleset 
composability.  Furthermore,  the  pD*  semantics  has 
low 
having  PTIME 
entailment  complexity  without  variables  in  the 
ontology  and  NPTIME  entailment  complexity  with 
variables included.  

combinations 

of 

entailment 

complexity: 

3.1. Selective Rule Loading Algorithm 

to 

ruleset  according 

The selective rule loading algorithm dimensions a 
selected 
the  amount  of 
semantics  contained  in  the  target  ontology.  There 
are  two  motivating  factors  behind  selective  rule 
loading algorithm: (1) the diversity in the amount of 
semantics  included  in  different  ontologies,  ranging 
from  lightweight  taxonomies  using  (partial)  RDFS 
to ontologies with complicated concept descriptions 
using full OWL 1 or OWL 2  expressiveness [6, 33], 
and (2) the often very fine-grained semantics carried 
by  each  RS  entailment  rule.  Therefore,  by  avoiding 
the  loading  of  unnecessary  rules,  a  reasoner  can 
construct  a  smaller  RETE  network  and  hence  can 
reduce memory consumption and reasoning time.  

The  selective  rule  loading  algorithm  employs 
what  we  call  rule-construct  dependencies 
to 
describe  the  use,  as  well  as  the  generation  of  a 
construct  in  a  rule.  Each  entry  in  rule-construct 
dependencies  describes  the  terms  from  a  given 
vocabulary V,  where  VU, are required for a rule r 
to fire and which terms are generated  when the rule 
fires.  The  former  type  of  terms  is  called  premise 
terms  of  rule  r,  written  as             V ,  and  the 
latter  consequent  terms  of  rule  r,  written  as      
       V .  For  pD*  rules,  V  is  therefore  the  pD* 
vocabulary  (refer  to  [44]  for  the  set  of  pD* 
vocabulary).  Based  on  the  above  definitions,  each 
entry  of  the  rule-construct  dependencies  set  should 
look like 

For  example,  let         ,         ,  and          be 
rule  variables  (to  clarify,  in  the  remaining  part  of 
this  paper  rule  variables  are  denoted  in  this  way), 
then the rule-construct dependency for rule rdfs9  

(?v rdfs:subClassOf ?w)   (?u rdf:type ?v) 

(?u rdf:type ?w) 

is therefore 

A full list of all rule-construct dependencies for pD* 
rules can be found in [43].  

Before  the  selective  rule  loading  algorithm  is 
described  in  a  more  formal  manner,  some  notations 
are defined. Here we use O to denote the ontology to 
be  reasoned,  R  the  set  of  entailment  rules,  and     
the entailment closure of O  reasoned  by R. From a 

facts  as  what  included  in  WMj  where  j  is  the  last 
RETE  cycle.  Let     be  the  set  of  terms  from  the 
vocabulary V and that appears in O,  we then define 
    to  be  the  set  of  terms  from  V  and  also 

appearing  in    .  We  term   
   a  R-closure  of   . 
From  the  definitions  we  can  infer  that  a  rule       

will be fired in the reasoning of O only if        
A corollary would be that only if each premise term 
of a rule exists in the original target ontology or as a 
consequent  term  of  other  rules,  can  the  rule  be 
considered  as  a  candidate  rule  to  be  selected.  In 
other words, if some premises of   do not appear in 
   then the rule will not be fired and hence there is 

no need to load it. 

Based  on  the  above  analysis,  the  selective  rule 
loading  is  therefore  a  calculation  of  a  selective 

ruleset 
    . Figure 1 describes an iterative algorithm  for 

computing   

 . It takes an ontology O and a ruleset R as 

/* This procedure describes an algorithm for computing a 
selective ruleset   
  */ 
inputs. It outputs a selective ruleset   
PROCEDURE SELECT_RULES (O, R) 
1: 
2: 
3: 
4:  
5: 
6: 
7: 
8: 
9: 
10: 
11: 
12: 
13: 
14: 
15: 
16: 
17: 
18: 
19: 
20: 
21: 
22: 
END PROCEDURE 

r    empty 

WHILE TRUE 

                              premises ( )  

               END IF 
        END FOR 

                RETURN   
        END IF 
END WHILE 

                 consequences ( ) 

   END IF 

rules.    

to  already  selected 

 ,  it  checks  if    is  a  subset  of   

This procedure iteratively builds up   

   from    
    is 
according 
initialized  to  be    (line  3).  For  each  rule  r  that  is 

not  already  in   
  (line 12) and then add 
(line 8-11). If yes, add r to   
    (line  13-15).  This  is  because  there  is  a 
    to   
chance  that  r  will  be  fired  adding  its  consequent 
terms into the ontology. This process iterates until a 
fixpoint  is  reached  (see  line  7  and  line  19-20).  The 
premises(r)  and  consequences(r)  are  two  functions 
that correspondingly retrieve the set of premises and 
the  set  of  consequents  from  the  rule-construct 
dependency of r. 

The procedure           in Figure 1 is used to 
initialize    .  This  procedure  can  be  designed 
differently  according  to  the  ruleset  and  the  set  of 
vocabulary V on which the ruleset operates. Here we 
use a simple algorithm specifically designed for pD* 
rules.  In  general,  this  algorithm  constructs      by 
checking  the  appearance  of  pD*  constructs  in  O 
using pD* triple patterns. For example, if a match is 
found  between  a  triple  in  O  and  the  pD*  triple 
pattern  (?c  rdf:type  rdfs:Class),  terms  rdf:type  and 
rdfs:Class will be added into   . All patterns in pD* 
rules  are  checked  against 
the  ontology.  This 
algorithm is described in Figure 2. The definition for 
the  function  match  can  be  found  in  section  2.2.  To 
save  some  space,  only  a  part  of  the  algorithm  is 
presented but a full list of triple patterns used in this 
algorithm  is  given  in  Table  1.  The  rest  of  the 
procedure can be inferred accordingly.  

/* This procedure describe an algorithm for computing   . It takes an 
ontology O as input and outputs the set of constructs used in O. */ 
PROCEDURE FIND_TERMS (O) 
1: 
2: 
3: 
4: 
5: 
6: 
7: 
8: 
... 

        IF                                     THEN 

        END IF 
        IF                                          THEN 

        END IF 
... ... 
END FOR 
RETURN    

END PROCEDURE 

Fig. 2. An algorithm for computing    based on pD* semantics 

Fig. 1. An algorithm for computing the selective ruleset 

Table 1: pD* triple patterns 

pD* triple patterns 

?p rdfs:subPropertyOf ?q 
?v rdf:type ?w 
?c owl:equivalentClass ?d 
?r owl:hasValue ?o 
?p owl:inverseOf ?q 
?r owl:onProperty ?p 

?p rdfs:domain ?d 
?p rdfs:range ?d 
?c rdfs:subClassOf ?d 
?c rdf:type rdfs:Class 
?p rdf:type rdf:Property 
?v owl:sameAs ?w 
?r owl:allValuesFrom ?d 
?r owl:someValuesFrom ?d 
?p rdfs:equivalentPropertyOf ?q 
?p rdf:type rdfs:ContainerMembershipProperty 
?p rdf:type owl:FunctionalProperty 
?p rdf:type owl:InverseFunctionalProperty 
?p owl:equivalentProperty ?q 
?p rdf:type owl:TransitiveProperty 
?p rdf:type owl:SymmetricProperty 

an 

assertion 

The algorithm presented in Figure 2 should use a 
small amount of computational resources and hence 
is  suitable  to  run  on  resource-constrained  devices. 
However it is incomplete  for some situations  where 
OWL/RDFS constructs are used in uncommon ways. 
For  example,  a  RDFS/OWL  Full  ontology  could 
(ex:subSpeciesOf 
contain 
rdfs:subPropertyOf 
where 
rdfs:subClassOf is used in the object position, which 
pD*  triple  patterns  will  fail  to  detect  and  therefore 
can lead to missing rdfs9, rdfs11 and rdfs12c (given 
that  rdfs:subClassOf  is  not  used  anywhere  else  in 
the  ontology  in  a  common  way).  A  brute-force  but 
effective 
to  check  each 
RDFS/OWL  constructs  used  in  pD*  rules  in  all 
subject,  predicate  and  object  positions  but  this  will 
introduce a lot more overhead.  

solution  could  be 

rdfs:subClassOf) 

In 

rules. 

this  paper 

The  selective  rule  loading  algorithm  is  designed 
to  be  independent  of  a  ruleset.  SELECT_RULES 
operates on any vocabulary  V and ruleset R. Hence 
theoretically  it  can  be  applied  to  other  OWL 
reasoning rulesets such as OWL 2 RL rules as  well 
as  domain-specific 
the 
implementation  of  FIND_TERMS  is  designed  for 
pD*  semantics  only  (Figure  2).  However  as 
mentioned  earlier,  for  each  ruleset  and  vocabulary, 
FIND_TERMS  can  be  implemented  accordingly  as 
per Figure 2. In fact, our theoretical analysis of ruleconstruct dependencies for OWL 2 RL rules  in [43] 
would  indicate  that  the  selective  rule  loading 
algorithm can  operate on OWL 2 RL rules. Finally, 
the  selective  rule  loading  algorithm  is  independent 
of  reasoning  algorithms,  and  therefore  it  can  be 
applied  for  both  rule-entailment  reasoning  and 
resolution-based reasoning. 

limitation  of 

the  selective  rule 

loading 
algorithm  is  that  it  lacks  flexibility  in  adapting  the 
selective  ruleset  to  an  ontology  with  evolving 
terminology:  the  selected  ruleset  preserves  only 

rules  required  for  reasoning  the  original  ontology, 
and  terms  added  later  during  evolution  may  require 
deselected  rules.  A  naive  and  inefficient  solution  to 
this  problem  would  be  to  re-execute  selective  rule 
loading  algorithm  as  well  as  reasoning  whenever  a 
new term is added.  

3.2. Two-Phase RETE Algorithm 

at 

In general the two-phase RETE algorithm applies 
RETE  optimizations 
runtime  based  on 
characteristics  of  a  particular  ontology.  This 
algorithm  is  inspired  by  the  related  work  in  [23]. 
However, instead of performing a full a-priori RETE 
execution for collecting statistics of facts (ontology), 
a  novel  interrupted  RETE  construction  mechanism 
is designed where statistic collection, RETE network 
construction and fact matching are entwined. Rather 
than constructing the RETE network in one go as in 
the  original  RETE, 
interrupted  RETE 
construction  mechanism  constructs  the  network  in 
two  phases,  the  construction  of  the  alpha  network 
and  the  construction  of  the  beta  network,  and  inbetween  a  fact  matching  is  performed  on  the  alpha 
network  for  ontology  statistics  gathering.  Thus  the 
interrupted  RETE  construction  mechanism  can 
collect  statistics  required  for  composing  a  more 
ontology-specific  and  efficient  RETE  network 
without  resulting  in  unnecessary  consumption  of 
extra  resources  (unlike  with  a  full  a-priori  RETE 
execution). 

the 

to 

anything 

(T1-T3), 

conforming 

The  remaining  part  of  this  section  explains  the 
two-phase  RETE  algorithm  in  detail  using  an 
illustrative  example  comprised  of  an  ontology 
snippet  and  two  pD*  rules,  i.e.  rdfs9  and  rdfp15 
(Figure  3). The  ontology  snippet  basically  says  that 
Car  is  a  sub-class  of  Vehicle,  Fiat  is  a  sub-class  of 
Car 
the 
WithAnEngine  restriction  should  have  a  component 
as an Engine (T4-T7), myCar is a Car and one of the 
three  components  myCar  has  is  an  azrTurbo  which 
is  an  Engine  (T8-T12).  By  applying  rule  rdfs9  and 
rdfp15 to this ontology, it should infer that myCar is 
something  of  type  WithAnEngine  (I13),  and  myCar 
is  also  a  Vehicle  (I14).  This  example  is  crafted  for 
illustration  purposes  and  therefore  the  reasoning 
results  may  not  be  very  interesting  to  practical 
problems.  Furthermore, 
it  contains  a  named 
restriction  in  order  to  better  demonstrate  the  twophase  RETE  algorithm,  which  is  however  rarely 
seen in practical OWL ontologies.  

rdfp15 

(?v rdfs:subClassOf ?w)    (?u rdf:type ?v) 

(?u rdf:type ?w) 

(?v owl:someValuesFrom ?w)    (?v owl:onProperty ?p)    

(?u ?p ?x)    (?x rdf:type ?w) 

 (?u rdf:type ?v) 

Example ontology 
T1 
T2 
T3 
T4 
T5 
T6 
T7 
T8 
T9 
T10 
T11 
T12 

I13 
I14 

ex:Car rdf:type rdfs:Class . 
ex:Car rdfs:subClassOf ex:Vehicle . 
ex:Fiat rdfs:subClassOf ex:Car .  
ex:Engine  rdf:type rdfs:Class . 
ex:WithAnEngine rdf:type owl:Restriction . 
ex:WithAnEngine owl:onProperty ex:hasComp . 
ex:WithAnEngine owl:someValuesFrom ex:Engine . 
ex:myCar rdf:type ex:Car . 
ex:azrTurbo rdf:type ex:Engine .  
ex:myCar ex:hasComp ex:azrTurbo . 
ex:myCar ex:hasComp ex:alcon .  
ex:myCar ex:hasComp ex:energyMX1 . 

ex:myCar rdf:type ex:WithAnEngine . 
ex:myCar rdf:type ex:Vehicle . 

Fig. 3. Rule rdfs9, rdfp15 and an example ontology 

3.2.1 First Phase 

The  first  phase  constructs  an  alpha  network  and 
gathers  statistics  about  the  ontology.  To  reduce 
memory consumption in the alpha network, an alpha 
node  sharing  mechanism  is  used:  rule  conditions 
with the same triple patterns share an alpha node. In 
this  way  the  amount  of  resources  required  for  both 
pattern  matching  and  storing  tokens  can  be  shared. 
In the example, the  condition (?u rdf:type ?v)  from 
both  rdfs9  and  rdfp15  ((?x  rdf:type  ?w)  in  rdfp15) 
share the same alpha node. 

Then, rather than continue to build a beta network 
as per the  original  RETE algorithm, the ontology is 
matched against the alpha network only (without the 
presence  of  a  beta  network).  As  joins  are  not 
performed,  this  matching  should  be  fast.  Matched 
triples  are  cached  in  the  corresponding  alpha  node, 
and statistics about the ontology, such as the number 
of  matched  facts  per  condition,  or  join  selectivity 
factors  and  so  on,  can  then  be  gathered.  These 
statistics can be used in the next phase for ontologyspecific beta network construction.  

In this research only the number of facts matched 
to  each  condition  is  collected.  As  will  be  shown  in 
the  second  phase,  this  data  can  optimize  join 
sequences  in  an  effective  manner.  The  (partial) 
RETE network  for our example after the first phase 
should look like Figure 4. 

Fig. 4. The RETE network after the first phase 

3.2.2 Second Phase 

The  second  phase  first  builds  an  optimized  beta 
network  using  statistics  collected  in  the  first  phase, 
and then  completes the  first  RETE cycle by  joining 
facts  cached  in  the  alpha  network.  Two  heuristics 
are  applied  to  optimize  the  beta  network:  a  most 
specific condition first heuristic and a pre-evaluation 
of  join  connectivity  heuristic.  Both  heuristics  are 
widely  used  in  previous  work  for  RETE  join 
sequences optimization [22, 23, 32, 46, 50].  Details 
for  the  two  RETE  optimizations  can  be  found  in 
Section 2.2. 

Apply  Most  Specific  Condition  First  Heuristic  to 
Optimize the First RETE Cycle 

Section 2.2 introduces three criteria for evaluating 
a  conditions  specificity.  Among  them,  using  the 
number  of  facts  matched  to  each  condition  when 
RETE  reasoning  ends  is  directly  derived  from  the 
definition of condition specificity and therefore it is 
considered to be the most effective [32]. However to 
gather  this  data,  previous  research  often  performs  a 
full  a-priori  RETE  reasoning  over  the  fact  base 
(ontology). Furthermore, in order to find the optimal 
RETE  network  for  the  ontology,  all  possible  RETE 
join sequences are enumerated and evaluated against 
complicated  cost  models  [22,  23].  The  previous 
approach  should  work  well  for  systems  requiring 
constant re-reasoning over the same fact base (where 
a  one-off  resource-intensive  optimization  is  then 
worthwhile).  However  for  sensor-rich  systems 
various ontologies are often used, a-priori reasoning 
and  evaluating  all  possible  join  structures  against  a 
complex  cost  model  for  each  ontology  would  incur 
significant  optimization  overhead  offsetting 
the 
performance/resource gains.   

One may notice that statistics collected in the first 
phase  can  only  represent  the  status  of  the  RETE 
network in RETE cycle 1. As an iterative process, a 
RETE  algorithm  generates  inferred  facts  in  each 
RETE cycle. Hence statistics such as the number of 

number of triples matched for each condition collected in the 
       . This procedure 
first phase as condition specificity, i.e.   
   all alpha nodes for conditions of r after first phase 
takes   
as input and output a join sequence    optimized according to 
ontology statistics. 
 */ 
PROCEDURE ORDER_JOIN (  

1: 

FOR EACH   
         FOR EACH j FROM 1 TO             

2: 

3: 
4:                                  IF (             

5:                                                       
6:                                      JUMP TO 2: 
7:                                  END IF 
8:                         END FOR 
9:                END FOR 
10:              RETURN    
END PROCEDURE 

Fig. 5. The RETE network after the first phase 

Figure 6 shows the join sequences of our example 
after  the  application  of  the  most  specific  condition 
first heuristics. The join sequence of rdfs9 is already 
ordered  with  the  most  specific  in  the  first  and  the 
least specific one last and hence  no change is  made 
on  the  join  sequence.  The  rule  rdfp15  has  the 
positions  of  the  last  two  conditions  switched,  as  a 
wildcard  condition  is  always  the  least  specific 
condition  according  to  our  chosen  criterion  for 
specificity estimation. 

facts  matched  to  each  condition  and  join  selectivity 
factor  could  vary  from  one  RETE  cycle  to  another. 
When  represented  using  the  notations  introduced  in 
Section  2.2,  the  number  of  facts  matched  to  each 
condition  gathered  in  the  first  phase  is  written  as 
       |                            and  in  most 

       |  where  l  represents  the 
cases  |  
final RETE cycle.  

       |   |  

Then  the  question  is:  will  statistics  collected  in 
the  first  phase  be  sufficient  for  join  sequence 
optimization?  To  find  out  the  answer,  an  empirical 
study  was  performed  which  looked  into  the  RETE 
networks  generated  for  reasoning  19  small-sized 
ontologies.  These  ontologies  are  from  various 
domains  and  with  different  characteristics  (a  list  of 
all  used  ontology  is  given  in  Table  3).  Results 
indicate  that  for  most  of  studied  ontologies,  the 
majority of reasoning is carried out in the first RETE 
cycle:  15  out  of  a  total  of  19  ontologies  have  on 
average  75%  joins  performed  in  the  first  RETE 
cycle  and  for  the  remaining  4  ontologies  this 
percentage  is  still  above  50%;  furthermore  an 
average  of  83%  inferred  facts  are  generated  in  the 
first cycle. Therefore it appears to be appropriate to 
order  join  sequences  by  applying  optimization 
heuristics  based  on  statistics  collected  only  in  the 
first  phase.  Furthermore  collecting  statistics  in  the 
first  phase  lowers  the  optimization  overhead  as 
required by a-priori RETE reasoning of the ontology. 
Based  on  the  discussion  above,  using  the  most 
specific  condition  first  heuristic 
to  order  join 
sequence is then reduced to sorting conditions by the 
number of matched facts. Figure 5 gives the pseudo 
code  for  a  simple  implementation  of  this  heuristic, 
based  on 
insertion  sort  algorithm.  This 
implementation  is  used  in  our  prototype  resourceconstrained  reasoner  but  a  better  sorting  algorithm 
could be used to enable faster condition re-ordering 
when  a  large  entailment  ruleset  is  involved.  The 
pseudo 
the  RETE  notations 
introduced in section 2.2. Let a    be a join sequence, 
the  function              is  defined 
insert  a 
condition  into  the  join  sequence      at  a  specified 
position. 

employs 

code 

the 

to 

condition  in  the  join  sequence  after        that  can 
connect to        . If there is  no such a      , then 
      is  left  in  place,  and  the  algorithm  continues  to 
check  the  connectivity  of               in  the  same 
way.  Figure  7  describes  the  above  algorithm  in 
pseudo code.  

/* This procedure implements an algorithm for checking join 
connectivity. It takes a join sequence    as input and output a 
  . This procedure  
 */ 
PROCEDURE CONNECT_CONDITIONS (  ) 
1: 
2: 

var_prev   { } 
var   { } 

FOR j FROM 2 TO             

3: 
4:                       var_prev   variables(                     ) 

5: 
6: 
7: 

8: 

9: 

        var   variables(     ) 
        IF var_prev  var = { } THEN 
                ins   empty 

                FOR m FROM j TO            
                        IF variables(     )  var_prev = { } 

                               ins         
                        END IF 

10: 
11: 
12:                             END FOR 
13: 

                IF ins = empty THEN 

                        JUMP TO 4: 

14: 

15: 
16: 
17:                                     j   j+1 
18: 
19:                                 END IF 
20:                     END IF 
21:              END FOR 
22:              RETURN    
END PROCEDURE 

Fig. 7. The RETE network after the first phase 

The  function  variables  receives  conditions  as 
arguments  and  returns  the  set  of  variables  used  in 
given  conditions.  The  function            removes 
a specified condition from the join sequence. Line 6 
and  9  respectively  check  the  connectivity  between 
        and       and  between         and      .  As 
the  join  sequences  for  rdfs9  and  rdfp15  in  our 
example  are  already  connected,  the  application  of 
this heuristic will not change their join sequences.  

Fig. 6. An illustration of the most specific condition first heuristic 

in the two-phase RETE algorithm 

Ensure Connectivity of an Optimized Join Sequence 
and  Maintain  As  Much  As  Possible  the  Optimized 
Join Sequence  

The  pre-evaluation  of  join  connectivity  heuristic 
is  applied  after  the  most  specific  condition  first 
heuristic.  It  ensures  a  join  sequence  is  connected, 
namely  a  condition  should  share  common  variables 
with  conditions  it  joins  with.  As  the  pre-evaluation 
of join connectivity  heuristic  operates on an already 
optimized join sequence, changing the join sequence 
for  connectivity  will  to  some  extent  impair  the 
previous  optimization.  Therefore  the  aim  here  is  to 
ensure  the  connectivity  of  a  join  sequence  in  the 
meantime  maintains  as  much  as  possible  of  the 
already ordered join sequence.  

Similarly,  let    be  the  join  sequence  optimized 
the  most  specific  condition  first  heuristic, 
by 
                     )  the  ith  condition,             
    |  |  the partial join  sequence  from       to      . 
Our  algorithm  for 
join 
connectivity heuristic then scans    from start to end. 
For  each  condition       that  is  found  not  connected 
to        , the algorithm inserts in front of       the 
first  condition                     that  is  connected 

the  pre-evaluation  of 

Join Sequences and Finish the RETE Reasoning  

to 

the 

the 

The two-phase RETE algorithm then constructs a 
beta  network  according 
join  sequence 
produced  by  CONNECT_CONDITIONS.  The  first 
RETE cycle resumes and tokens  stored in the alpha 
memory  (because  of 
initial  matching  for 
statistics  collection)  are  passed  along  the  beta 
network  joining  to  each  other  as  per  the  normal 
RETE  algorithm.  Figure  8  compares 
two 
example RETE networks after all facts are matched.  
Tokens are listed in the box under the corresponding 
alpha/beta  nodes.  It  can  be  seen  that  the  amount  of 
tokens  in  the  alpha  network  of  Figure  8b  is  smaller 
than  those  in  the  counterpart  alpha  network  in 
Figure  8a,  because  of  the  use  of  the  node  sharing 
mechanism. The re-ordering of the join sequence of 
rule  rdfp15  causes  a  reduction  of  cached  tokens  in 
the  beta  network.  Both  algorithms  correctly  deduce 
tokens I13 and I14. 

the 

8(b) The complete RETE network constructed by the two-phase 

RETE algorithm 

Fig. 8. The complete RETE network 

3.3.3 Discussion  

It  is  clear  that  the  two-phase  RETE  algorithm  is 
only  applicable  to  RETE-based  algorithms,  but 
selective  rule  loading  algorithm  is  independent  of 
any  rule-based  reasoning  algorithms,  as  long  as  RS 
rules  are  used.  As  pointed  out  earlier,  the  selective 
rule  loading  algorithm  will  need  re-execution  to 
handle  a  changing  ontology.  However  unlike  the 
selective 
evolving 
terminology  is  not  a  problem  for  the  two-phase 
RETE  algorithm  as  it  loads  the  entire  ruleset.  Thus 
changes  of 
reflected 
immediately  in  the  reasoning  without  re-executing 
the entire composition.  

the  ontology  can  be 

algorithm, 

loading 

rule 

8(a) The complete RETE network constructed by the normal 

RETE algorithm 

Only  the  number  of  facts  matched  to  each 
condition in the first phase is used  here to optimize 
join  sequences.  This  could  lead  to  a  situation  that 
join  sequences  optimized  according  to  this  statistic 
are  effective  mostly  for  the  first  (few)  RETE  cycle 
(although  through  our  study  it  has  been  observed 
that  most  operations  and  resource  consumption 
happen  in  the  first  RETE  cycle).  We  envisage  that 
with  more  diverse  types  of  ontology  statistics, 
optimized join sequences can be even more effective 
not  only  for  the  first  (few)  RETE  cycles  but  for 
subsequent RETE cycles. It is also important to note 
that  the  two-phase  RETE  algorithm  optimisation  is 

As  a  result  the  authors  would  argue  that  with  some 
code-level  modification,  none,  one,  or  both  of  the 
two  reasoner  optimisations  could  be  applied  in  any 
rule-entailment reasoner implementation. 

4. COROR: A Composable Rule-entailment Owl 
Reasoner for Resource Constrained Devices 

The reasoner composition algorithms described in 
section  3  were  prototyped  as  a  proof-of-concept 
resource-constrained 
reasoner  named  COROR. 
COROR  is  only  implemented  to  investigate  if  the 
reasoner  composition  approaches  designed  can 
reduce  resource  consumption  for  rule-entailment 
reasoning.  Therefore 
the  minimal  number  of 
components was implemented with core components 
being 
the  composable  RETE  engine  and  a 
framework  for  loading,  parsing  and  manipulating 
ontologies. Many other components of practical use, 
such  as  a  query  interface,  a  remote  reasoning 
interface and so on, were omitted from this minimal 
implementation.  However  they  should  work  well 
with  our  composition  algorithms.  An  overview  of 
the COROR components is given in Figure 9. 

Fig. 9. An overview of COROR 

The  target  platform  for  COROR  is  Sun  SPOT 
sensor platform v4.0 (blue)4. Hardware and software 
specifications  of  this  platform  are  given  in  Table  2. 
The  choice  of  Sun  SPOT  sensor  platform  is 
motivated  by  two  aspects:  it  is  a  representative  of 
typical resource constrained devices and it has a full 
range  of  tools  supporting  Java  programming  using 
Netbeans  and 
testing  using  a  fully  functional 
emulator. 

4http://www.sunspotworld.com/ 

Table 2. Specs of the Sun SPOT sensor platform 

Flash 

180MHz 32 bit ARM920T 
512KB 
4MB 
Squawk VM 
J2ME CLDC 1.1 

than 

Rather 

implementing 

every  COROR 
component  from  scratch,  it  was  decided  to  refactor 
and  reuse  some  existing  software.  The  Jena 
framework  [49]  is  a  cut-down  version  of  Jena  for 
J2ME.  It  is  used  in  COROR  for  ontology  parsing 
and  manipulation.  However  as  reasoning  capability 
has  been  removed  from  Jena,  the  Jena  RETE 
engine was refactored and ported to run on Jena.  

in 

Triple  is  the  only  type  of  fact  for  Jena  RETE 
engine (and therefore COROR). A triple in COROR 
is  comprised  of  three  RDF  nodes  each  of  which 
respectively  represents  its  subject,  predicate  or 
object.  RDF  nodes  used 
the  ontology  are 
maintained  in  a  RDF  node  pool  in  COROR.  Only 
one  real  node  is  constructed  for  a  URI  and  the 
reference to the RDF node is used in different triples. 
Types  of  RDF  nodes  that  can  be  used  in  a  triple 
include  URI  nodes,  literal  nodes,  and  blank  nodes. 
Besides  the  above  three  RDF  node  types,  a  triple 
pattern  can  also  have  rule  variable  nodes  to 
represent  variables.  A  URI  node  and  a  literal  node 
keep  the  actual  URI/literal  string  in  the  node.  All 
triples are stored in a triple pool implemented using 
a J2ME Vector object. No index is built for the triple 
pool  nor  for  all  alpha/beta  memories.  Hence 
accessing  the  triple/token  is  performed  using  linear 
search.  The  triple  pool  also  serves  as  the  working 
memory  for  the  RETE  engine.  Tokens  used  in  the 
RETE  network  were  implemented  to  use  a  dual 
array:  one  array  for  rule  variables  and  the  other  for 
bound values of corresponding variables. The reason 
for the dual array is to enable node sharing.  

To improve resource efficiency for triple storage, 
a hash of URI/literal strings or an indexed  mapping 
dictionary  between identifiers (e.g. a  Long number) 
and URI/literal strings can speed up node searching 
and  comparison.  It  is  worth  noting  that  as  our 
research  concentrates  on  the  amount  of  resources 
that  can  be  reduced  by  reasoner  composition 
algorithms,  very  few  software  engineering  level 
optimizations  were 
such 
optimizations  should  not  be  exclusively  applicable 
to  our  composition  algorithms  and  they  can  be 
implemented 
stable  COROR 
implementation. 

used.  However, 

a  more 

on 

The  pD*  semantics  was  chosen 

the 
semantics  for  COROR.  It  was  implemented  as  39 
entailment rules in the Jena format: including 16 D* 
entailment rules and 23 P entailment rules. Rules for 
detecting clashes (XML-clash, part D-clash, P-clash, 
see  more  about  clashes  in  [44])  are  not  handled  in 
this  implementation.  This  is  consistent  with  the 
assumption  made  by  previous  work  that  in  a 
resource-constrained environment the consistency of 
ontology is either ensured by the ontology developer 
or checked offline by an ontology server [3, 38]. 

composition 

involved).  Third,  as  opposed 

The  choice  of  pD*  rules  instead  of  OWL  2  RL 
rules  is  motivated  by  a  number  of  reasons.  First,  at 
the time the implementation and the evaluation were 
undertaken  there  was  not  a  set  of  Jena  compatible 
OWL  2  RL  rules  available.  Second,  compared  to 
pD*  rules  implementing  OWL  2  RL  rules  as  Jena 
rules could be more error-prone (given that RDF list 
is  heavily 
to 
constructing  a  fully-fledged  resource-constrained 
reasoner, the main aim of this work is to study how 
reasoner 
resource 
consumption  for  rule-entailment  reasoning.  Hence 
pD*  was  chosen  as  a  more  straightforward  way  to 
carry  out 
the 
composition  algorithms  introduced  in  Section  3  are 
independent  of  rulesets  and  therefore  using  pD* 
rules does not exclude the possibility of OWL 2 RL 
rules  in  the  future.  In  section  3.5  of  [43]  ruleconstruct  dependencies  of  OWL  2  RL  rules  are 
discussed.  It  establishes  that  OWL  2  RL  can  work 
with  composition  algorithms.  Considering  the  shift 
from  OWL  1  to  OWL  2  for  most  Semantic  Web 
tools in the last few years, this topic will be studied 
in the near future. 

research.  Nevertheless, 

reduce 

this 

can 

The  analysis  of  rule-construct  dependencies  for 
pD*  rules  was  performed  manually.  It  was  a  onceoff  effort  and  rule-construct  dependencies  are  kept 
as  plain  text  in  a  local  text  file.  The  rule-construct 
dependencies file is loaded and parsed in memory at 
the beginning of selective rule loading algorithm. In 
the  future  when  domain-specific  rules  are  also 
integrated,  an  automatic  rule  analysis  procedure 
needs to be developed. 

Composition algorithms are implemented to work 
together and individually. We use different names to 
distinguish 
the  combinations  of  composition 
algorithms. They are 1:  COROR-noncomposable, 2: 
COROR-selective,  3:  COROR-two-phase,  and  4: 
COROR-hybrid.  The  above  modes  respectively 
correspond  to  the  use  of  none,  one,  or  two 
composition  algorithms. 
the  hybrid  mode, 

In 

selective  rule  loading  algorithm  first  produces  a 
selective  ruleset  and  then  the  two-phase  RETE 
algorithm  builds  a  customized  RETE  network 
according to the ontology to be reasoned over. 

5. Evaluation 

The aim of this evaluation  was to explore if, and 
to  what  extent  reasoner  composition  can  reduce 
resource  consumption  for  rule-entailment  OWL 
reasoning.  To  do  this,  the  evaluation  tested  and 
compared the performance of  the different  COROR 
composition  modes  to  reason  over  ontologies.  Two 
items  need  to  be  clarified  here:  (1)  the  term 
reasoning  refers  to  computing  all  entailments  of 
ontology  according  to  the  given  ruleset,  and  (2)  the 
term  performance  refers  to  both  time  performance 
and memory performance.  

5.1. Experiment Design and Execution 

the 

of 

the 

change 

rule-entailment 

Two  experiments  were  performed.  The  first 
experiment  was  an  intra-reasoner  comparison  that 
investigated 
reasoning 
performance  of  COROR  with  and  without  our 
composition  algorithms.  In  our  case,  four  COROR 
composition  modes  were  compared  side  by  side. 
inter-reasoner 
The  second  experiment  was  an 
comparison  that  compares  our  composable  ruleentailment reasoner with some other rule-entailment 
reasoners  from  the  state  of  the  art.  This  second 
experiment  studied  if  a  composable  rule-entailment 
reasoner  would  be  more  suitable  to  run  on  a 
resource-constrained device in comparison with off-
the-shelf 

performance  perspective).  To  do 
this  second 
experiment,  COROR-hybrid  was  compared  with  4 
other  in-memory  rule-entailment  reasoners:  Bossam 
0.9b45,  Jena  v2.6.3  (with  forward  reasoning  only, 
termed 
paper), 
BaseVISorv1.2.1, 
v3.0.10. 
COROR-hybrid  was  used  because  it  represents  the 
performance  gain  of  both  modes  together.  Selected 
reasoners have similar semantics to that of COROR, 
avoiding the bias that one reasoner is slower or uses 
more  memory  because  it  implements  a  lot  more 
the  other.  SwiftOWLIM  and 
semantics 
BaseVISor  adopted 
the  pD*  entailment  with 
axiomatic  triples  and  consistency  rules;  Jena  was 
configured  to  use  the  same  ruleset  as  COROR;  the 
expressivity  of  Bossam  (closed  source)  was  not 

this 
swiftOWLIM 

Jena-forward 
and 

reasoners 

(from 

than 

in 

time  on  either 

its  website 5 , 
defined  at 
publications [24], or implementation,  which made it 
difficult  to  judge  its  inference  capability.  Pellet  is 
also  included  only  to  give  readers  an  impression  of 
how  COROR  performs  compared  to  a  full-fledged, 
complete  DL-tableau  reasoner.  However  since  the 
expressivity of Pellet is larger than pD*, it is unfair 
to compare the performance  of Pellet  with COROR 
side by side, and this is also not our intention.  

using 

Memory and time used by each reasoner to reason 
over  an  ontology  are  measured  for  the  performance 
comparison.  Reasoning  time  is  measured  by  taking 
the  difference  of  the  time  recorded  before  and  after 
System.CurrentTimeMills(). 
reasoning, 
Memory  required  by  reasoning  is  measured  by 
subtracting  the  free  memory  from  the total  memory 
(Runtime.getRuntime().totalMemory()-
Runtime.getRuntime().freeMemory()) 
the 
reasoning,  when  the  RETE  network  reaches  its 
maximum  size  (all  entailments  are  fully  calculated 
and all tokens are cached) and memory measured at 
this moment should reach its maximum. 

after 

for 

Measurements 

time  and  memory  were 
performed  separately  in  different  executions  to 
reduce  interference  between  each  other.  The  Java 
methods  used 
to  measure  memory  are  not 
particularly 
reliable.  They  produce  only  an 
approximation  of  the  memory  usage.  To  reduce  the 
statistical errors in each time/memory measurement, 
each result of time/memory used in the evaluation is 
an  average  of  10 
individual  measurements. 
Furthermore  garbage  collection  was  performed  to 
release  garbage  so  interference  from  non-recycled 
garbage  memory  could  be  reduced  as  much  as 
possible.  This  was  done  by  explicitly  calling 
System.gc()  20 
each  memory 
measurement.  Therefore 
the  results  should  be 
sufficiently  accurate  for  the  evaluation  goal  of  this 
paper.  A  threshold  of  30  minutes  was  set  to  avoid 
excessively  long  reasoning.  Any  reasoning  process 
still running longer than this threshold was manually 
terminated. 

times  before 

inter-reasoner 

The intra-reasoner comparison  was performed on 
the  Sun  SPOT  sensor  platform  board  emulator.  It 
has  a  180MHz  processor,  512KB  RAM  and  4MB 
ROM.  The 
comparison  was 
performed  on  a  desktop  computer  as  reasoners 
selected  for  comparison  were  written  in  J2SE  and 
cannot run on Sun SPOT (which runs J2ME CLDC 
1.1). It is non-trivial to port them to a J2ME CLDC 
1.1  platform,  but  COROR  can  run  natively  on  a 

5http://bossam.wordpress.com/ 

J2SE  platform.  The  experiment  environment  for 
inter-reasoner  comparison  was:  Eclipse  Helios  with 
Java  SE  6  Update  14  with  128MB  maximum  heap 
size, Dual Core CPU @ 2.4GHz, 3.25GB RAM. 

Commonly  used  ontology  benchmarking  tools 
such  as  LUBM  [66]  could  have  been  used  to  carry 
out  performance  experiments  but  even  the  smallest 
dataset  generated  by  LUBM  (synthetic  data  about 
one department in an university) contains from 6000 
to  8000  triples,  which  requires  more  than  the 
threshold for COROR to reason on a Sun SPOT. For 
our evaluation we selected more appropriate smaller 
ontologies  (as  listed  in  Table  3).  The  expressivity, 
number  of  classes/properties/individuals,  and  the 
size  both  before  and  after  COROR  reasoning  are 
listed  (in  number  of  triples).  The  selection  of  the 
ontologies  was  based  on  three  factors:  (1)  they  are 
from  different  domains,  which  to  some  extent  can 
represent the diversity of characteristics of ontology 
used  in  resource-constrained  systems,  (2)  they  vary 
in  expressivities,  avoiding  unintentional  biases  that 
some  OWL  constructs  are  over-  or  under-used,  and 
(3) they are  well-known and commonly  used so are 
relatively  free  from  errors.  URIs  of  the  selected 
ontologies are given at the end of the paper.  

Table 3: Characteristics of ontologies used in the experiments 

Expressivity  Cls  Prop  Indv  Before  After 

teams  

owls-profile  ALCHIOF(D)  54 
Koala 

ALCON(D) 

university 

SIOF(D) 

ALHI(D) 

Beer 
mindswapper ALCHIF(D) 
Foaf 

ALCHIF(D) 
mad_cows  ALCHOIN(D)  54 
Biopax 

ALCHF(D) 

73  126 

Food 
mini-tambis  ALCN 
amino-acid  SHOF(D) 

atk-portal  ALCHIOF(D)  169  147 
Wine 

SHOIN(D) 

0  1080 

1  1465 

75  1499 

16  161  1833 

ALCF(D) 

Pizza 
tambis-full  SHIN 
Nato 

ALCF(D) 

0  1867 

395  100 

0  3884  10959 

194  885 

0  5924  15746 

5.2. Results and Discussions 

This section presents and discusses results of the 
experiments undertaken. 

number is 6 for COROR-two-phase, 2 for COROR-
selective,  but  none  for  COROR-noncomposable. 
Whenever the reasoning process cannot be achieved 
only  in  RAM,  extensive  paging  of  RAM  state 
required,  delaying 
to/from 
reasoning 
power 
consumption.  

(Flash)  storage 

time 

is 

and 

costing  more 

Figure  10a  and  Figure  10b  show  the  correlation 
between memory/time consumption and the sizes of 
reasoned  ontology.  The  choice  of  sizes  of  reasoned 
ontology  rather  than  sizes  of  original  ontology  is 
because  memory  measurements  are  collected  at  the 
end  of  reasoning  when  all  inferences  are  deduced. 
Trend lines are plotted for different COROR modes. 
In  general  the  memory  consumption  shows  a  linear 
increase  with  size  of  reasoned  ontology.  The 
different  slopes  of  trend  lines  indicate  that  the 
memory  consumption  of  COROR-noncomposable 
increases much faster than the other COROR modes. 
Reasoning  time  shows  a  quadratic  polynomial 
increase  with  size  of  reasoned  ontology.  Similarly, 
COROR-noncomposable  shows  a  much  steeper 
increase in  time than the rest. The time required by 
COROR-selective  to  perform  reasoning  is  less  than 
COROR-noncomposable  but  still  shows  a  much 
steeper  increasing  trend  than  that  of  the  other  two 
composable COROR modes.  

(a) Memory vs No. triple after reasoning 

5.2.1 Intra-reasoner comparison 

Time  and  memory  consumptions  required  by  the 
four  COROR  composition  modes  in  the  intrareasoner  comparison  are  listed  in  Table  4.  For  an 
ontology  that  requires  time  more  than  the  preset 
threshold  (30  minutes),  manual 
is 
imposed  and  therefore  no  results  were  collected  for 
these cases. 

termination 

Table 4: Results for inter-reasoner comparison (time in Second 

and memory in KB). NC = COROR-noncomposable; SL = 

COROR-selective; TP = COROR-Two-Phase, SL+TP = 

COROR-hybrid 

SL+TP 

teams 

profile 

koala 

beer 

Time Mem  Time Mem  Time Mem  Time Mem 

24.7  717  17.4  493 

7.5  219 

7.5  208 

41.9  867  27.2  459 

11.5  251 

11.6  232 

58.7  997  42.3  732 

14.8  276 

15.0  268 

70.1  1284  48.9  797 

16.8  329 

17.3  313 

university 

121.3  1069  76.0  883 

25.8  289 

25.9  284 

mindswappers 309.5  1800  189.4  738 

49.5  422 

50.3  401 

foaf 

517.6  2276  343.2  1579 

80.6  514 

81.8  500 

mad_cows 

609.2  2267  416.1  1526 

98.1  518 

97.5  509 

biopax-level1  N/A  N/A  760.2  1922  169.8  688  176.3  683 

food 

N/A  N/A  836.3  1721  164.8  711  163.7  696 

miniTambis 

N/A  N/A  N/A  N/A  321.3  960  320.1  947 

amino-acid 

N/A  N/A  N/A  N/A  307.3  838  307.3  824 

atk-portal 

N/A  N/A  N/A  N/A  999.2  1472  984.7 1455 

wine 

pizza 

N/A  N/A  N/A  N/A 1550.6  1716 1561.2 1698 

N/A  N/A  N/A  N/A  364.3  1064  384.3 1049 

tambis-full 

N/A  N/A  N/A  N/A  N/A  N/A  N/A  N/A 

nato 

N/A  N/A  N/A  N/A  N/A  N/A  N/A  N/A 

As  shown  in  Table  4,  a  composable  COROR 
(COROR  with  one  or  two  composition  algorithms 
enabled,  refer  to  columns  SL,  TP,  SL+TP)  greatly 
outperforms  a  noncomposable  COROR  (COROR 
with  no  composition  algorithm  enabled,  refer  to 
column  NC)  in  both  memory  and  time.  CORORselective  uses  on  average  35%  less  memory  than 
COROR-noncomposable (with a std. dev. 13%) and 
on  average  33%  less  time  (with  a  std.  dev.  4%). 
COROR-hybrid  uses  on  average  74%  less  memory 
than  COROR-noncomposable  (with  a  std.  dev.  3%) 
and on average 78% less time (with a std. dev. 6%). 
COROR-hybrid  uses  on  average  75%  less  memory 
than  COROR-noncomposable  (with  a  std.  dev.  3%) 
and on average 78% less time (with a std. dev. 6%). 
For  8  ontology  (i.e.  teams,  owls-profile,  koala, 
university, beer, mindswappers, mad_cows, and foaf) 
COROR-hybrid  requires  less  than  the  RAM  size  of 

0.00.51.01.52.02.501000200030004000500060007000Memory (MB)NCSLTPSL+TPTriplesNCSLSL+TPTPSun SPOT physical memoryCOROR, and hence the amount of tokens generated 
and  cached  in  alpha  network  (#TK)  and  beta 
network 
respective 
indicator for memory consumption of alpha network 
the beta network.  

(#TK)  was 

therefore 

the 

(b) Time vs No. triple after reasoning 

Fig. 10. Time/Memory vs No. triple after reasoning 

Figure  10b  also  enables  the  estimation  of  the 
maximum  ontology  size  that  different  COROR 
modes  can  handle  under  evaluation  settings  (Sun 
SPOT, 30 minutes time threshold). From Figure 10b, 
it  can  be  seen  from  the  trend  lines  that:  with  a  30 
min threshold, 180MHz CPU and 512KB RAM, the 
maximum  supported  ontology  size  is  around  2900 
triples (after reasoning) for COROR-noncomposable, 
around  3200  triples  (after  reasoning)  for  COROR-
selective,  around  6800  triples  (after  reasoning)  for 
COROR-two-phase  and  COROR-hybrid.  Knowing 
the  maximum  ontology  size  supported  by  each 
COROR  mode  could  allow  users  to  determine 
whether  an  ontology  is  suited  for  reasoning  with 
COROR  with  given  hardware  specifications  and 
threshold requirement. This can be achieved by first 
reasoning  over  the  ontology  using  a  desktop  ruleentailment  reasoner  using  pD*  rules,  and  then 
collecting  the  size  of  the  reasoned  ontology.    By 
adjusting  the  time  threshold,  COROR  will  have  a 
different maximum supported ontology size.  

  To 

investigate 

Although  both  COROR-two-phase  and  CORORselective  greatly  reduce  the  reasoning  time  and 
memory  consumption,  it  was  observed  that  in  the 
experiment  COROR-hybrid  performed  no  better 
than  COROR-two-phase. 
this 
observation, an in-depth examination was performed 
of RETE networks constructed by  the four COROR 
modes.  In  general  match  operations  and 
join 
operations  dominate  the  execution  of  a  RETE 
algorithm. We therefore chose the number of match 
operations  (#M)  and  the  number  of  join  operations 
(#J)  as  respective  indicators  for  the  amount  of  time 
spent  in  the  alpha  network  and  the  amount  of  time 
spent  in  the  beta  network.  Similarly,  the  amount  of 
tokens  generated  and  cached  in  the  network  (#TK) 
was  selected  as  an  indicator  of  memory  usage  for 

By measuring these indicators, it was noticed that 
compared  with  COROR-noncomposable,  COROR-
two-phase has less #M and #TK but has almost the 
same  #J  and  #TK.  This  implies  that  the  memory 
and  time  gain  of  COROR-selective  came  mainly 
from  alpha  nodes  of  deselected  rules  but  the  beta 
network benefit little from deselected rules. A close 
examination of the deselected rules shows that their 
join sequences are already optimized by the original 
pD*  rule  author.  Hence  in  an  execution  of  a 
COROR-noncomposable 
reasoning,  even  when 
these  deselected  rules  are  loaded  into  the  RETE 
engine, they still perform  very  few  superfluous join 
operations,  leading  to  a  nearly  empty  beta  network. 
For this reason, deselecting these rules in executions 
of  COROR-selective  does  not  achieve  substantial 
memory or time improvement in beta network. 

The  same  set  of  manually  optimized  pD*  rules 
was  also  used  for  COROR-two-phase  but  Table  4 
indicates  that  COROR-two-phase  yielded  much 
more  performance  gain  than  that  of  COROR-
selective.  Examination  of 
the  RETE  networks 
generated by COROR-two-phase executions reveal a 
similar  situation  that  most  performance  gain  came 
from the construction of a shared alpha network but 
the  join  sequence  reordering  heuristics  had  little 
effect  on  reducing  #J  and  #TK.  In  fact,  only  two 
rules,  rdfp11  and  rdfp15,  had  their  join  sequences 
reordered. For rdfp11, the first two conditions in the 
join  sequence  swapped  position,  leading  to  no 
change  in  #J  and  #TK.  For  rdfp15,  the  last  two 
conditions  in  the  join  sequence  are  switch  over 
first 
according 
heuristic,  leading  to  a  small  reduction  of  #J  and 
#TK.  However,  as  the  rule  rdfp15  accounts  for  a 
very  small  portion  of  the  total  #TK  in  the  whole 
RETE  network  (only  0.91%  for  the  wine  ontology 
and  even  smaller  for  the  other  tested  ontology),  the 
performance gain from reordering the join sequence 
of rdfp15 is subtle. 

the  most  specific  condition 

to 

that  COROR-selective  achieves 

From  the  above  analysis,  we  can  deduce  that  the 
reason 
less 
performance improvement than that of COROR-two-
phase  is  mainly  because  of  the  construction  of  a 
shared  alpha  network  in  the  two-phase  RETE 
algorithm.  In  fact,  conditions  are  highly  shared 

04008001,2001,6002,00001000200030004000500060007000Time (seconds)NCSLTPSL+TPTriplesNCSLTPSL+TP1,800Manual termination of reasoningrules,  e.g. 

the  condition 

(?v 
among  pD* 
owl:sameAs  ?w)  is  shared  by  rdfp6,  rdfp7,  rdfp9, 
rdfp10  and  rdfp11  and  the  wildcard  condition 
(?v ?p ?l)  is shared by 21 rules.  As only one alpha 
node  is  constructed  for  each  shared  condition,  the 
more  conditions  shared  among  rules  and  also  the 
more  rules  share  a  condition,  the  less  #J/#TK  per 
rule  and  hence  the  more  performance  gain  from  a 
shared  alpha  network.  On  the  other  hand,  it  also 
shows that for rulesets with highly shared conditions 
a  shared  alpha  network  can  save  more  resources  in 
alpha  network  than  merely  removing  parts  of  the 
alpha  network.  Since  COROR-hybrid  is  only  a 
simple  combination  of  the  selective  rule  loading 
algorithm  and  the  two-phase  RETE  algorithm,  the 
above  analysis  also  explains 
the  very  similar 
performance  of  COROR-two-phase  and  CORORhybrid (Table 4). 

To  establish  that  the  heuristics  introduced  in  the 
second  phase  of  the  two-phase  RETE  algorithm  do 
lead to a performance gain, we deliberately changed 
the  join  sequence  of  three  rules,  i.e.  rdfp1,  rdfp2, 
and  rdfp4,  to  make  them  less  optimized,  but 
maintaining  the  exact  same  semantics.  Modified 
rules  are  respectively  named  rdfp1m,  rdfp2m  and 
rdfp4m  (Figure  11).  The  performance  of  COROR-
two-phase 
and  COROR-noncomposable  were 
measured  again  using  the  same  settings  as  those  in 
Table  4  (on  the  same  Sun  SPOT  emulator  with  the 
same  set  of  ontologies),  using  both  (original  and 
modified)  rulesets.  Results  are  given  in  Figure  12. 
The  word  original  and  modified  are  affixed  to 
the  corresponding  COROR  mode  to  distinguish  if 
the original or the modified ruleset is used.  

rdfp1 
(?p rdf:type owl:FunctionalProperty)   (?u ?p ?v)   (?u ?p ?w)   

notLiteral(?v) 

(?v owl:sameAs ?w) 

rdfp1m 
(?u ?p ?v)   (?u ?p ?w)   (?p rdf:type owl:FunctionalProperty)   

notLiteral(?v) 

(?v owl:sameAs ?w) 

rdfp2 

(?p rdf:type owl:InverseFunctionalProperty)   (?u ?p ?w)   

(?v ?p ?w) 

(?u owl:sameAs ?v) 

rdfp2m 

rdfp4 

(?u ?p ?w)   (?v ?p ?w)   

(?p rdf:type owl:InverseFunctionalProperty) 

(?u owl:sameAs ?v) 

(?p rdf:type owl:TransitiveProperty)   (?u ?p ?v)   (?v ?p ?w) 

rdfp4m 

(?u ?p ?w) 

(?u ?p ?v)   (?v ?p ?w)   (?p rdf:type owl:TransitiveProperty) 

(?u ?p ?w) 

Fig. 11. Original and modified version of the rule rdfp1, rdfp2 

and rdfp4 

less  optimized  rules 

rules  does  not  change  much 

It is apparent from Figure 12a and Figure 12b that 
COROR-noncomposable-modified  uses  a  lot  more 
memory  and  time  than  COROR-noncomposable-
original  does  in  reasoning  over  the  same  ontology. 
Compared  with  results  presented  in  Table  4,  four 
more ontologies, i.e. Beer, Mindswapper, mad_cows, 
and  foaf,  cannot  be  reasoned  over  by  COROR-
noncomposable-modified  within  the  time  threshold, 
which indicates that poor performance  will result in 
running  a  set  of 
in  a 
noncomposable  reasoner.  However,  the  use  of  the 
modified 
in 
time/memory  performance  for  COROR-two-phase. 
Inspection into the RETE networks showed that join 
sequences  of  modified  rules  are  reordered  by  join 
sequence  reordering  heuristics  in  two-phase  RETE 
algorithm  to  the  same  join  sequences  as  they  are  in 
the original ruleset. This demonstrates that even the 
crude  and  simplistic  heuristic  used  in  the  fully 
automated join reordering process in COROR yields 
results  at  least  as  performant  as  rulesets  manually 
reordered  by  rule-authoring  specialists,  with  the 
added  advantage  that  the  reordering  applied  in 
COROR is also tuned to the particular ontology, and 
procuring  a  hand-optimised 
for  each 
ontology  is  infeasible.  Sub-optimal  join  ordering 
would  also  be  common  in  manually  authored 
application-specific rules  since these rules would be 
written  by  domain  experts 
rule-
authoring-experts.  Moreover,  compared 
the 
reduction  of  #M  and  #TK  in  COROR-two-phase-
modified, the reduction of #J and  #TK in COROR-
two-phase-modified  is  more  substantial,  therefore 
indicating that the majority performance gain comes 
from  the  join  sequence  reordering  in  the  second 
phase of the two-phase RETE algorithm.  

ruleset 

rather 

than 

to 

(b) Time performance 

Fig. 12. Reasoning performance of COROR-two-phase and 

COROR-noncomposable for both modified and original ruleset 

Based on the above discussion, it can be inferred 
that the two-phase RETE algorithm would be better 
at handling a ruleset with long and sub-optimal join 
sequences and with highly shared conditions, whilst 
the  selective  rule  loading  algorithm  would  work 
better on rules with shorter join sequences (no more 
than 2 conditions) or where most conditions are not 
shared among rules. 

5.2.2 Inter-reasoner comparison  

is 

comparable 

to 

Jena-forward 

Results  of  the  inter-reasoner  comparison  are 
given  in  Figure  13  (memory  performance  in  Figure 
13a and time performance in Figure 13b). As shown 
in  the  figures,  the  time  performance  of  CORORhybrid 
and 
BaseVISor.  However,  in  contrast  to  results  of  the 
intra-reasoner  comparison  presented  in  Table  4, 
where  COROR-hybrid  uses  much  less  reasoning 
time than  COROR-noncomposable, COROR-hybrid 
only slightly outperforms Jena-forward in reasoning 
time.  Considering  the  modifications  made  to  port 
Jena  RETE  engine  to  Sun  SPOT  (as  described  in 
section 4), we infer that the main reason causing this 
difference  is  that  J2SE  container  classes  used  in 
Jena-forward  are  much  more  optimized  than  the 
their  naive  counterparts  implemented  by  authors  of 
this research and Jena authors.  

SwiftOWLIM  is  the  fastest  reasoner  for  most 
ontologies  in  the  inter-reasoner  comparison  (except 

for  Tambis-full  where  BaseVISor  was  the  fastest). 
Bossam is also fast for some ontologies. For smaller 
ontologies  such  as  Teams,  OWLS-profile,  Beer, 
Bossam can compete with SwiftOWLIM, but it fails 
with  errors  for  four  ontology  including  Koala, 
University, tambis-full and NATO all of which are 
successfully reasoned over by other reasoners. Pellet 
generally  has  quite  constant  performance  for  most 
selected  ontologies 
their  sizes. 
However  for  smaller  ontologies  it  uses  more  time 
than  the  other  rule-entailment  reasoners.  Pellet 
requires  much  more  time  to  reason  over  wine  and 
mindswapper.  This  is  because  very  expressive 
structures  used  in  the  terminology  (which  in  the 
Wine  ontology  were  specifically  designed  to  stress 
the  reasoner)  slow  down  the  DL  tableau  reasoning 
process.  Pellet  does  not  have  results  for  the  Beer 
ontology because of inconsistencies.  

regardless  of 

In  the  experiment  the  memory  performance  of 
COROR  is  much  better  than  all  other  reasoners  for 
the tested ontologies. This is  especially the case  for 
smaller  ontologies.  The  memory  usage  of  Bossam 
and  Jena-forward  grows  much  faster  than  that  of 
COROR-hybrid as the size and the complexity of an 
ontology  increases.  For  example,  for  ATK-portal 
Bossam  uses  6  times  more  memory  than  COROR 
and  for  Wine  it  uses  13  times  more  memory  than 
COROR. The memory footprint for swiftOWLIM is 
much  larger  than  COROR  even  for  very  small 
ontology.  It  uses  20MB  of  memory  to  reason  over 
teams  which  is  15  times  larger  than  that  used  by 
COROR.  This  shows  that  swiftOWLIM  trades 
increased  memory  usage  for  time  improvements. 
BaseVISor hides its reasoning process from external 
inspection  so  it  is  not  possible  to  measure  its 
memory  usage  and  therefore  it  is  missing  from  the 
memory  comparison.  The  overall  results  indicate 
that a much smaller memory footprint is needed for 
COROR  (-hybrid)  to  reason  over  small  sized 
ontologies  without  sacrificing  time  performance. 
This  demonstrates 
from  a  performance 
perspective it is much more feasible to use COROR 
in resource-constrained environment. 

that 

(b) Time performance 

Fig. 13. Results of the inter-reasoner comparison 

6. Conclusion 

reduced 

fault-tolerance, 

There  are  many  reasons  why  some  semantic 
reasoning  should  be  performed  at  or  close  to  edgenodes  in  a  semantic  network  of  nodes,  such  as 
improved 
centralized 
processing load, reduced raw-data traffic and routing 
overhead,  more  localized  decision  making  and 
aggregation, faster reaction in time-critical scenarios, 
supporting  larger  distributed  knowledge-bases,  and 
so  on,  and  all  reasons  could  contribute  to  a  robust 
system  in  critical  situations.  In  most  cases  edgenodes  are  more  resource-constrained,  and  in  an 
extreme  case  of  a  semantic  sensor  network,  nodes 
are extremely low-specification. To enable semantic 
reasoning  in  such  cases,  resource-efficient  semantic 
reasoning  needs 
to  be  designed  for  resourceconstrained environments. Our research investigates 
the  use  of  reasoner  composition  to  reduce  the 
resource  consumption  of  rule-entailment  reasoning. 
Two  algorithms,  a  selective  rule  loading  algorithm 

and a two-phase RETE algorithm,  were designed to 
compose  an  optimized  reasoning  process,  without 
sacrificing  the  generality  or  semantics  supported  in 
the  reasoner.  A  performance  evaluation  of  a  naive 
proof-of-concept  implementation  of  the  algorithms 
indicates  that  reasoner  composition  based  only  on 
optimising the ruleset used can save on average 35% 
of the reasoning  memory and 33% of the reasoning 
time,  and  composition  based  on  extending  the 
reasoning algorithm can save on average 74% of the 
reasoning memory and 78% of the reasoning time. A 
comparison  between  our  implementation  and  other 
rule-entailment  reasoners  from  the  state  of  the  art 
(including  a  mobile  reasoner)  shows  that  our 
implementation  uses  much  less  memory  than  the 
others 
semantics)  without 
compromising  on  time.  The  results  show  reasoner 
composition  could  be  considered  a  pre-requisite  to 
enable 
semantic 
in 
resource-constrained environments. 

rule-entailment 

reasoning 

(with 

similar 

This  work  is  partially  supported  by  the  Irish 
Government 
in  Network  Embedded  Systems 
(NEMBES), under the Higher Education Authority's 
Program  for  Research  in  Third  Level  Institutions 
(PRTLI) cycle 4 and Science Foundation Ireland via 
grant 
("Federated,  Autonomic 
Management  of  End-to-End  Communications 
Services"). 

08/SRC/I1403 
