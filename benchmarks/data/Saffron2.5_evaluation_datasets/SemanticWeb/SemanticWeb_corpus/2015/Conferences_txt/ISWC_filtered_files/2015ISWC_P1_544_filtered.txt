General Terminology Induction in OWL

Viachaslau Sazonau(B), Uli Sattler, and Gavin Brown

The University of Manchester, Oxford Road, Manchester M13 9PL, UK

{sazonauv,sattler,gbrown}@cs.manchester.ac.uk

Abstract. Automated acquisition, or
learning, of ontologies has
attracted research attention because it can help ontology engineers build
ontologies and give domain experts new insights into their data. How-
ever, existing approaches to ontology learning are considerably limited,
e.g. focus on learning descriptions for given classes, require intense supervision and human involvement, make assumptions about data, do not
fully respect background knowledge. We investigate the problem of general terminology induction, i.e. learning sets of general class inclusions,
GCIs, from data and background knowledge. We introduce measures that
evaluate logical and statistical quality of a set of GCIs. We present methods to compute these measures and an anytime algorithm that induces
sets of GCIs. Our experiments show that we can acquire interesting sets
of GCIs and provide insights into the structure of the search space.

1 Introduction

An ontology is a machine-processable representation of knowledge about a
domain of interest. Ontologies are encoded in formal languages, such as the Web
Ontology Language [8], OWL, underpinned by expressive Description Logics,
DLs [1]. OWL ontologies are widely-used to represent and share knowledge in
application areas such as medicine, biology, astronomy, defence and others.1 An
ontology can contain data and background knowledge (terminology) where both
may be incomplete. One might benefit from finding informative correlations in
their data taking background knowledge into account. Those correlations may
suggest new axioms for the background knowledge or start new inquiries about
the data.

However, the problem of terminology induction is generally hard. Firstly, an
ideal solution should represent a coherent, self-contained, expert-level modelling.
Due to high expressivity of OWL and its Open World Assumption (OWA), the
search space can be vast or even infinite depending on the language chosen. Sec-
ondly, as usual, the quality of the result depends on the quality of the data which
can be incorrect, noisy or insufficient. Ideally, new knowledge should respect the
existing knowledge along with the data in order to be maximally informative
and avoid contradictions.

Thus, some restrictions and assumptions that simplify the problem are nec-
essary. Another consequence is that any induced knowledge is hypothetical only

1 http://bioportal.bioontology.org/
c Springer International Publishing Switzerland 2015
M. Arenas et al. (Eds.): ISWC 2015, Part I, LNCS 9366, pp. 533550, 2015.
DOI: 10.1007/978-3-319-25007-6 31

V. Sazonau et al.

and requires a domain expert judgement. The contributions of this paper are as
follows.

 We state the problem of general terminology induction, i.e. learning sets,
called hypotheses, of general class inclusions, GCIs, from data (ABox) and
background knowledge (TBox).

 We view the problem as multi-objective and define quality criteria for a
hypothesis: readability, logical quality, and statistical quality. We define quality measures for a hypothesis that respect the OWA, interactions between
axioms in the hypothesis, and interaction of the hypothesis with the background knowledge.

 We have designed and implemented methods to compute the quality measures.
 We have designed, implemented and evaluated an anytime algorithm for
general terminology induction. We have gained insights into the structure of
the search space and developed heuristics to find out promising hypotheses.
The experiments show that we can indeed learn interesting hypotheses.

2 Preliminaries

We assume the reader to be familiar with DLs [1] and OWL [8]. The following
nomenclature is used throughout this paper. O = T  A is an ontology where
T , A are TBox and ABox, respectively. NC, NR, NI are disjoint and countably
infinite sets of class, property, and individual names, respectively.  is a sig-
nature, T , A, O are signatures of T ,A,O, respectively. ind(O) = NI  O is
a set of individual names occurring in O.  is a general class inclusion, GCI,
also called axiom. A, B, X, Y are atomic classes (class names), C, D are complex
classes (class expressions), R is a property, a, b, c, d are individuals. mod(O, )
is a module [5] of an ontology O given a signature . C is a set of (possibly
complex) classes. H is a hypothesis, H is a set of hypotheses. In the following,
ABox and TBox are called data and background knowledge, respectively.

3 Related Work

Ontology learning approaches can be characterised along several dimensions. The
first one is a type of the data source, e.g. texts, RDF(S), an oracle (a domain
expert), positive and negative examples for a class along with the ABox. The second one is a type of the output knowledge, e.g. class descriptions, class inclusions,
and its expressivity. The third dimension is methods used: natural language
processing, machine learning, association rules mining, oracle queries, Formal
Concept Analysis (FCA), least common subsumer (LCS) computation, etc. The
fourth dimension is semantics used that can differ from the OWL semantics, e.g.
the Closed World Assumption (CWA). One more characteristic is appreciation
of available background knowledge. Finally, the degree of domain expert involvement into the learning process greatly varies across approaches. A survey can be
found in [12].
?

?

?
We concentrate on learning from instance-level data, i.e. both class and property assertions. Among the approaches aimed at this type of input data are class
description learning, CDL [3,6,11], knowledge base completion, KBC [2], association rules mining, ARM [17].

The main method of CDL is machine learning, in particular, Inductive Logic
Programming, ILP [13]. The goal is to find a good description (class expres-
sion) of a given class name from a set of positive and negative examples [11] for
it, i.e. learning is supervised. The class description must cover all positive and
none of the negative examples. Learning is essentially a search in the space of
class expressions guided by refinement operators and heuristics. The background
knowledge can be used to optimize the search by exploiting the classification hier-
archy. To supervise learning, a domain expert has to provide additional information in form of positive and negative examples for a given class, which can
be difficult. As a consequence, there are techniques to sample examples from
data. In particular, instances of the class are taken as its positive examples and
the CWA is made to obtain its negative examples. However, this way can cause
problems [10]. Another method of CDL is finding the least common subsumer
(LCS) [3]. LCS is computed from the most specific class (MSC) of each instance
of a target class. The method, however, is only applicable to weakly expressive
languages.

KBC is based on Formal Concept Analysis (FCA) [7]. It is aimed at acquiring
(in some sense) complete knowledge bases, in contrast to CDL. KBC requires
to define a set of class expressions in advance which can be hard. The degree
of domain expert involvement is high as the expert judges axioms and has to
supply a counterexample in the case of rejection. One more limitation is that
standard FCA can only be applied under the CWA and the OWA of OWL
requires modifications of FCA [2].

ARM is yet another approach to ontology learning [17]. Association rules are
mined from transaction tables where columns are predefined class expressions
which, similarly to the case of KBC, can be difficult to define in advance. In contrast to KBC, ARM, however, permits acquiring axioms that have counterex-
amples. In contrast to CDL, ARM induces class inclusions and demands neither
positive nor negative examples. The approach focuses on weakly expressive lan-
guages. Among other restrictions are its CWA and little appreciation of interaction between induced axioms and the background knowledge, as well as mutual
interactions between induced axioms, since they are acquired independently.

Thus, ontology learning approaches simplify the problem in different aspects.
As a result, there is no approach that has all following capabilities: learns sets of
GCIs, appreciates interactions between axioms within the set and interactions of
the set with the background knowledge, uses standard OWL semantics, requires
no supervision, does not demand frequent human interventions.

4 Settings and Assumptions

This paper is aimed at addressing the problem of inducing general terminological
knowledge from data and background knowledge which together constitute the

V. Sazonau et al.

input ontology. New knowledge is acquired in form of hypotheses. A hypothesis
is a set of axioms which does not contradict the input ontology, i.e. consistent
with it, and carries new information, i.e. informative for it.
Definition 1. (Hypothesis) An axiom  is informative for an ontology O if
O  . A set H of axioms (GCIs) is called a hypothesis for an ontology O if H
is consistent with O, i.e. O  H    , and each   H is informative for O.
A hypothesis is evaluated by quality criteria: readability, statistical quality,
and logical quality. Clearly, a hypothesis can be better on one criterion and worse
on another. Therefore, we view terminology induction as a multi-objective problem where objectives are quality measures corresponding to the quality criteria.
Hypotheses are presented to a domain expert who accepts some of them and
rejects others. In order to suggest, or recommend, good hypotheses first, a preference relation based on quality measures is imposed on the set of hypotheses.
In this paper, we apply the following settings.

(i) We use OWL and its standard semantics.

(a) We allow for the usual OWA, i.e. for an instance a and a class C it is
possible that O  C(a) and O  (C)(a). As a consequence, data can
be regarded as just incomplete.

(b) Data normally consists of both class and property assertions, e.g. people
with family relations, proteins with interactions between them.
(c) We consider any logic for which subsumption, O |= C  D, and instance
checking, O |= C(a), are decidable. We use OWL ontologies and reason-
ers.
(ii) Any input ontology O is consistent, i.e. data contains no noise which causes

inconsistency.

(iii) Learning is unsupervised, i.e. no additional information is required in form

of positive or negative examples.

(iv) A set C of target (possibly complex) classes is fixed and finite.

The goal of induction is finding good hypotheses over classes C, or C-
hypotheses. In the following, we only consider C-hypotheses and omit C from
the name. We also define C
Definition 2. (C-Hypothesis) Given an ontology O, a hypothesis H for O is
called a C-hypothesis if   H implies  = C  D, where C, D  C

 := C  {C | C  C}.

.

It makes sense to establish a correspondence, sufficient for the task at hand,

between an ontology O and classes C, which we call projection.
Definition 3. (Projection) A projection  of an ontology O to C is
  a  ind(O)}.

(O, C) := {D(a) | O |= D(a)  D  C
?

?

?
Thus, a projection is a set of positive and negative class assertions over classes
C entailed by O. A projection can be viewed as a table where rows are labelled
with individuals ind(O) and columns are labelled with classes C. Each cell with
indices a, C can contain one of three possible values: 1 if O |= C(a), 0 if
O |= C(a), ? if O  C(a) and O  C(a). Although there are similarities
with a transaction table of ARM, our table view is imaginary only and it permits
question marks. We will use the table view for better presentation of examples,
see Example 1 and Table 1.
Example 1. Given C = {A, B,R.B}, T = ,

A = {A(a1), A(a2), A(a3), A(a4), (A)(b), (A)(c), B(c)

R(a1, b), R(a2, b), R(a3, b), R(a4, c)}.

Table 1
A B R.B

a1 1 ? ?
a2 1 ? ?
a3 1 ? ?
a4 1 ? 1
b 0 ? ?
c 0 1 ?

We use the projection to evaluate how well a hypothesis fits
the known data assuming it is correct on the unknown data.
Indeed, due to the OWA, a hypothesis can make assumptions
on the unknown data by turning question marks into ones or
zeros. If a hypothesis makes too many assumptions, it may
be too strong, e.g. H = {  CCC}. Therefore, it is
necessary to evaluate how brave a hypothesis is.
Definition 4. (Assumption) An assumption of a hypothesis
H in an ontology O given C is
(H,O, C) := {D(a) | O  D(a)  O  H |= D(a)  D  C

  a  ind(O)}.
As a consequence, (H,O, C) (O, C) =  for any hypothesis H. Requiring
O  (D)(a) in Definition 4 is not necessary because if O |= (D)(a) then
H is not a hypothesis due its inconsistency with O. Hypotheses making fewer
assumptions are preferred according to Occams razor.

One can think of suggesting hypotheses as single axioms. However, this approach ignores interactions between axioms that can influence the quality of the
hypothesis. Two axioms, which are logically good individually, do not necessarily create a logically good hypothesis. For example, a hypothesis can become
redundant, e.g. H = {A  B,B  A}, see Section 5.2. In fact, a set of two
logically good axioms is not necessarily a hypothesis. For example, given that
{A  B} and {B  C} are hypotheses for O, a set {A  B, B  C} is not
a hypothesis for O if O |= (A  C)(a). Similar to logical quality, two axioms
which are statistically good individually may not create a good hypothesis
which is discussed below, see Section 5.3.

5 Quality Criteria and Measures for a Hypothesis

5.1 Syntactic Length as a Readability Measure

Readability is the ease with which a hypothesis can be read and understood by
a human. One of possible measures of readability is the usual syntactic length of
a hypothesis.

V. Sazonau et al.

Definition 5. (Syntactic Length) Let A, C, D be (possibly complex) classes, A 
NC a class name, R  NR a property name, a  NI an individual name. The
syntactic length of a GCI is defined as follows: |C  D| := |C| + |D|, where
|| = || = |A| := 1, |C| := 1 + |C|, |C  D| = |C  D| := 1 + |C| + |D|,
|R.C| = |R.C| := 1 + |C|, |  nR.C| = |  nR.C| := 1 + n + |C|. The
syntactic length of a hypothesis H is |H| :=

H ||.
?

?

?
5.2 Logical Quality

Logical quality evaluates logical properties of a hypothesis: logical strength and
redundancy. Logical strength is commonly called generality in machine learning.
Definition 6. (Logical Strength) A hypothesis H is weaker (more general) than
another hypothesis H if H |= H and H  H.

A hypothesis can contain axioms which are superfluous, or redundant, within
the hypothesis, even if those axioms are informative. For example, axiom A  C
is redundant in hypothesis {A  B, B  C, A  C} and axiom B  A is
redundant in hypothesis {A  B,B  A}. Axioms can also have redundant
parts. For example, D is a redundant part of axiom A  C  D in hypothesis
{A  B  D, A  C  D}.
Definition 7. (Redundancy) A hypothesis H is redundant if there exists a
hypothesis H such that H  H and |H| < |H|. Otherwise, H is non-redundant.
Lemma 1. If a hypothesis H is non-redundant, then |H| = min{|H| | H  H}.
We define the logical strength and redundancy of a hypothesis H regardless
of O. The reason is that an axiom   H, which is informative for O and nonredundant in H, can be interesting, even if it is not informative for O  H\{}.
Such axiom reveals yet only implicit (and possibly unknown) relation between
classes. Additionally, the search for good hypotheses would require entailment
checking O  H |= H which could make it infeasible for hard ontologies.

5.3 Statistical Quality

Statistical quality criteria are aimed at selecting hypotheses that best represent
data given background knowledge. In order to comply with the standard OWL
semantics and its OWA, we consider the statistical quality of a hypothesis as
two-fold. Firstly, hypotheses differently fit data along with background knowl-
edge. Secondly, hypotheses make different number of assumptions in data given
background knowledge, i.e. some hypotheses are more cautious than others. Statistically better hypotheses have greater fitness and lower braveness.
?

?

?
Fitness and Braveness. In order to evaluate the statistical quality of a hypoth-
esis, we exploit the idea that axioms can encode regularities in the data. Those
regularities can be used to compress the data, i.e. to present it in a shorter
way. This is the fundamental principle of the minimum description length induction [4,16]. According to it, the better a hypothesis fits the data, the shorter
description of the data it provides.

A standard way of measuring the description length is using syntactic mea-
sures. However, syntactic measures do not respect logical
interactions of a
hypothesis with data and background knowledge. Therefore, we introduce a
semantic measure of the description length. We define fitness and braveness
of a hypothesis as follows.
Definition 8. (Description Length, Fitness, Braveness) The description length
of an ABox B given an ontology O = T  A is

minSize(B,O) := min{|B|

| B  O  B  O}.

Given an ontology O, a set C of classes, and a hypothesis H, let  := (O, C)
and  := (H,O, C). Then
(i) fitness of H is f it(H,O, C) := ||  minSize(,T  H),
(ii) braveness of H is bra(H,O, C) := minSize(,O).

As a consequence of Definition 8, all semantically equivalent hypotheses have

the same fitness and the same braveness which is stated by Lemma 2.
Lemma 2. Given an ontology O, a set C of classes, and two hypotheses
H1, H2, if H1  H2 then f it(H1,O, C) = f it(H2,O, C) and bra(H1,O, C) =
bra(H2,O, C).

Fitness of a hypothesis indicates how well the projection
can be shrunk using the hypothesis and background knowl-
edge, i.e. a better shrinkage corresponds to a better fitness.
Braveness of a hypothesis measures how many assumptions it
makes in the data given the background knowledge. Respecting Occams razor, hypotheses of lower braveness (or more cautious) are pre-
ferred, see Example 2.
Example 2. The projection  is given by Table 2, T = . For H1 = {A  B}
f it(H1,O, C) = |B(a)| = 1, bra(H1,O, C) = |B(b)| = 1. For H2 = {B  A}
f it(H2,O, C) = |B(a)| = 1, bra(H2,O, C) = 0. Hence, H2 is statistically better
than H1.

Table 2
A B
a 1 1
b 1 ?

Two axioms which are statistically good individually
may or may not create a good hypothesis, see Example 3.
Example 3. The projection is given by Table 3, T = .
Hypotheses H1 = {A  B}, H2 = {B  C}, H3 = {A 
C} are individually statistically confident: f it(H1,O, C) =

Table 3
A B C
a ? ? 1
b 1 1 1
c 1 1 1

V. Sazonau et al.

f it(H2,O, C) = f it(H3,O, C) = 2. However, hypothesis H23 = H2  H3 has
the same fitness as H2, H3: f it(H23,O, C) = 2. On the other hand, hypothesis
H12 = H1  H2 has the fitness twice as big as one of H1, H2: f it(H12,O, C) = 4.
In addition, axioms in the hypothesis can enforce each other, see Example 4.
Example 4. The projection is given by Table 4, T = {B  C}. Hypotheses
H1 = {A  B}, H2 = {C  D} individually have zero fitness. So, the fitness of
collective hypothesis H12 = H1  H2 is greater than the total fitness of H1 and
H2: f it(H12,O, C) = 3.

Although projection simplifies induction, we may lose some
information, in particular, relations between individuals. The
latter can result in the overestimation of hypothesiss assump-
tion. In Example 1 let hypothesis H = {A  B}, then
(H,O, C) = {B(b), (R.B)(a1), (R.B)(a2), (R.B)(a3)}.
However, (R.B)(a1), (R.B)(a2), (R.B)(a3) are, in fact, the
consequences of B(b) and should not be counted. Braveness correctly handles
this: bra(H,O, C) = |{B(b)}| = 1. Illusive assumptions can also be forced by
background knowledge and braveness handles this as well, see Example 5.
Example 5. The projection  is given by Table 5, T = {B  C  D}
and H = {A  B, B  D}. The assumption of H is (H,O, C) =
{B(a), B(b), D(a), D(b)} and the braveness is bra(H,O, C) = |{B(a), B(b)}| = 2.

Table 4
A B C D
a 1 ? ? 1
b 1 ? ? 1
c 1 ? ? 1

As a consequence of Definition 8, fitness and braveness are
semantically sound and syntax independent measures of the
statistical quality of a hypothesis. They take into account both
the interaction of a hypothesis with the background knowledge
and interactions between axioms within the hypothesis. The
measures respect the standard OWL semantics, in particular, they deal with
its OWA and, consequently, with incomplete data. Finally, they demand no
supervision, such as positive or negative examples, and no additional information
besides the input ontology.

Table 5
A B C D
a 1 ? 1 ?
b 1 ? 1 ?

Computing Fitness and Braveness. Computing fitness and braveness
requires finding the size of the minimal projection and assumption, respectively.
These may not be unique. All minimal subsets can be found using a hitting set
tree algorithm [14]. However, this may require an exponential number of reasoner
updates which is computationally expensive given that the fitness and braveness
are computed for each hypothesis.
Fortunately, there is a more efficient way to compute the fitness and braveness
of a hypothesis avoiding reasoner updates. The idea is to introduce into O fresh
}, and exploit
names for classes from C
the inferred class hierarchy of OX. The function minSizeU p(B,OX) computes

, i.e. OX = O  {XC  C | C  C
?

?

?
an upper bound of the description length minSize(B,O), which is used for
calculating fitness and braveness (see Definition 8):
minSizeU p(B,OX) := |B|  |redun(B,OX)|, where

redun(B,OX) := {D(a)  B | there is C  OX s.t. either

(i) OX |= C  D  OX  D  C  (OX |= C(a)  C(a)  B) or
(ii) OX |= C  D  D = unique(D,OX)}, where

unique is a function s.t. unique(D,OX)= D

implies OX |= D D.
minSizeU p(B,OX) is based on detecting redundancy of B given OX,
redun(B,OX), which is the set of those class assertions that can be easily
inferred from B  OX after full classification of OX. This avoids costly reasoner updates: a reasoner can be executed just once for each hypothesis to
classify classes and individuals. However, minSizeU p(B,OX) can overestimate
minSize(B,O) if some redundancy is missed by it. Hence, fitness can be underestimated and braveness can be overestimated, i.e. we may label a hypothesis
worse than it is.

6 General Terminology Induction

According to Definition 1, we only consider hypotheses which are logically sound,
i.e. informative and consistent with the background knowledge and data. The
goal of the induction is finding among those hypotheses ones which have maximal
fitness and minimal braveness, or better represent the data.

We impose a readability constraint on a hypothesis: it must not exceed a
given syntactic length. The logical weakness of a hypothesis is reflected by its
braveness: weaker hypotheses have a lower braveness and are preferred (respect-
ing their fitness) according to Occams razor. A redundant hypothesis has the
same fitness and braveness as its non-redundant counterpart but a greater length
that might be occupied by better axioms. We state the problem of general terminology induction in OWL as follows.
Definition 9. (General Terminology Induction) Given an ontology O and a set
C of classes, the problem of general terminology induction is to find all best
hypotheses which do not exceed length .

Thus, as in ILP, we view induction as search in the space of hypotheses
restricted by a language bias, determined by C and  in our case. We regard
the process of constructing hypotheses as being equivalent to ranking them in a
justified way which is based on fitness and braveness.

6.1 Dominance and Anytime Algorithm

So far, the comparison of hypotheses and terms better, best have not been
fully defined. We now define an order on hypotheses via dominance.

V. Sazonau et al.

Definition 10. (Dominance) Given an ontology O and a set C of classes, a
hypothesis H dominates a hypothesis H, written H  H, if H = H and either
(i) f it(H,O, C) > f it(H,O, C)  bra(H,O, C)  bra(H,O, C), or
(ii) f it(H,O, C)  f it(H,O, C)  bra(H,O, C) < bra(H,O, C).

C: a set of concepts
: maximal syntactic length of a hypothesis
stop: termination criteria

By Definition 10 dominance  is a strict partial order, i.e. two different
hypotheses may be incomparable. Best hypotheses are those which are dominated by no other hypotheses. Definition 10 considers only two competitive
objectives: fitness and braveness. In addition, we compare hypotheses only if
they have the same signature because otherwise interesting hypotheses could be
discarded.
The size of the search space depends on C and . It varies from 2  |C|2
(if a hypothesis is restricted to be a single axiom) to 2|C|2 (if a hypothesis is
permitted to include all possible axioms). Consequently, the explicit enumeration
can be infeasible. We employ an anytime algorithm, Algorithm 1, that attempts
to explore promising regions of the search space first.
Algorithm 1. induceHypotheses(O, C, , stop)
1: inputs
2: O: an ontology
3:
4:
5:
6: outputs
7:
8: do
9: OX = O  {XC  C | C  C}
10:
11:
12:
13: while H =  and stop is not satisfied do
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

classify OX and compute the projection
Hinit  {{C  D} | {C  D} is a hypothesis  C, D  C
H  Hinit, Hbest  
H  choose(H,OX )
H  H\{H}
classify OX  H and compute the assumption of H
compute fitness and braveness of H using minSizeU p
Hbest  Hbest  {H}
if H is not complete

end while
remove dominated hypotheses from Hbest
return Hbest

Hbest: best hypotheses

}

% extensions are possible
Hext  {H  H | H  Hinit  |H  H|    H  H / H  Hbest}
H  H  Hext

% add all direct extensions of H

then

end if

The longer Algorithm 1 runs, the better hypotheses it returns. It can be
interrupted at any point which is specified by the termination criteria stop, e.g.
a timeout, maximal number of iterations, quality threshold, etc. The algorithm
processes the whole search space if stop does not prevent it from doing so.
?

?

?
The function choose(H,O) determines which regions of the search space are
explored first. Various heuristics can be applied to guide the search. We use the
following heuristic for choose(H,O): select H  H with maximal

q(H,O) :=

| H|  

:=CD  H(sup(,O)    [cov(,O)  sup(,O)]),

where

sup(,O) := |ins(C  D,O)|
cov(,O) := |ins(C,O)|
ins(C,O) := {a  ind(O) | O |= C(a)}
  (0,)

is coverage of ,

is support of ,

is a predefined penalty of unsupported coverage.

are instances of C,

The heuristic chooses hypotheses that have smaller signatures and consist of
axioms with larger support and smaller unsupported coverage. More importantly,
it forces Algorithm 1 to firstly explore hypotheses with connected axioms (due
to 1/| H|) of higher independent statistical quality. The higher the penalty  is,
the more likely it is for cautious hypotheses to be evaluated first. If Algorithm 1
enumerates the full search space, then the heuristic does not affect the outcome.
Only in this case Algorithm 1 is guaranteed to be complete.

Although a reasoner is updated just once per hypothesis, computing the
fitness and braveness can still be expensive if the ontology is computationally
hard. This can result in a small number of evaluated hypotheses once the termination criteria stop are satisfied. Incremental reasoners, such as FaCT++ [15],
can improve the performance if a hypothesis is not big. Hence, besides readability and size of the search space, the length of a hypothesis may affect the
performance of computing its fitness and braveness.

6.2 Choice of Classes

So far, we have assumed that a set C of interesting classes is known. For example,
it can be defined by a domain expert. Unfortunately, this can be a difficult problem on its own. There are several possibilities to automate the choice of target
classes. First, one can extract all subclasses, including complex ones, occurring
in the ontology O. These are suitable candidates because they are explicitly
asserted in the ontology which implies that a domain expert is more likely to
find them sensible and interesting.

However, an ontology can have poor terminological knowledge, in particular,
it can contain mostly atomic classes. In this case, classes C can be generated
from some signature   O using a target class language, see Example 6.
Example 6. The signature is  = {A1, A2, R1, R2} and target class language is
G = {X | X  }  {X  Y | X, Y  }  {R.X | X  } (OWLs structural
equivalence is employed to avoid duplicates). Then, the set of classes is generated
as follows: C := {A1, A2, A1  A2,R1.A1,R1.A2,R2.A1,R2.A2}.

If the ontology signature is large and our class language is expressive, the
produced set of class expressions can be vast. One way to deal with the problem

V. Sazonau et al.

is to determine unpromising classes in C and discard them. Another way is to
select a signature of interest   O of manageable size and construct classes C
from it using a language G.  can be specified by a domain expert which may be
hard due to the lack of knowledge, large ontology signature, etc. Alternatively,
 can be selected automatically.

Since we run our experiments on OWL ontologies which we are not familiar
with and do not have access to domain experts, we select a signature  of an
ontology O with respect to A using the modular structure of the ontology as
follows:  := M, where M = module(T , A) (we use -modules [5]).
This approach yields class and property names that are logically connected
with A and discards logically disconnected ones (those can be numerous). We
construct classes C from  using a language G. Finally, we discard classes from
C that have no instances.

7 Implementation and Evaluation

7.1

Implementation

Tools and Hardware. All algorithms are implemented in Java 7 using OWL
API (3.5.0). We use the OWL 2 DL reasoner FaCT++ (1.6.3) [15] which supports
incremental reasoning. The experiments are executed on the following machine:
Linux Ubuntu 14.04.2 LTS (64 bit), Intel Core i5-3470 3.20 GHz, 8 GB RAM.

7.2 Evaluation

Evaluation Goals. By Definition 9, the solution of the general terminology
induction problem is a set of hypotheses. It depends on the following parameters:
an ontology O, a set C of classes, and a maximal length . The evaluation aim
is to empirically assess the influence of these parameters on the solution. More
specifically, the experiments are aimed at answering the following questions.

Q1 Where are we likely to find good hypotheses: in more expressive languages

for C or bigger values of ?

Q2 How does expressivity of the language and maximal length of a hypothesis

influence the performance of computing the fitness and braveness?

Q3 Can we acquire hypotheses that seem plausible, so that we can use them
to enrich our background knowledge, or that tell us interesting information
about our data?

Choice of Ontologies. We conduct the empirical evaluation on a corpus of
ontologies selected from related work [6,10] including DL-Learner datasets,2
Prot eg e OWL,3 and TONES4 repositories. The Kinship ontology is obtained

2 https://github.com/AKSW/DL-Learner
3 http://protegewiki.stanford.edu/index.php/Protege Ontology Library
4 http://owl.cs.manchester.ac.uk/repository/
?

?

?
from UCI Machine Learning Repository.5 We have selected the ontologies based
on the following criteria. Firstly, data contains both class and property asser-
tions, at least 15 individuals. Secondly, ontology classification takes less 10 min-
utes. Thirdly, we are sufficiently confident that we understand the topic of the
ontology. The corpus is available online.6
Table 6 describes the corpus where we use the following metrics. |ind(A)|,
CA, RA are numbers of individuals, concept and property assertions in the
ABox, respectively. degree(A), conn(A) are the average degree and average number of individuals in a connected component, respectively. | A|, | T | are sizes of the
ABox and TBox signature. Jac( A, T ) is the Jaccard index of ABox and TBox
signatures, open(A,T ) is the average number of question marks per individualclass name pair.

Table 6. Ontologies and their metrics

)
?

?

?
)
T
A

,

(
c
a

)

,

(
n
e
p
o

|

A

|

|

T

|

)

(
n
n
o
c

|

)

(
d
n
i
|
?

?

?
150 40
3.8 10

(
e
e
r
g
e
d
AL 150
Alzheimer
854 5.7

Arch

26 1.4

BasicFamily

95 3.1
ALC(D) 22372 22372 40666 1.8
Carcinogenesis

Cinema
76 1.7
SHOIN (D)
Earthrealm
203 1.2
ALCH(D)
Economy
555 1.2
ALCOIF 17941 17941 47248 2.6 8970.5 52
Financial
GeoSkills ALCHOIN (D) 2592 4681 3896 1.5
AL(D)
275 1080 3.9
?

?

?
40 1.7

525 1508 3.6
AL(D)
975 2883 3.0
MDM073 ALCHOF (D)
169 1.5

AL(D) 14145 14145 26533 1.9
61.5 60
SHOIN (D)
724 1636 2.3
2.8 64
AL(D) 2979 2979 6008 2.0 175.2 20

0 0.96
13 0.77 0.53
10.3
1 0.67
65.8 113 146 0.77 0.65

37 0.19 0.88
7.4 23 2482 0.01 0.89
5.3 29 380 0.04 0.94
76 0.68 0.54
13.9 569 618 0.90 0.69

11 0.82 0.90

12 18
4 0.16 0.81
4 25
40 0.55 0.65
975 18
22 0.82 0.97
2.0 82 215 0.38 0.51
91 0.66 0.99
78 0.82 0.96
49 0.41 0.97

Heart
Kinship

Mammographic

Mutagenesis

Suramin
?

?

?
Evaluation Setup. To answer the raised questions, we set up the following
experimental pipeline. Given an ontology O, for each combination of a class
language G and maximal length  we run Algorithm 1 with the timeout stop set
to 10 minutes. Once Algorithm 1 terminates, we record the fitness and braveness
of each hypothesis in the output set. We also record the average hypothesis
evaluation time which comprises computing the fitness and braveness. Finally,
we store all hypotheses if their number is less than 100 and only 100 hypotheses
of maximal q(H,O) otherwise.
5 https://archive.ics.uci.edu/ml/datasets/Kinship
6 http://www.cs.man.ac.uk/sazonauv/tbox induction/corpus/

V. Sazonau et al.

We choose maximal length  from {2, 4, 6, 8, 10}. In order to generate classes
C, we use the process described in Section 6.2. The signature is  := M, where
M = module(T , A). We investigate 5 class languages Gi, such that Gi  Gi+1
(duplicates are avoided by the means of OWLs structural equivalence):

G1 := {X | X  };
G2 := G1  {XM | XM is a possibly complex subclass in M};
G3 := G2  {X  Y | X, Y  };
G4 := G3  {R.X | X, R  };
G5 := G4  {X  R.Y | X, Y, R  }.

7.3 Results

Dependence of fitness and braveness on language and length is shown on Figure 1.
For each ontology the experiment is executed as described above. The values
obtained are normalised, i.e. divided by the maximal value. Then, the values are
aggregated across the corpus and the average value is reported per cell.

G5

G4

G3

G2

G1

G5

G4

G3

G2

G1

(a) Fitness

(b) Braveness

Fig. 1. Dependence of fitness (a) and braveness (b) on language expressivity and maximal length: darker colours reflect greater numbers

Our first observation is that some languages and lengths result in no hypotheses induced which happens if a class language is not expressive enough or hypothesis length is too low. We aggregate and average only over non-empty values.
An expected observation is that increasing expressivity is useless if an ontology
is poor, e.g. contains few relations in the data and axioms in the background
knowledge. On the other hand, if an ontology is rich, increasing expressivity may
or may not be fruitful.

Figure 1 shows that increasing length always results in hypotheses of higher
fitness and mostly, but not always, of higher braveness since added axioms may
make no assumptions or repeat the assumptions already made. Increasing expressivity also generally leads to higher fitness and higher braveness. However, the
changes are not as gradual as for length, in particular, braveness seems irregular.
?

?

?
Interestingly, we observe that G2 consistently outperforms G3 in fitness, despite
G2  G3, which can be explained as follows. On the one hand, the search space
considerably increases from G2 to G3. On the other hand, G3 appears to be
less fruitful than G2 (compare to G4 and G5). As a result, it becomes harder to
find equally good hypotheses in the same time. Thus, the answer to Q2 is that
increasing expressivity and length promises better fitness but commonly worse
braveness.

We also observe that the average hypothesis evaluation time does not vary
widely. Thus, the answer to Q2 is that performance does not degrade significantly
for higher expressivity and length. The performance of evaluating a hypothesis
is as follows: less than 0.1 second for 8 ontologies, from 0.1 to 1 second for 4
ontologies, from 1 to 10 seconds for 4 ontologies, and around 15 seconds for 1
ontology. The results can be found online.7

Table 7. Examples of hypotheses induced within 10 minutes

Ontology

Alzheimer

Arch

BasicFamily

Cinema

Earthrealm

Economy

Financial

Mammographic

Mutagenesis

Examples of hypotheses

Drug  getsReplacedBy.Substituent
Substituent  hasP olatisation.P olar
hasP olatisation.P olar  isHAcceptor.HAcceptor
construction  hasP illar.pillar
hasP arallelpipe.wedge  hasP illar.f reeStandingP illar
touches.pillar  lef tof.pillar
hasChild.P erson  P erson
hasP arent.P erson  P erson
hasP arent.F emale  hasP arent.M ale
M ovie  hasF orActor.Actor
M ovie  hasF orGenre.Genre
hasF orActor.{Eastwood}  hasF orGenre.{W estern}
hasF orDirector.{Burton}  hasF orActor.{Depp}
hasDef aultU nit.BaseU nit  hasDef aultU nit.ComplexU nit
hasDef aultU nit.{second}  T imeRelatedQuantity
hasDef aultU nit.{meterP erSecond}  DrySeasonDuration
N ation  IndependentState
economyT ype.EconomicDevelopmentLevel
 economyT ype.IM F DevelopmentLevel
Account  hasStatementIssuanceF requency.M onthly
isOwnerOf.Account  Client
hasM argin.spiculated  hasShape.irregular
hasShape.irregular  hasDensity.low
Compound  hasBond.Bond1
inBond.Hydrogen3  Bond1
inBond.Oxygen40  inBond.N itrogen38
M an  spouseOf.W oman
knows.M an  M an
relativeOf.M an  M an

In order to answer Q3, we act as domain experts and eyeball the induced
hypotheses. We aim at finding plausible and interesting hypotheses. Some results
7 http://www.cs.man.ac.uk/sazonauv/tbox induction/results/

V. Sazonau et al.

are shown in Table 7. Firstly, we observe that induced hypotheses can, in fact,
enrich the background knowledge, see Table 7. If the background knowledge is
poor, as in BasicFamily and Cinema, or even absent, as in Alzheimer, hypotheses
seem to be a good starting point for modellers. If the background knowledge is
incomplete, hypotheses appear to be interesting missing bits, e.g. for Economy,
Financial, NTN, and Mutagenesis.

Secondly, we observe that hypotheses can reveal interesting relations in our
data. This can expose new knowledge about the domain and help to understand
the data. For example, hypotheses discover relations between particular actors,
directors, and movie genres from Cinema. Another example is Mammographic
where we can learn relations between diagnostic observations, e.g. having irregular shape implies having lower density. Such hypotheses can potentially inform
doctors of yet unknown relations in their data, facilitate future research in the
domain, and lead to data improvements, e.g. a supplement of images of tumours
that have irregular shape and high density.

Thirdly, hypotheses can contain strange axioms which may help us high-
light, on the one hand, odd or erroneous modelling and, on the other hand, inaccurate or abnormal data. We observe this for Arch inducing touches.pillar 
lef tof.pillar (why is there nothing to the right?) and for Earthrealm inducing
hasDef aultU nit. {meterP erSecond}  DrySeasonDuration (wrong unit?).
Thus, we can answer Q3 positively.

Although we use different settings and the goal of induction is different,
we make some comparison of our results with related work. In particular, we
consider the supervised CDL and its implementation DL-Learner [11]. Given a
set of positive and negative examples for a target class construction in Arch,
it searches for definition construction  hasP illar.(f reeStandingP illar 
lef tof.supports.). As Table 7 shows, our approach induces a weaker definition of construction along with some related knowledge. For Cinema we observe
that descriptions of different movie types are induced, e.g. EastwoodM ovie
 hasF orActor.{Eastwood}, EastwoodM ovie  hasF orGenre.{W estern}.
For NTN the definition M an  spouseOf.W oman is induced. Thus, although
our approach is unsupervised, it shows the potential to learn class definitions.

8 Discussion and Future Work

The evaluation shows that our approach is able to induce interesting hypotheses.
On the one hand, they can potentially be helpful to build and improve the
background knowledge. On the other hand, hypotheses seemingly discover new
knowledge about the domain and help us understand the data. Interestingly,
they may help us identify modelling errors and data flaws.

Although the search space is vast, general terminology induction is feasible.
It is encouraging given that statistically and logically sound measures are used
to evaluate a hypothesis and this requires reasoning. We observe that larger and
more expressive hypotheses are generally better and still feasible.

As for future work, we will investigate more informed ways of constructing a
set of promising initial classes, e.g. using techniques from CDL, along with new
?

?

?
algorithms and heuristics for search space exploration. We will also attempt to
extend the methodology to deal with noisy data that causes inconsistency, e.g.
using techniques from [9]. We plan to investigate learning property hierarchies.
We intend to go beyond the corpus and carry out case studies with domain
experts to evaluate our approach in more detail. We also consider other scenar-
ios, e.g. how acceptance or rejection of a hypothesis affects other hypotheses,
how hypotheses can be used for predicting class memberships of individuals,
terminology abduction and what if analysis of data under the OWA.
