Explaining and Suggesting Relatedness

in Knowledge Graphs

Giuseppe Pirr`o(B)

Institute for High Performance Computing and Networking,

Italian National Research Council (ICAR-CNR), Rende, CS, Italy

pirro@icar.cnr.it

Abstract. Knowledge graphs (KGs) are a key ingredient for searching,
browsing and knowledge discovery activities. Motivated by the need to
harness knowledge available in a variety of KGs, we face the following
two problems. First, given a pair of entities defined in some KG, find an
explanation of their relatedness. We formalize the notion of relatedness
explanation and introduce different criteria to build explanations based
on information-theory, diversity and their combinations. Second, given a
pair of entities, find other (pairs of) entities sharing a similar relatedness
perspective. We describe an implementation of our ideas in a tool, called
RECAP, which is based on RDF and SPARQL. We provide an evaluation
of RECAP and a comparison with related systems on real-world data.

1 Introduction

Knowledge Graphs (KGs) maintaining structured data about entities are becoming a common support for browsing, searching and knowledge discovery activ-
ities. Search engines like Google, Yahoo! and Bing complement search results
with facts about entities in their KGs. An even large number and variety of
KGs, based on the Resource Description Framework (RDF) data format, stem
from the Linked Open Data project [8]. Fig. 1 (a) shows information provided
by the Google KG when giving the entity F. Lang as input; it reports some facts
about the director along with relationships with other entities. Fig. 1 (b) and
Fig. 1 (c) show information, encoded in RDF, about F. Lang taken from DBpedia
and LinkedMDB, respectively. Note that the Google KG suggests entities like T.
von Harbou as related to F. Lang with a short comment saying that T. von Harbou was F. Langs former spouse. However, what is the mechanism behind this
suggestion? What is the relationship between F. Lang and other entities like H.
Hitchcock? KGs like DBpedia and LinkedMDB fail short when it comes to both
explain the relatedness between an arbitrary pair of entities and suggest related

Part of this work was done while the author was working at the WeST institute,
University of Koblenz-Landau, Germany. This work was partially supported by the
EU Framework Programme for Research and Innovation under grant agreement no.
611242 (SENSE4US) and by the Cyber Security Technological District financed by
the Italian Ministry of Education, University and Research.

c Springer International Publishing Switzerland 2015
M. Arenas et al. (Eds.): ISWC 2015, Part I, LNCS 9366, pp. 622639, 2015.
DOI: 10.1007/978-3-319-25007-6 36
?

?

?
(a)

Metropolis

(Film)

dbpo:
director

dbpo:

birthName

dbpo:

producer

dbpo:
writer

Das 

Testament

lmdb:

producer

lmdb:
director

Fritz 
Lang

foaf:
page

Fritz 
Lang

Thea von 
Harbou

dbpo:
spouse

dbpo:

influenced

dbpo:

occupation

(b)

rdf:
type

lmdb:
name

(c)

People also search for

Former spouse

Ex-wife

Fig. 1. F. Lang in the Google KG (a), DBpedia (b), and LinkedMDB (c).

entities. We contend that the usage of a standard data format (i.e., RDF) and
the availability of a standard querying infrastructure (i.e., SPARQL endpoints)
open new perspectives toward explaining relatedness and querying KGs by using
pairs of entities as input.

The first problem that we face concerns how to build relatedness expla-
nations. This has applications in several areas including: terrorist networks,
to uncover the connections between two suspected terrorists [20]; co-author
networks, to discover interlinks between researchers [5,6]; generic exploratory
search. The need for relatedness explanations also emerged in the context of
the SENSE4US FP7 project1, which aims at creating a toolkit to support information gathering, analysis and policy modeling. Here, relatedness explanations
are useful to investigate and show topic connectivity2, thus enabling to find out
previously unknown information that is of relevance, understand how it is of
relevance, and navigate it.

Although the problem of finding connectivity structures between entities has
been studied (e.g., [3,19]), existing approaches do not offer comprehensive mechanisms for building different types of relatedness explanations and controlling
the amount of information to be included. Moreover, these approaches miss the
possibility to query KGs.

The second problem that we tackle concerns querying KGs. KGs behind
search engines (e.g., Google) provide limited querying capabilities, typically
accepting one entity as input. KGs based on RDF (e.g., DBpedia) provide rich
querying capabilities but require familiarity with languages like SPARQL [7]

1 http://www.sense4us.eu
2 A module of the SENSE4US toolkit extracts topics from policy documents.

G. Pirr`o

and the underlying data/schema [10]. Our strategy recalls the query by example
approach; given a pair of entities as input, we leverage their relatedness explanation to learn a query pattern, which is used to identify other (pairs of) related
entities. Our approach goes beyond existing entity suggestion mechanisms mainly
based on the syntactic analysis of query logs and pages [13]. We now provide an
example about the two main challenges faced in this paper, that is, how to build
relatedness explanations and how to query KGs by giving entities as input.

1.1 Overview of the Approach

Syd is fond of science-fiction films; he has heard about two German directors
named Fritz Lang and Thea von Harbou and is interested in their relatedness.

By giving F. Lang and T. von Harbou as input to RECAP, the tool implementing our framework, Syd gets the explanation in Fig. 2 (a). This explanation
is more informative than the short comment (i.e., former spouse) provided by
the Google KG and combines information from Freebase and DBpedia. The
explanation includes the top-20 most informative paths (out of 240) at max.
distance 2; informativeness is defined in terms of edge labels occurrences. RECAP
allows to generate different types of explanations (Fig. 2 (b)) and also provides
information about nodes/edges (Fig. 2 (c)).

RECAP goes beyond related approaches (e.g., REX [4], Explass [2]) that provide visual information about connectivity as it allows to build different types of
explanations (e.g., graphs, sets of paths), thus controlling the amount of information visualized. RECAP has the advantage of not requiring any data preprocess-
ing; information is obtained by querying (remote) SPARQL endpoints. Moreover,

Freebase

owl:sameAs

(a)

DBpedia

owl:sameAs

Die Nibelungen

(c)

Thea von Harbou

(b)

(c)

Fig. 2. The explanation perspective of the RECAP tool.
?

?

?
dbpo:spouse

dbpo:starring

d
b
p
o
:
s
p
o
u
s
e

d
b
p
o
w

:

r
i
t
e
r

dbp:Gale_Anne_Hurd    dbp:James_Cameron
dbp:Edgard_Rice_Buroughs   dbp:Florence_Gilbert
dbp:Stanley_Donen   dbp:Jeanne_Coyne

dbp:Vidhu_Vinod_Chopra   dbp:Renu_Saluja

(c)

(d)

(e)

Fig. 3. The querying perspective of RECAP. Path patterns (a), explanation (b), query
pattern (c), SPARQL query (d), and suggested entities (e) ranked by popularity
(PageRank [17] in this case).

RECAP can combine information from multiple KGs. In the previous example, the
combination of Freebase and DBpedia allowed to discover an additional episode
of Die Nibelungen series (missing in DBpedia), that is, Kriemhilds Revenge,
co-written by F. Lang and T. von Harbou. Last but not least, RECAP also allows
to query KGs by using pairs of entities as input.

Given a pair of entities, RECAP finds other (pairs of) related entities by learning a SPARQL query from their relatedness explanation. By continuing our
example, suppose that Syd gives the pair (F. Lang, T. von Harbou) as input
to RECAP with the aim to discover other entities. Fig 3 (b) shows one possible
explanation that RECAP uses to learn a SPARQL query. The explanation merges
paths conforming to the pattern in Fig 3 (a). Fig 3 (c) abstracts the explanation
by replacing nodes with variables.

The SPARQL query generated (shown in Fig 3 (d)) allows to find pairs of
related entities that can be locally ranked. The top-5 pairs of entities found by
RECAP, and ranked by their popularity, are show in Fig. 3 (e). As an example, for
the pair (Gale Ann Hurd, James Cameron) we can reconstruct a similar pattern
as that shown in Fig. 3 (b): J. Cameron was married with G. A. Hurd, he wrote
the movie The Terminator where L. Hamilton (also married to J. Cameron)
starred.

1.2 Related Work

There is solid body of work about (i) finding structures (e.g., paths, subgraphs)
connecting entities [3,9,12,12,15,19]; (ii) learning relationships between enti-
ties; (iii) discovering and/or visualizing connectivity information between entities [2,4,9]. Differently from (i), RECAP focuses on the problem of providing

G. Pirr`o

concise explanations by leveraging path informativeness and/or a diversity cri-
terion. As for (ii), systems like PATTY [16] mainly focus on learning semantic
relationships. RECAP has a different departure point; it explains relatedness in
the form of graphs that can be dynamically configured to include the desired
amount of information. As for (iii), Table 1 compares RECAP with related systems
in terms of: KG supported (KG), output (O), filtering capabilities (F), querying capabilities (Q), requirement of local data (L). RECAP differs from related
systems in the following main respects: as for KG, RECAP is KG-independent;
it only requires the availability of a (remote) query endpoint. Moreover, RECAP
can combine information from multiple KGs. As for O and F, RECAP focuses on
building different types of explanations in the form of graphs or (sets of) paths
by leveraging informativeness (to estimate the relative importance of edges),
diversity (to include rare edges) and their combinations. Moreover, RECAP is the
only approach that can be used to query KGs (Q). As for L, neither does RECAP
assume local availability of data nor any data preprocessing. A more detailed
comparison between RECAP and related systems on real data is discussed in
Section 4.

Table 1. Comparison of RECAP with closely related systems.

System
REX [4]
RelFinder [9]
Explass [2]

Yahoo!
DBpedia
DBpedia

Graph
Graph
Paths

No
No

Yes (only paths)

Q L
No Yes
No Yes
No Yes

Any

Graph/Paths Yes (paths and graphs) Yes No

1.3 Contributions and Outline

The framework that we are going to introduce poses several challenges, among
which: (i) how to capture the notion of relatedness explanation between enti-
ties? we leverage informativeness of paths and a diversity criterion to construct
different types of explanations; (ii) how to query KGs? we isolate the structure
of an explanation to learn a SPARQL query; (iii) how to make RECAP readily
available? We use RDF, the SPARQL query language, and SPARQL endpoints.
The contributions of this paper are as follows: (i) a framework for building relatedness explanations; (ii) different path ranking strategies; (iii) a mechanism to
query KGs by giving entity pairs as input; (iv) a KG-agnostic implementation
of our framework; (v) an extensive experimental evaluation.

The remainder of the paper is organized as follows. Section 2 introduces
the problem and gives some background. Section 3 presents the explanation
framework. Section 4 discusses an evaluation of the performance of RECAP and
a comparison with related work. We draw some conclusions and sketch future
work in Section 5.
?

?

?
2 Problem Formalization

Motivation. The goal of this paper is to facilitate the discovery and explanation
of knowledge in knowledge graphs. Part of this research was motivated by the
SENSE4US project where explanations are a useful support to discover connectivity between topics emerging from policy documents.
Input. We consider as input a pair (ws, wt) of entities defined in some knowledge graph G. We focus on RDF knowledge bases K=G, O, A where G is a
knowledge graph (KG), O is an ontology/schema, and A is a query endpoint.
Assumptions. The framework that we are going to introduce works on top existing knowledge bases. Our approach has to be flexible enough to be applied to
different KGs as dictated by the SENSE4US project, which considers a variety
of KGs in the LOD cloud. Hence, we consider the access to knowledge bases via
the query endpoint A. Neither this requires local availability of the data (e.g.,
local copies) nor any complex data processing infrastructure from the user side.
The computations are reduced to the evaluation of a set of queries against A
plus some local refinement.
Desired Output. Given K=G, O, A and a pair of entities (ws, wt)  G, the
output can be of two different types. It can be an explanation Ge(ws, wt)  G.
To produce the graph Ge, our explanation algorithm only considers nodes/edges
in the set of paths between ws and wt. Given a set of paths, we define different
mechanisms to rank/select paths to be included in Ge.

e where nodes in Ge are replaced with variables. Gv

When focusing on querying KGs, the output is a set of (pairs of) entities.
We isolate the structure of an explanation into an explanation pattern. Given
a graph Ge, representing an explanation, an explanation pattern considers a
graph Gv
e is used to generate
a SPARQL query that is evaluated against A. Results of such query can be
locally ranked (e.g., via PageRank [17]).
Basic Definitions and Background. We now define what a knowledge graph (KG)
is and outline the fragment of the query language supporting the implementation
of our framework. Although there are several KGs today available (e.g., Yahoo!,
Google) we will focus on those encoded in RDF3. The choice of RDF is merely
practical; data in RDF is widely and openly accessible on the Web for querying
via SPARQL [7]. Let U (URIs) and L (literals) be countably disjoint infinite
sets. An RDF triple is a tuple of the form U  U  (U  L) whose elements are
referred to as subject, predicate and object, respectively. As we are interested in
discovering explanations in terms of nodes and edges carrying semantic meaning
for the user, we do not consider blank nodes.
Definition 1 (Knowledge Graph). Given a set T of RDF triples, a KG is
a multigraph G=V,E where V ={s | (s, p, o)  T}  {o | (s, p, o)  T} and
E = {(s, p, o)  T}.

3 A list is available at http://lod-cloud.net

G. Pirr`o

For the purposes of this paper, we will consider the most basic form of
SPARQL queries, that is, Basic Graph Patterns (BGPs). We shall also make
usage of the COUNT aggregate operator. Let V be a set of SPARQL variables,
that is, strings starting with the ? symbol, U a set of URIs and L a set of liter-
als. A triple pattern is a triple of the form (U  L  V)  (U  V)  (U  L  V).
BGPs are sets of triple patterns that can be combined via algebraic operators;
we will make usage of the join operator (represented by the symbol . in the
SPARQL syntax).

3 The RECAP Approach

We see an explanation as a concise representation of the relatedness between
entities in terms of edges (carrying a semantic meaning via RDF predicates) and
other entities. As graphs are a natural and flexible way to represent and visualize
information about interlinked entities in a variety of scenarios, we represent
explanations as graphs.
Definition 2 (Explanation). Given a knowledge base K=G, O, A and a pair
of entities (ws, wt) where ws, wt G, an explanation is a tuple of the form
E=(ws, wt, Ge), where ws, wt  Ge, Ge  G, and Ge is connected.

The above definition is very general; it only states that two entities are connected via nodes and edges in a graph Ge, which is a subgraph of the knowledge
graph G and has an arbitrary structure. The challenging aspect is how to uncover
the structure of Ge by accessing G only via queries against the endpoint A. To
tackle this challenge we shall characterize the desired properties of Ge. Consider
the explanation shown in Fig. 4 (a); Ge contains two types of nodes: nodes such
as n1, n3, n4 that do belong to some path between ws and wt and other nodes
such as n2 that do not.

Fig. 4. An explanation (a) and a pattern graph (b).

Although the edge (n2, p1, n3) can contribute to better characterize n3, it is
in a sense non-necessary as it does not directly contribute to explain how ws and
wt are related. Hence, we introduce the notion of necessary edge.
Definition 3 (Necessary Edge). An edge (ni, pk, nj)G is necessary for an
explanation E=(ws, wt, Ge) if it is in a simple path (no node repetitions) between
ws and wt.
?

?

?
The necessary edge property enables to refine the notion of explanation into that
of minimal explanation.
Definition 4 (Minimal Explanation). Given K=G, O, A and a pair of
entities (ws,wt) where ws, wt G, a minimal explanation is an explanation
E=(ws, wt, Ge) where Ge is obtained as the merge of all simple paths between
ws and wt.

Minimal explanations enable to focus only on nodes and edges that are in some
path between ws and wt thus preserving connectivity information only. The
challenge is now how to retrieve minimal explanations.

Consider the explanation shown in Fig. 4 (a) (ignoring the dashed node and
edge). Ge could be retrieved by matching the pattern graph Gp in Fig. 4 (b)
(nodes and edges are query variables) against G. If the structure of Gp were
available, one could find Ge. Unfortunately such structure, that is, the right way
of joining query variables representing nodes and edges in Gp is unknown before
knowing Ge. As the building blocks of minimal explanations are paths between
ws and wt, finding these paths is crucial.

Generally speaking, paths between entities can have an arbitrary length; in
practice it has been shown that for KGs like Facebook the average distance
between entities is bound by a value k  5 [21]. Considering paths of length k is
also in line with the goal of providing explanations of manageable size that can
be visualized/interpreted by users. Finally, related approaches like Explass [2]
and REX [4] also considered bounded-length paths. Fig. 5 summarizes the explanation algorithm.

Algorithm 1: Building Relatedness Explanations

Input: A pair (ws,wt) of entities, an integer k, the address of a query endpoint A
Output: A graph Ge representing an explanation

(1) Find paths: we describe in Section 3.1 an approach based on SPARQL queries

against A to retrieve paths between ws and wt of length k.

(2) Rank paths: We describe in Section 3.2 different mechanisms to rank paths by

considering informativeness and diversity.

(3) Select and merge top-m paths: we discuss in Section 3.3 different ways of selecting

ranked paths to build an explanation.

Fig. 5. An overview of the relatedness explanation algorithm.

3.1 Finding Paths Between Entities

We now describe the structure of queries used to retrieve paths via the
endpoint A.

G. Pirr`o

Definition 5 (k-connectivity Pattern). Given K=G, O, A, a pair of entities (ws,wt) where ws, wt G and an integer k, a k-connectivity pattern is a tuple
=ws, wt,Q, k where Q is a set of SPARQL queries composed by joining k
triple patterns.

Note that SPARQL 1.1 supports property paths (PPs) [7], that is, a way
for discovering routes between nodes in an RDF graph. However, since variables
cannot be used as part of the path specification itself, PPs are not suitable for
our purpose; we need information about all path elements (i.e., nodes and edges)
to build explanations.
Example 6 (Example of k-connectivity Pattern). The 2-connectivity pattern between F. Lang (:FL) and T. von Harbou (:TvH) contains the following
set of queries Q:

SELECT DISTINCT * WHERE{:FL ?p1 ?n1. ?n1 ?p2 :TvH}
SELECT DISTINCT * WHERE{:FL ?p1 ?n1. :TvH ?p2 ?n1}
SELECT DISTINCT * WHERE{?n1 ?p1 :FL. :TvH ?p2 ?n1}
SELECT DISTINCT * WHERE{?n1 ?p1 :FL. ?n1 ?p2 :TvH}

Definition 7 (Path). Given K=G, O, A and a k-connectivity pattern
=ws, wt,Q, k, a path  is a set of edges: (ws, wt)=ws
pk wt,
ni  G i  [1, q], pj  G j  [1, k] and   {,}.

p3n3..nq

p1n1

p2n2

3.2 Ranking Paths

The number of paths connecting two entities ws and wt can be large. Considering
the merge of all paths, as done in minimal explanations (see Definition 4), can
be an obstacle toward concise explanations. Therefore, we introduce different
criteria to rank paths, a subset of which (e.g., top-m) can be merged to form an
explanation.
Ranking By Path Informativeness
The first approach to estimate the informativeness of a path connecting a pair of
entities (ws,wt) G leverages the informativeness of its constituent RDF predicates [18].
Definition 8 (Predicate Frequency Inverse Triple Frequency). Given a
knowledge graph G=V,E, an entity w  G and a predicate p appearing in
some triple involving w, the incoming pfw
o (p) predicate
frequency are shown in equation (1) and equation (2), respectively. The Inverse
Triple Frequency of p (itf(p)) and the pfitf are shown in equation (3) and
equation (4), respectively.

i (p) and outgoing pfw

(1)

pfw

o (p, G) =

(2)

|Eo(w)|(p)
|Eo(w)|

 itf

pfw

i (p, G) =

|Ei(w)|(p)
|Ei(w)|
|E|
|E|(p)

itf(p, G) = log

(4)
where |Ei(w)|(p) (resp., |Eo(w)|(p)) is the number of triples in G where the
predicate p is incoming (resp., outgoing) in w, |Ei(w)| (resp., |Eo(w)| ) is the

pfitfx(p, G) = pfx

(3)
?

?

?
k=1

dbpo:
writer

k=2

Fritz 
Lang

Fritz 
Lang

dbpo:
spouse

dbpo:
writer

The 

Indian Tomb

Thea von 
Harbou

(a)

Fritz 
Lang

Thea von 
Harbou

Fritz 
Lang

dbpo:
director
k=3

dbpo:
wrtiter

k=2

?v

dbpo:Film

dbpo:
writer

?v1

dbpo:
starring

?v2

dbpo:Film

dbpo:Person

(b)

Thea von 
Harbou
dbpo:
spouse

Thea von 
Harbou

dbpo:
director

k=2

dbpo:

screenplay

Die Nibelungen

dbpo:
editing

k=2

Spione

dbpo:
writer

Fritz 
Lang

Fritz 
Lang

Thea von 
Harbou

(c)

Thea von 
Harbou

Fig. 6. Ranking: (a) most informative paths; (b) most informative patterns; (c) most
diverse paths.

total number of incoming (resp., outgoing) triples including w. |E|(p) is the
number of triples including p. In equation (4), pfitfx(p, G) can use pfw
i (p, G)
or pfw

o (p, G).

p wt be a path
Definition 9 (Path Informativeness). Let (ws, wt)=ws
between ws and wt in G of length k=1. The informativeness of  is defined as:
(5)
p wt can be obtained by conThe informativeness of the path (ws, wt)=ws
sidering p as an incoming edge to ws. For paths having length k > 1, we have:

o (p, G) + pfitfwt

I(, G) = [pfitfws

i (p, G)]/2

I(, G) =

I((ws, w1), G) + ... + I((wk, wt), G)

k

(6)

pq wt,
pq wt,

p1 n1
p1?v1

p2 n2..nq
p2?v2..?vq

Ranking by Pattern Informativeness
We now introduce informativeness based on path patterns. A path pattern generalizes a path by replacing nodes with variables.
Definition 10 (Path Pattern). Given a path (ws, wt)=ws
a path pattern is an expression of the form (ws, wt)=ws
where ?vi i  {1, 2, ...q} are variables and q  k.
As an example, the path in the bottom-part of Fig. 6 (a) is abstracted in the pattern in the top-part of Fig. 6 (b). The usage of variables in place of intermediate
entities enables to represent in a more concise way information about a set of
paths. The pattern in the top-part of Fig. 6 (b) enables to capture the fact that
F. Lang and T. von Harbou have co-written 11 movies (bindings of the variable
?v) according to DBpedia. Information in the ontology O (when available) can
help to more precisely characterize the nature of intermediate entities by considering their rdf:type (Fig. 6 (b)). RECAP includes a pattern-based exploration
of the connectivity between ws and wt along with the possibility to generate
explanations including all paths matching a pattern (see Fig. 3 (a)).
Definition 11 (Path pattern informativeness). Let P be the set of patterns obtained from a set of paths P. The informativeness of a path pattern
  P is:

(7)

I(, G) = log

|P|
|(, G)|

G. Pirr`o

where | P | is the number of patterns and | (, G) | is the number of paths
sharing  in G.
Ranking By Path Diversity
The most informative paths of length k=2 between F. Lang and T. von Harbour often include predicates related to the fact that they have co-written movies
(e.g., The Indian Tomb and Metropolis); this will potentially discard other predicates appearing in paths with low informativeness. To cope with this aspect, we
introduce path diversity.
Definition 12 (Path Diversity). Given a source entity ws  G, a target entity
wt  G and two paths 1(ws, wt) and 2(ws, wt) we define path diversity as:

(1, 2) =

|Labels(1)  Labels(2)|
|Labels(1)  Labels(2)|

(8)

where Labels() denotes the set of labels (RDF predicates) in a path. Fig. 6
(c) shows the two most diverse paths at distance 2 between F. Lang and T. von
Harbou. As it can be observed, the predicate dbpo:screenplay is included; such
predicate is never present in the top-10 most informative paths.

3.3 Selecting and Merging Paths

The last step of the explanation algorithm concerns path selection. Table 2
describes different strategies that given a value m, select a subset (but E)
of paths (patterns) according to one of the three approaches described in
Section 3.2. Moreover, two strategies combine path (pattern) informativeness
and diversity. The strategy in the last line of Table 2 does not merge paths and
is used by RECAP to enable pattern-based explorations of the relatedness between
ws and wt. We discuss an evaluation of the different strategies in Section 4.

Table 2. Path selection/merging strategies.

Symbol Meaning

Merge all of paths

E
m Merge the top-m most informative paths
E
E
m Merge paths belonging to the top-m most informative path
E Merge paths whose value of diversity falls in [(maxr),max]

patterns

where max is the max diversity and r is a % value.

E, Merge the results of E
E, Merge the results of E

Set of all paths (no merge)

m and E
m and E
?

?

?
3.4 Querying KGs by Example

We now describe the second building block of our framework, that is, an algorithm (shown in Fig.7) to query KGs by giving a pair of entities as input. In
what follows we outline the steps, but (1), of Algorithm 2 after introducing
explanation patterns.

Algorithm 2: Knowledge Graph Querying

Input: A pair (ws,wt) of entities, an integer k, the address of a query endpoint A
Output: A set of ranked (pairs of) entities

(1) Find an explanation E=(ws, wt, Ge) between ws and wt by using Algorithm 1.
(2) Build the entity query pattern Qe.
(3) Query the KG with Qe (via A) and get a set of (pairs of) entities.
(4) Rank the answers to Qe.

Fig. 7. An overview of the query answering algorithm.

Definition 13 (Explanation Pattern). Given
explanation E=
ws, wt, Ge, an explanation pattern is a tuple E=?ws, ?wt, Gv
e where
e={TP1,TP2,...,TPk} is a query graph and TPi=(U LV)U (U LV), 1 <
Gv
i < k, is a triple pattern not containing variables in predicate position. Moreover,
for i > 1 |var(TPi)var(TPi1)| = 1.

an

In the above definition, Gv

In the above definition, TPi, i  [1, k] are triple patterns in Gv

e is the query graph obtained from Ge by replacing
all nodes into an explanation with query variables. Basically, an explanation
pattern generalizes the structure of an explanation by keeping edge labels only.
Explanation patterns are used to generate entity query patterns.
Definition 14 (Entity Query Pattern). An entity query pattern is a
SPARQL query of the form: SELECT DISTINCT ?ws ?wt WHERE{TP1. TP2. TPk.}
e and ?ws
and ?wt are variables used in lieu of the entities in input. Query patterns are
automatically derived; our algorithm neither requires familiarity with SPARQL
nor with the underlying data/schema. The evaluation of a query pattern returns
a set of pairs of entities.
Ranking of Results
Our approach for querying KG learns an entity query pattern Qe from a relatedness explanation. Since the evaluation of Qe can return a large number of
results, our algorithm includes a ranking component. The problem of ranking
results of SPARQL queries has been already studied (e.g., [1,14]) and is not the
main purpose of the present paper.

G. Pirr`o

Inspired by the Google KG, we consider a simple result ranking mechanism based on the popularity of entities; specifically, we leverage the PageRank [17] algorithm. Given a pair of entities (w1, w2) returned when evaluating Qe
(obtained from step (3) Algorithm 2), we estimate their popularity as (P R(w1)
+P R(w2)) /2, where P R(wi) is the PageRank value of the entity i. We leave as a
future work the investigation of more sophisticated result-ranking mechanisms.

4 Implementation and Evaluation
We have implemented our ideas in the RECAP tool, which uses JavaFX4 for the
GUI and the Jena5 framework to handle RDF data and SPARQL queries.

4.1 Evaluating the Explanation Generation Component

We start by discussing the evaluation of the explanation component of RECAP.
Experimental setting. We considered two KGs: DBpedia (DB)6 and Freebase7
(FB). We adopt the dataset of 26 pairs used to evaluate Explass [2] and set k4
as done in Explass. We use as reference graph for the computation of informativeness scores (see Def. 8 and Def. 11) the graph obtained by merging all paths.
Experiments have been performed on a MacBook Pro with a 2.8 GHz i7 CPU
and 16GBs RAM.

Experiment 1: Performance Evaluation: we investigate the performance of
RECAP for increasing values of k8 in terms of: (i) obtaining paths; (ii) computing
explanations. Results that follow are the average of 5 runs. Fig. 8 (a) and (b)
show the running times. Clearly, the higher k the higher the running time for
path retrieval. The multi-thread implementation of RECAP allows to keep the
time for finding paths on average around 6.4 secs for DB and 12 secs for FB when
k4. When executing the queries sequentially (results are not reported for sake
of space) the running times can be up to 30 times higher. We observed in another
experiment on DB (not reported for sake of space) that local data reduces the
running times by 60% on average. However, this has the disadvantage that
both a local processing infrastructure and local data are required.

Running times on DB for generating the different types of explanations
described in lines 1-4 of Table 2 are shown in Figs. 9 (a). We report results
on DB as this KG has been used in the (qualitative) comparison of RECAP with
related approaches (see Section 4.1). Nevertheless, we report results on the combination DB-FB in Fig. 9 (b).
Generally speaking, E explanations can be generated very fast; here, no
path ranking/filtering is performed. However, E can be very big, which makes
4 http://docs.oracle.com/javafx/
5 https://jena.apache.org/
6 http://dbpedia.org/snorql
7 http://lod.openlinksw.com/sparql
8 In particular, for each k, all paths of length k are generated
?

?

?
Fig. 8. Path retrieval in DB (a) and FB (b). Y-axis: time(ms) in log-scale; X-axis: entity
pair.

Fig. 9. Explanations in DB (a) and DB/FB (b). Y-axis: time(ms) in log-scale; X-axis:
entity pair.

the interpretation by users difficult, as we will discuss in Experiment 2. E explanations that use diversity (we considered r=25%) are more expensive as they
require the computation of distances between paths, for which RECAP leverages
a multi-thread approach. Explanations based on path informativeness E
m (we
considered m=5) require to compute pfitf scores; RECAP computes these scores
in parallel and using the merge of all paths as reference graph thus not performing any remote query. Explanations based on pattern informativeness E
m (we
considered m=5) are less expensive since they do not analyze the informativeness
of all edges in a path. The most expensive explanations (not reported here for
sake of space) are those combining path/pattern informativeness and diversity
requiring 6 secs for k4. When compared to related system (see Section 4.1),
RECAP has been judged the fastest system in the overall task of generating different types of relatedness explanation. In terms of size (results not reported for
sake of space), E are the biggest one; their size can include up to 4000 paths
(k4) for pair 12 (C. Bale, C. Nolan) in FB.
5 are smaller; the typical size is 8 nodes and 7
edges. E
5 have variable size as it depends on the number of paths for each
of the top-5 most informative patterns. In general these are bigger than E
explanations (15 nodes and 12 edges). Note that E

5 explanations enable to
focus on specific aspects as they include all the instantiations of each of the
top-5 most informative path patterns. The sizes of E are in the same order
of magnitude as E
5 ; however E explanations guarantee to also include rare
edges potentially discarded by path or pattern informativeness. The typical size

Explanations of type E

G. Pirr`o

of an explanation combining (top-5) path/pattern informativeness and diversity
(r=25%) is 20 nodes and 15 edges. The possibility, featured by RECAP, to
decide the amount of information to be included into an explanation is crucial
toward understanding relatedness.
Experiment 2: Interpreting Explanations: This experiment aims at: (i)
investigating whether RECAP provides useful explanations to the user; (ii) comparing RECAP against two related systems online available9, that is, Explass [2]10
and RelFinder [9]11. We used DB for the comparison as Explass and RelFinder
only work on this KG.
Setting. Twenty participants were assigned each six random pairs among the 26
entity pairs. They were shown how the three systems work and asked to use each
system (with no other support) in order to understand the relatedness between
entities in a pair. Following the methodology in [2] participants were given a
set of six questions; the response to each question was given with an agreement
value from 1 (min) to 5 (max). Q6 was not considered in [2]; we included it
to understand how users perceive the performance of the systems in terms of
running time. Results are reported in Table 3.

Table 3. Questions/responses: means (standard deviation).

Question

Q1: Information overview
4.55(0.65) 3.05(0.77)
Q2: Easiness in finding information
4.45(0.55) 4.05(0.63)
Q3: Easiness in comparing/synthesizing info 4.62(0.62) 3.10(0.82)
Q4: Comprehensive support
4.81(0.73) 3.42(0.77)
Q5: Sufficient support to the task
4.67(0.81) 3.28(0.86)
Q6: Running time
4.82(0.48) 4.12(0.72)

3.82(0.75)
3.85(0.67)
4.06(0.61)
4.15(0.79)
4.23(0.83
3.18(0.52)

RelFinder Explass

According to questions Q1-Q5, users perceived RECAP and Explass as better
supports to the explanation task. Users reported that RelFinder does not allow
the flexible creation of explanation (e.g., by grouping paths into patterns), which
makes it hard to control the amount of information shown. In general, RECAP
was judged to be a more comprehensive solution; it provides both a graphbased and pattern-based exploration of results and several ways of controlling
the amount of information to be shown. While RECAP and RelFinder quickly
provide information immediately after retrieving paths, Explass requires a much
longer time. On Q6 Explass was judged to be the less compelling system. RECAP
was judged higher than the other two systems in all questions via LSD post-hoc
tests (p < 0.05). The inter-annotator agreement was of 0.85.
Combining multiple KGs. We tested RECAP on the combination of DB and FB
(see Fig. 9 (d)). Starting from DB, for the source/target entities we looked at

9 REX [4] is not available for public usage
10 http://ws.nju.edu.cn/explass/
11 http://www.visualdataweb.org/relfinder/relfinder.php
?

?

?
owl:sameAs links to the corresponding FB entities. We then merged the set of
paths from each KG by using owl:sameAs links. Users (75%) perceived the
combination of multiple KGs as very useful toward more comprehensive expla-
nations. This is especially true when KGs cover the same domain with different
levels of detail (FB was judged more comprehensive than DB). The combination
also produces graphs of bigger size. Indeed, the functionality of RECAP allowing
to filter information to be included into an explanation was judged very useful
(participants thought E were too big when k 3).

4.2 Evaluating the Querying Component

We now discuss the evaluation of the querying component of RECAP.
Experimental setting. We used the dataset of 18 pairs defined by Jayaram et
al. [11] and considered DBpedia as KG. In order to rank query results, we compute PageRank values for the latest version of DBpedia and stored them in a
local Lucene12 index.

Table 4. Accuracy of RECAP (m=10).

Table 5. Accuracy of RECAP (m=15).

Pair P@m nDCG Pair P@m nDCG
P1
P2
P3
P4
P5
P6
P7
P8
P9

0.91
0.82
0.73
0.67
0.74
0.82
0.72
0.69
0.81

0.87
0.75
0.72
0.81
0.82
0.84
0.78
0.62
0.79

0.94
0.92
0.87
0.72
0.83
0.85
0.81
0.77
0.85

0.91
0.78
0.78
0.89
0.85
0.86
0.84
0.72
0.82

P10
P11
P12
P13
P14
P15
P16
P17
P18

Pair P@m nDCG Pair P@m nDCG
P1
P2
P3
P4
P5
P6
P7
P8
P9

0.78
0.78
0.71
0.62
0.68
0.64
0.62
0.61
0.78

0.81
0.72
0.65
0.71
0.68
0.70
0.68
0.62
0.67

0.83
0.74
0.71
0.74
0.71
0.71
0.72
0.67
0.74

0.82
0.79
0.72
0.68
0.73
0.72
0.71
0.68
0.81

P10
P11
P12
P13
P14
P15
P16
P17
P18

Evaluation Metrics. The aim of this experiment is to measure how precise are the
results returned by RECAP as compared to a gold-standard. We measure the accuracy on a query by considering: (i) Precision-at-m (P@m): the percentage of the
top-m results in the ground truth; (ii) Normalized Discounted Cumulative Gain
reli
(nDCG): the cumulative gain of the top-m results is DCGm=rel1 +
log2(i);
it penalizes the results if the ground truth result is ranked low. DCGm is normalized by IDCGm, the cumulative gain for an ideal ranking of the top-m results.
Thus nDCGm= DCGm

m

1=2

IDCGm .

We report results for m=10 (in Table 4) and m=15 (in Table 5) that consider
top-10 and top-15 entity pairs, respectively. We use E
10 (top-10 most informative
paths) explanations, at step (1) of Algorithm 2, to generate entity query patterns.
As it can be observed, the usage of PageRank scores, as a mechanism to
weight the importance of query results, brings acceptable performance. In the

12 https://lucene.apache.org/

G. Pirr`o

majority of the 18 pairs, the P@10 score is above 0.75. In some cases like P1 (i.e.,
Nike, Tiger Woods) RECAP was able to identify almost all the other entities (in
the gold standard), among which M. Jordan, and K. Bryant (also sponsored by
Nike). When the value of m increases performance decreases. However, usually
providing top-10 results13 is an acceptable compromise. Note that the nDCG is
in most of the cases above 0.7; in this measure, the DCG emphasizes pairs of
entities that appear early in the set of results. We leave as a future work the
investigation of more sophisticated ranking mechanisms.

In terms of running time, the overhead introduced (besides explanation gen-
eration) by Algorithm 2 consists in the access to the Lucene index to retrieve
PageRank scores and the computation of their average value. Typically, the overall running time for path finding, explanation generation and result ranking is
10 secs.

5 Concluding Remarks and Future Work

We have introduced a framework to generate different types of relatedness expla-
nations, possibly including information from multiple KGs. Our work is motived
by the SENSE4US FP7 project, where there is the need to find topic connectivity
information.

We have faced another important problem: querying KGs by using entities
as input. As of today, either KGs provide limited querying capabilities (e.g.,
by accepting one entity as input) or require familiarity with languages such as
SPARQL besides the underlying schema/data. We have shown how the usage
of the relatedness explanation between a pair of entities can help in learning
SPARQL queries to find other pairs of related entities. We plan to investigate
optimization mechanisms to reduce the running time for path finding. One approach could be to leverage the ontology O to generate candidate queries according
to paths between entities at the schema level, rank these queries, and check the
most promising.
