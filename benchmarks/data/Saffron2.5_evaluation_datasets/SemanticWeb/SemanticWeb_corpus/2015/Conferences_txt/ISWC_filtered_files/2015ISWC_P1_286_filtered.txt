CogMap: A Cognitive Support Approach

to Property and Instance Alignment

Jan N oner, David Martin, Peter Z. Yeh, and Peter F. Patel-Schneider(B)

AI Research Group, Nuance Communications, Sunnyvale, CA 94085, USA

{david.martin,peter.yeh,peter.patel-schneider}@nuance.com

jan.noessner@gmail.com,

Abstract. The iterative user interaction approach for data integration
proposed by Falconer and Noy can be generalized to consider interactions
between integration tools (generators) that generate potential schema
mappings and users or analysis tools (analyzers) that select the best
mapping. Each such selection then provides high-confidence guidance
for the next iteration of the integration tool. We have implemented this
generalized approach in CogMap, a matching system for both property
and instance alignments between heterogeneous data. The generator in
CogMap uses the instance alignment from the previous iteration to create high-quality property alignments and presents these alignments and
their consequences to the analyzer. Our experiments show that multiple
iterations as well as the interplay between instance and property alignment serve to improve the final alignments.

1 Introduction

In recent years, companies have spent more and more effort in building knowledge
graphs based on light-weight ontologies, which incorporate data from multiple
heterogeneous sources (which we will henceforth call information stores). A
key challenge of these efforts is determining the best alignment of the schema
of a new store to the ontology of the knowledge graph, while minimizing the
manual analytical effort required of a human knowledge engineer.

Most of the current ontology alignment systems, such as those evaluated
recently in the annual ontology alignment evaluation initiative [1], have several
limitations. Most of these alignment algorithms solve one integration problem
(deriving a mapping between two ontologies) using a fully-automated, one-shot
approach. Thus, they are often not able to improve by iterating over previous
alignments. Partly for this reason, the results of fully automated algorithms
are often error prone [32] and cannot be reliably used for high-quality data
integration.

Currently much information to be integrated is obtained from non-ontological
sources such as relational databases or XML documents. Classical ontology alignment systems are often not able to process this data [1]. To address this need,
systems like OntoDB [20] and standards like D2RQ [3] have emerged. However,

c Springer International Publishing Switzerland 2015
M. Arenas et al. (Eds.): ISWC 2015, Part I, LNCS 9366, pp. 269285, 2015.
DOI: 10.1007/978-3-319-25007-6 16

J. N oner et al.

these solutions do not include semi-automated alignment algorithms which take
instance information into account.

Our approach, implemented in the CogMap system, follows a cognitively-
inspired, iterative approach. With multiple iterations the system is able to
improve over time, since it builds on the results of previous iterations (or, in
the case of the first iteration, seed queries given by the user). At each iteration,
the results are augmented with new information that has been verified by a user
or automated verification capability.

CogMap uses instance information to perform property alignment. While
most state-of-the-art schema alignment algorithms do not take instance information into account, focusing exclusively on the alignment of classes and properties and mainly considering their labels or structural information [7,29], using
instance matching has attended more and more attention over the last years [16].
CogMap explores instances by not only focusing on data properties but
also taking object properties into account. In the case of databases, it follows
foreign keys; with RDF information stores it explores sub-tags. To the best of our
knowledge, there exists no other approach which explores the space of potential
mappings between information stores as we do.

CogMap is not restricted to the alignment of information based on formal
ontologies. It also supports relational databases and XML documents, which can
serve either as the source or the target of an alignment. In addition, CogMap
allows support for other data formats to be added in a modular fashion.

2 Related Work

Many schema alignment systems have been developed in ontology matching. The
development of these systems has largely been driven by the available benchmark
datasets of the ontology alignment evaluation initiative. An overview of the
current systems and their evaluation is given by Grau et al. [14]. The most
important datasets, however, cover only a small problem space.

Although most ontology matching systems ignore instances, there exists
a strand of literature which combines schema alignment and instance alignment [16]. Bilke and Naumann [2] developed an approach that first aligns
instances and uses this information for schema alignment. Their evaluation is
based on artificially populated data whereas we employ real-world data information stores like Freebase and DBpedia. Bilke et al. [1,26], Thor et al. [34],
Gal [12], and Leme et al. [24] use instances to align schema and resolve conflicts.
Another fully automated system that integrates both schema and instance alignment is Paris [33]. Its algorithms are, however, resource intensive, in some cases
taking days to produce a solution. In contrast, the CogMap algorithms are much
less resource intensive and can be run on a typical desktop computer. Wang et
al. investigates the problem of having only a few non-overlapping instances by
approaching the mapping problem as a classification problem. However, this
approach is limited to mapping concepts and ignores properties. Duan et al. [5]
use hashing techniques to speed up instance-based matching. Nunes et al. [30]
?

?

?
present an instance-based algorithm for complex data property matching. A
prominent example is the system RiMOM, which dynamically combines several
alignment strategies including instance alignment [25]. Due to its recent excellent achievements at the ontology alignment evaluation initiative, we chose this
system for our evaluation.

To the best to our knowledge, none of these approaches is exploring object
properties with an iterative cognitive support approach. QuickMig [4] is a
migration tool for database systems which follows a semi-automated approach.
However, it considers only exact value matches and their results are not used to
improve the ongoing iterations.

A smaller number of systems utilize learning. A prominent example is
SILK [1719] which learns expressive linking rules by using genetic program-
ming. However, its target user is a technical expert who can, e.g., analyse complex
matching trees while CogMap focuses on domain experts by hiding technical
complexity. LIMES [27] focuses on runtime improvements by using the triangle
inequality. However, it does not allow a user-centric iterative approach. Further-
more, neither system is able to map data properties to object properties, which
is required by the real-world datasets we examined. (See the algorithm section
for details.)

Recently, the ontology alignment evaluation initiative initiated an interactive track which simulates interactive matching [31], where a human expert is
involved to validate mappings found by the matching system. The client was
modified to allow interactive matchers to ask an oracle, which emulates a perfect user. The interactive matcher can present a correspondence to the oracle,
which then tells the user whether the correspondence is right or wrong. How-
ever, the initiative uses a dataset which does not contain any instance data and
thus is not suitable for evaluating our approach. The two most successful participating systems 2014 were AML[11] with respect to gained f-measure due to
the interactive approach and LogMap [22] with respect to efficiency (number
of interactions required). We have included both systems in our evaluation.

Tools have been developed to support the alignment of databases to ontolo-
gies. One example is OnTop (ontop.inf.unibz.it), which provides a Prot eg e
plug-in to facilitate the creation of integration rules. OnTop focuses on fast execution of already existing data integration rules, but not on the (semi-)automated
construction of them. Furthermore, its target ontology is assumed to be small
and to contain only schema information but no instance information. There have
also been attempts to build graphical tools for supporting the user in data inte-
gration. Karma [23], for example, loads data from different information stores
and uses instance information for schema alignment. However, its approach is
different from our algorithm. Karma learns the general structure of fields based
on previous alignments made whereas CogMap operates on instance informa-
tion. Two disadvantages of Karmas approach are that it generally assumes that
fields (e.g., ids) have similar structures in different datasets and its algorithms
require a large amount of training data.

J. N oner et al.

3 The Cognitive Support Approach

Researchers in ontology and schema matching have recently recognized the need
for various types of cognitive support in aligning complex conceptual models
[8,10]. Most approaches are based on advanced visualization of the models to be
integrated and the mappings created by the user [13]. While the appropriate use
of visualizations is known to be a key aspect for successful manual data integra-
tion, visualizations quickly reach their limits in the presence of very complex or
very large models.

Fig. 1. The cognitive support model for data integration by Falconer [9].

Fig. 2. Modified cognitive support Model as implemented in CogMap.
?

?

?
As a result, recent work has tried to go beyond pure visualization support to
include cognitively efficient interaction strategies to support the user [9]. Falconer
[8] proposed an interactive strategy for data integration where the integration
task is distributed between the user and the tool (Figure 1). The MappingAssistant [32] project used a modified cognitive support model for data integration,
focusing on detecting and correcting incorrect data integration rules.

In our implementation of the cognitive support model, which we call
CogMap, we go one step further and allow the user to be either a human user
or an intelligent automated agent. Thus, in our implementation of the cognitive
support model (Figure 2), we distinguish between an analyzer and a generator,
instead of a user and a tool.1

In each iteration, CogMap extracts data based on the results of previous
iterations (or, in the first iteration, based on given seed queries), generates property correspondence suggestions, and computes the consequences for the top
suggestions. These consequences are the instances that would be aligned if the
analyzer selects (verifies) this property correspondence. Next, CogMap sends
the ranked correspondences and their consequences to the analyzer. The analyzer then inspects this information and selects a correspondence. The selected
property correspondence, and the resulting instance alignment, are added to
the evolving results sets, which allows the system to improve its suggestions in
subsequent iterations. The algorithm terminates when no properties remain to
generate new correspondences, or no correspondence is selected by the analyzer.
CogMap is designed to cope with many different types of data stores,
in many different formats. We currently have implemented support for RDF
accessed via SPARQL, relational databases, and general XML based files. This
list is easily extended by implementing our Connector interface.

4 Algorithm

The primary focus of the CogMap algorithm (Algorithm 1) is to construct
property correspondences and instance alignments. An alignment (or mapping)
consists of a set of correspondences. According to Euzenat et al. [6], a correspondence is a 4-tuple es, et, r, c, where es and et are source and target entities, r
is a semantic relation, and c is a confidence value (usually, c  [0, 1]). Like most
ontology alignment systems [1], we focus on equivalent relations es, et,, c.

The algorithm can be split into three phases. The data extraction (lines
4-6) and data exploration (lines 13-16) phases are only executed in the first
iteration (i = 0). The alignment generation and selection (lines 7-12) phase
is repeated until no more correspondences are found. This phase includes the
decision making of the analyzer. The following subsections will explain the phases
in more depth.

1 As stated, the analyzer can be a human. As this paper is about the effectiveness
of the overall approach, we only use simple agents. Sophisticated automated agents
or humans can utilize world knowledge or judgements to select bettter alignments
instead of just picking the highest-scoring ones.

J. N oner et al.

4.1 Data Extraction and Exploration

We adapt the terms data property, object property, and instances from the
semantic web literature, extending them to databases and XML documents in
an obvious fashion. For example, instances include database rows and XML
nodes.

In the data extraction phase, we extract all data property names and their
corresponding values for M instances into a source table Ts and a target table
Tt (line 5). The left block (2nd column) of Table 1 illustrates the general form
of Ts and Tt after extraction.

In the data exploration phase, we explore the search space by following object
properties. In other words, for each object property op of an instance i, we
examine the object which is the value of that property. Then, for each data

Algorithm 1 High-level algorithm of CogMap.
Input: Ss, St: Seed queries for source and target
Input: M : number of extracted instances of each information store (default: 5000)
Input: k: number of suggestions (default: 5)
Output: X ,Y: Set of user-verified property correspondences and instance correspon-
dences

getAlignments
1: X ,Y  
2: i  0
3: repeat

 Data Extraction
if i=0 then

4:
5:

8:
9:

10:
11:
12:

13:
14:
15:

Ts, Tt  Extract M instances and their data properties and values based on
seeds Ss and St.

6:
7: Xi  Compute top-k property correspondence suggestions based on Ts, Tt and

end if
 Alignment Generation and Selection
Y (if not empty).
for every x  Xi do

Yx  Compute instance alignment consequences for x based on Ts, Tt and Y
(if not empty).

end for
Analyzer selects the optimal x  Xi based on Xi and {Yx| x  Xi}.
add x to X , Y  Yx.
 Data Exploration
if i=0 then

Is, It  Extract source and target instance sets from instance alignment Y.
Ts, Tt  Extend tables by following the object-property assertions of Is and
It.

end if
i  i + 1

16:
17:
18: until No more suggestions found
?

?

?
Table 1. General form of source and target table. Initially, the direct data properties and the corresponding data are imported (left block). Second, and subsequent,
steps further explore the data by including object properties (right blocks). (dp=data
property, op= object property, i = instance, and v = value).

dp1
v1,1
i1


im vm,1






dp1,a
dpn
v1,1,a
v1,n


vm,n vm,1,a

opa





dpn,a
v1,n,a

vm,n,a

opb




property of that object, we add its value to the row for i. Thus, the right blocks
of Table 1 (opa, opb, ) are added during this phase. The reason this exploration
happens at the end of the first iteration (i = 0, line 13-16) is that there may
exist many object properties to follow. This often leads to a large amount of
data. Thus, the idea is to restrict the exploration to the smaller instance sets
Is and It. These instance sets are extracted from the first instance alignment Y
(line 15) such that Is = {es|es, et,, c  Y} and It = {et|es, et,, c  Y}.
Then, CogMap only follows the object properties for the instance sets Is and
It which are usually much smaller.

For RDF repositories we utilize SPARQL queries to access data. We do not
rely on the completeness of domain and range restrictions for extracting prop-
erties, since they are often poorly defined (e.g., in DBpedia). Instead we take
the distinct set of all properties of the relevant instances (those retrieved by S
for extraction or those identified as values of object properties for expansion)
as the relevant data properties. For relational data extraction, we just add the
limit M to the seed SQL query S, execute the query, and store the result in
table T . For exploration, we follow the foreign keys according to the definitions
in the database schema. For XML files, we extract every attribute and every
direct child node that has a primitive value from the initial XPath expression
S. We store both attribute and child node values as data properties in Table 1.
For the exploration phase, we inspect the children x of all non-primitive nodes.
From these nodes, we again store the values of every attribute and every direct
child node that has primitive values.

For some properties, a given instance may have multiple values. For sim-
plicity, we consider only single values in this presentation. In practice, we have
found the concatenation of multiple values to be effective. More sophisticated
strategies will be developed in future work. On the other hand, there may exist
properties and/or instances which have almost no assertions, especially in large
RDF knowledge bases and XML documents. To ensure the effectiveness of the
approach, those assertions might need to be ignored. To cope with that issue,
CogMap has an optional parameter  to filter instances and data properties
with sparse value assertions.

J. N oner et al.

4.2 Alignment Generation and Selection

The goal of CogMap is to establish instance correspondences and correspondences between data and object properties. In doing so, the space of ontology
elements (in RDF stores) or schema elements (in relational and XML stores)
that CogMap considers is constrained by the seed information S. In addition,
instance alignments are constrained by the domains and ranges of property align-
ments. For example, if we align a property es =id to a property et =movieName,
then the resulting instance alignment is bounded by Ss =movie and St =film as
domains. Thus, there is no need to consider possible correspondences involving
other instances in the information stores.

In addition to the standard data-property to data-property, object-property
to object-property, and instance to instance correspondences, we also support
object- and data-property to data-property correspondences (Example: et =
film/country/./name/ and es = movie/language).

Line 7 of Algorithm 1 first computes the top-k property correspondence suggestions Xi. In the first iteration (i = 0), CogMap uses all available instance
data since no instance alignment exists yet (Y = ). For computational reasons,

Table 2. Selected implemented components.

Aggregators

Unions or joins of sets of correspondences. Average, maximum, or multiples of confidence values if correspondences share the same source entity es and target entity
et.
Name
TopKFilter

Filters

Description
Returns the top-k correspondences with the highest confidence value c.
Returns a functional one-to-one alignment. We implemented a greedy strategy. First, it orders the correspondences in descending order. Then, it traverses through the
list and drops all correspondences whose entities es or et
have been already matched.
Description

Property Matchers

OneToOneFilter

Name
PropertyNameMatcher Matches properties according to their name.
ValueLengthMatcher Matches properties p1 and p2 with close average value
DistinctValueMatcher

length / close percentages of distinct row entries l1 and l2.
The similarity is computed with M in(l1, l2)/(M ax(l1, l2).
If instance alignments Y = , we align properties by concatenating all values of all instances for each property and
compute the string similarity. If Y = , we compute the
property similarities for every instance pair in Y separately
and average over the results.

InstanceBasedMatcher

Align instances by concatenating every value for every property and computing their
string similarity. If a specific property p is given, consider only values of that property.
If an instance alignment Y is given, traverse through that alignment and update the
similarities based on the string values of all the given property value(s).

Instance Matchers
?

?

?
we use an implicit cutoff at this initial stage. In the following iterations, we can
improve the suggestions by considering the instance correspondences from the
previous iteration and comparing the property values only for the instance pairs
in Y. In these iterations, we do not apply any threshold but rank the correspondences at the end.

Then, we compute the consequences Yx for the top-k suggestions Xi (line 8-
10). That is, for each of those suggested property correspondences, we compute
the instance alignment that will result if this correspondence is selected by the
analyzer. Initially (i = 0), all source instances are compared against all target
instances. In following iterations the instance alignment Y from the previous
iteration is used to compute the new alignment. The threshold applied for the
instance alignment equals the confidence value c of e1, e2,, c  Xi.

The value of k is relatively unimportant here. As long as a correct correspondence is in the top-k suggestions, the results of the approach will not be
significantly affected. We have found that k = 5 is generally adequate to achieve
this condition, and results in a reasonable load on human analysts. In an automated setting it would be easy to use a larger k, which might produce slightly
better results at the price of of somewhat longer run times (to score the extra
suggestions).
Finally, the analyzer selects the optimal x  Xi based on the suggestions Xi
and the consequences {Yx| x  Xi}. As noted above, this selection can either
be made by a human user or by an automated selection function that takes the
confidence value and the suggestions into account. In this paper, we use only
a simple automated agent that selects the best-scoring alignment. Employing
humans or more-sophisticated agents would presumably produce better results,
but then any advantage of the approach might only come from the intelligence in
the human or agentusing a simple agent means that the benefits come from the
overall alignment philosophy. (We plan to address elsewhere the user interface
issues associated with supporting selections by a human.) After selection of x,
we update the seed property alignment X and the seed instance alignment Y for
the next iteration.

CogMap supports many different components to match instances and prop-
erties. Every Filter, Aggregator, and Matcher is a component. Each component has an execute() method, which returns a set of alignments.

The components are organized as a tree. The Matchers form the leaves. They
take a source table Ts, a target table Tt, a set of previously verified property
alignments X and a set of instance alignments Y from the previous iteration as
input. An Aggregator executes every component in the list cs and aggregate
the results. It might, for example, just take the maximum confidence value c
of all correspondences with equal entities es and et. A Filter reduces the size
of the alignment of its component after executing it. A simple filter might, for
example, only return the correspondences for which confidence values c are above
a certain threshold.

Table 2 lists a selection of implemented components and a short explanation
of their functionality. CogMap incorporates mechanisms to deal with different

J. N oner et al.

Table 3. Benchmark Statistics.

Benchmark (1)
DBpedia EPG Freebase Fandango

Benchmark (2)

People
?

?

?
Cast
?

?

?
Film
?

?

?
Movie
?

?

?
1,045,474 6,857

247,608

100,959

Format
Data Properties
Object Properties
Instances

(A)

OneToOneFilter

(B)

OneToOneFilter

UnionAvgAggregator

UnionAvgAggregator

InstanceBasedMatcher

PropertyNameMatcher

Fig. 3. Experiment Configurations.

date and number formats, which are omitted here for brevity, and easy interfaces
to facilitate new component development. Figure 3 provides example trees built
from these components.

5 Evaluation

We have selected two natural alignment tasks using real-world data, assessed the
performance of CogMap on them benchmarks, and compared its performance
with that of AML, LogMap, and RiMOM.

5.1 Benchmarks

The first benchmark aligns all people from DBpedia in FOAF format (wiki.
dbpedia.org/Downloads39#persondata) with cast information of all programs
playing on TV in the U.S. over a two week window from a commercial Electronic Program Guide (EPG) database. The second benchmark aligns Freebase
films (www.freebase.com/film/film) with movie data from Fandango (www.
fandango.com). Table 3 provides details on the number of instances and properties of each benchmark.

We designed and selected these benchmarks because existing state-of-the-
art ontology alignment benchmarks, e.g., in the Ontology Alignment Evaluation Intiative campaigns2, lack sufficient instance data, which are required by

2 See oaei.ontologymatching.org
?

?

?
CogMap. Because of the size of these benchmarks, it was not possible to prepare in advance an official gold-standard. Instead, a human judge was employed
to grade the correctness of the alignment results, which we discuss below.

5.2 Experiment Setup

We used the following experiment setup to answer three key questions:

 What is the impact of implementing a cognitive support model for align-

ment?

 What is the impact of using instance data for alignment?
 How general is our solution?

We first setup our solution, CogMap, using configuration (A) in Figure 3. A
description of each component used in configuration (A) can be found in Table 2.
CogMap analyzes the property correspondences, and selects the one with the
highest confidence value to iterate on (see lines 11 and 12 of Algorithm 1).

We then created two variants of CogMap to answer the first two experimental questions above. We first created a variantcalled InstMapby ablating
the cognitive support model used by CogMap. InstMap still uses instance data
but does not iterate on the results to further improve alignment.

We also created a second variantcalled Baselineby ablating both the cognitive support model and the use of instance data. Baseline performs alignment
using a property name matcher, but the other configuration components are the
same (see configuration (B) in Figure 3).

Moreover, we selected three state-of-the-art ontology alignment systems [15]
to compare CogMap against, in order to assess its practical impact. The three
systems are AML [11], LogMap [22], and RiMOM [25]. AML is focused on
computational efficiency and designed to handle very large ontologies. It is
the leading system in the conference and anatomy tracks of the 2014 ontology alignment evaluation, in terms of f-measure. LogMap provides a scalable
logical ontology alignment framework. RiMOM automatically combines multiple alignment strategies with the goal of finding the optimal alignment results.
We selected these systems because they are the most established systems in the
2014 ontology alignment evaluation, and an executable version is available to
the public.

Finally, we applied all systems above to both Benchmarks 1 and 2 to assess
their generality, and hence answer the third experimental question. Unless otherwise noted, we set the number of instances to use from each benchmark to
M = 5000, and the fraction of non-null values required for each property to
 = 0.1. We also converted each benchmark into the RDF OWL syntax because
many of the ontology matching systems compared cannot directly consume
databases or XML files.

All experiments were run on a desktop PC with 4GB of RAM and an Intel
i5 duo-core processor. We used Fast-Join [37] as the underlying matching algorithm for instances. Fast-Join combines both token-based similarity (Jaccard,

J. N oner et al.

Table 4. Benchmark 1 results.

Baseline AML LogMap RiMOM InstMap CogMap

nDCG@3
nDCG@6
P @3
P @6
Runtime in sec

0.38
0.35
0.33
0.33
0.4

0.76
0.51
0.67
0.33
2.7

0.38
0.25
0.33
0.17
9.1

0.38
0.48
0.33
0.50
5.8

0.76
0.74
0.67
0.67
1.9

1.00
0.89
1.00
0.83
3.0

Cosine, or Dice) and string edit distance. Moreover, it is currently the fastest
matching algorithm (see [21]), by implementing efficient pruning and hashing techniques, with soundness and completeness guarantees. This efficiency is
required because of our large benchmarks, which make it infeasible to compare
every source instance with every target instance.

The output of each system was graded by a human judge familiar with the
data sources in each benchmark3 using the metrics of Precision at n (P @n) and
the normalized (logarithmic) Discounted Cumulative Gain at n (nDCG@n) [38]
where n denotes that the top-n results. Precision P is defined as:

P =

and nDCG is defined as:

|correct correspondences|
|retrieved correspondences|

nDCG =
?

?

?
rel1 +
(1 +
?

?

?
n
i=2

n
i=2

reli
log2i

log2i)

where reli is 1 if the correspondence at position i is correct and 0 else. nDCG@n
gives more weight to correct correspondences that are ranked higher.

5.3 Results and Discussions

Tables 4 and 5 show the results for benchmarks 1 and 2, respectively. From
these results, we observed that CogMap outperformed InstMap in most cases.
We attribute this improvement to the only difference between the two systems:
CogMap uses a cognitive support model while InstMap does not. Hence, the
use of a cognitive support model has a positive impact on alignment results.

We also observed that InstMap outperformed Baseline in all cases. We
attribute this improvement to the only difference between the two systems: the
use of instance data. For example, Baseline could not correctly align the following
data properties in benchmark 1 by matching just the names of these properties.

first name  givenName
last name  surName
full name  name

3 Determining the correctness of the correspondences produced by each system was
simple for the human judge. We thus believe that the use of a human judge in this
manner did not introduce any biases and did not affect the comparison.
?

?

?
Table 5. Benchmark 2 results.

Baseline AML LogMap RiMOM InstMap CogMap

nDCG@3
nDCG@6
nDCG@9
nDCG@12
P @3
P @6
P @9
P @12
Runtime in sec

0.38
0.25
0.20
0.17
0.33
0.17
0.11
0.08
2.6

0.76
0.60
0.68
0.68
0.67
0.50
0.67
0.67
7.0

0.38
0.49
0.39
0.38
0.33
0.50
0.33
0.33
21.7

0.38
0.38
0.30
0.25
0.33
0.33
0.22
0.17
33.2

1.00
0.90
0.79
0.69
1.00
0.83
0.67
0.50
20.5

1.00
1.00
1.00
0.85
1.00
1.00
1.00
0.75
29.1

Fig. 4. Results for varying  (number of non-null values) for CogMap and InstMap
for both benchmarks. For high , nDGC and runtime decrease because fewer alignment
candidates remain.

However, InstMap correctly found these alignments because of the overlap
between the instances of these properties. Hence, these results show that the
use of instance data also has a positive impact on performance.

Finally, we observed that CogMap out performed all three state-of-the-art
ontology matching systems compared, i.e. AML, LogMap, and RiMOM. We
attribute this improvement to the following factors:

J. N oner et al.

Fig. 5. Results for varying M (number of instances) for CogMap and InstMap for
both benchmarks. The more instances included, the higher the overlap and hence better
results (nDCG).

 CogMap uses instance data for alignment.
 CogMap uses an iterative cognitive model for alignment.
 CogMap can ignore rarely used properties by using the  parameter.

Given the different characteristics of these two benchmark, the results above
suggest the general utility of an alignment system like CogMap that combines a
cognitive support model with the use of instance data. Moreover, the additional
computation does not contribute to a significant increase in runtime. Across
both benchmarks, CogMap had comparable (or better) runtime than the other
state-of-the-art systems compared.

Figures 4 and 5 show the impact of varying  (the fraction of non-null
values required for each property) and M (the number of instances used) for
CogMap and InstMap on both benchmarks. These results demonstrate the relative robustness of CogMap to these parameter settings compared to InstMap,
and further demonstrate the positive impact of using a cognitive support model.
For example, we observed on both benchmarks that the performance of CogMap
only became negatively impacted for larger values of , which was in contrast to
InstMap. Similarly, the performance of CogMap increased at a faster rate compared to InstMap as M was increased, and plateaued sooner than InstMap.
?

?

?
6 Conclusion and Future Work

This paper presents a cognitive based approach for aligning properties by taking
instance information into account. The approach is implemented in the system
CogMap which iteratively suggests property correspondences and their consequences in terms of instance alignments. In each round, the system is able to
improve these alignments based on the user verifications of the previous round.
Experiments show that the cognitive based approach outperforms both a baseline approach and the purely instance-based approach.

Currently, the system is restricted to aligning instances and properties. In
future work, we will enable class alignments and complex matchings [36]. These
complex matchings will be described using the R2RML standard (www.w3.org/
TR/r2rml).

We will extend exploration of the knowledge sources. First, we will integrate
object properties that are more than one hop away. This will require efficient
pruning techniques to avoid an intolerable blowup of both data size and processing requirements. Second, we will use the organization of the knowledge structure
(ontologies and schemas, when they are specified) to widen the search space by,
e.g., exploring the data of the superclasses.

The knowledge structures will also help to improve the alignment itself by
including ideas from [28,29]). Additionally, tree structure learning algorithms,
inspired by [35], will be used to learn the optimal composition of matching trees.
Finally, we plan to explore the possibility of integration into Karma [23],

which we believe would provide a suitable graphical user interface.
