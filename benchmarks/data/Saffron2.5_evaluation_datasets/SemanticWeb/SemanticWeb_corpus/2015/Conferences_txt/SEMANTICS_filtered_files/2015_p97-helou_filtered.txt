Cross-Lingual Lexical Matching with Word Translation and

Local Similarity Optimization

Mamoun Abu Helou

DISCo, University of Milan-Bicocca

Matteo Palmonari

DISCo, University of Milan-Bicocca

Viale Sarca,336

20126 Milano, Italy

Viale Sarca,336

20126 Milano, Italy

mamoun.abuhelou@disco.unimib.it

matteo.palmonari@disco.unimib.it

ABSTRACT
Cross-Lingual Mapping (CLM ) establishes semantic relations between source and target concepts to align two resources lexicalized in different languages, e.g., ontologies,
thesauri, or concept inventories, or to enrich a multilingual
resource. In this paper, we focus on purely lexical matching
algorithms to support CLM between lexically-rich resources,
where concepts can be identified by synsets. The key idea of
these algorithms is to use the results of word translations as
evidence to map synsets lexicalized in different languages.
We propose a new cross-lingual similarity measure inspired
by a classification-based mapping semantics. Then we apply a novel local similarity optimization method to select
the best matches for each source synset. To evaluate our
approach we use wordnets in four different languages, which
have been manually mapped to the English WordNet. Results show that despite our method uses only lexical information about the concepts, it obtains good performance and
significantly outperforms several baseline methods.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous

Keywords
cross-lingual mapping, lexical resource, similarity measure

1.

INTRODUCTION

According to a recent survey 1, web pages are lexicalized in as much as 166 different languages world-wide, 38 of
which cover at least the 0.01% of the total website content
world-wide. As of March 2015, more than 1,000,000 Open
Government Datasets have been also published online by national and local governments from more than 40 countries
in 24 different languages2. Language-based resources such

1http://w3techs.com/technologies/overview/content langu-
age/all
2http://logd.tw.rpi.edu/iogds data analytics

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SEMANTiCS 15, September 15-17, 2015, Vienna, Austria

c 2015 ACM. ISBN 978-1-4503-3462-4/15/09. . . $15.00
DOI: http://dx.doi.org/10.1145/2814864.2814888

as ontologies, thesauri, and dictionaries, have been shown to
provide valuable support for data integration in languagespecific applications. Merging these resources is a crucial
task to support the integration of information lexicalized in
different languages [15].

Cross-Lingual Mapping (CLM) is the task of establishing
semantic relations between concepts lexicalized in different
languages [26] to align two language-based resources [25, 14,
19, 26], or to create multilingual resources with rich lexicalizations [28, 22, 21]. Because of the size of lexical resources,
manual CLM requires considerable effort, which makes it
unfeasible at large scale. Automatic cross-lingual matching
methods can be used either to compute mappings automat-
ically, even at the price of accuracy [12], or to support semiautomatic mapping workflows by recommending mappings
to lexicographers [22].

In this paper we present a cross-lingual lexical matching
method to map lexically-rich language resources, i.e., resources that associate each concept with a set of synonym
words. In the rest of the paper we therefore refer to a concept
using the term synset, borrowed from the WordNet terminology [20]. Given a source synset s in one language, our
method tries to find the best match for s among the set of
target synsets in another language. The principles of our
method, which considers only the concepts lexicalization,
can be sketched as follows. We collect a large set of candidate matches for a source synset by translating each of
its synonym words (without trying to disambiguate their
meaning) using a machine translation system and a large
multilingual knowledge base. The words returned by the
translation are used also to rank the candidate matches by
computing their similarity with the source synset. We use
a disambiguation technique to select the mappings, i.e., to
assign the best matches to each source synset using a novel
Local Similarity Optimization Algorithm (LSOA).

To evaluate our approach we use cross-lingual mappings
manually established by lexicographers between four wordnets (Arabic, Italian, Slovene and Spanish) and the English
WordNet. Using gold standards based on these wordnets has
two main advantages: they contain a very large number of
mappings that cover different families of languages and resources of different size. Experiments show that our matching method outperforms several alternative methods and is
able to find mappings of reasonably good quality between
two language resources, at least when the target resource
covers a larger number of synsets than the source one.

The paper is structured as follows. In Section 2, we overview

related works. In Section 3, we overview our approach, and


In Section 4 we present the lexical similarof the paper.
ity measure.
In Section 5, we explain our local similarity optimization algorithm. The experimental design, the
gold standard datasets, the baseline methods, the evaluation measures, and the results are explained in Sections 6.
Conclusions and future works are in Section 7.

2. RELATED WORK

A first research area relevant to the CLM problem addressed by our work paper include approaches proposed to
build multilingual lexical ontologies (wordnets) [20, 28, 22].
The expand and merge models [28] are the main approaches
used in the development of multilingual wordnets.
In the
merge model, synsets of a pre-existing resource in one language (i.e., thesaurus) are aligned to the most equivalent
synset in English. In the expand model, English synsets are
translated in the respective languages. The main advantage
of these approaches is to avoid the expensive manual elaboration of the semantic hierarchy in new languages. The English WordNet hierarchy is used as reference for most of the
wordnets. Most of the developed wordnets have been manually compiled using these approaches by lexicographers [28,
22].

Automatic approaches have been proposed to reduce the
lexicographers workload. In [5], several heuristics to identify suitable translations have been used, e.g. based on
words monosemy and polysemy. Similar heuristics have
been applied to support lexicographers in building the MultiWordNet [22], by suggesting a set of potential mappings.
Multilingual corpora have been used as translation resources
to expand English synsets with lexicalizations in other lan-
guages, e.g., in the Slovene wordnet [13]. The above approaches mainly used translation resources to show potential
matches to lexicographers, without attempting to automatically match the wordnets.

A supervised method to automatically expand English
synsets with lexicalizations in other languages was also proposed [12]. This method does not match two different language resources, but learns to determine the best translation
for English synsets by taking into account, bilingual dictio-
naries, structural information in the English WordNet, and
corpus frequency information.

An unsupervised method was used to build, BabelNet,
the largest available multilingual knowledge system as of today [21]. BabelNet has been built by expanding the English
WordNet synsets with lexicalizations in other languages by
using a variety of Web-based multilingual resources. English WordNet synsets have been mapped to Wikipedia en-
tries, and lexicalizations in other languages are obtained by
using inter-lingual links of Wikipedia. Synsets for which
Wikipedia entries cannot be found have been expanded using automatic translations of English sense-tagged sentences;
translations of monosemous English words have been collected using Google Translate and directly included in the
expanded lexicalizations. The core of BabelNet consists of
the lexicalizations obtained with these methods, also named
BabelNet synsets. Later, synsets lexicalizations are expanded
with more lexical resources: Wiktionary, WikiData, Omega-
Wiki, and 25 wordnets that are mapped to the English
WordNet, which are available in the Open Multilingual WordNet [7]. While BabelNet automatic methods focus on expanding lexicalizations of English synsets with several re-

sources (expand model), we present an unsupervised automatic method to match two language resources lexicalized
in different languages (merge model). However, in our experiments we will evaluate the usefulness of mappings found
by our matching method to enrich synset lexicalizations in
languages other than English.

A second research area relevant to the work presented in
this paper is cross-lingual ontology matching [25, 26], where
machine translation tools and other Web-based language resources have been used, e.g. [14, 19]. Wiktionary was used
as a source of lexical knowledge to match English and French
ontologies [19]. Wikipedia inter-lingual links are used as an
intermediate resource for cross-lingual linking tasks [8, 17] .
These approaches have been applied to map structured and
lexically-poor ontologies (concepts are often represented by
at most one label). Our approach targets lexically-rich but
possibly unstructured resources, for which the structural information used by those approaches is not available.

The contributions of our method to the state-of-the-art
in the CLM field can be summarised as follows.
It does
not require training data, e.g., parallel corpora, like other
supervised cross-lingual methods [25, 12, 13]. It can be in
principle applied to any language for which word-to-word
translations are available (the machine translation resource
and the multilingual knowledge base used in this paper cover
respectively 90 and 271 languages).
It introduces a mapping selection method that significantly improves the quality
of an automatically generated alignment even when applied
after purely lexical, recall-oriented and efficient techniques
for candidate match retrieval and similarity evaluation. In
particular, it should be noticed that a similar optimizationbased mapping selection method introduced in mono-lingual
ontology matching [11] - which inspired our work - was based
on the computation of similarity among every source and
target concept and cannot scale to the size of language resources considered in our paper. In this paper, we compute
a locally optimal assignment for a small number of lexically
related synsets, which can be applied to large mapping prob-
lems. In addition, although we do not consider structural information in our CLM method, thus covering CLM scenarios
where lexically-rich resources are not structured, structural
matching methods [24, 26, 11] can be easily incorporated in
the similarity evaluation step of our method, without major
changes to our approaches.

3. APPROACH OVERVIEW

Before explaining our cross-lingual matching method, we
introduce the terminology used in the paper and describe
the approach used to generate word translations.

3.1 Lexicalizations of concepts

Synsets are used in wordnets to organize natural language
words into synonym sets. Each synset represents one underlying concept, i.e., a set of words (synonyms) that share the
same meaning in a given context [20].
If W is the set of
words represented in a wordnet, a synset s  P(W ) is a
set of words s = {w1, ..., wn}. A synset can contain one
word (synonymless) or many words (synonymful ) [1]. We
will use the set notation w  s to state that word w is a
member of synset s. Wordnet supports two types of rela-
tions: semantic relations that link synsets (e.g., hypernymy,
hyponymy), and lexical relations that link individual words
(e.g., antonymy, synonymy).


when it is a member of many synsets. In the paper the superscript + on the right-hand corner of a word, e.g., table+,
indicates a polysemous word. A word is monosemous, i.e.,
has only one meaning when it is a member of only one synset.
Given the set of word W and the set of synset S defined in
a wordnet, the function senses: W 7 P(S) returns the set
of synsets that a word belongs to, defined by senses(w) =
{s|w  s}. For example, the English word table+ is a member of eight synsets, i.e., it has eight senses in the English
WordNet; one of these senses means a set of data arranged
in rows and columns, which has the word tabular array
as a synonym word. A synset is a monosemous-synset
if it contains at least one monosemous word, e.g., {table+,
tabular array}.

3.2 Synset Translation

We define the translation of a source synset s in one language L1 into a target language L2 with a translation resource D, as a function sT ransL2
D : s 7 P(P(W L2 )) that
maps a synset s into a set of sets of words, each of which is
the output of the translation of some w  s. Figure 1 illustrates the translation of the Italian synset {tavola+, tabella}
into English.

Translations are obtained using external resources. In our
experiments we use two resources to obtain automatic trans-
lations: Google Translate 3 and BabelNet 4. Google Translate is a statistical machine translation system provides a
Web access service. BabelNet is a multilingual semantic
network. Nodes, called BabelNet synsets, represent concepts
or named entities, which are lexicalized in several languages.
BabelNet is arguably the state of the art multilingual knowledge system with lexicalizations built from several lexical
resources [21]. The BabelNet translation of a source word w
in a language L1 into a target language L2 is given by every
word in L2 that lexicalizes some BabelNet synset that also
contains w. We chose these resources for several reasons.
They generate translations for a large number of languages
and have been frequently used in several cross-lingual ontology matching approaches [26], BabelNet has incorporated a
large number of lexical resources. They return useful translations for a large number of synsets in different languages,
as discussed in previous work [1].

We used Google Translate and BabelNet to construct bilingual dictionaries for every pairs of non-English and English languages in our gold standards, similarly as performed
in [1]. To ensure the largest possible coverage, we merged
translations obtained of both directions (e.g., It-to-En, and
En-to-It) using Google Translate into one dictionary we called
MT dictionary. We extracted two bilingual dictionaries
from synsets in BabelNet: BabelNet core synsets (BNcore),
which are constructed with translation obtained by: sensetagged sentences that are collected from Wordnet and Wiki-
pidia, automatic translations of monosemous words (M W H
strategy), and Wikipedia inter-lingual links; and BabelNet
synsets (BN), all synsets in BabelNet, which include BNcore,
and lexicalizations obtained from Wikitionary, WikiData,
OmegaWiki, and Wikipedia redirection links. BNcore is a
subset of BN . We excluded the open multilingual WordNet
translations [7], which we use as our gold standards.

3https://translate.google.com/
4BabelNet version 2.5, http://babelnet.org/download

Figure 1: Cross-lingual matching method overview

3.3 Lexical Cross-Lingual Mapping

Cross-Lingual Mapping (CLM) has been defined in ontology matching as the task of finding mappings between
concepts of a source ontology, lexicalized in a language L1,
and concepts of a target ontology, lexicalized in a language
L2 [26]. This definition also applies to our CLM task, despite we consider collections of synsets rather than structured ontologies. Mappings can represent different relations
between source and target synsets. In this paper, we consider only equivalence mappings, i.e., mappings that specify
that a source and a target synsets have equivalent meaning.
Mappings can be defined as triples hs, t, w , where s and t
are synsets respectively of the source and target collections,
and w  [0; 1] is a weight representing the confidence that
the synsets s and t have an equivalent meaning. The set of
mappings returned by the CLM task is also called alignment,
As in most of related work, we also assume that the cardinality of an alignment containing equivalence mappings is
1:1, i.e., each source synset is mapped to at most one target
synset.

Given two collections of synsets lexicalized in different lan-
guages, our cross-lingual matching method tries to find
and add the best match for each source synset to the align-
ment. Our approach incorporate two main components. 1)
Candidate match retrieval. This component retrieves,
for each source synset s, a set of candidate matches T =
{t1, ..., tn} lexicalized in L2. Figure 1 shows the association between the Italian synsets {tavola+, tabella} and a
set of candidate synsets in English. Two subtasks are performed sequentially, synset-translation is used first, then the
sense-lookup is applied to the words returned by synset-
translation; every target synset that contains at least one
word translation is included in T . 2) Similarity Evalua-
tion. This component computes a weight for each candidate
match using a novel lexical similarity measure, explained in
detail in Section 4.

These above components are used in an algorithm that selects the mappings to include in the final alignment; our algorithm includes a disambiguation technique based on a Local Similarity Optimization Algorithm. We explain in detail
our algorithm in Section 5. Intuitively, we want to include in
the alignment a mapping between every source synset and
their best candidate matches. However, it should be noticed
that it is not always possible to include a mapping for every synset (a problem common also to baseline methods).
Different candidate matches may be evaluated to be equally
good for a source synset based on the available evidence, i.e.,
a tie occurs among a set of top-ranked matches; in this case,
the mapping for s is undecidable, and no mapping for s
is included in the final alignment. We call this TopOne se-

Synset-Translation Sense-Lookup Candidate Match Retrieval  tavola+ tabella list table table board plank panel slab desk tabular array t1 {board+} t2  {board+, table+} t3  {board+} t4 {table+ , tabular array} t5  {table+} t6  {table+, tabulate+ ,...} t26  {panel+ }  :   : t40  {list+ , listing} t41  {number+, list+ } : : : Similarity  Evaluation Mapping Selection { s, t1, 0.36 } { s, t2 , 0.29 } { s, t3 , 0.29 } { s, t4 , 0.36 } { s, t5 , 0.25 } { s, t6  , 0.22 } :  {table+, tabular array} {tavola+ , tabella} {tavola+ , tabella} diner be hardly applied to resources of the size considered in our
work, because it runs on a bipartite graph representing the
whole alignment.

The mapping selection method proposed in this paper is
based on merging locally optimal assignments computed for
each source synset. For each source synset s, a locally optimal assignment between a (small) set S of source synsets
lexically related to s, and a (small) set T of target synsets,
is computed; these pre-alignment mappings are stored with
their weights. Every pre-alignment mapping hs, t, w such
that s is not pre-aligned to any other mapping is included
in the final alignment.
If a source synset s is mapped to
more than one target synsets that have the same weight,
the match for s is undecided. In the following, we present
our Local Similarity Optimization Algorithm (LSOA), we
explain in details how we compute each locally optimal as-
signment.

We compute the locally optimal assignments using the
Hungarian method, which operate over a weight matrix. We
create a matrix M = S  T for each source synset s such
that: S contains s and a set of synsets lexically related to
s, and T consists of all the candidate matches for synsets in
S. The matrix is created by iteratively expanding S and T .
The set S is populated using a disambiguation context for
the synset s, i.e., source synsets for which we are likely to
find mappings in conflict with s. To determine the disambiguation context of a synset s we build a graph G = (V, E),
called disambiguation graph. Each node v  V is a
synset, which contains a set of words {w1, w2, ..., wn}. Two
synsets are connected by an edge iff they have at least one
word in common. We call polysemic relation the relation
represented by edges in the disambiguation graph because it
is based on polysemic words shared by synsets. The graph is
built iteratively from s, by performing a breadth-first search
over the polysemic relation with a depth limit. The depth
is defined by a maximum path length p, which is given as
input to the algorithm. Figure 3 shows an excerpt of the disambiguation graph for the Italian synset {tavola+, tabella}
with p = 1.

Once the disambiguation graph is built, and the candidate
matches for each synset in the graph are retrieved, we can
populate the weight matrix. The values of the cells represent the weights assigned to the synsets si and ti, which is
computed with CM as described in Section4; are defined as
follows: cell[i; j] = wij if the weight (similarity) assigned to
the synsets si and ti, cell[i; j] = 0 if wij. After the population of the weight matrix, a locally optimal assignment can
be computed using the Hungarian algorithm. The execution
of the algorithm consists of a series of iterative steps that
generate mappings with a maximum weight.

Observe that the similarity between a source synset s and
every target synset in the disambiguation graph, which is not
a candidate match for s, is equal to 0. In fact, whenever the
similarity between a source synset s and a target synset t is
different from 0, t will appear among the candidate matches
for t by definition. For this reason LSOA always finds mappings among the candidate matches of source synsets.

Figure 3 shows an illustrative example of a disambiguation graph built from the Italian synset {tavola+, tabella};
each node is attached with a descending order candidate
matches based on their similarity weights. Three nodes
have undecidable mappings because of ties. With LSOA the
selection task considers the synsets in the disambiguation

Figure 2: Classification-based similarity measure

lection approach, which preserves the 1:1 cardinality of the
alignment by electing at most one mapping for each map-
ping. However, sometimes, it might be useful to consider a
brave selection approach, such that every mapping that is
considered among the best matches is included in the align-
ment. We call the latter approach TopSet and we will use
it for evaluation purposes in Section 6.

4. LEXICAL SIMILARITY MEASURE

In this section we introduce (to the best of our knowledge)
a novel cross-lingual similarity measure (for short, CM),
which is inspired by the classification-based interpretation
of mappings semantics [4]. We use the F1-measure to compute the similarity between a source synset s and a target
synset t. The F1-measure is the harmonic mean of recall
and precision measures. Precision is defined as the number words returned by translation of s that occur in t, over
the total number of word returned by translation of s, i.e.,
P (s, t) = |{sT ransL2
D (s)}|. Recall is defined as the number of words returned by translation of s
that occur in t, over the total number of words in t. The
recall R(s, t) = |{sT ransL2

D (s)}t|/|{sT ransL2

D (s)}  t|/|t|.

The weight of a mapping between s and t synsets is given
as F1-measure: F1(s, t) = (2  P (s, t)  R(s, t))/(P (s, t) +
R(s, t)). For a more in depth analysis of the semantics of
cross-lingual mappings, details are provided in [3].

For example Figure 2 shows the CM computed between

the Italian synset {tavola+, tabella} and its candidate matches
in English. For instance, the weight for the target synset
{board+, control board,...} is computed as follows:
recall
R = 2

5 , precision P = 2

9 , and F1 = 0.29.

5. MAPPING BY MERGING LOCALLY

OPTIMAL ASSIGNMENTS

Given a ranked set of candidate matches, a simple approach for mapping selection is to map a source synset to its
best match (when a tie does not occur). However, this solution does not consider that the decision about a mapping can
be influenced by other mappings that are relevant for the de-
cision, e.g., mappings between other source synsets and the
same candidate matches. In previous work on mono-lingual
ontology [11], the mapping selection problem is viewed as an
instance of the Assignment Problem [9]: mappings included
in the alignment are those that maximize the similarity of
the whole alignment. Finding a global optimal solution for
an assignment problem requires combinatorial algorithms
(e.g., the Hungarian Method [18]), which are costly in terms
of memory usage and execution times and cannot be applied
to match thousands, or dozens of thousands of synsets (e.g.,
wordnets). Even the efficient solution proposed in [11] can

{tavola+, tabella+}It board plank panel slab diner list table tabular array c synset-translation 110.41110.50.50.50.50.330.250.17R0.220.220.220.220.110.110.110.110.110.110.110.110.11P0.360.360.290.360.200.200.180.180.180.180.170.150.13F1 {board,table } {plank,board } {(cid:271)oard,(cid:272)ontrol (cid:271)oard, panel,... } {table,tabular array } {board}  {board}  {gore,panel} {panel,venire  {jury,panel} {mesa,table}  {display board,board,...  {diner,dining car,...}  {add-in,board,cuit board,...}Candidate Synsets. . .  . . .  . . .  . . .  Recall = 2/5  Precision= 2/9 F1-measure=0.29  instrument panel control panel control board desk P&OWS 

{tavola+}It 

 Candidate Matches  

board, table  
 plank,board  
 board,control (cid:271)oard,panel,...  

 table,tabular array  
 board  
board  
 diner  

 . . . 


 Candidate Matches  

 board,table  
 plank, board  
 table, tabular array  
 plate  
 board  
 diner  

 . . . 


{tavola+, tabella}It 

{tavolo+, tavola+}It 

Disambiguation Graph 
      Path Length: p = 1 

{asse+, tavola+}It 

 Candidate Matches  

 plank,board  
board, table  

 board,control (cid:271)oard,panel,...                   0.29  


 table,tabular array  
 plate  
 board  
 diner  

 . . . 

Candidate Synsets

 Candidate Matches  

 board,table  
 plank,board  
 board,control (cid:271)oard,panel ,... 

 table,tabular array  
 . . . 

...


(cid:271)


(cid:272)


(cid:271)


: Tie 

: Polysemic relation 

: Synset 


{tavola+, tabella}

{asse+, tavola+}

{tavolo+, tavola+}

{tavola+}

0.36 0.36 0.29 0.36 0 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0

0 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.17 0.15 0.13

0.36 0.40 0.29 0.36 0 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.17 0.15 0.13

0.40 0.40 0.31 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0

0.50 0.50 0.22 0.25 0 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0

0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.22 0.20 0.17

0 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.22 0.20

Weight Matrix 

Figure 3: Illustrative example of the LSOA

graph. Once LSOA assigns the target synsets to the synset
{asse+, tavola+}. This reduces the likelihood that the English synset {plank+, board+} will be the target synset of
{tavola+, tabella} or {tavola+, tabella}. Then, the English
synsets {board+, table+} and {table+, tabella}, are assigned
to the source synsets {tavolo+, tavola+}, and {tavola+, tab-
ella}, respectively, in order to maximize the total similarity.
Mappings involving polysemous but synonymless synsets (for
short, P &OW S) are harder to filter out within the mapping
selection task [1]; for the synset {tavola+} still a tie occurs
and the algorithm will randomly select one target synset out
of the T opSet candidate matches. P &OW S synsets can be
correctly selected only if all the conflicting synsets (synsets
in the graph) are correctly disambiguated, and the target
synset is also not P &OW S synset [1].

6. EXPERIMENTS

The main goal of our experiments is to evaluate the performance of the proposed matcher in mapping a lexical resource
in a language other than English to the English WordNet,
i.e., in the Cross-Lingual Mapping (CLM) tasks. To this
end, we will compare mappings found with our matcher to
mappings available in gold standard alignments. In addition,
we want to evaluate the completeness of the lexicalizations
that can be found in the target language by using alignments
automatically created with our matcher. We want to evaluate to which extent that our mapping approach can be helpful to enrich a multilingual semantic network. In this case,
we will use gold standard alignments to evaluate the richness of the lexicon found using our mapping approach when
compared to the lexicon found using different approaches
proposed in related work [21].

We evaluate our matching method considering three different configurations: using our similarity measure CM with
selection based on T opOne mappings, i.e., ranking the similarity weights without applying the optimization method,
CM with selection based on LSOA at p=1, and CM with
selection based on LSOA at p=2. We compare these approaches with two baseline approaches: Monosemous Words
Heuristic, and Majority Voting. We run our experiment over
a system with 8GB RAM and 1.33GHz core. We used MT
dictionary for translation in the above configurations.

Next, we first describe the gold standards used in our ex-
periments. Then we describe the baseline approaches. Then
we describe the design of the experiments and the adopted
evaluation measures. Finally, we discuss the results and
highlight potential future directions

Table 1: Size of the wordnets: gold standards

Words
Word senses
Synsets

En

147306
206941
117659

Ar


It


Slv


Es


6.1 Datasets and Gold Standards

Four non-English wordnets, which are manually constructed

and mapped to the English WordNet (En) [20], are used
as our cross-lingual mapped concepts gold standards. The
wordnets are for Arabic (Ar) [23], Italian (It) [22], Slovene
(Slv) [13] and Spanish (Es) [16].The wordnets used in the
experiments have been built using different approaches and
cover different families of languages: a Germanic language
(English), two Ramoance languages (Italian and Spanish),
a Balto-Slavic language (Slovene), and a Semitic language
(Arabic). Moreover, English, Spanish, and Arabic languages
are among the top five spoken languages in the world. Italian and Slovene languages are selected also because they
represent minority languages. Table 1 reports the size of
these wordnets in terms of words, word senses and synsets.

6.2 Baseline Approaches

We compared the performance of our matching approach

with two baseline approaches described next.

6.2.1 Monosemous Words Heuristic

The monosemous words heuristic (MWH) selection method
is based on the assumption that a monosemous word in a
source language is translated into a monosemous word in
a target language. MWH applies only on monosemous-
synsets. Given a monosemous-synset s, the monosemous
words are translated, then word translation is used in sense-
lookup. If the same candidate synset is returned, i.e, they
are all monosemous word in the target language, it will be
selected as the corespondent synset.

MWH has been adopted in several state-of-the-art sys-
tems, e.g., [21], as heuristic because it has the advantage
that several correct mappings can be selected in a simple
way. However, synsets in different languages, which have
an equivalent meaning, can be lexicalized in different types
of words [2]. For example, the Italian monosemous-synset
{forchetta} is mapped to the English synset {fork+}, which
is a P &OW S synset. In Figure 1 the Italian monosemous
word tabella is translated into three words in English; two
are polysemous words. Therefore, the correct target synsets
cannot be selected with MWH. If we consider the translation
of its synonym word, the word tavola+, we can collect additional evidence that can be used to select the corespondent
synset.

6.2.2 Majority Voting

The majority voting (MV) selection method considers the
multiset union of the candidate matches that are returned by
every sense-lookup task for a source synset. Given a source
synset s, the set of candidate matches can be defined as the
set Ts = {tn
D (s)}, where n
indicates the number that the target synset ti is returned
by sense-lookup task. MV determines the best target synset
by selecting the most frequent synset.

i |ti  sense(w)  w  sT ransL2

For example, in Figure 1 the target synsets that have the
highest frequency are {board+, table+}3 and {table+, tabular
array}3; each appear three times in the candidate matches
set. These are undecidable mappings; a tie has been oc-


Translation

core

M T

Synsets
Monosemous
All
Monosemous
All
Monosemous
All

Ar


It


Slv


Es


curred. Nevertheless, two synsets are nominated as best
target synsets that include the correct match, instead of
considering all candidate matches in Ts.

6.3 Experiments & Results

We performed three experiments. In the first two experiments we focus on the CLM task, and we evaluate the accuracy of the mappings found with different matchers against
gold standards. We use the well-known performance measures Precision, Recall, and F1-Measure, to quantify the
performance of the matchers. Results are computed as the
macro-average measures [27]. The recall, precision, and F1measure are expressed in terms of percentage, ranging from
0% to 100%. In a third experiment, we focus on the task
of enriching a multilingual semantic network with lexicalizations in one language. These lexicalizations are obtained
with a matcher that aligns a large lexical resource in one
language, e.g., English WordNet, to a lexically-rich resource
in another language (e.g., an Italian thesaurus). To this end
we compute the Recall of the lexicalizations found with a
given method against the lexicalizations defined in the gold
standards. This measure tell us for a source synset, how
many words in the target synset, in the target language, are
returned with the enrichment method. Observe that since
we cannot expect that such lexicalizations are complete for
every language, measuring precision is not meaningful.

Experiment 1. In this experiment we evaluate the performance of the alignments found with every matchers. When
a matcher is not able to decide about a mapping because a
tie occurs (undecidable mappings), the mapping is considered not correct. We compare the three configurations CM,
CM+LSOAp=1, and CM+LSOAp=2 with the baseline appraoches MWH, MV.

We first evaluate the performance measures against the
whole gold standard. Then we perform the same evaluation
using a subset of the gold standard, which is defined by the
mappings such that the target synset is among the candidate matches found for the source synset by the candidate
match retrieval phase. We call this subset upper-bound
gold standard. We define the upper-bound of a matcher as
the synset-coverage; which evaluates if the translation function of the source synset returns at least one word of the correspondent synset, as defined in [21, 1]. This is motivated
by the observation that every selection method described
in this paper can select only matches that are found in the
candidate match retrieval step. Thus candidate match retrieval define an upper-bound for the recall of every method.
By evaluating the alignment found by the selection methods
with this subset of the gold standard, we want to evaluate
the performance of the selection methods in isolation from
the limits imposed by the candidate match retrieval phase.
In Table2 we report the upper-bounds of the matchers; as
a relative numbers of covered synsets in the gold standards
with different translation resources. We report the synsetcoverage for the monosemous-synsets and all synsets in the
gold standards. Table 3 reports the performance measures

compared to mappings in the all gold standard datasets and
upper-bounds. For LSOA we report results obtained with
disambiguation graph with path length p, for values p = 1
and p = 2; experimentally at p = 3 the system fails to perform all mapping tasks; on average execution time is 15 and
115 minuets at p = 1 and p = 2, respectively.

Experiment 2. In this experiment we want to understand
if correct mappings can be found in the set of T opSet candidate matches. In the selection methods used so far source
synsets for which a tie occurs are not mapped to any target synset, i.e., their mapping is undecided. However, if the
correct mapping can be found among these top-ranked candidate matches, and if this set is reasonably small, one could
use interactive matching approaches, e.g., based on crowdsourcing [10], to decide about the mappings. For this reason,
in this experiment, we consider as correctly retrieved mapping if it appears in a set of top-ranked candidate matches,
with cardinality less or equal than C, for different values of C.
Also in this case, compare the selection methods both with
the entire gold standards and upper-bounds datasets. Table 4 reports the recall for MV, CM and LSOAp=1 methods
with T opSet sets at cardinality  C; for C= 1 (T opOne),
5, and all the T opSet set.
In T opSet setting, a mapping
is considered correct for a source synset, whenever the correct match for the synset is included in the the set of its
top-ranked candidate matches. Observe that every mapping
that is counted as correct in T opOne setting, is also counted
as correct in T opSet setting. We report results for LSOA
only at p = 1 as it has reasonable execution time. Mappings
with MWH are not selected based on a similarity weight, so
it is not included.

Experiment 3. In this experiment we compare the lexicalizations of languages other than English found by mapping non-English lexical resources to English WordNet, with
three matching configurations: MV, CM, and LSOAp=1 compared to three sets of synsets, we call baseline-synsets. As
baseline-synsets we use BabelNet synsets [21]: BNcore and
BN; and a third set is synsets constructed with MT where
correspondent synsets are obtained directly with the translation of the source synsets, i.e., with out employing any selection method. We compare the lexicalization of synsets returned with the different matchers at T opOne setting, with
the lexicalization in the baseline-synsets. The intuition behind the baseline-synsets, on the one hand, is to highlight
the advantage of using selection methods w.r.t direct translation (i.e., MT), which presents a naive simple matching
approach; on the other hand, is to compare the returned
synsets w.r.t synsets in BabelNet, which is arguably considered as state-of-the-art multilingual knowledge system.

6.4 Discussion

Results in Table 3 shows that the performance of MV,
CM, and LSOA for the monosemous-synsets outperform the
MHW approach. MWH has higher precision than the other
methods, because large number of the monosemous-synsets
are synonymless synsets [1]. However, MWH has the lower
recall, this can be explained by two main observations. First,
MWH single-out the monosemous words form its synonym
words, which excludes several potential candidate matches.
Second, monosemous words that are considered as specific
domain words, are covered by the translation resource less
than the polysemous words, which are considered as frequent
and generic domain words to a great extent [1, 2].


Lang.

Synsets

F1

Ar

It

Slv

Es

Ar

It

Slv

Es

Monos.
All
Monos.
All
Monos.
All
Monos.
All

Monos.
All
Monos.
All
Monos.
All
Monos.
All


















































F1


w.r.t. gold standards


w.r.t. upper-bounds


F1


CM+LSOAp=1

F1

CM+LSOAp=2

F1


Table 4: Recall of T opSet selection at different cardinality

w.r.t. gold standards

w.r.t. upper-bounds

Synset
Ar Monos.

It

All
Monos.
All

Slv Monos.

All

Es Monos.

All

 1


 5


all


 1


 5


all


CM+LSOAp=1

 1


 5


all


 1


 5


all


 1


 5


all


CM+LSOAp=1

 1


 5


all


Table 3 shows that MV and CM results are comparable;
MV achieves higher precision measures than CM, whereas
CM achieves higher recall measures than MV. An interesting direction, is to study how to join the force of both meth-
ods, e.g., linear weighted combination matcher [11]. Observe
that, in Table 3 the precision measures are the same for gold
standards and upper-bounds datasets.

LSOA method reports the best measures w.r.t the other
methods. LSOA take advantage over MV and CM by considering mappings in the disambiguation graph. In Figure 3 the
Italian synsets {tavola+, tabella} cannot be disambiguated
with MV or CM because a tie occurs; which resulted in
undecidable mappings. With LSOA the selection task of
the synset {board+, tavola+} increases the likelihood that
the correct mapping of the synsets {asse+, tavola+} and
{tavola+, tabella} is selected. With LSOA we get much
closer to the upper-bounds for the monosemous-synsets (Ta-
ble 2), again this is because mapping tasks are not performed
in isolation of other mapping tasks, and synonymful synsets
that contains some monosemous and polysemous words are
better disambiguated with the context graph by resolving
the undecidable mappings.

In Table 3 recall measures still have considerable margins
w.r.t upper-bounds in Table 2. In fact, Table 3 reports the
performance with the T opOne selection setting, that is, we
did not consider cases when ties occur; Table 4 details this
case. Observe that recall measures are remarkably enhanced
when we consider the ties (undecided mappings).

Table 5 shows that the synsets that are obtained with MV,
CM, and LSOA methods significantly much richness than
baseline-synsets. Observe that, MC has the highest recall,
this in due to the fact that it selects the target synsets in a
such way that it maximizes the recall, whereas LSOA tries
to obtain a trade-off between precision and recall.

Table 5: Recall of the enrichment task

En to MT

Ar

It

Slv

Es

BNcore


BN MV


CM CM+LSOAp=1


7. CONCLUSIONS &FUTURE WORK

We presented a novel cross-lingual lexical selection ap-
proach. We introduced a new cross-lingual similarity measure inspired by a classification-based mapping semantics,
to measure the similarity between two synsets lexicalized in
deferent languages. We used a disambiguation technique to
assign the best match to each source synset using a novel
local similarity optimization algorithm. We evaluated our
approach using wordnets in four different languages, which
have been manually mapped to the English wordnet. Experiments showed that our approach significantly improves
the quality of an automatically generated alignment even
when applied after purely lexical, recall-oriented and efficient techniques for candidate match retrieval and similarity
evaluation. Moreover, experiments showed that by using a
merge model first, and enriching synsets lexicalizations using the found mappings with our approach afterwards, one
can obtain richer lexicalizations for the source synsets.

For future work we plan to enhance the recall of our ap-
proach, we plan to extend the Hungarian algorithm not to
stop after the generation of the (first) best mapping but to
continue to the generation of the best top  k mappings,
similar to work presented in [6]. Another interesting direction that we may consider is to build adaptive cross-lingual
matching method. We propose a semi-automatic procedure
that is capable of speeding up and improving pre-defined
cross-lingual mappings using a crowdsourcing based model.
We may consider an interactive matching process that im-


wordnets for automatic construction of the slovene
wordnet. In Z. Vetulani and H. Uszkoreit, editors,
LTC, LNCS. Springer, 2007.

[14] B. Fu, R. Brennan, and D. OSullivan. A configurable

translation-based cross-lingual ontology mapping
system to adjust mapping outcomes. J. Web Sem.,
2012.

[15] J. Garcia, E. Montiel-Ponsoda, P. Cimiano,

A. Gomez-Perez, P. Buitelaar, and J. McCrae.
Challenges for the multilingual web of data. Web
Semantics: Science, Services and Agents on the World
Wide Web, 11(0), 2011.

[16] A. Gonzalez-Agirre, E. Laparra, and G. Rigau.

Multilingual central repository version 3.0. In LREC,
2012.

[17] S. Hertling and H. Paulheim. Wikimatch - using

wikipedia for ontology matching. In 7th International
Workshop on Ontology Matching, 2012.

[18] H. W. Kuhn. The Hungarian Method for the

Assignment Problem. Naval Research Logistics
Quarterly, 2(12), March 1955.

[19] F. Lin and A. Krizhanovsky. Multilingual ontology

matching based on wiktionary data accessible via
sparql endpoint. CoRR, 2011.

[20] G. A. Miller. Wordnet: A lexical database for english.

Commun. ACM, 38(11):3941, Nov. 1995.

[21] R. Navigli and S. P. Ponzetto. Babelnet: The

automatic construction, evaluation and application of
a wide-coverage multilingual semantic network.
Artificial Intelligence, 193(0):217  250, 2012.

[22] E. Pianta, L. Bentivogli, and C. Girardi.

Multiwordnet: developing an aligned multilingual
database. In Proc. of the 1st GWC, January 2002.
[23] H. Rodriguez, D. Farwell, J. Farreres, M. Bertran,

M. A. Marti, W. Black, S. Elkateb, J. Kirk, P. Vossen,
and C. Fellbaum. Arabic wordnet: Current state and
future extensions. In The 4th GWC, 2008.

[24] P. Shvaiko and J. Euzenat. Ontology matching: State
of the art and future challenges. IEEE Trans. Knowl.
Data Eng., 25(1):158176, 2013.

[25] D. Spohr, L. Hollink, and P. Cimiano. A machine

learning approach to multilingual and cross-lingual
ontology matching. ISWC11, 2011.

[26] C. Trojahn, B. Fu, O. Zamazal, and D. Ritze.

State-of-the-art in Multilingual and Cross-Lingual
Ontology Matching. Springer, 2014.

[27] V. A. Vincent. Macro- and micro-averaged evaluation

measures, 2013.

[28] P. Vossen. Eurowordnet: A multilingual database of

autonomous and language-specific wordnets connected
via an inter-lingualindex. International Journal of
Lexicography, 17(2):161173, June 2004.

proves an alignment using the user feedback [10]. In par-
ticular, when we consider the undecidable mappings, which
provides higher recall. We plan to estimate the mapping
tasks difficulty based on the lexical characterization of concepts under evaluation, i.e., to estimate the ambiguity level
as discussed in [1], and accordingly we estimate the expected
users efforts (number of user to validate the task). Validation process will be analogues to the LSOA matcher; each
time users validate a mapping, other mapping tasks are updated according to the users feedback. Overall we plan to
follow the latter research direction to use in a mapping model
to ease the construction of a lexical-semantic ontology in
the context of the Arabic Ontology project [3].

Acknowledgments
This work was supported in part by COMSODE project
(FP7-ICT-611358) and SIERA project (FP7-INCO-295006).
