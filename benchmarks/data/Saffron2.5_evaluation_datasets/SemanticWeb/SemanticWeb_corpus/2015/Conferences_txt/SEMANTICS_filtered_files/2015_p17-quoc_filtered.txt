An elastic and scalable spatiotemporal query processing

for linked sensor data

Hoan Nguyen Mau Quoc

Danh Le Phuoc

Insight Centre for Data Analytics (formerly DERI)

Insight Centre for Data Analytics (formerly DERI)

National University of Ireland, Galway

University Road, Galway, Ireland

hoan.quoc@insight-centre.org

National University of Ireland, Galway

University Road, Galway, Ireland
danh.lephuoc@nuigalway.ie

ABSTRACT
Recently, many approaches have been proposed to manage sensor data using Semantic Web technologies for effective heterogeneous data integration. However, our research survey revealed that these solutions primarily focused
on semantic relationships and still paid less attention to
its temporal-spatial correlation. Most semantic approaches
do not have spatiotemporal support. Some of them have
served limitations on providing full spatiotemporal support
but have poor performance for complex spatiotemporal aggregate queries. In addition, while the volume of sensor data
is rapidly growing, a challenge of querying and managing the
massive volumes of data generated by sensing devices still
remains unsolved. In this paper, we propose a spatiotemporal query engine for sensor data based on Linked Data
model. The ultimate goal of our approach is to provide an
elastic and scalable system which allows fast searching and
analysis on the relationships of space, time and semantic in
sensor data. We also introduce a set of new query operators in order to support spatiotemporal computing in linked
sensor data context.

Keywords
Internet of things, Graph of things, Linked Stream Data,
Real-time Search Engine

1.

INTRODUCTION

The Internet of Things(IoT) is the network of physical
objects embedded with sensors that are making real-time
observations about the world as it happens. With the estimation of having 50 billion connected objects by 2020[4],
there will be an enormous of sensor observation data being
continuously generated per second. This sensor observation
data source, combining with existing data and services in the
Internet, is promising a wide range of innovative and valuable applications and services in Smart Cities, Smart Grid,
Industry 4.0, Intelligent Transportation Systems, etc. To be
able to extract meaningful information from heterogeneous

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SEMANTiCS 15, September 15-17, 2015, Vienna, Austria

c 2015 ACM. ISBN 978-1-4503-3462-4/15/09. . . $15.00
DOI: http://dx.doi.org/10.1145/2814864.2814869

sensor data sources in a variety of formats and protocols,
Semantic Web community has extended RDF data model,
that has been widely used for representing deep web data,
to connect dynamic data streams generated from IoT, e.g.
sensor readings, with any knowledge-base to create a single graph as an integrated database serving any analytical
queries on a set of nodes/edges of the graph [21, 2, 19, 12].
However, most of current approaches of using RDF data
model for managing sensor data, called Linked Sensor Data,
assume that RDF stores are able to handle queries on fast
update streams in conjunction with massive volume of data.
In fact, sensor data is always associated with spatiotemporal contexts, i.e, they are produced in specific locations
at specific time. Therefore, all sensor data items can be
represented in three dimensions: semantic, spatial and tem-
poral. Consider the following example: What is the average temperature in last 30 minutes in Dublin city. This
simple example poses an aggregate query on weather temperature readings of all weather stations in Dublin city. In
this example, the semantic dimension describes the average
temperature of Dublin city. The spatial dimension describes
the place (Dublin city) the sensor observes. The temporal
dimension describes time when the temperature values were
generated (last 30 minutes). Unfortunately, supporting such
multidimensional analytical queries on sensor data is still
challenging in terms of complexity, performance and scala-
bility. In particular, these queries imply heavy aggregation
on large amount of data points along with computationintensive spatial and temporal filtering conditions. Moreov
er, the high update frequency and large volume of natures
of our targeted systems (ten thousand updates per seconds
on billions of records already in the storage) will add up
the burden of answering the query within some seconds or
milliseconds. On top of that, by their nature, such systems
need to scale to millions of sensor sources and years of data.
Motivated by such challenges, in this paper, we propose
an elastic spatiotemporal query engine, which is able to in-
dex, filter and aggregate a high throughput of sensor data
together with a large volume of historical data stored in
the engine. The engine is backed by distributed database
management systems, i.e., OpenTSDB for temporal data
and ElasticSearch for spatial data, so that it enables us to
store billion data points and ingest million records per second while it is still able to query live data streaming from
sensor sources. Our contributions are as follows:

1. A proposed distributed spatiotemporal sub-graph par-

titioning solution which significantly improves spatiotemporal aggregate query performance.


temporal and semantic query operators supports computation of implicit spatial and temporal properties in
RDF-based sensor data.

3. An extensive performance study of the implementation
using large real-world sensor datasets along with a set
of spatiotemporal benchmark queries.

The remainder of the paper is organised as follows.

In
section 2, we review the related work of current existing
solutions. Section 3 describes our system architecture. In
Section 4, we present in details the implementation of this
framework and its infrastructure to store and query sensor
data. An experimental evaluation of this implementation
follows in Section 5. Finally, we conclude and discuss the
future work at the last section.

2. RELATED WORK

In this section, we will analyse the shortcomings for related work regarding supporting spatial temporal queries
over RDF-based sensor data. Current standard query language for RDF, i.e, SPARQL 1.1, does not support spatiotemporal query patterns on sensor data. Recently, there
are several complimentary work on supporting spatiotemporal queries on RDF. For example, to enable spatiotemporal
analysis, in [20], Perry et al. propose SPARQL-ST query
language and introduce the formal syntax and semantics
of their proposed language. SPARQL-ST is extended from
SPARQL language to support complex spatial and temporal queries on temporal RDF graphs containing spatial ob-
jects. With the same goal of SPARQL-ST, Koubarakis et
al. propose st-SPARQL[10]. They introduce stRDF as a
data model to model spatial and temporal information and
stSPARQL language to query against stRDF. Another example is [6], Gutierrez et al. propose the framework that
introduces temporal RDF graphs to support temporal reasoning on RDF data. In this approach, the temporal dimension is added into the RDF model. The temporal query language for temporal RDF graphs is also provided. However,
the aforementioned work commonly focuses on enabling spatiotemporal query features, but hardly any of them fully
address the performance and scalability issues of querying
billions of triples.

In the light of dealing with performance and scalability of
RDF stores, many centralized and distributed RDF repositories have been implemented to support storing, indexing and
querying RDF data, such as Clustered TDB[18], Inkling[16],
RDFStore 1, Jena[15], 4Store2. These RDF repositories are
fast and able to scale up to many millions of triples or few
billions of triples. However, none of the systems takes spatiotemporal feature into consideration.

Toward supporting spatial queries on RDF store, Brodt
et al.[1] and Virtuoso3 utilize RDF query engines and spatial index to manage spatial RDF data. [1] uses RDF-3x as
the base index, and adds spatial index for filtering entities
before or after RDF-3x join operations. Another example
is OWLIM[9], which supports geospatial index in its Standard Edition (SE). However, none of them systematically

address the issue of elasticity and scalability for spatiotemporal analytic functions to deal with massive volume of sensor data. The technical detail and the index performance
are also not mentioned in such systems. Moreover, these
approaches only support limited spatial functions, and the
spatial entities have to follow the GeoRSS GML[22] model.
Such systems are not aware of temporal nature of Linked
Sensor Data which might be distributed over long time span.
For example, in our evaluations, most of the data is continuously archived for 10 months or even 30 years for weather
data. Therefore, such systems can easily run into scalability
issues when the data grows. In one of our experiments with
Virtuoso [11], this triple store crashed after few week ingesting weather sensors reading from 70,000 sensor stations
and the system hardly can answer any simple queries on few
billions of triples in the store. Taking such short-commings
into consideration, our work presented in this paper is a new
evolution of our series of efforts [13, 11, 17] on managing sensor data together with other related work in the community.
The main focus of this work is designing a query engine that
is able to support complicated spatiotemporal queries tailor
of managing Linked Sensor Data while the engine is capable of dealing mentioned performance and scalability issues.
The design of such engine is presented in the next section.

3. SYSTEM DESIGN

To systematically address the short-comings in the previous section, we exploit multidimensional nature of Linked
Sensor Data to parallelise the writing and query operations
on distributed computing nodes. This decision is also inspired by the study of
[8, 14] showing that, the processing
in Big RDF Graph could be parallelised efficiently by partitioning the graph into smaller subgraphs to store in multiple
processing nodes. Following will be the overview of the architecture associated with indexing strategies to deal with
temporal and spatial aspects of the Linked Sensor Data.
The implementation details of this architecture is presented
in the next section.

3.1 Overview of the architecture

This section will present in details our scalable spatiotemporal query engine tailored for sensor data. The system architecture is shown in Figure 1.

Temporal)

Rules)

Spa0al)
Rules)

Text)
Rules)

Triple$pa2erns$

Recogni6on$

Rules$

RDF$

Parser$

Triple$Analyzer$

Triple$Router$

Raw$
data$

En6ty$

Producer$

Data$Manager$

RDF$

Triple)
Storage)

Elas6c$
record

s$

TSDB$

records$

Spa6al,$

Text$En6ty$

Indexer$

Temporal$

En6ty$
Indexer$

Open)TSDB)

Query$

Results$

Query$Processing$

Module$

Search$

Figure 1: System architecture

http://rdfstore.sourceforge.net
http://4store.org
https://github.com/openlink/virtuoso-opensource

In this architecture, the input data will be fed from sensor devices into system. The data format is RDF or raw.
For example, the data can be JSON, XML, text... Due to


tioned above, when the data is fed to the system, the Triple
Analyser will partition the input data based on RDF predicates that imply the spatial and temporal context. The
example is shown in Figure 2 and 3. In addition, in order to
support text-based search functionalities, the Triple Analyser can also partition the data if the property value is text
literal. The RDF predicate patterns that imply spatial, text
and temporal context of the triples will be defined by user
in Triple Patterns Recognition Rules.

For the Data Manager, we choose OpenTSDB4 and ElasticSearch5 as underlying storages for such partitioned sub-
graphs. Their table structures have the flexible data structure enables us to store subgraphs which share a similar
graph shape. This also solves the aforementioned issue of
multi-valued properties that is problematic for both traditional property tables and relational tables. Such table
structures is able to store a flexible number of attributes
in the same table without using list, set, or bag attributes.
Moreover, HBase and ElasticSearch provide spatial and temporal indexing schema that lead to two types indexing mechanisms introduced in following subsections. Other triple patterns that are not proper to be indexed in these types will
be stored in a native triple storage. We currently use JenaTDB to store such generic subgraph which is a small static
datasets and can be easily loaded into RAM of a standalone
workstation for the sake of boosting performance.

The incoming data from sensor devices is routed to destined storages via the data routing flow illustrated in Figure
1. The fetching operations are built as an asynchronous
tasks. The engine then evaluates the triple received and
buffers the data streams for further processing. Data is
routed to Triple Analyser to process based on Triple Patterns Recognition Rules. The Triple Patterns Recognition Rules is a set of triple patterns defining the rule to
extract the values from streaming data. Values are extracted
will be indexed properly based on their characteristics and
graph patterns. The stream subgraphs that have spatial
context will be routed to ElasticSearch to index and the
ones have time series or numeric values will be transferred
to OpenTSDB cluster. The details of these two indexing
mechanisms are presented below. On the other hand, the
stream subgraphs do not match any spatial and temporal
rules will be stored in the normal triple storage.

3.2 Spatial-driven Indexing

To enable spatial computation on spatial properties attached to a subgraph, we store and index such subgraph as
a document in ElasticSearch storage as illustrated in Figure 2. Along with spatial attributes, ElasticSearch allows us
to add date-time, string attributes to be indexed on which
we can filter by the range condition. That means a combination of spatial computation, free text search and time-based
filter are supported in our SPARQL endpoint (see example
queries in Appendix).

To build a document for inserting into the ElasticSearch
storage, the Spatial and Text Entity Indexer will extract
RDF triples which comprise geographical information, text
labels and date-time values. Triples which are filtered by this
component will be constructed to ElasticSearch record and
then stored in ElasticSearch storage. Both bulk and near

http://opentsdb.net/
https://www.elastic.co/

s"

p"

o"

:dublinAirport- geo:lat-

53.1324^xsd:float-

:dublinAirport- geo:long-

18.2323^xsd:float-

:dublinAirport- rdfs:label-
:dublinAirport- rdf:type-

Dublin-Airport-

ssn:FeatureOfInterest-

Spa0al"rules:"
?s-geo:lat-?lat.-
?s-geo:long-?long.-

Triple"

Analyzer"

Text"rules:"
?s-rdfs:label-?label.-

:dublinAirport- Rdfs:label-

Dublin-Airport-

:dublinAirport- Geo:lat-

53.1324^xsd:float-

:dublinAirport- Geo:long-

18.2323^xsd:float-

ElasXcSearch-Doc-

{-
--"uri":-"hJp://graphoLhings.org/resource/
PhysicalPlace/8e8wyehe",-
label":-Dublin-Airport",-
}-

ElasXcSearch-Doc-

{-
"uri":-"hJp://graphoLhings.org/resource/
PhysicalPlace/drywv0kt",-
"geo":-"drywv0kt",-
"coords":-"44.78255844116211,-
-V69.38337707519531",-

"locaXon":-{-
--------"type":-"point",-
--------"coordinates":-[18.2323,-53.1324-]-
---------}-
}-

En0ty"
Indexer"

ElasXc-
Search-

Figure 2: Store spatial subgraphs as ElasticSearch document

real time (NRT) updates are possible with ElasticSearch.
Bulk updates are accomplished using Hadoop Map/Reduce
and NRT are performed through direct HTTP calls.

3.3 Temporal-driven indexing

A large amount sensor data is fed as time series of numeric values such as temperature, humidity, wind speed. For
these time series data, we choose OpenTSDB (Open Time
Series Database) which uses HBase as the underlining scalable database. OpenTSDB is able to ingest millions of time
series data points per second. As shown in Figure 1, input
triples which comprise numeric values and timestamps are
analysed based on predefined temporal recognition rules in
Triple Analyzer. The Entity Indexer extracts numeric measurements to construct time series rows to insert to HBase
via parallel Time Series Daemons (TSD) running on different machines of our cluster.

Along with numeric values and timestamps, some metadata can be added to each datapoint of OpenTSDB. Such
metadata can be used to filter the values by the information
encoded in the meta data. Therefore, we extract spatial
context and sensing context of the numeric sensor readings
as a temporal subgraphs. This graph is used to construct
necessary metadata to add to the data points that will be
then inserted to OpenTSDB. The metadata is chosen by
the frequencies used as filtering parameters in the SPARQL
queries. For example, the coordinate where the sensor reading is captured will be used to covert to geohash value. Together with the geohash value, other properties such as type
of sensor, type of reading are also encoded as filtering tags.
The example is illustrated in figure 3.

metric$

Dme$

value$

tag$

$dp0$

$1288946927$

$14$

$geohash=dp0m5c4e$$

put$

$sensorId=u0qxpf3m_TemperatureSensor_noaa_109340$$$$$$$$readingtype=temprature$$$

tag$

tag$

Figure 3: TSDB Put example


IMPLEMENTATION

In this section, we will present in details the implementation of our engine based on the architecture presented in
the previous section. We firstly introduce our data partitioning strategy, which paces the way for parallelising the query
processing of the distributed computing nodes.
In partic-
ular, two main types of query operators will be executed
in parallel are spatial and temporal operators, which are
then handled by ElasticSearch and OpenTSDB respectively.
The provided query language is an extension of SPARQL
1.1. towards supporting spatiotemporal computation. To
coordinate the semantic, spatial and temporal operator, we
implement the query executor by using query delegation approach which breaks the query into smaller query execution
tasks that can be delegated to underlying query processing
engines, ie., Jena, ElasticSearch, OpenTSDB. The details
will be followed in next subsections.

4.1 Data partitioning strategy

For storing historical observation data of sensors, we choose
OpenTSDB, which uses HBase as a underlining scalable
database. By default, OpenTSDB stores input data in the
tsdb HBase table. At the beginning, data is inserted into
just one region of tsdb table. If the number of the records in
the first region exceeds the given threshold, the region will
split into two roughly equal-sized child regions. As more and
more data is inserted, the regions will split recursively. This
is an expensive operation and heavily affects to insert per-
formance. To avoid this costly splitting, we divide the tsdb
table at predefined level beforehand using the pre-splitting
method. For different data sources, the pre-splitting strategy might be varied as it is very dependent upon the characteristics of the data. For our system, we design geohashbased data partitioning strategy in order to take advantage
of the spatial-temporal correlation feature of sensor data.
We implement this technique by firstly calculating the geohash prefix for all sensors based on their locations and then
embedded it in the rowkey prefix. The details of rowkey
design scheme are shown in Figure 4. For the sensors located in the same area, their data will be ordered by time
and stored in the same Hbase regions. Figure 5 illustrates
the geohash-based data partitioning strategy. Our experiments show that, this strategy significantly increases the
insert performance of system.

geohash_)me_fullGeoHashTag_fullGeoHashValue_sensorIdTag_sensorIdValue_readingTypeTag_readingTypeValue!
|AAAAAAAA|!|AAAAAAAA|!|AAAAAAAAAAAAAAAAAAA|!|AAAAAAAAAAAAAAAAAAAAA|! |AAAAAAAAAAAAAA|! |AAAAAAAAAAAAAAAA|! |AAAAAAAAAAAAAAAAAAAA|!|AAAAAAAAAAAAAAAAAAAAAAA|!
metric! )me!

tagv1!

tagk1
!

!dp0!!1288946927! geohash=dp0m5c4e!!

sensorId=u0qxpf3m_TemperatureSen
sor_noaa_109340!

tagk2
!

tagv2
!

tagk3
!
readingtype=temprature!!

tagv3
!

Figure 4: Rowkey design scheme

4.2 Spatial Operators

To query spatial relationships between resources, we first
define operators to extract the spatial properties of a single
entity and spatial region, then define operators to query relationships between these extracted spatial properties. Based
on topological spatial relations identified by Egenhofer and
Herring [3], the engine currently supports some main spatial operators like spatial:withinCircle, spatial:intersectBox
and spatial:withinBox..., that are aligned with the GeoDistance and Geo Bounding Box Filter functions in Elastic-
Search. Moreover, to filter the query results, we allow the

t1(

t2(

t10(

t15(

tk(

tk+1(

tn(

<me(

longitude(

longitude(

a0#


0u#

t5#

t(


zy5#

Data(

0u1$9xz(

a01$bxq(

.......(

t5a$s5u(

u0q$zy5(

region(1(

region(2(

region(n$1(

region(n(

HBase(Regions(

Figure 5: Geohash-based data partitioning

user to specify the semantic description and time conditions
in the input parameters. The Query 1 in the Appendix illustrates the example of the semantic constraints (filtering by
dul:PhysicalPlace) on spatial entities and the time condition
(1h-ago).

4.3 Temporal Operators

Similar to spatial query, to allow queries on temporal relationships between entities and events, we first identify the
temporal properties [5] of single entities and events and then
define operators for searching relationships between the extracted temporal properties. In our engine, we also extend
the temporal operator not only for searching relationships
but also be able to perform aggregate operations with the
time constraints. Some temporal-based aggregation operators are described in following:

 temporal:sum: calculates the sum of all reading data
points from all of the time series or within the time
span if down sampling.

 temporal:avg : calculates the average of all observation values across the time span or across multiple time
series.

 temporal:min: returns the smallest observation value

from all of the time series or within the time span.

 temporal:max : returns the largest observation value

from all of the time series or within a time span.

 temporal:values:

list all observation values from all

of the time series or within the time span.

 temporal:sensors:

list all sensors which have valid
observation value from all of the time series or within
a time span.

To provide more flexible features to the user, for the aggregation operator, a new parameter, called groupin, is pro-
vided. This new feature helps the user to avoid retrieving
millions of data points when requesting data for a long time
span which can cause a burden on bandwidth or slow responding time. When the query is executed in OpenTSDB,
the groupin value will be translated to downsampling value
which informs the time-series database to break the long
span of data into smaller spans and merge the data for the
smaller span into a single data point. The aggregation functions will perform the same calculation as for an aggregation


ple time series at a single time stamp, downsampling works
across multiple data points within a single time series over
a given time span. The Query 6 is an example of average
function with the groupin value is 1day.

4.4 Query Delegation Model

The query processing module in Figure 1 will be implemented by query delegation model which breaks the input
query into subqueries that can be delegated to underlying sub-components that are ElasicSearch, OpenTSBD and
Jena ARQ 6 query engines.
In this model, a SPARQL
query can be represented by SPARQL Query Graph Model
(SQGM) [7]. A query translated into SQGM is a planar
rooted directed graph consisting of operator as nodes and
data flows as edges of a binary tree.
In SQGM, an operator processes and generates either an RDF graph (a set
of RDF triples), a set of variable bindings, or a Boolean
value. Any operator has the properties input and output.
An evaluation process is a post-order traversal of the tree,
during which the data are passed from the previous node
to the next. In this tree, each child nodes can be executed
individually as asynchronous tasks, which can be carried
out in different processes on different computers. Therefore,
our system delegates some of those evaluation tasks to different distributed backend repositories, which can provide
certain function sets, e.g., geospatial functions (by Elasic-
Search), temporal functions (by OpenTSDB) and achieve
the best performance in parallel. Figure 6 shows an example of SQGM tree on which a spatial filer node is delegated
a geospatial query to ElasticSearch.

JOIN%

FILTER%

BGP%

?obs%ssn:observedBy%?sensor%

BGP%

Condi3on%

?sensor%a%ssn:Sensor%

?sensor%spa3al:withinCircle(67.03%
H178.91%10.0%km)%

{%
%%%"filter":%{%
%%%%%%"geo_distance":%{%
%%%%%%%%%"conference.coordinates":%{%
%%%%%%%%%%%%"lon":%H178.91,%
%%%%%%%%%%%%"lat":%67.03%
%%%%%%%%%},%
%%%%%%%%%"distance":%10km",%
%%%%%%%%%"distance_type":%"arc"%
%%%%%%}%
%%%}%
}'%

Figure 6: Delegating the evaluation nodes to different backend repositories

5. EXPERIMENTAL EVALUATION

To demonstrate the performance and scalability of our ap-
proach, we evaluate our system on a physical setup and a
cloud setup. The physical setup dedicates to a live deployment at http://graphofthings.org which has been ingesting
and serving data from more than 400k sensor data sources
since June, 2014. For the cloud setup, we repeat the process
carried in the physical setup on the Google Cloud.

5.1 Experimental Setup

Physical setup. The Physical setup is a physical cluster
of 4 servers running on the shared network backbone with
10Gbps bandwidth. Each server has following configuration:

2x E5-2609 V2 Intel Quad-Core Xeon 2.5GHz 10MB Cache,
Hard Drive 3x 2TB Enterprise Class SAS2 6Gb/s 7200RPM
- 3.5 on RAID 0, Memory 32GB 1600MHz DDR3 ECC
Reg w/Parity DIMM Dual Rank. One serves is dedicated as
front-end server and coordinating the cluster, other 3 servers
are used to stored data as and run as processing slaves.

Cloud setup. For the elasticity and scalability demon-
stration, we then evaluated our engine using a virtual cluster deployment on Google Cloud. The configuration of the
Google Cloud instances we use for all experiments is n1-
standard-2 instance, i.e, 7.5 GB RAM, 1 virtual core with
2 Cloud Compute Units, 100 GB instance storage, Intel Ivy
Bridge platform. In this evaluation, we more focus on showing how the system performance scales when increasing the
number of processing nodes, rather than comparing with the
performance on physical cluster.

Dataset. In order to give a practical overview of the en-
gine, we evaluate it on some realistic scenarios which cover
both complexity, performance and scalability aspects. To
meet such requirements, we choose the dataset consists of
more than 400K sensing objects allocated around the world
and cover various aspects of data distribution. The window
of archived data spreads over 40 years. The dataset has more
than 8.5 billion sensor reading records which represented in
SSN observation triple layout (15-40 triples/records). Hence,
the data contains approximately 127-340 billion triples if it
is stored in a native RDF store. For that reasons, we prepare
two main datasets for the experiments. Among them, the
biggest one is weather data sources containing about 8,1 billions historical sensor data from 80,000 sensors around the
world (LSM [11] and NOAA 7). Another one is a transportation dataset containing 360 millions spatial records. This
spatial dataset is collected from 317,000 flights and 20,000
ships in the past 8 months. To evaluate the engines full-text
search functionality, we use the Twitter dataset which has 5
millions of tweets. The detailed statistics of these datasets
are listed in Table 1.

Table 1: Dataset

Sources


Flight
Ship

Twitter

Sensing objects Historical Data Archived window

21k
60k
317k
20k



7,900M
185M
317M
51M
5M

since 1/1971
since 6/2014
since 8/2014
since 2/2015
since 8/2014

To evaluate the query processing performance, we prepared various types of queries to cover almost popular analytical query contexts over historical sensor data and meta-
data. The SPARQL representation of the queries can be
found in the Appendix.

5.2 Experimental Results

5.2.1 Evaluating performance

In this subsection we evaluate the efficiency of our approach with respect to data indexing and query execution on
our physical cluster. Figure 7 describes the insert throughput of the system spatial, temporal and textual data.

The results show that the system can reach 55000 record-
s/s for the temporal data and there is no significant difference when the number of data point increases from 0.02 to

https://jena.apache.org/documentation/query/

http://www.noaa.org


50000"

40000"

30000"

20000"

10000"


0"


95,000$

90,000$

85,000$

80,000$

75,000$

70,000$


12000#

11800#

11600#

11400#

11200#

11000#

10800#

10600#

10400#

10200#

10000#

Number'of'spa9al'object'(million)'

Number'of'Data'point'(billion)'

0.02" 0.15" 0.39" 0.53" 0.79" 1.05" 1.31" 1.54" 1.83" 2.14" 2.45" 2.77" 3.08" 3.39" 3.72" 4.08" 4.54" 4.95" 5.59" 6.30" 7.07" 7.78"

38$

179$

315$

360$

487$

530$

572$

588$

658$

Number'of'textual'object'(million)'

0.64#

1.43#

2.16#

2.41#

2.97#

3.53#

4.38#

(a) Average temporal index throughput

(b) Average spatial index throughput

(c) Average full-text index throughput

Figure 7: Index performance

7.78 billion. This is because we pre-split the data regions in
advance based on the geohash prefix so that the data that
is close in space and time will be distributed into the same
specific regions. When the amount of data stored in a region
reaches to the threshold value, it needs to resplit. Together
with the splitting process, the related data has to be transferred and distributed again. This step will cause additional
cost and will affect the insert performance. This explains
the slight decreasing in the Figure 7a when the data size
reaches to 5 billions.

Figures 7b and 7c describe the performance of spatial index and full-text index in ElasticSearch. The results reveal
that the increasing of the number of data point can reduce
significantly the insert performance. For the text data, the
system needs to analyse the value and break it into the set
of substrings. Consequently, the cost is higher and the performance of insert is much lower compares to spatial index.
We have performed three types of experiments for the
query execution. We evaluate performance in terms of average query execution time. The experiments are follows:

(Exp.1) For each test query we create single query instance.
We execute each query instance and compute the average query execution time over the whole data set.

(Exp.2) For each test query we compute the average query

execution time by varying data time collected.

(Exp.3) For each test query we generate multiple query
instances from 10 to 1,000 at the same time. We then
execute these instances in parallel and compute the
average query execution time.

Table 2: Query execution time on physical cluster

Query

Execution
Time(ms)

Q1

Q2

Q3 Q4 Q5

Q6

Q7

Q8

For the Exp.2., the results in Figure 8 show that the execution time of Q1, Q3, Q4, Q5, Q6, Q7 increases slightly
but takes less than 0.8 seconds to response for the whole
dataset. However, the execution time of Q2 and Q8 sharply
rises from 0.5 secs to 2.8 secs and from 1.3 secs to 3.9 secs
respectively. This is because the more moving objects are
added, the considerably heavier work load is pushed to ElasticSearch cluster. On the contrary, for the queries on temporal data, the query execution time is almost steady due
to our effective data partitioning strategy in OpenTSDB.

The results of Exp.3 are shown in Figure 9. The execution time for all the queries increases when more query instances are added. In particular, when the number of query
instances increases from 500 to 1,000, the query execution
time also increases rapidly due to the growing workload applied to the system.


4500"

4000"

3500"

3000"

2500"

2000"

1500"

1000"

500"

0"

Q1"

Q2"

Q3"

Q4"

Q5"

Q6"

Q7"

Q8"

Time'

Figure 8: Average query execution time by time collected

Complexity S-T

Te

S-T S-T-Agg

S-T-Agg

S-T-Agg

5.2.2 Evaluating scalability

S: spatial, T: temporal, Te: text, Agg: aggregate function

Table 2 shows the result of Exp.1. The execution time is
different for each query due to its feature. For the queries
with high selectivity like Q2 and Q8, the performance is
poor. This is caused by not only for the large number of
moving spatial object that needs to be filtered but also because of the query complexity. For example, in both Q2 and
Q8, in addition to the spatial criteria, there is a time condition to filter all spatial objects appearing during that time
period.

In this subsection, we measure how system performance
scales when adding more nodes to the cluster. We vary the
number of nodes in Google Cloud cluster with 2, 4, 8, 12
nodes respectively.

Figure 10 presents the average index throughput of spa-
tial, temporal and text objects when increasing the number
of nodes. The results reveal that the index performance
linearly increases with the size of cluster. This is because
scaling out of the cluster causes the the working data need
to be indexed on each machine to be small enough to fit into


40040#

30040#

20040#

10040#

40#

Q1#

Q2#

Q3#

Q4#

Q5#

Q6#

Q7#

Q8#

6000"

5000"


4000"


3000"

2000"

1000"

0"

Q1"

Q2"

Q3"

Q4"

Q5"

Q6"

Q7"

Q8"

10#

100#

250#

500#

1000#

Number'of'clients'

2"

4"

8"

12"

Nodes&

Figure 9: Average query execution time by varying number
of clients

Figure 11: Average query execution time by varying number
of cluster nodes

main memory, which dramatically reduces the needed disk
I/O.

query performance. Whilst our system still has its limi-
tation, it is a step towards providing high performance spatiotemporal query engine in the IoT world.

7. ACKNOWLEDGMENTS

This publication has emanated from research supported
in part by Irish Research Council under Grant No. GOIPD
/2013/104 and by European Union under Grant No. FP7-
ICT-608662 (VITAL).

spa+al"

temporal"

text"


25000"

20000"

15000"

10000"

5000"

0"

2"

4"

8"

12"

Nodes'

Figure 10: Average index throughput by varying number of
cluster nodes

In the following, we look at the query performance as
shown in Figure 11. In this figure, the Query 8 scales perfectly with the cluster size while the others have a slightly
decreased on query execution time. Our explanation is that
along with the scaling out of cluster, the amount of data processing on each node is reduced significantly. The amount
of data need to be processed is based on the complexity of
the query. On the other hand, this is also the case for Query
8 which requires the spatiotemporal computation on a large
number of moving flight during the last hour. Moreover,
to complete the required processing, a further aggregation
operation on each flight is performed to calculate the surrounding average air temperature.

6. CONCLUSIONS AND FUTURE WORK

The need for efficient querying on massive amount of sensor data lies at the heart of most sensor data analytics plat-
form. In this paper, we present our recent effort on leveraging the linked data and NoSQL technologies to effectively
manage sensor data. Our approach provides not only a complex spatiotemporal query functions to the users but also
proves the ability to handle billions of sensor data. Our experimental results show that this approach is both fast and
scalable.

For the future work, we expect to adapt a distributed
triple store to our system. Furthermore, we are implementing some query optimisation algorithms to speed up the
