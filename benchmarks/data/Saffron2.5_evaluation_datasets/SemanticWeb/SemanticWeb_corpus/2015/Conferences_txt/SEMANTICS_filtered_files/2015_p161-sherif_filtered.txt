An Optimization Approach for Load Balancing in Parallel

Link Discovery

Mohamed Ahmed Sherif

University of Leipzig

Augustusplatz 10

04109 Leipzig, Germany

Axel-Cyrille Ngonga Ngomo

University of Leipzig

Augustusplatz 10

04109 Leipzig, Germany

sherif@informatik.uni-leipzig.de

ngonga@informatik.uni-leipzig.de

ABSTRACT
Many of the available RDF datasets describe millions of resources
by using billions of triples. Consequently, millions of links can
potentially exist among such datasets. While parallel implementations of link discovery approaches have been developed in the past,
load balancing approaches for local implementations of link discovery algorithms have been paid little attention to. In this paper,
we thus present a novel load balancing technique for link discovery
on parallel hardware based on particle-swarm optimization. We
combine this approach with the Orchid algorithm for geo-spatial
linking and evaluate it on real and artificial datasets. Our evaluation suggests that while naive approaches can be super-linear on
small data sets, our deterministic particle swarm optimization outperforms both naive and classical load balancing approaches such
as greedy load balancing on large datasets.

INTRODUCTION

1.
With the constant growth of Linked Data sources over the last years
comes the need to develop highly scalable algorithms for the discovery of links between data sources. While several architectures
can be used to this end, previous works suggest that approaches
based on local hardware resources suffer less from the data transfer bottleneck [18] and can thus achieve significantly better runtime than parallel approaches which rely on remote hardware (e.g.,
cloud-based approaches [13]). Moreover, previous works also suggest that load balancing (also called task assignment [20]) plays a
key role in getting approaches for Link Discovery (LD) to scale.
However, load balancing approaches for local parallel LD algorithms have been paid little attention to so far. In particular, mostly
naive implementations of parallel LD algorithms have been integrated into commonly used LD framework such as SILK [8] and
LIMES [16].

The load balancing problem, which is know to be NP-complete [20],
can be regarded as follows: Given n tasks 1, ..., n of known computational complexity (also called cost) c(1), ..., c(n) as well as m
processors, distribute the tasks i across the m processors as evenly
as possible, i.e., in such a way that there is no other distribution
which would lead to a smaller discrepancy from a perfectly even

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SEMANTiCS 15, September 15-17, 2015, Vienna, Austria
c 2015 ACM. ISBN 978-1-4503-3462-4/15/09 ...$15.00.
DOI: http://dx.doi.org/10.1145/2814864.2814872

distribution of tasks. Consider for example 3 tasks 1, 2 respectively 3 with computation complexities 3, 4 resp. 6. An optimal
distribution of these tasks amongst two processors would consist
of assigning 1 and 2 to the one of the processor (total costs: 7)
and task 3 to the other processor (total costs: 6). No other task
distribution leads to a more balanced load of tasks.

In this paper, we address the research gap of load balancing for
link discovering by first introducing the link discovery as well as
the load balancing problems formally. We then introduce a set of
heuristics for addressing this problem, including a novel heuristic
dubbed deterministic particle swarm optimization (DPSO). This
novel heuristic employs the basic insights behind particle swarm
optimization (PSO) to determine a load balancing for link discovery tasks in a deterministic manner. Our approach is generic and
can be combined with any link discovery approach that can divide
the LD problem into a set of tasks within only portions of the input
datasets are compared, including methods based on blocking (e.g.,
Multiblock [8]) and on space tiling (e.g., [17]). We evaluate our
approach on both synthetic and real data.

2. PRELIMINARIES
The formal specification of LD adopted herein is akin to that proposed in [16]: Given two sets S respectively T of source respectively target resources as well as a relation R, our goal is to find
the set M  S  T of pairs (s, t)  S  T such that R(s, t). If R is
owl:sameAs, then we are faced with a deduplication task. Given
that the explicit computation of M is usually a demanding endeavor,
M is usually approximated by a set  M = {(s, t, (s, t))  S T  R+ :
(s, t)  }, where  is a distance function and   0 is a distance
threshold. For example, geo-spatial resources s and t are described
by using single points or (ordered) sets of points, which we regard
as polygons. Most algorithms for LD achieve scalability by first
dividing S respectively T into non-empty subsets S 1 . . . S k resp.

T1 . . . Tl such that

Si=1

S i = S and

Sj=1

T j = T . Note that the differ-

ent subsets of S respectively T can overlap. In a second step, most
time-efficient algorithms determine pairs of subsets (S i, T j) whose
elements are to be compared. All elements (s, t) of all Cartesian
products S i  T j are finally compared by means of the measure 
and only those with (s, t)   are written into M.

The idea behind load balancing for LD is to distribute the computation of  over the Cartesian products S i  T j over n processors.
We call running  through a Cartesian product S i  T j a task. The
set of all tasks assigned to a single processor is called a block. The
cost c() of the task  is given by c(S i  T j) = |S i|  |T j| while the
cost of a block B is the sum of the cost of all its elements, i.e.,


c(B) = PtB
NP-hard. Hence, we refrain from trying to find a perfect solution in
this paper. Rather, we aim to provide a heuristic that (1) achieves
a good assignment of tasks to processors while (2) remaining computationally cheap. We measure the quality of an assignment by
measuring the mean squared error (MSE) to a potentially existing
perfect solution. Let B1, ..., Bm be the blocks assigned to our m pro-
cessors. Then, the MSE is given by

Algorithm 1: Naive Load Balancer
input : T  {1, ..., n} : set of tasks of size n

m : number of processors

output: B  {B1, ..., Bm} : a partition of T to a list of m blocks of

tasks

1 i  1;
2 foreach task  in T do

addTaskToBlock(, Bi);
i  (i mod m) + 1;

c(Bi) 

i=1


j=1

c(B j)


(1)

5 return B;

It is obvious that there might not be a solution with an MSE of 0.
For example, the best possible MSE when distributing the 3 tasks
1, 2 respectively 3 with computation complexities 3, 4 respectively 6 over 2 processors is 0.5.

3. LOAD BALANCING ALGORITHMS
The main idea behind load balancing techniques is to utilize parallel processing to distribute the tasks necessary to generate the solution to a problem across several processing units. Throughput max-
imization, response time minimization and resources overloading
avoidance are the main purposes of any load balancing technique.
We devised, implemented and evaluated five different load balancing approaches for linking geo-spatial datasets.

In each of the following algorithms, as input, we assume having a
set T of n tasks and a set of m processors. Through each of the al-
gorithms, we try to achieve load balancing among the m processors
by creating a list of balanced task blocks B = {B1, ..., Bm} with size
m, where each processor pi will be assigned its respective block Bi.

In order to ease the explanation of the following load balancing
algorithms, we introduce a simple running example where we assume having a set of four tasks {7, 1, 8, 3} where the superscript
of the task stands for its computational cost. Moreover, we assume
having two processing units p1 and p2. The goal of our running
example is to find two balanced tasks blocks B1 and B2 to be assigned to p1 respectively p2. In the following, we present different
approaches for load balancing.

3.1 Naive Load Balancer
The idea behind the naive load balancer is to divide all tasks between all processors based on their index and regardless of their
complexity. Each task with the index i is assigned to the processor
with index ((i + 1) mod m) + 1. Hence, each of the m procesmm tasks. Algorithm 1 shows the pseudosors is assigned at most l n
code of our implementation of a naive load balancing approach in
which tasks are assigned to processors in the order of the input set.
Applying the naive load balancer to our running example we get
B1 = {7, 8}, B2 = {1, 3} and MSE = 30.25.

3.2 Greedy Load Balancer
The main idea behind the greedy load balancing [5] technique is
to sort the input tasks in descending order based on their com-
plexity. Then, starting from the most complex task, the greedy
load balancer assigns tasks to processors in order. This approach
is basically a heuristic that aims at achieving an even distribution
of the total task complexity over all processors. The pseudo code
of the greedy load balancer technique in presented in algorithm 2.
Back to our running example, the greedy load balancer first sorts
the example tasks (line 2) to be {8, 7, 3, 1}. Then, in order, the

Algorithm 2: Greedy Load Balancer
input : T  {1, ..., n} : set of tasks of size n

m : number of processors

output: B  {B1, ..., Bm} : a partition of T to a list of m blocks of

balanced tasks

1 T  descendingSortTasksByComplexity(T );
2 i  1;
3 foreach task  in T do

addTaskToBlock(, Bi);
i  (i mod m) + 1;

6 return B;

tasks are assigned to the task blocks (line 4) to have B1 = {8, 3},
B2 = {7, 1} with MSE = 2.25.

3.3 Pair-Based Load Balancer
The pair-based load balancing [14] is reminiscent of a two-way
breadth-first-search. The approach assigns processors tasks in pairs
of the form (most complex, least complex). In order to get most homogeneous pairs, the algorithm first sorts all input tasks according
to tasks complexities. Afterwards, from the sorted list of tasks, the
pair based algorithm generates the  n
 pairs of tasks where the pair

i is computed by selecting ith and (n  i + 1)th tasks from the sorted
list of tasks. The pseudo-code of the pair based technique is shown
in Algorithm 3.

The pair-based load balancer starts dealing with our running example tasks by sorting them to be {1, 3, 7, 8} (line 1). After-
wards, the algorithm generates tasks pairs as (first, forth) and (sec-
ond, third) to have B1 = {1, 8}, B2 = {3, 7} with MSE = 0.25.

Algorithm 3: Pair Based Load Balancer
input : T  {1, ..., n} : set of tasks of size n

m : number of processors

output: B  {B1, ..., Bm} : a partition of T to a list of m blocks of

balanced tasks

1 T  sortTasksByComplexity(T );
2 i  1;
3 for i  n/2 do

addTaskToBlock(i, Bi);
addTaskToBlock(ni+1, Bi);
i  i + 1;

7 return B;


The particle swarm optimization (PSO) [12][10][11] is a populationbased stochastic algorithm. PSO is based on social psychological
principles. Unlike evolutionary algorithms, in a typical PSO, there
is no selection of individuals, all population members (dubbed as
particles) survive from the beginning to the end of a algorithm. At
the beginning of PSO, particles are randomly initialized in the problem solution space. Over successive iterations, particles cooperatively interact to improve of the fitness of the optimization problem
solutions. PSO is normally used for continuous problems but that
it has been extended to deal with discrete problems [22][4] such as
the one at hand.

In order to model our problem in terms of the PSO technique, we
consider the input tasks T as the particles1 to be optimized. The
aim here is to to balance the size of the blocks (i.e., the total complexity of tasks included in each block) as well as possible. To
adapt the idea of the PSO to load balancing, we define the fitness
function as the task complexity difference between the most overloaded task block and least underloaded task block. Formally, The
PSO fitness function is defined as

F = c(B+)  c(B),

(2)

where B+ = arg max
BB

c(B) and B = arg min
BB

c(B) are the most and

least loaded blocks respectively, and B is the list of all task blocks.

Initially, the PSO based load balancing approach starts like the
naive approach (see Algorithm 1). All particles are distributed
equally into the task blocks regardless of tasks complexities, i.e.,
each block now contains at most  n
 particles. We dubbed the task

block list as Best Known Positions (BKP). Afterwards, PSO computes the fitness function to the initial BKP and saves it as Best
Known Fitness (BKF). Until a termination criterion is met, in each
iteration, PSO performs the particles migration process. This process consists of first assigning a random velocity v to each particle
p included in a block Bi, where v  N and 0  v  m. If v , i,
p is moved to the new block Bv, otherwise p stays in its block Bi.
After moving all the particles, the PSO computes the new fitness F.
If the new fitness F is less than BKF, PSO updates both BKF and
BKP.

Note that the termination criteria can be defined independently of
the core PSO algorithm. Here, we implemented two termination
criteria: (1) minimum fitness threshold and (2) maximum number
of iterations. If the minimum fitness threshold is reached in any
iteration the algorithm terminates instantly and the BKP is returned.
Otherwise, the BKF is returned after reaching the maximal number
of iterations. The pseudo-code of the PSO load balancing technique
in presented in algorithm 4.

Back to our running example, assume we set the maximal number
of iterations to 1 (I = 1). First, the PSO initializes B1 = {7, 8},
B2 = {1, 3} (lines 35) and the best known Fitness F = 11
(lines 9). Then, the PSO clones B1 and B2 to B
1 respectively B

(line 12). Assume that PSO generates random velocity v = 1 for
1. For 8, assume
7 (line 16). Then, 7 stays in its current block B
v = 2 (which is different than 8s block B
1), then 8 migrates to B

(line 18). For 1 and 3 assume v = 1 which make both 1 and 3
2 = {1, 3, 8} with
stays in B
the new fitness function F  = 5 (line 21) and as F  < F then B and

2. Consequently, we have B

1 = {7}, B

Algorithm 4: Particle Swarm Optimization Load Balancer
input : T  {1, ..., n} : set of tasks of size n

m : number of processors
F : fitness function threshold (zero by default)
I : number of iterations

output: B  {B1, ..., Bm} : the Best Known Particles positions as

a list of m blocks of balanced tasks

1 Initialize Particles Best known Position B
2 i  1;
3 foreach task  in T do

addTaskToBlock(, Bi);
i  (i mod m) + 1;

6 Initialize best known Fitness F;
7 B+  getMostOverloadedBlock(B);
8 B  getLeastUnderloadedBlock(B);
9 F  c(B+)  c(B);
10 i  1;
11 while i < I do
B  B;
Move each task  (particle) to new position based on a
random particle velocity v
foreach block B  B do

foreach particle   B do

, B then

v  generateRandomVelocity(0, m);
if B

migrateParticleToBlock(, B
v);
If better fitness achieved update result
B+  getMostOverloadedBlock(B);
B  getLeastUnderloadedBlock(B);
F   c(B+)  c(B);
if F  < F then

F  F ;
B  B;

if F == F then

return B;

i  i + 1;

28 return B;

F are updated by B respectively F  (lines 2224). The PSO terminates as it is reached the maximum number of iterations (line 11)
and returns B1 = {7}, B2 = {1, 3, 8} with MSE = 6.25.

3.5 DPSO Load Balancer
The PSO load balancer (see section 3.4) has a main drawback of
being an indeterminism approach. This drawback is inherited from
the fact that the PSO is a heuristic algorithm that depends up on a
random selection of velocity for moving particles. In order to overcome this drawback, we propose the Deterministic PSO (DPSO).

The DPSO starts in the same way as the PSO by partitioning all the
n tasks to m task blocks, where m equals the number of processors.
In this stage, each block contains at most n/m tasks regardless of
tasks complexities. Until a termination criterion is met, in each
iteration the DPSO:

1In the rest of the paper we will use the terms tasks and particles
interchangeably.

1. Finds the most overloaded block B+ = arg max
BB

c(B) and the


BB

c(B), where B is the

list of all task blocks.

Algorithm 5: DPSO Load Balancer
input : T  {0, ..., n} : set of tasks of size n

2. Sort tasks within B+ based in their complexities.

3. As far as a better balancing between B+ to B is met, DPSO
perform task migration, where DPSO moves task per task in
order from B+ to B.

4. Compute fitness function as c(B+)  c(B).

Here, We implement two termination criteria akin with the ones defined previously for PSO: (1) minimum fitness threshold (F) and (2)
maximum number of iterations (I). The pseudo code of the DPSO
load balancing algorithm in presented in algorithm algorithm 5.
Note that the termination criteria can be defined independently of
the core DPSO algorithm. For instance, fitness function convergence could be considered as the termination criterion.

The deterministic nature of DPSO comes from the fact that (1)
DPSO only moves tasks from most overloaded block B+ to the
lest underloaded block B, i.e. no random particles migration as
in PSO, (2) DPSO sorts B+ tasks before it starts the task migration
process. Sorting insures migration of smaller tasks first in which
away an optimal load balancing between most and least loaded
blocks is achieved in each iteration.

Assume we apply the DPSO for our running example for one iteration (I = 1). First, the DPSO initializes B1 = {7, 8}, B2 = {1, 3}
(lines 25). Then, DPSO sorts tasks within the most overloaded
block B+ which is B1 (line 8) to be {7, 8} (line 11). Consequently,
the DPSO migrates 7 from B+ = B1 to B = B1 (line 13). Finally,
as the DPSO finds that c(B+) < c(B) it breaks (line 14) and returns
the result B1 = {8}, B2 = {1, 3, 7} with MSE = 2.25.

4. EVALUATION
The aim of our evaluation was to quantify how well DPSO outperforms traditional load balancing approaches (i.e. naive, greedy and
pair-based). To this end, we measured the runtime for each of the
five load balancing algorithms for both synthetic and real data. In
the following, we begin by presenting the algorithm and data that
we used. Thereafter, we present our results on different datasets.

4.1 Experimental Setup
For our experiments, the parallel task generation was based on the
Orchid approach [17]. The idea behind Orchid is to improve the
runtime of algorithms for measuring geo-spatial distances by adapting an approach akin to divide-and-conquer. Orchid assumes that
it is given a distance measure . Thus all pairs in the mapping M
that it returns must abide by (s, t)  . Overall, Orchid begins by
partitioning the surface of the planet. Then, the approach defines
a task as comparing the points in a given partition with only the
points in partitions that abide by the distance threshold  underlying the computation. A task is the comparison of all points in two
partitions.

m : number of processors
F : fitness function threshold (zero by default)
I : number of iterations

output: B  {B0, ..., Bm} : the Best Known Particles positions as

a list of m blocks of balanced tasks

1 Initialize Particles Best known Position B
2 i  1;
3 foreach task  in T do

addTaskToBlock(, Bi);
i  (i mod m) + 1;

6 i  1;
7 while i < I do

B+  getMostOverloadedBlock(B);
B  getLeastUnderloadedBlock(B);
Balance B+ and B+ by migrating particles (tasks) from sorted
B+ to B
B+  sortTasksByComplexity(B+);
foreach particle   B+ do

migrateParticleToBlock(, B);
if c(B+) < c(B) then

break;

Compute fitness function F as the complexity difference
between most and least loaded tasks
F  c(B+)  c(B);
if F == F then

return B;

i  i + 1;

21 return B;

the synthetic dataset polygons sizes from one to ten points. The
variation of sizes of polygons was based on a Gaussian random dis-
tribution. Also, the (latitude, longitude) coordinates of each point
are generated akin with the Gaussian distribution.

We used three publicly available datasets for our experiments as
real datasets. The first dataset is the Nuts3. We chose this dataset
because it contains fine-granular descriptions of 1,461 geo-spatial
resources located in Europe. For example, Norway is described
by 1981 points. The second dataset, DBpedia4, contains all the
731,922 entries from DBpedia that possess geometry entries. We
chose DBpedia because it is commonly used in the Semantic Web
community. Finally, the third dataset was LinkedGeoData, contains
all 3,836,119 geo-spatial objects from http://linkgeodata.org
that are instances of the class Way.5. Further details to the datasets
can be found in [17].

All experiments were carried out on a 64-core server running OpenJDK 64-Bit Server 1.6.0_27 on Ubuntu 12.04.2 LTS. The processors were 8 quad-core Intel(R) Core(TM) i7-3770 CPU @ 3.40 GHz
with 8192 KB cache. Unless stated otherwise, each experiment was

We performed controlled experiments on five synthetic geographic
datasets2 and three real datasets. The synthetic datasets were created by randomly generating a number of polygons ranging between 1 and 5 million polygons in steps of 1 million. We varied

2All synthetic dataset are available at https://github.com/
AKSW/LIMES/tree/master/evaluationsResults/lb4ld

3Version 0.91 available at http://nuts.geovocab.org/data/
is used in this work
4We used version 3.8 as available at http://dbpedia.org/
Datasets.
5We used the RelevantWays dataset (version of April 26th, 2011)
of LinkedGeoData as available at http://linkedgeodata.org/
Datasets.


PSO approach we ran it 5 times in each experiment and provide
the mean of the five runs results. The approaches presented herein
were implemented in the LIMES framework.6 All results are available at the project web site.7

4.2 Orchid vs. Parallel Orchid
We began by evaluating the speedup gained by using parallel implementations of Orchid algorithm. To this end, we first ran experiments on the three real datasets (Nuts, DBpedia and Linked-
GeoData). First, we computed the runtime of the normal (i.e., non-
parallel) implementation of Orchid [17]. Then, we evaluated the
parallel implementations of Orchid using the aforementioned five
load balancing approaches. To evaluate the speedup gained from
increasing the number of parallel processing units, we reran each
of the parallel experiments with 2, 4 and 8 threads. Figure 1 shows
the runtime results along with the mean squared error (MSE) results
of the experiments.

Our results show that the parallel Orchid implementations using
both PSO and DPSO outperform the normal Orchid on the three
real datasets. Particularity, when dealing with small dataset like
NUTS (see Figure 1 (a)), PSO and DPSO achieve up to three times
faster than the non-parallel version of Orchid. When dealing with
larger dataset such as LinkedGeoData (see Figure 1 (e)), PSO and
DPSO are capable of achieving up to ten times faster than the nonparallel version of Orchid. This fact shows that our load balancing
heuristics deployed in PSO and DPSO are capable of achieving
superlinear performance [1, 2] when ran on two processors. This
is simply due to the processor cache being significantly faster than
RAM and thus allowing faster access to data and therewith also
smaller runtimes. On the other side, greedy and pair-based load
balancing fail to achieve even the run time of the normal Orchid.
This fact is due to the significant amount of time required by greedy
and pair-based load balancing algorithms for sorting tasks prior to
assigning them to processors.

4.3 Parallel Load balancing Algorithms Eval-

uation

We performed this set of experiments with two goals in mind: First,
we wanted to measure the run time taken by each algorithm when
applied to different datasets. Our second aim was to qualify the
quality of the data distribution carried out by each of the implemented algorithm using MSE. To this end, we ran two sets of exper-
iments. In the first set of experiments, we used the aforementioned
three datasets of Nuts, DBpedia and LinkedGeoData. The result
of this set of experiments are presented in Figure 1. In the second set of experiments, we ran our five load balancing algorithms
against a set of five synthetic randomly generated datasets (see subsection 4.1 for details). The results are presented in Figure 2.

Our results suggest that DPSO and PSO outperform the naive approach in most cases. This can be seen most clearly in Figure 2
(note the log scale). DPSO is to be preferred over PSO as it is deterministic and is thus the default implementation of load balancing
currently implemented in LIMES. Still, the improvements suggest
that preserving the integrity of the hypercubes generated by Orchid still leads to a high difference in load across the processors as
shown by our MSE results. An interesting research avenue would

6http://limes.sf.net
7https://github.com/AKSW/LIMES/tree/master/
evaluationsResults/lb4ld

thus be to study approaches which do not preserve this integrity
while guaranteeing result completeness. This will be the core of
our future work.

5. RELATED WORK
Load balancing techniques have been applied in a large number of
disciplines that deal with big data. For handling massive graphs
such as the ones generated by social networks, [21] introduces
two message reduction techniques for distributed graph computation load balancing. For dealing with federated queries, [3] proposes an RDF query routing index that permits a better load balancing for distributed query processing.
[14] proposes two approaches for load balancing for the complex problem of entity resolution (ER), which utilize a preprocessing MapReduce job to analyze the data distribution.
[13] demonstrates a tool called Dedoop for MapReduce-based ER of large datasets that implements
similar load balancing techniques. A comparative study for different load balancing algorithms for MapReduce environment is
presented in [7].

Finding an optimal load balancing is known to be NP-complete.
Thus, [15] provides two heuristics for distributed grid load balanc-
ing, one is based on ant-colony optimization and the other is based
on particle-swarm optimization. Yet, another heuristic (dubbed as
BPSO) is proposed by [9]. BPSO is a modified binary particleswarm optimization for network reconfiguration load balancing.
[19] proposes an artificial bee-colony-based load balancing algorithm for cloud computing.

The study [1] introduces superlinear performance analyses of realtime parallel computation. The study shows that parallel computers
with n processors can solve a computational problem more than n
times faster than a sequential one.
In another work [2], the superlinear performance concluded to be also possible for parallel
evolutionary algorithms both theoretically and in practice.

6. CONCLUSION AND FUTURE WORK
In this paper, we presented and evaluated load balancing techniques
for link discovery. In particular, in the PSO approach, we applied
particle-swarm optimization to optimize the distribution of tasks of
different sizes over a given number of processors. While the PSO
outperforms classical load balancing algorithms, it has the main
drawback of being indeterministic in nature. Therefore, we proposed the DPSO, where we altered the task selection of PSO for
ensuring deterministic load balancing of tasks. We combined the
aforementioned approaches with the Orchid algorithm. All the implemented load balancing approaches are evaluated on real and artificial datasets. Our evaluation suggests that while naive approaches
can be super-linear on small data sets, our deterministic particle
swarm optimization outperforms both naive and classical load balancing approach such as greedy load balancing on large datasets as
well as a datasets which originate from highly skewed distributions.

Although we achieve reasonable results in terms of scalability, we
plan to further improve the time efficiency of our approaches by
enabling the splitting of one task over more than one processor.
As an extension of DPSO, we plan to implement a caching tech-
nique, which enables DPSO to be used on larger datasets that can
not be fitted in memory [6]. While DPSO was evaluated in combination with Orchid in this paper, we will study the combination of
our approach with other space tiling and/or blocking algorithms for
generating parallel tasks.


Number of threads

(a) Nuts runtime


Naive
Greedy
PairBased


Orchid

Naive
Greedy
PairBased


Orchid

Naive
Greedy
PairBased


Number of threads

(b) Nuts MSE

Naive
Greedy
PairBased


Number of threads

(c) DBpedia runtime

Number of threads

(d) DBpedia MSE

Naive
Greedy
PairBased


Orchid


Naive
Greedy
PairBased


Number of threads

(f) LinkedGeoData MSE

Number of threads

(e) LinkedGeoData runtime

Figure 1: Runtime and MSE generated when applying Orchid [17] vs. parallel implementations of Orchid using naive, greedy, pair
based, PSO and DPSO load balancing algorithms against the three real datasets of Nuts, DBpedia and LinkedGeoData using 2, 4 and
8 threads


Naive
Greedy
PairBased


Naive
Greedy
PairBased


 1M

 2M

 3M

 4M

 5M

 1M

 2M

 3M

 4M

 5M

Number of polygons

(a) 2 threads runtime

Number of polygons

(b) 2 threads MSE


 1M


 1M

Naive
Greedy
PairBased


Naive
Greedy
PairBased


 2M

 3M

 4M

 5M

 1M

 2M

 3M

 4M

 5M

Number of polygons

(c) 4 threads runtime

Number of polygons

(d) 4 threads MSE

Naive
Greedy
PairBased


Naive
Greedy
PairBased


 2M

 3M

 4M

 5M

 1M

 2M

 3M

 4M

 5M

Number of polygons

(e) 8 threads runtime

Number of polygons

(f) 8 threads MSE

Figure 2: Runtime and MSE generated when applying parallel implementations of Orchid using naive, greedy, pair based, PSO and
DPSO load balancing algorithms against the five synthetic datasets of sizes 1, 2, 3, 4 and 5 million polygons using 2, 4 and 8 threads


Parts of this work were supported by the GeoKnow project (GA
318159) and the BMWi project SAKE.

[11] James Kennedy. Particle swarm optimization. In

Encyclopedia of Machine Learning, pages 760766.
Springer, 2010.
