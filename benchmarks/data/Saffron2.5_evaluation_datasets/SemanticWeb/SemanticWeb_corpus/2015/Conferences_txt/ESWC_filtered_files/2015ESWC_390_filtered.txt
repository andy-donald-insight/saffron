Automating RDF Dataset Transformation

and Enrichment

Mohamed Ahmed Sherif(B), Axel-Cyrille Ngonga Ngomo, and Jens Lehmann

Department of Computer Science, University of Leipzig, 04109 Leipzig, Germany

{sherif,ngonga,lehmann}@informatik.uni-leipzig.de

Abstract. With the adoption of RDF across several domains, come
growing requirements pertaining to the completeness and quality of RDF
datasets. Currently, this problem is most commonly addressed by manually devising means of enriching an input dataset. The few tools that
aim at supporting this endeavour usually focus on supporting the manual
definition of enrichment pipelines. In this paper, we present a supervised
learning approach based on a refinement operator for enriching RDF
datasets. We show how we can use exemplary descriptions of enriched
resources to generate accurate enrichment pipelines. We evaluate our
approach against eight manually defined enrichment pipelines and show
that our approach can learn accurate pipelines even when provided with
a small number of training examples.

1 Introduction

Over the last years, the Linked Data principles have been used across academia
and industry to publish and consume linked data [16]. With this adoption of
Linked data come novel challenges pertaining to the integration of these datasets
for dedicated applications such as tourism, question answering, enhanced reality
and many more. Providing consolidated and integrated datasets for these applications demands the specification of data enrichment pipelines, which describe
how data from different sources is to be integrated and altered so as to abide
by the precepts of the application developer or data user. Currently, most developers implement customized pipelines by compiling sequences of tools manually
and connecting them via customized scripts. While this approach most commonly leads to the expected results, it is time-demanding and resource-intensive.
Moreover, the results of this effort can most commonly only be reused for new
versions of the input data but cannot be ported easily to other datasets. Over
the last years, a few frameworks for RDF data enrichment such as LDIF1 and
DEER2 have been developed. The frameworks provide enrichment methods such
as entity recognition [22], link discovery [15] and schema enrichment [4]. However,
devising appropriate configurations for these tools can prove a difficult endeav-
our, as the tools require (1) choosing the right sequence of enrichment functions

1 http://ldif.wbsg.de/.
2 http://aksw.org/Projects/DEER.html.
c Springer International Publishing Switzerland 2015
F. Gandon et al. (Eds.): ESWC 2015, LNCS 9088, pp. 371387, 2015.
DOI: 10.1007/978-3-319-18818-8 23

M.A. Sherif et al.

and (2) configuring these functions adequately. Both the first and second task
can be tedious.

In this paper, we address this problem by presenting a supervised machine
learning approach for the automatic detection of enrichment pipelines based on a
refinement operator and self-configuration algorithms for enrichment functions.
Our approach takes pairs of concise bounded descriptions (CBDs) of resources
{(k1, k
i is the enriched version of ki. Based on
these pairs, our approach can learn sequences of atomic enrichment functions
that aim to generate each k
i out of the corresponding ki. The output of our approach is an enrichment pipeline that can be used on whole datasets to generate
enriched versions.

n)} as input, where k

1) . . . (kn, k

Overall, we provide the following core contributions: (1) We define a supervised machine learning algorithm for learning dataset enrichment pipelines based
on a refinement operator. (2) We provide self-configuration algorithms for five
atomic enrichment steps. (3) We evaluate our approach on eight manually defined
enrichment pipelines on real datasets.

2 Preliminaries
Enrichment: Let K be the set of all RDF knowledge bases. Let K  K be
a finite RDF knowledge base. K can be regarded as a set of triples (s, p, o) 
(R  B)  P  (R  L  B), where R is the set of all resources, B is the set of
all blank nodes, P the set of all predicates and L the set of all literals. Given
a knowledge base K, the idea behind knowledge base enrichment is to find an
enrichment pipeline M : K  K that maps K to an enriched knowledge base K
with K = M(K). We define M as an ordered list of atomic enrichment functions
m  M, where M is the set of all atomic enrichment functions. 2M is used to
denote the power set of M, i.e. the set of all enrichment pipelines. The order
of elements in M determines the execution order, e.g. for an M = (m1, m2, m3)
this means that m1 will be executed first, then m2, finally m3. Formally,
?

?

?
if K = K,

(m1, . . . , mn), where mi  M, 1  i  n otherwise,

M =

(1)

where  is the empty sequence. Moreover, we denote the number of elements of
M with |M|. Considering that a knowledge base is simply a set of triples, the
task of any atomic enrichment function is to (1) determine a set of triples +
to be added the source knowledge base and/or (2) determine a set of triples 
to be deleted from the source knowledge base. Any other enrichment process can
be defined in terms of + and , e.g. altering triples can be represented as
combination of addition and deletion.
In this article we cover two problems: (1) how to create self-configurable
atomic enrichment functions m  M capable of enriching a dataset and (2) how
to automatically generate an enrichment pipeline M. As a running example, we
use the portion of DrugBank shown in Fig. 1. The goal of the enrichment here is
?

?

?
Fig. 1. RDF graph of the running example. Ellipses are RDF resources, literals are
rectangular nodes. Gray nodes stand for resources in the input knowledge base while
nodes with a white background are part of an external knowledge base.

to gather information about companies related to drugs for a market study. To
this end, the owl:sameAs links to DBpedia (prefix db) need to be dereferenced.
Their rdfs:comment then needs to be processed using an entity spotter that will
help retrieve resources such as the Boots Company. Then, these resources need
to be attached directly to the resources in the source knowledge base, e.g., by
using the :relatedCompany property. Finally, all subjects need to be conformed
under one subject authority (prefix ex).
Refinement Operators: Below, we give definitions of refinement operators and
their properties. Refinement operators have traditionally been used, e.g. in [11],
to traverse search spaces in structured machine learning problems. Their theoretical properties give an indication of how suitable they are within a learning
algorithm in terms of accuracy and efficiency.

Definition 1 (Refinement Operator and Properties). Given a quasiordered space (S, ) an upward refinement operator r is a mapping from S to
2S such that s  S : s  r(s)  s  s. s is then called a generalization of s. A pipeline M2  M belongs to the refinement chain of M1  M
iff i  N : M2  ri(M1), where r0(M) = M and ri(M) = r(ri1(M)).
A refinement operator r over the quasi-ordered space (S, ) can abide by the
following criteria. r is finite iff r(s) is finite for all s  S. r is proper if
s  S, s  r(s)  s = s. r is complete if for all s and s, s  s implies
that there is a refinement chain between s and s. A refinement operator r over
the space (S, ) is redundant if two different refinement chains can exist between
s  S and s  S.

3 Knowledge Base Enrichment Refinement Operator
Our refinement operator expects the set of atomic enrichment functions M as
input and returns an enrichment pipeline M as output. Each positive example
e  E is a pair of CBDs (k, k), with k  K and k  K, the K stands

M.A. Sherif et al.

Fig. 2. Ibuprofen concise bound description before and after enrichment

for the enriched version of K. Note that we model CBDs as sets of RDF triples.
Moreover, we denote the resource with the CBD k as resource(k). For our running
example, the set E could contain the pair shown in Fig. 2a as k and in Fig. 2b
as k.
The set of all first elements of the pairs contained in E is denoted source(E).
The set of all second elements is denoted target(E). To compute the refinement
pipeline M, we employ an upward refinement operator (which we dub ) over
the space 2M of all enrichment pipelines. We write M  M when M is a
subsequence of M, i.e., m
i is the ith
element of M resp. M.
Proposition 1 (Induced Quasi-Ordering).  induces a quasi-ordering over the
set 2M.
Proof. The reflexivity of  follows from each M being a subsequence of itself.
The transitivity of  follows from the transitivity of the subsequence relation.
Note that  is also antisymmetric.

We define our refinement operator over the space (2M,) as follows:

i = mi, where mi resp. m

i  M  m

(M) =

M ++ m

( ++ is the list append operator)

(2)
?

?

?
mM

We define precision P (M) and recall R(M) achieved by an enrichment pipeline

on E as
?

?

?
P (M) =
?

?

?
ksource(E)
?

?

?
M(k)
?

?

?
ksource(E)

M(k)

ktarget(E)
?

?

?
k
?

?

?
, R(M) =
?

?

?
ksource(E)
?

?

?
M(k)
?

?

?
ktarget(E)
?

?

?
ktarget(E)

k
?

?

?
The F-measure F (M) is then

F (M) =

2P (M)R(M)
P (M) + R(M)

.

k
?

?

?
.

(3)

(4)

Using Fig. 2a from our running example as source and Fig. 2b as target with
the CBD of :Iboprufen being the only positive example, an empty enrichment
4 and an F-measure of
pipeline M =  would have a precision of 1, a recall of 3
7. Having defined our refinement operator, we now show that  is finite, proper,

complete and not redundant.
?

?

?
Proposition 2.  is finite.
Proof. This is a direct consequence of M being finite.
Proposition 3.  is proper.
Proof. As the quasi order is defined over subsequences, i.e. the space (2M,),
and we have |M| = |M| + 1 for any M  (M),  is trivially proper.

Proposition 4.  is complete.
Proof. Let M resp. M be an enrichment pipeline of length n resp. n with

M  M. Moreover, let m
(M). Hence, by applying  n  n times, we can generate M from M. We can

thus conclude that  is complete.

i be the ith element of M. Per definition, M ++ m
?

?

?
n+1

Proposition 5.  is not redundant.

Proof.  being redundant would mean that there are two refinement chains that
lead to a single refinement pipeline M. As our operator is equivalent to the list
append operation, it would be equivalent to stating that two different append
sequences can lead to the same sequence. This is obviously not the case as
each element of the list M is unique, leading to exactly one sequence that can

generate M.

4 Learning Algorithm

The learning algorithm is inspired by refinement-based approaches from inductive logic programming. In these algorithms, a search tree is iteratively built up
using heuristic search via a fitness function. We formally define a node N in a
search tree to be a triple (M, f, s), where M is the enrichment pipeline, f  [0, 1]
is the F-measure of M (see Eq. 4), and s  {normal, dead} is the status of the
node. Given a search tree, the heuristic selects the fittest node in it, where fitness
is based on both F-measure and complexity as defined below.

4.1 Approach

For the automatic generation of enrichment pipeline specifications, we created
a learning algorithm based on the previously defined refinement operator. Once
provided with training examples, the approach is fully automatic. The pseudocode of our algorithm is presented in Algorithm 4.1.
Our learning algorithm has two inputs: a set of positive examples E and a
set of atomic enrichment operators M. E contains pairs of (k, k) where each
k contains a CBD of one resource from an arbitrary source knowledge base
K and k contains the CBD of the same resource after applying some manual
enrichment. Given E, the goal of our algorithm is to learn an enrichment pipeline
M that maximizes F (M) (see Eq. 4).

M.A. Sherif et al.

As shown in Algorithm 4.1, our approach starts by generating an empty
refinement tree  which contains only an empty root node. Using E, the algorithm then accumulates all the original CBDs in k (Source(E)). Using the
same procedure, k is accumulated from E as the knowledge base containing
the enriched version of k (Target(E)). Until a termination criterion holds
(see Sect. 4.3), the algorithm keeps expanding the most promising node (see
Sect. 4.2). Finally, the algorithm ends by returning the best pipeline found in :
(GetPipeline(GetMaxQualityNode())).

Having a most promising node t at hand, the algorithm first applies our refinement operator (see Eq. 2) against the most promising enrichment pipeline Mold
included in t to generate a set of atomic enrichment functions M  (Mold).
Consequently, using both kold (as the knowledge base generated by applying Mold
against k) and k, the algorithm applies the self configuration process of the current atomic enrichment function m  SelfConfig(m, kold, k) to generate a
set of parameters P (a detailed description for this process is found in Sect. 5).
Afterwards, the algorithm runs m against kold to generate the new enriched
knowledge base knew  m(kold, P ). A dead node N  CreateNode(M, 0,
dead) is created in two cases: (1) m is inapplicable to kold (i.e., P == null) or
(2) m does no enrichment at all (i.e., knew is isomorphic3 to kold). Otherwise, the
algorithm computes the F-measure f of the generated dataset knew. M along
with f are then used to generate a new search tree node N  CreateNode(M,
f, normal)). Finally, N is added as a child of t (AddChild(t, N)).

4.2 Most Promising Node Selection
Here we describe the process of selecting the most promising node t   as
in GetMostPromisingNode() subroutine in Algorithm 4.1. First, we define
node complexity as linear combination of the nodes children count and level.
Formally,
Definition 2. (Node Complexity). The complexity of a node N = (M, f, s)
in a refinement tree  is a function c : N    [0, 1], where c(N, ) =  |Nd|
|| +
, |Nd| is number of all Ns descendant nodes, || is the total number of nodes
 Nl
d
in , Nl is Ns level, d is s depth,  is the children penalty weight,  is the
level penalty weight and  +  = 1. Seeking for simplicity, we will use the c(N)
instead of c(N, ) in the rest of this paper.
We can then define the fitness f(N) of a normal node N as the difference between
its enrichment pipeline F-measure (Eq. 4) and weighted complexity. f(N) is zero
for dead nodes. Formally,
Definition 3. (Node Fitness). Let N = (M, f, s) be a node in a refinement
tree , Ns fitness is the function

f(N) =
?

?

?
F (M)    c(N)

if s = dead,
if s = normal.

(5)

3 http://www.w3.org/TR/rdf11-concepts/.
?

?

?
where M is the enrichment pipeline contained in the node N,  is the complexity
weight and 0    1.
Note, that we use the complexity of pipelines as second criterion, which makes
the algorithm (1) more flexible in searching less explored areas of the search
space, and (2) leads to simpler specification being preferred over more complex
ones (Occams razor [3]). The parameter  can be used to control the trade-off
between a greedy search ( = 0) and search strategies closer to breadth first
search ( > 0). The fitness function can be defined independently of the core
learning algorithm.

Consequently, the most promising node is the node with the maximum fitness
through the whole refinement tree . Formally, the most promising node t is
f(N), where N is not a dead node. Note that if several
defined as t = arg max
nodes achieve a maximum fitness, the algorithm chooses the shortest node as it
aims to generate the simplest enrichment pipeline possible.
Algorithm 4.1. EnrichmentPipelineLearner(E +,M)

N

comment: initialize 
  CreateRootNode()
k  Source(E)
k  Target(E)
repeat







do

comment: Expand most promising node of 
t  GetMostPromisingNode()
Mold  GetPipeline(t)
M  (Mold)
comment: Create a child of t for each m  M
for each m  M


kold  Mold(k)
P  SelfConfig(m, kold, k)
knew  m(kold, P )
if P == null or knew == kold

then

else
?

?

?
N  CreateNode(M, 0, dead)

f  F(m)
N  CreateNode(M, f, normal)

AddChild(t, N)

until TerminationCriterionHolds()
return (GetPipeline(GetMaxQualityNode()))

4.3 Termination Criteria
The subroutine TerminationCriterionHolds() in Algorithm 4.1 can check
several termination criteria depending on configuration: (1) optimal enrichment

M.A. Sherif et al.

pipeline found (i.e., a fixpoint is reached), (2) maximum number of iterations
reached, (3) maximum number of refinement tree nodes reached, or a combination of the aforementioned criteria. Note that the termination criteria can be
defined independently of the core learning algorithm.

5 Self-Configuration

To learn an appropriate specification from the input positive examples, we need
to develop self-configuration approaches for each of our frameworks atomic
enrichment functions. The input for each of these self-configuration procedures is
the same set of positive examples E provided to our pipeline learning algorithm
(Algorithm 4.1). The goal of the self-configuration process of an enrichment function is to generate a set of parameters P = {(mp1, v1), . . . , (mpm, vm)} able to
reflect E as well as possible. In cases when insufficient data is contained in E to
carry out the self-configuration process, an empty list of parameters is returned
to indicate inapplicability of the enrichment function.

5.1 Dereferencing Enrichment Functions

The idea behind the self-configuration process of the enrichment by dereferencing
is to find the set of predicates Dp from the enriched CBDs that are missing
from source CBDs. Formally, for each CBD pair (k, k) construct a set Dp  P
as follows: Dp = {p : (s, p, o)  k}\{p : (s, p, o)  k}. The dereferencing
enrichment function will dereference the object of each triple of ki given that
this object is an external URI, i.e. all o in ki with (s, p, o)  ki, o  R and o is
not in the local namespace of the dataset will be dereferenced. Dereferencing an
object returns a set of triples. Those are filtered using the previously constructed
property set Dp, i.e. when dereferencing o the enrichment function only retains
triples with subject o and a predicate contained in Dp. The resulting set of triples
is added to the input dataset.
We illustrate the process using our running example: In the first step, we
compute the set Dp = {:relatedCompany, rdfs:comment} which consists of the
properties occurring in the target but not in the source CBD. In the second step,
we collect the set of resources to dereference, which only consists of the element
db:Ibuprofen. In the third step, we perform the actual dereferencing operation
and retain triples for which the subject is db:Ibuprofen and the predicate is
either :relatedCompany or rdfs:comment. In our example, no triples with predicate :relatedCompany exist, but we will find the desired triple (db:Ibuprofen,
rdfs:comment, "Ibuprofen ..."), which is then added to the input dataset.

5.2 Linking Enrichment Function
The aim of link discovery is as follows: Given two sets Rs  R of source resources
and Rt  R of target resources, we aim to discover links L  Rs  Rt such that
for any (s, t)  L we have (s, t)   where  is a similarity function and  a
?

?

?
threshold value. The goal of the linking enrichment function is to learn so called
link specifications including a similarity function  and a threshold . To this
aim, we rely on an unsupervised hierarchical search approach, which optimizes
a target function akin to F-measure. The search space of all link specifications is
split into a grid and the approach computes the objective function for all points in
the grid. Thereafter, the region surrounding the point which achieves the highest
score is selected as new search space. This approach is applied iteratively until
a stopping condition (e.g., a maximal number of iterations) is reached. More
details can be found at [18].

5.3 NLP Enrichment Function

The basic idea here is to enable the extraction of all possible named entity types.
If this leads to the retrieval of too many entities, the unwanted predicates and
resources can be discarded in a subsequent step. The self-configuration of the
NLP enrichment function is parameter-free and relies on FOX [17]. The application of the NLP self configuration to our running example generates all possible
entities included in the literal object of the rdfs:comment predicate. The result
is a set of related named entities all of them related to our ex:Iboprufen object
by the default predicate fox:relatedTo as shown Fig. 3a. In the following 2 sections we will see how our enrichment functions can refine some of the generated
triples and delete others.

5.4 Conformation Enrichment Functions

The conformation-based enrichment currently allows for both subject-authority-
based conformation and predicate-based conformation. The self-configuration
process of subject-authority-based conformation starts by finding the most frequent subject authority rk in source(E). Also, it finds the most frequent subject
authority rk in the target dataset target(E). Then this self-configuration process
generates the two parameters: (sourceSubjectAuthority, rk) and (target
SubjectAuthority, rk). After that, the self-configuration process replaces each
subject authority rk in source(E) by rk.

Back to our running example, the authority self-conformation process generates the two parameters (sourceSubjectAuthority, ":") and (targetSubject
Authority, "ex:"). Replacing each ":" by "ex:" generates, in our example, the
new conformed URI "ex:Iboprufen".
We define two predicates p1, p2  P to be interchangeable (denoted p1  p2)
if both of them have the same subject and object. Formally, p1, p2  P : p1 
p2  s, o | (s, p1, o)  (s, p2, o).

The idea of the self-configuration process of the predicate conformation is to
change each predicate in the source dataset to its interchangeable predicate in
the target dataset. Formally, find all pairs (p1, p2) | s, p1, o  k  s, p2, o 
k  (s, p1, o)  k  (s, p2, o)  k. Then, for each pair (p1, p2) create two selfconfiguration parameters (sourceProperty, p1) and (targetProperty, p2).
The predicate conformation will replace each occurrence of p1 by p2.

M.A. Sherif et al.

Fig. 3. Ibuprofen CBD after NLP and predicate conformation enrichment

In our example, let us suppose that we ran the NLP-based enrichment first
then we got a set of related named entities all of them related to our ex:Iboprufen
object by the default predicate fox:relatedTo as shown in Fig. 3a. Subsequently,
applying the predicate conformation self-configuration will generate (source
Property, fox:relatedTo) and (targetProperty, ex:relatedCompany)
parameters. Consequently, the predicate conformation module will replace
fox:relatedTo by ex:relatedCompany to generate Fig. 3b.

5.5 Filter Enrichment Function

The idea behind the self-configuration of filter-based enrichment is to preserve
only valuable triples in the source CBDs k and discard any unnecessary triples
so as to achieve a better match to k. To this end, the self-configuration process
starts by finding the intersection between source and target examples I =
kk. After that, it generates an enrichment function based on a SPARQL
(k,k)E
query which is only preserving predicates in I. Formally, the self-configuration
results in the parameter set P =
?

?

?
pKKP

p.

Back to our running example, let us continue from the situation in the previous section (Fig. 3b). Performing the self-configuration of filters will generate
P = {fox:relatedTo}. Actually applying the filter enrichment function will
remove all unrelated triples containing the predicate fox:relatedTo. Figure 4
shows a graph representation for the whole learned pipeline for our running
example.

Fig. 4. Graph representation of the learned pipeline of our running example, where
d1 is the positive example source presented in Fig. 2a and d6 is the positive example
target presented in Fig. 2b.
?

?

?
6 Evaluation

The aim of our evaluation was to quantify how well our approach can automate
the enrichment process. We thus assumed being given manually created training
examples and having to reconstruct a possible enrichment pipeline to generate
target CBDs from the source CBDs. In the following, we present our experimental setup including the pipelines and datasets used. Thereafter, we give an
overview of our results, which we subsequently discuss in the final part of this
section.

6.1 Experimental Setup

We used three publicly available datasets for our experiments:

1. From the biomedical domain, we chose DrugBank 4 as our first dataset. We
chose this dataset because it is linked with many other datasets5, from which
we can extract enrichment data using our atomic enrichment functions. For
our experiments we deployed a manual enrichment pipeline Mmanual, in which
we enrich the drug data found in DrugBank using abstracts dereferenced from
DBpedia, then we conform both DrugBank and DBpedia source authority
URIs to one unified URI. For DrugBank we manually deployed two experimental pipelines:
 M 1

DrugBank = (m1, m2), where m1 is a dereferencing function that dereferences any dbpedia-owl:abstract from DBpedia and m2 is an authority
conformation function that conforms the DBpedia subject authority6 to
the target subject authority of DrugBank 7.

 M 2

DrugBank = M 1

DrugBank ++ m3, where m3 is an authority conformation
function that conforms DrugBanks authority to the Example authority8.
2. From the music domain, we chose the Jamendo9 dataset. We selected this
dataset as it contains a substantial amount of embedded information hidden in literal properties such as mo:biography. The goal of our enrichment
process is to add a geospatial dimension to Jamendo, e.g., the location of a
recording or place of birth of a musician. To this end, we deployed a manual enrichment pipeline, in which we enrich Jamendos music data by adding
additional geospatial data found by applying the NLP enrichment function

4 DrugBank is the Linked Data version of the DrugBank database, which is a repository of almost 5000 FDA-approved small molecule and biotech drugs, for RDF
dump see http://wifo5-03.informatik.uni-mannheim.de/drugbank/drugbank dump.
nt.bz2.

5 See http://datahub.io/dataset/fu-berlin-drugbank for complete list of linked dataset

with DrugBank.

6 http://dbpedia.org.
7 http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugs.
8 http://example.org.
9 Jamendo contains a large collection of music related information about artists and

recordings, for RDF dump see http://moustaki.org/resources/jamendo-rdf.tar.gz.

M.A. Sherif et al.

against mo:biography. For Jamendo we deploy manually one experimental
pipeline:
Jamendo = {m4}, where m4 is an NLP function that find locations in
 M 1
mo:biography.

3. From the multi-domain knowledge base DBpedia [12] we used the class
AdministrativeRegion for our experiments. As DBpedia is a knowledge base
with a large ontology, we build a set of five pipelines of increasing complexity:
DBpedia = {m5}, where m5 is an authority conformation function that
 M 1
conforms the DBpedia subject authority to the Example target subject
authority.

 M 2

DBpedia = m6 ++ M 1

DBpedia, where m6 is a dereferencing function that

dereferences any dbpedia-owl:ideology.

 M 3

DBpedia = M 2

DBpedia ++ m7, where m7 is an NLP function that finds

all named entities in dbpedia-owl:abstract.

 M 4

DBpedia = M 3

DBpedia ++ m8, where m8 is a filter function that filters

for abstracts.

 M 5

DBpedia = M 3

DBpedia ++ m9, where m9 is a predicate conformation
function that conforms the source predicate dbpedia-owl:abstract to
the target predicate of dcterms:abstract.

Altogether, we manually generated a set of eight pipelines, which we then applied
against their respective datasets. The evaluation protocol was as follows: Let M
be one of the manually generated pipelines. We applied M to an input knowledge
base K and generated an enriched knowledge base K = M(K). We then selected
a set of resources in K and used the CBD pairs of selected resources and their
enriched versions as examples E. E was then given as training data to our
algorithm, which learned an enrichment pipeline M. We finally compared the
triples in K (which we used as reference dataset) with the triples in M(S)
to compute the precision, recall and F-measure achieved by our approach. All
generated pipelines are available at the project web site10.

All experiments were carried out on a 8-core PC running OpenJDK 64-Bit
Server 1.6.0 27 on Ubuntu 12.04.2 LTS. The processors were 8 Hexa-core AMD
Opteron 6128 clocked at 2.0 GHz. Unless stated otherwise, each experiment was
assigned 6 GB of memory. As termination criteria for our experiments, we used
(1) a maximum number of iterations of 10 or (2) an optimal enrichment pipeline
found.

6.2 Results

We carried out two sets of experiments to evaluate our refinement based learning
algorithm. In the first set of experiments, we tested the effect of the complexity
weight  to the search strategy of our algorithm. The results are presented
in Table 1. In the second set of experiments, we test the effect of the number
of positive examples |E| on the generated F-measure. Results are presented in
Table 2.
10 https://github.com/GeoKnow/DEER/tree/master/evaluations/pipeline learner.
?

?

?
Configuration of the Search Strategy. We ran our approach with varying
values of  to determine the value to use throughout our experiments. This parameter is used for configuring the search strategy in the learning algorithm, in particular the bias towards simple pipelines. As shown in Sect. 4.2, this is achieved
by multiplying  with the node complexity and subtracting this as a penalty
DrugBank.
from the node fitness. To configure , we used the first pipeline M 1
The results suggest that setting  to 0.75 leads to the best results in this particular experiment. We thus adopted this value for the other studies.

Table 1. Test of the effect of  on the learning process using the Drugbank dataset,
where |E| = 1, M is the manually created pipeline, |M| is the complexity of M , M
is the pipeline generated by our algorithm, and In is the number of iterations of the
algorithm.



|M|

|M|

|| In P (M
61 10 1.0

) R(M
0.99

) F (M
0.99

)

0.25 3

0.50 3

0.75 3

1.0

61 10 1.0

61 10 1.0

4 1.0

61 10 1.0

0.99

0.99

1.0

0.99

0.99

0.99

1.0

0.99

Effect of Positive Examples. We measured the F-measure achieved by our
approach on the datasets at hand. The results shown in Table 2 suggest that
when faced with data as regular as that found in the datasets Drugbank, DBpedia and Jamendo, our approach really only needs a single example to be able
to reconstruct the enrichment pipeline that was used. This result is particularly interesting, because we do not always generate the manually created reference pipeline described in the previous subsection. In many cases, our approach
detects a different way to generate the same results. In most cases (71.4 %) the
pipeline it learns is actually shorter than the manually created pipeline. However,
in some cases (4.7 %) our algorithm generated a longer pipeline to emulate the
Jamendo the manual configumanual configuration. As an example, in case of M 1
ration was just one enrichment function, i.e., NLP-based enrichment to find all
locations in mo:biography. Our algorithm learns this single manually configured
enrichment as (1) an NLP enrichment function that extracts all named entities
types and then (2) a filter enrichment function that filters all non-location triples.
Our results also suggest that our approach scales when using a small number of
positive example as on average the learning time for one positive example was
around 48 s.

7 Related Work

Linked Data enrichment is an important topic for all applications that rely on
a large number of knowledge bases and necessitate a unified view on this data,

M.A. Sherif et al.

Table 2. Test of the effect of increasing number of positive examples in the learning
process. For this experiment we set  = 0.75. M is the manually created pipeline,
|M| is the size of M , TM (KB) is the time for applying M to the entire dataset, M
is
the pipeline generated by our algorithm, Tl is the learning time, || is the size of the
refinement tree  , In is the number of iterations performed by the algorithm, and all
times are in minutes.

M 1

DBpedia

M 2

DBpedia

M 3

DBpedia

M 4

DBpedia

M 5

DBpedia

M 1

DrugBank 1

M 2

DrugBank 1

M 1

Jamendo
?

?

?
|E|
?

?

?
|M| TM (KB)
?

?

?
0.2
0.2

|M| TM(KB) Tl
1.3
?

?

?
1.3

1.6
1.8

|| In P (M
?

?

?
1 1.0
1 1.0
?

?

?
23.3

14.7

0.4
0.6

25.5

3.5
3.6

25.2
22.8

10.9
10.4
?

?

?
0.1

15.2
15.1

0.1
0.3

0.1
0.2

4.1
3.4

0.1
0.1

10.6
10.4

0.2

0.3 55

6.1 55
0.1 55

0.7 13
0.9 13

0.7 13
0.9 13

1 1.0
9 0.99

9 1.0
9 0.99

2 0.99
2 0.99

2 1.0
2 1.0

0.1 61 10 0.99
0.1 61 10 0.99

0.1 61 10 1.0
0.1 61 10 1.0

0.1 13
0.1

2 0.99
1 0.99

) R(M

) F (M

)

1.0
1.0

0.99
1.0

0.99
0.99

0.99
1.0

1.0
1.0

0.99
0.99

0.99
0.99

0.99
0.99

1.0
1.0

0.99
0.99

0.99
0.99

0.99
0.99

1.0
1.0

0.99
0.99

0.99
0.99

0.99
0.99

e.g., Question Answering frameworks [13], Linked Education [6] and all forms
of semantic mashups [9]. In recent work, several challenges and requirements to
Linked Data consumption and integration have been pointed out [14]. For exam-
ple, the R2R framework [2] addresses those by enabling the publish of mappings
across knowledge bases that allow to map classes and defined the transformation
of property values. While this framework supports a large number of transforma-
tions, it does not allow the automatic discovery of possible transformations. The
Linked Data Integration Framework (LDIF) [21], whose goal is to support the
integration of RDF data, builds upon R2R mappings and technologies such as
SILK [10] and LDSpider11. The concept behind the framework is to enable users
to create periodic integration jobs via simple XML configurations. Still these
configurations have to be created manually. The same drawback holds for the
Semantic Web Pipes12 [20], which follows the idea of Yahoo Pipes13 to enable
the integration of data in formats such as RDF and XML. By using Semantic
Web Pipes, users can efficiently create semantic mashups by using a number of

11 http://code.google.com/p/ldspider/.
12 http://pipes.deri.org/.
13 http://pipes.yahoo.com/pipes/.
?

?

?
operators (such as getRDF, getXML, etc.) and connect these manually within
a simple interface. KnoFuss [19] addresses data integration from the point of
view of link discovery. It begins by detecting URIs that stand for the same realworld entity and either merging them together or linking them via owl:sameAs.
In addition, it allows to monitor the interaction between instance and dataset
matching (which is similar to ontology matching [7]). Fluid Operations Information Workbench14 allows to search through, manipulate and integrate datasets
for purposes such as business intelligence. [5] describes a framework for semantic enrichment, ranking and integration of web videos, and [1] presents semantic enrichment framework of Twitter posts. Finally, [8] tackles the linked data
enrichment problem for sensor data via an approach that sees enrichment as a
process driven by situations of interest. To the best of our knowledge, the work
we presented in this paper is the first generic approach tailored towards learning
enrichment pipelines of Linked Data given a set of atomic enrichment functions.

8 Conclusions and Future Work

In this paper, we presented an approach for learning enrichment pipelines based
on a refinement operator. To the best of our knowledge, this is the first approach
for learning RDF based enrichment pipelines and could open up a new research
area. We also presented means to self-configure atomic enrichment pipelines so
as to find means to enrich datasets according to examples provided by an end
user. We showed that our approach can easily reconstruct manually created
enrichment pipelines, especially when given a prototypical example and when
faced with regular datasets. Obviously, this does not mean that our approach
will always achieve such high F-measures. What our results suggest is primarily
that if a human uses an enrichment tool to enrich his/her dataset manually, then
our approach can reconstruct the pipeline. This seems to hold even for relatively
complex pipelines.

Although we achieved reasonable results in terms of scalability, we plan to further improve time efficiency by parallelising the algorithm on several CPUs as well
as load balancing. The framework underlying this study supports directed acyclic
graphs as enrichment specifications by allowing to split and merge datasets. In
future work, we will thus extend our operator to deal with graphs in addition to
sequences. Moreover, we will look at pro-active enrichment strategies as well as
active learning.
