Information Extraction for Learning

Expressive Ontologies

Giulio Petrucci1,2(B)

1 Fondazione Bruno Kessler, Via Sommarive, 18, 38123 Trento, Italy

2 University of Trento, Via Sommarive, 14, 38123 Trento, Italy

petrucci@fbk.eu

Abstract. Ontologies are used to represent knowledge in a formal and
unambiguous way, facilitating its reuse and sharing among people and
computer systems. A large amount of knowledge is traditionally available
in unstructured text sources and manually encoding their content into
a formal representation is costly and time-consuming. Several methods
have been proposed to support ontology engineers in the ontology building process, but they mostly turned out to be inadequate for building
rich and expressive ontologies. We propose some concrete research directions for designing an effective methodology for semi-supervised ontology
learning. This methodology will integrate a new axiom extraction technique which exploits several features of the text corpus.

1 Introduction

According to a widely accepted definition, an Ontology is a formal representation of a shared conceptualization (see [15]): dealing with a formal and explicit
representation of a commonly agreed understanding of a domain, can help to
overcome the problem of ambiguity in knowledge representation and sharing. In
the Semantic Web scenario, ontolgies provide the conceptual scheme for metadata carrying the explicit data semantics, endowing machines with the capability
to interpret data unambiguously and perform reasoning over them.

Several approaches, known as Ontology Learning, have been proposed along
the years to facilitate the encoding of knowledge from large textual sources,
which are massively available, into ontologies. Such approaches still experience
some severe limitations, specially trying to extract complex and expressive formalizations like axioms.

The intended contribution of our work is twofold. First, we aim to provide
a novel automatic technique, capable to abstract the formulation of OWL DL
axioms from large text corpora. Second, we intend to combine such technique
together with other state-of-the-art methods in a full methodology that will
support the ontology engineer in the axiom extraction process, reducing its cost.
The paper is structured as follows. In Sect. 2 we will depict the State of the
Art in Ontology Learning. In Sect. 3 we state our research problem and the contribution we want to give. In Sect. 4 we will describe our research methodology.
In Sect. 5 we present our preliminary results. In Sect. 6 we present our evaluation
plan. Finally, Sect. 7 concludes the paper.
c Springer International Publishing Switzerland 2015
F. Gandon et al. (Eds.): ESWC 2015, LNCS 9088, pp. 740750, 2015.
DOI: 10.1007/978-3-319-18818-8 47
?

?

?
2 State of the Art

Human knowledge is often carried by large unstructured textual sources. Building an ontology from such sources can be resource-intensive and time consuming.
To support ontology engineers in such process, several approaches have been pro-
posed, known as Ontology Learning (see [7]), which build upon well-established
techniques from Natural Language Processing, Machine Learning, Information
Retrieval, Knowledge Acquisition and Ontology Engineering. The various tasks
relevant in the Ontology Learning process and their mutual dependencies have
been organized in the Ontology Learning Layer Cake (see Fig. 1), a conceptual
sketch of a generic ontology learning layered architecture in which each layer is
associated with a task and its output is the input for the one on the top of it.

Fig. 1. The Ontology Learning Layer Cake

Recently, the ontology engineering community has been pursuing the ambitious goal to extract increasingly complex information ranging from terms, to
relations, to hierarchies, and finally to axioms. Even if the fully automatic acquisition of knowledge by machines is still a long term goal, several automatic and
semi-automatic techniques, along with several ready-to-use tools, have been proposed for each layer of the cake.

Term Extraction. The goal of this task is to identify all the relevant terms in
the text corpus. Frequency based criteria or metrics from Information Retrieval
(TF, TF-IDF, TIM-DRM) are used in [8,13,18,20,30,37], together with the
C-value/NC-value metric from Computational Linguistics. Linguistic features
are exploited in [18,30,33] while a combined approach is presented in [44]:
extracted terms are connected as nodes in a graph structure, ranked and filtered on the basis of metrics from Graph Theory (Betweenness, Centrality, ecc)
or Information Retrieval and finally selected according to some voting schemes
(Majority, Intersection, ecc). In [40] a machine learning based approach has been
shown capable to adapt to different domains with a reduced training effort.

Synonym Extraction. In order to group together terms with similar meaning, the
well known distributional hypothesis is largely exploited as in [8,29,33]. Other

G. Petrucci

Text Mining clustering techniques are used in [13] while external resources as
WordNet1 are used in [30].

Concept Learning. At this layer, concepts must be induced in an intensional way.
Clustering algorithms and Latent Semantic Indexing (LSI) are used in [13] while
in [33] sets of terms considered synonyms are casted into new concepts named
after the more occurring term. WordNet Domains and other resources are used in
[36] to associate the newly extracted keyphrases with existing concepts. Context
similarity in [8,29] induces concepts definition over sets of terms.

Concept Hierarchy. The goal of this task is to learn taxonomic relations over
extracted concepts. Lexico-syntactic patterns from [17] are used in [8,44]. The
approach presented in [35] uses syntactic features to train a binary classifier to
predict if two nouns are in a taxonomic relation. In [33] taxonomy is induced
with a hierarchical clustering process while in [29] a complex context similarity
measure is used to add new concepts in an existing ontology. In [21,37] a graph
with concepts as vertices is built via pattern-driven web search operations, while
in [14] a similarity metric over a vector space model is used to evaluate term
relatedness. Task-specific algorithms are then used in order to turn these graphs
into taxonomic trees.

Relation Learning. The output of this layer is the set of all relations among
concepts and individuals, eventually organized in a hierarchical order. Linguistic
patterns and metrics from Graph Theory are used in [44]. Linguistic patterns are
used also in [18] in order to detect verb-based relations. In [29], context similarity
is taken as an evidence of a generic conceptual relation. Statistical significance
of co-occurrence is used in [32] in order to predict a relation between two terms.
In [9], patterns expressed in an ad hoc formalism are used to detect instances
of known relations. Syntactic features are used in [8] in order to detect general
relations, while a combination of a set of patterns together with WordNet is
used for mereological relations. The assumption of an intrinsic redundancy in
large corpora is exploited in [16] in order to apply graph mutual reinforcement
between a set of relation lexico-syntactic patterns and corresponding matching
instances. Iterative approaches are followed in [1,5,42] starting from a small
hand-crafted set of patterns, matching textual relation instances are used to
extend the pattern set. Parse tree feature spaces are used in [6] in order to train
tree kernel based classifiers which can predict occurrences of predefined relations.
Hand-crafted rules are used in [2] in order to label a set of examples which will be
used to train a classifier. This semi-supervised approach has been extended in [11]
with POS-tags patterns acting as syntactic constrains and a large dictionary of
relation acting as a lexical constraint, in order to improve the extractions quality.
A pairwise vector space clustering is used in [34] in order to detect recurring
patterns and identify relation instances. A clustering technique is used also in
[26] in order to detect generic relations for given type signatures. Ontological

1 http://wordnet.princeton.edu/.
?

?

?
resources like Freebase,2 YAGO3 and Wikipedia4 infoboxes are exploited in [25,
27,28,41] as sources of evidence in order to match textual relation instances
from which different types of features are extracted to train classifiers or cluster
instances.

Axiom Learning. The last layer addresses the problem of axioms learning. The
usage of lexico-syntactic patterns has been exploited in [39] for generating formal
class description from definitional sentences. This approach has been followed in
[38] in order to detect disjointness among classes using external lexical resources,
underlying ontologies and Pointwise Mutual Information (PMI) relying on the
Web as a source of evidence. A radically different approach is presented in [31]
where Discourse Representation Theory (DRT, see [19]) is used in order to represent the content of a single document in formal structures called Discourse
Representation Structures (DRSs). Those structures are then mapped to OWL
constructs applying a set of translation rules and exploiting several external lexical (FrameNet5 and VerbNet6) and ontological (FOAF,7 DBpedia,8 Dolce+DnS
Ultralite9) resources. The translation from DRSs to OWL constructs relies on a
set of Ontology Design Patterns (ODPs). An ODP is a reusable successful solution to a recurrent modeling problem 10 and can be seen as a sort of template to
be used in some particular and recurring situations. Acting as constraints, they
are supposed to ensure quality for the final ontology construction. In [22], an system for the extraction of EL++ concepts definitions from text is presented. Text
fragments involving concepts from the Snomed CT11 ontology are matched and
their lexical and ontological features are used to train a maximum entropy classifier in order to predict the axiom describing the involved entities. Users can
provide their feedback, helping the system to correct the underlying model.

3 Problem Statement and Contribution

Reasoning-based applications can infer new knowledge beyond what explicitly
stated in the domain representation they rely on. Their power of reasoning
depends on the expressivity of such representation: an ontology provided with
complex TBox axioms can act as a valuable support for the representation and
the evaluation of a deep knowledge about the domain it represents.

Automatic learning of expressive TBox axioms is a complex task. From a
linguistic point of view, conjunctions, negations and disjunctions are difficult
2 http://www.freebase.com.
3 www.mpi-inf.mpg.de/yago-naga/yago/.
4 https://www.wikipedia.org/.
5 https://framenet.icsi.berkeley.edu/fndrupal/.
6 http://verbs.colorado.edu/mpalmer/projects/verbnet.html.
7 http://www.foaf-project.org/.
8 http://dbpedia.org.
9 http://www.ontologydesignpatterns.org/ont/dul/DUL.owl.
10 http://ontologydesignpatterns.org/wiki/Main Page.
11 http://www.ihtsdo.org/snomed-ct/.

G. Petrucci

to parse and interpret. The same is for detecting local contexts for universal
quantifications. Statistical relevance based metrics could be misleading as definitional sentences can appear infrequently in the corpus and the knowledge to be
encoded in a single axiom can be spread across several sentences.

State-of-the-art methods and tools capable to handle large text corpora still
appear to be more suitable to support the construction of light-weight ontolo-
gies. On the other hand, axiom extraction methods are intended to work in a
sentence-by-sentence translation modality, with constraints on the structure of
the sentence or the particular domain of interest. As a consequence, ontology
learning from large text corpora still remains a heavily manual process which
can end up having unsustainable costs. Therefore, we state our research problem
in the form of the following question:

How can semi-supervised techniques significantly help to extract relevant
expressive axioms from a large domain text corpus and therefore minimize human intervention in the process of ontology building?
Answering this question, the first contribution we aim to give is the design
of an automatic technique to extract TBox axioms from large text
corpora. Our target axioms are those that can be expressed through OWL DL
constructs. Each text corpus is assumed to have some inner coherence about
some domain. As in [43], we focus on corpora from which a significant amount of
ontological statements (definitions of concepts, their relations, ecc) rather than
factual statements can be extracted  like encyclopedic texts, textbooks or spec-
ifications. Setting this research goal, we hypothesize that statistical, linguistic
and semantic features of such text corpora can be exploited to train a system
capable to detect textual occurrences of axioms.

The second contribution is the organization of our new axiom extraction
technique and other state-of-the-art methods together with human activities
into a fully-fledged methodology, outlined in Fig. 2. The initial input is a text
corpus and the final output is a set of axioms which are intended to be expressive
conceptualization of the knowledge originally contained in the input corpus. The
four phases are:
1. Light-Weight Extraction (LE): state-of-the-art methods are used to extract

concepts, relations and concept hierarchies from the corpus.

2. User Selection (US): the ontology engineer can select a subset of the ontology elements produced by the previous phase or add new ones that may have
been missed. This interaction validates the knowledge extracted so far and
highlights what the engineer considers as more relevant.

3. Axioms Extraction (AE): the user can select a relation, concept or a pair
of concepts and the system will provide all the hypotheses of axioms involving
them, along with some textual evidence if possible. Hypotheses will be ranked
according to how much the system is confident that the single axiom actually
holds in the corpus.

4. User Feedback (UF): the user can express a judgment about the correctness
of the axiom hypotheses suggested by the previous phase, in order to remove
uncertainty.
?

?

?
Fig. 2. An outline for the proposed methodology

4 Research Methodology and Approach

In order to train a system capable to learn axioms from text, we first need
to identify which features of the text corpus must be exploited. As shown in
Sect. 2, several approaches relying on lexico-syntactic features and deep linguistic
analysis of sentences have been proposed. With some ongoing experiments we
are investigating the possibility to combine them with statistical relevance based
ones as in [30,44]. As definitory sentences could be strong points of evidence for
axioms holding in the text, we want to exploit them as well with deeper linguistic
analysis techniques, as in [31,37]. Some preliminary studies have been performed
using the BPMN 1.1 specification12 as the input text corpus and a corresponding
ontology, manually developed by an ontology engineer.

Starting from this, we plan to build a set of training examples: the inputs is a
text corpus and the expected output is a set of axioms actually holding according
to the corpus content. We have started investigating some available ontologies
to extract axioms from them and build our expected outputs. Usually, they are
not supplied with a corresponding text corpus so that we will have to build it
by ourselves harvesting the web or some other large document repositories. Our
idea is to train our system with these examples assuming that the system can
scale to other corpora maintaining an acceptable performance level, in a sort
of domain adaptation fashion. The approaches presented in [24] for keyphrases
extraction and in [12] for taxonomy induction exploited the same intuition. We
want to extend this investigation to the axiom extraction problem.

Our target ontology language is OWL DL. We plan to develop our system in
an iterative way, having different type of constructs as target for each step. So
far, we have planned to proceed according to the following hypothesis of work:

1. Atomic Negation, Concept Intersection, Inverse Properties, Nominals
2. Complex Concept Negation, Role Hierarchy, Cardinality Restrictions
3. Universal Restrictions, Limited Existential Quantification

5 Preliminary Results

During the first year of our work we focused on reviewing the State of the Art and
performing some exploratory activities. We outlined the preliminary version of
our methodology on the main idea of alternating manual and automatic tasks. In

12 http://www.omg.org/spec/BPMN/1.1/.

G. Petrucci

order to decide which techniques to use in the first phase of our methodology, we
analyzed several state-of-the-art methods. After some comparative experiments,
we opted for KX, a tool implementing the approach presented in [30] for the
term extraction task, as it turned out giving the best performance. We still have
to investigate further in order to decide which relation extraction and concept
hierarchy induction techniques to be adopted.

We tested several NLP tools over the BPMN 1.1 specification in order to
get a rough idea of their behavior. We performed deep syntactic analysis with
Mate-tools (see [3]) and with the Stanford Core NLP toolkit (see [23]) which
has been extended with a plugin (still under development) that annotates the
shortest path parse tree fragment (as in [6,41]) between pairs of relevant concepts
spotted by KX: such paths could be easily analyzed in order to check how they
can be useful for our work. We used SEMAFOR (see [10]) in order to extract all
the occurrences of the semantic frames collected in FrameNet. We also tried to
use Boxer (see [4]) to extract a deep semantic representation of the text based
on DRSs, which have the interesting property of being equivalent to First-Order
Logic formulae.

Recently we started building our training set according to analogous experiences in literature. In [29], the OpenCyc ontology is exploited for the domains of
Fisheries and Aquaculture and Economy and Finance. The Aquatic Sciences and
Fisheries Abstract (ASFA) thesaurus13 has been used as a corpus for the former,
while the the Harvey Glossary14 for the latter. In [37] the Economy and Finance
corpus has been extended with scientific journal or conference papers about the
topic. The Economist also provide a related glossary.15 In the same work, authors
followed the same procedure to build a corpus for the domain of Artificial Intel-
ligence, but no valuable ontology seems to be available. The BioPortal16 project
provides a large repository of ontologies for the biomedical domain. Several corpora for such domain17 are available as it has been explored in depth by the
NLP research community. However, as it comprises a large variety of different
topics, finding suitable corpus/ontology pairs may not be straightforward.

During the last weeks, further explorations suggested us the opportunity to
deepen some topics from the distributed compositional semantics field in order
to deal with linguistic issues in axiom detection.

6 Evaluation Plan

Our axiom extraction technique will be evaluated according to its capacity of
correctly identify axioms actually holding in the input text corpus. We plan to
continuously evaluate it using the axioms contained in out training set ontologies

13 http://www4.fao.org/asfa/asfa.htm.
14 http://biz.yahoo.com/glossary.
15 http://www.economist.com/economics-a-to-z.
16 http://bioportal.bioontology.org/ontologies.
17 e.g. http://www2.informatik.hu-berlin.de/hakenber/links/benchmarks.html.
?

?

?
as the gold standard. Well-known metrics like Precision and Recall could be used
in a quite straightforward way in this scenario.

Later, we plan to involve some human assessors in order to judge some
axioms, maybe verbalized, as actually holding in the text corpus or not. Our
idea is to involve more human assessors and measure the quality of the judgment using some inter-annotator agreement metric.

The final evaluation phase will focus on the effectiveness of our methodology
in term of cost reduction. Starting from the same specification we will build
again the BPMN ontology using our methodology. This will allow us to compare
both the correctness of the extracted axioms and the time spent for this task
with the ones of the corresponding hand-made ontology.

7 Conclusions

Several approaches have been proposed to support ontology engineers in the
tasks of building ontologies from large textual corpora. Looking at state-of-
the-art techniques for semi-automatic ontology engineering, despite the progress
made so far, they are not able to give significant support in rich and expressive
ontology building so that their cost can become unaffordable. To overcome this
limitation, we want to design a new axiom extraction technique to be integrated
into a a semi-automatic methodology together with other state-of-the-art meth-
ods. We hypothesize that available large textual and knowledge sources can be
used to train a domain-independent system capable of providing a human actor
some hypotheses about axioms holding among concepts in the knowledge contained in a text corpus. We also outline an evaluation process, involving both
automatic evaluation against a gold standard and human assessment in order
to evaluate relevance and correctness of the extracted axioms together with the
effectiveness of our methodology in term of human effort reduction.
