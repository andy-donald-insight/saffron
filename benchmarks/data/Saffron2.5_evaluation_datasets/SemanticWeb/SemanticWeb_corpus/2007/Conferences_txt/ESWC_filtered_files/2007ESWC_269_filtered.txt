Extracting Social Networks Among Various Entities

on the Web

Yingzi Jin1, Yutaka Matsuo2, and Mitsuru Ishizuka1

1 University of Tokyo, Hongo 731, Tokyo 113-8656, Japan

	
			 			

2 National Institute of Advanced Industrial Science and Technology

		

Abstract. Social networks have recently attracted much attention for their importance to the Semantic Web. Several methods exist to extract social networks
for people (particularly researchers) from the web using a search engine. Our goal
is to expand existing techniques to obtain social networks among various entities.
This paper proposes two improvements, i.e. relation identification and threshold tuning, which enable us to deal with complex and inhomogeneous commu-
nities. Social networks among firms and artists (of contemporary) are extracted
as examples: Several evaluations emphasize the eectiveness of these methods.
Our system was used at the International Triennale of Contemporary Art (Yoko-
hama Triennale 2005) to facilitate navigation of artists information. This study
contributes to the Semantic Web in that we increase the applicability of social
network extraction for several studies.

1 Introduction

Social networks explicitly exhibit relationships (called ties in social sciences) among individuals and groups (called actors). They have been studied in social sciences since the
1930s. To date, vastly numerous studies using social network analysis have been conducted [22]. In the context of the Semantic Web, social networks are crucial to realize
a web of trust that facilitates estimation of informations credibility and its providers
trustworthiness [10]. Ontology construction is also related to social networks: P. Mika
discusses the relation between the community and emergent ontology from a social
network perspective [18]. Information sharing and recommendation [19,9] on social
networks are other applications that are served by the Semantic Web. Our lives are influenced strongly by social networks without our knowledge of their implications. For
that reason, many applications are relevant to social networks [23].

Social networks are obtained from various sources, such as e-mail archives, FOAF
documents, and DBLP. For example, T. Finin et al. extract a social network from the
web by collecting FOAF documents [7]. Particularly, several studies have been undertaken to use a search engine to extract social networks from the entire web [11,16,17].
Co-occurrence of names on the web, which is basically obtained by posing a query including two names to a search engine, is commonly used as proof of relational strength.
Using a search engine to recognize the relation of two entities (or two words) has increasingly gained attention in the field of natural language processing [5,12,24].

E. Franconi, M. Kifer, and W. May (Eds.): ESWC 2007, LNCS 4519, pp. 251266, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Y. Jin, Y. Matsuo, and M. Ishizuka

This study is intended to expand current social-network mining techniques using a
search engine to obtain a social network among various entities. Specifically in this
paper, two improvements are proposed in order to apply our method to complex and
inhomogeneous communities: relation identification and threshold tuning. We extract
two social networks as examples: artists of contemporary art, and famous firms in Japan.
We must identify the relation types such as alliances and lawsuits; consequently, we
can make elaborate queries and apply text processing to extract a social network among
firms. Our algorithm adds a relation keyword to the search query to emphasize a specific
relationship. Extracting a social network of artists, on the other hand, requires adaptive
tuning of thresholds because the appearance of each artist on the web is completely
dierent. Optimal thresholds are sought to invent appropriate edges between entities.

Our contributions are summarized as follows: First, through the two improvements,
i.e. relation identification and threshold tuning, which respectively focus on complex
and inhomogeneous communities on the web, social network extraction becomes more
generally applicable to various entities. We argue the general social network extraction
in the last part of the paper, which can cultivate existing studies using social networks
in the Semantic Web. Second, because our method can extract relations from among
entities, it can output machine-processable knowledge about the relations automatically
from the information on the current web. Although some approaches exist to generate
RDF statements by web mining, our study provides an alternative; our intuition is that
extracting a social network might provide information that is only recognizable from
the network point of view. For example, the centrality of each firm is identified only
after generating a social network.

The next section introduces related studies. Section 3 describes the investigation of
dierent appearance of entities on the web and addresses our ideas to obtain various social networks from the web. Sections 4 and 5 introduce our case studies, which specifically investigate two types of networks: those of firms and artists. In Section 6, before
we conclude the paper, we propose a general architecture of social network extraction
and discuss applications of the extracted social networks to the Semantic Web.

2 Related Works

Numerous studies have obtained and analyzed social networks on the web: L. Adamic
collects relations among students from web link structure and text information, and
characterizes the social networks among Stanford students and MIT students [1]. T.
Finin describes a large collection of FOAF documents (over 1.5 million) from the web
and analyzes the structure of friendship networks in the Semantic Web [7]. Trust calculation [10] is a major application of social networks. Some studies seek other appli-
cations: A. McCallum and his group present an end-to-end system that automatically
integrates both e-mail and web content to help users maintain large contact databases
[6]. Aleman-Meza et al. use relational data from both FOAF and DBLP to detect relationships among potential reviewers and authors of scientific papers [2].

Several studies have particularly addressed use of a search engine for social network extraction. In the mid-1990s, H. Kautz and B. Selman developed a social network
extraction system called the Referral Web [11]. The system uses a search engine to retrieve web documents that include a given personal name. Recently, P. Mika developed
?

?

?
Flink, a system for extraction, aggregation, and visualization of online social networks
for the Semantic Web community [17]. A social network of 608 researchers from both
academia and industry is extracted and analyzed. The web-mining component of Flink,
similarly to that used in Kautzs work, employs co-occurrence analysis. The strength
of relevance of two persons, X and Y, is estimated by putting a query X AND Y to a
search engine: If X and Y share a strong relation, we can usually find much evidence
on the web such as links found on home pages, lists of co-authors in technical papers,
organizational charts, and so on. In Flink, the strength of relations among individuals is
calculated using the Jaccard coecient nXY nXY, where nXY represents the number
of hits yielded by the query X AND Y and nXY represents the number of hits by the
query X OR Y. The two researchers are considered to share a relation if the value is
greater than a certain threshold. The term Semantic Web OR ontology is added to the
query for name disambiguation.

Matsuo et al. developed a system called POLYPHONET, which also uses a search engine to measure the co-occurrence of names [15,16]. In their study, several co-occurrence
measures [13] have been compared, including the matching coecient (nXY), mutual
information, Dice coecient, Jaccard coecient, and overlap coecient. The overlap
coecient nXY  min(nX nY) performs best according to the experiments. In addition,
POLYPHONET was operated at several AI conferences in Japan and a couple of international conferences to promote participants communication. For disambiguating personal names, key phrases such as aliations are added to queries.

We regard the two studies by Mika and Matsuo as relevant precedent studies, and

propose some improvements to increase the applicability of that approach.

3 Extraction of Social Networks

3.1 Problem of Existing Methods

The fundamental idea underlying the existing studies by Mika and Matsuo is that the
strength of a relation between two entities can be estimated by co-occurrence of their
names on the web. The criteria to recognize a relation, such as the measure of cooccurrence and a threshold, are determined beforehand. An edge will be invented when
the relation strength by the co-occurrence measure is higher than the predefined thresh-
old. Although the approach is eective for extracting a social network of researchers, our
preliminary study indicates that it does not perform well for various entities on the web.
As the first reason, co-occurrence-based methods become ineective when two entities co-occur universally on numerous web pages. For example, when we want to infer
two firms relations from the web, we submit a query Matsushita AND JustSystem1
to a search engine. Consequently, we are referred to as many as 425,000 pages, for
which the Jaccard coecient is 0.031. However, this figure is unreliable considering
the media eect on the web. In the domain of firms, many relations are published in
news reports and on news releases that are distributed on the web. Many web pages
describe and comment on the relation if the news is given attention by media services
or people. Conversely, if it were not attention given, only a small number of pages

1 Both are names of famous Japanese corporations.

Y. Jin, Y. Matsuo, and M. Ishizuka

would describe the relations. Considering that media eects influence the number of
web pages, co-occurrence of names on the web is not always available to represent the
relational strength of two entities.

For the second reason, co-occurrence-based methods function ineectively when applied to inhomogeneous communities. An inhomogeneous community means, in this
paper, a community that includes people in dierent fields, dierent nations, or dierent
cultures, where a relation is dicult to obtain using a single criterion. The researchers
communities (of the same research field) usually present a homogeneous character; for
that reason, using a single criterion to calculate the relation works well. In contrast,
the international artist community is more inhomogeneous. For example, two Japanese
artists, Taisuke Abe and Jun Oenoki, have no prior relationship, but their Jaccard
coecient is high: 0.024. Two international artists Beat Streuli from Switzerland and
Nari Ward from Jamaica have co-participated in several exhibitions, but their coefficient is low: 0.0009. This happens because the community consists of many people
from dierent contexts. For that reason, it is dicult to precisely recognize the relation
using a single criterion.

We consider that the precedent studies on the research domain implicitly use the

following two assumptions:
Assumption 1. Generally, web pages are created according to results of two actors
co-participation in events. Therefore, the number of web pages is assumed to show
a useful correlation to the strength of two actors.

Assumption 2. A community to be extracted as a social network is assumed to be

homogeneous.

In the following section, we will introduce our improvements, relation identification
(in Section 3.2) and threshold tuning (in Section 3.3), which respectively mitigate violations of these assumptions. Furthermore, to emphasize the eectiveness of our methods,
we apply each method to our case studies: Extracting social networks of firms (in Section 4) and artists (in Section 5). A general extraction model bundling these dierent
extraction methods will be described in Section 6.

3.2 Relation Identification

In social sciences, the definition of a weak or strong tie might vary among contexts
[14]. For example, the frequency or degree of relations aects that strength; multiple
relations between two actors also can imply a stronger tie. In the firm case, the types of
relations define the strength: For example, a capital alliance relation is stronger than a
business alliance relation. Consequently, to present a tie among firms, it is appropriate
that we identify the concrete relations of firms. As a solution, we add some word or
combination of words to a search query. Using this strategy, we can eciently identify
relations among firms. For example, when we wish to extract lawsuit relations, we add
a term lawsuit. We issue a query Matsushita AND JustSystem AND lawsuit so that
the search engine will return the lawsuit pages that are associated with the two firms.
Then we can conduct text processing to these pages to validate the relations existence.
This idea is similar to keyword spices [20], which extend queries for domain-specific
web searches. Question answering systems also construct elaborate queries for using
search engines [21].
?

?

?
We call the keyword to be added a relation keyword. By adding relation keywords,
we can extract particular relations among entities, which can be a solution for validation
of Assumption 1. Below, we explain some issues about relation types and extraction of
relation keywords.

Relation Types. It is considered that a pair of entities has multiple relations. For ex-
ample, two firms share alliance and lawsuit relations. Each relation is typed in a more
detailed way. Alliance relations between firms include capital alliances and business al-
liances, where the former usually represents a stronger relation than the later. A lawsuit
relation has multiple stages: at some time, it will be settled by mutual accommodation
or by final judgement. Consequently, the relation can be typed into the claim phase and
the accommodation phase. For dynamic and complex relational networks, it is important to distinguish such typical and temporal relations for detailed analyses of social
networks [14,22].

Relation Keyword Extraction. To extract particular types of relations between firms,
we need some relation keywords. The intuitive method for finding relation keywords is
to select terms that appear often in target pages (where the target relation is described)
and which do not appear in other pages. Therefore, as a training corpus, we must collect annotated web pages that describe specific relations of the firms. Once we find
appropriate relation keywords, we can extract the relations among many firms.

Collecting and annotating the training corpus requires many hours of tedious work.
In our study, we also try to use a search engine to extract relation keywords. This method
is identical to that of Moris work [19], in which a specific word wc is assigned, which
can represent the relation most precisely. If we want to retrieve an alliance relation,
we add alliance (denoted as wc) to a search query; words that co-occur frequently
with it also become good clues to discern the relation. We use the Jaccard coecient
nwcwnwcw to measure relevance of word w to word wc. The words w with large Jaccard
coecients are also used as relation keywords aside from wc. It would saves costs of
annotating training data with relevance or non-relevance manually.

3.3 Threshold Tuning

In studies of social network analysis, network questionnaires have traditionally been
conducted. Typically, participants are asked Please name your four closest friends.
The respondents would then list the relations that are personally important. In other
words, the relation is recognized by a subjective criterion for each participant. We propose to use this subjective criterion for the solution against Assumption 2. For example,
even if the relation between Beat Streuli and Nari Ward is weaker than the objective
standard, it is important to Beat Streuli if there are no other persons with a stronger
relation. Consequently, we might add an edge between them.

We employ two criteria that correspond to objective and subjective importance of relations for actors. We first invent edges using objective criteria with a consistent threshold T . Then we invent edges using subjective criteria for actors who have no certain
number M of edges. This procedure alleviates the problem of some nodes having too
many edges and some nodes being isolated. The combination of two criteria enables

Y. Jin, Y. Matsuo, and M. Ishizuka

Maxtor

Minebea

Zhongcai

JapanCeraTech

Kyocera

NipponBroadcasting

LGElectronics

Nidec

Biomatics

IBMJapan

MatsushitaIndustrial

MihonCeratec

Sourcenext

Elecom

Victor

Fuji

ConnectTechnologies

Transware

Rakuten

Livedoor

Matsushita

SeikoEPSON

Toshiba

Justsystem

Intersil

Agere

Hynix

Towa

Ebank

TokyoElctric

Renesas

TokyoBectron

SumitomoMituiCard

Fujitsu

Hitachi

Opera

Niws

Kainos

Nifty

Intel

LiveRevolution

UFJNicos

Samsung

SixApart

Cybird

FujituoSoftware

Nikon

Broadcom

CanonStaar

Nidek

Alliance

Capital Alliance
Business Alliance

Lawsuit
Claim
Accommodation

Fig. 1. Social network of 60 firms in Japan

more exhaustive extraction for every node than the previous method, although it sometimes yields low precision. For that reason, we must find the appropriate parameters so
that the target network is extracted as precisely as possible.
Setting Parameters for Each Community. Parameters vary according to the domain
of a community. For example, T in the researcher community might be higher than that
in artist community, simply because researchers names are more likely appear on the
web than artists names. Therefore, some training data are necessary to learn the appropriate values for each target community. Simply, the parameters are tuned so that the
performance of relation identification is maximized: We maximize the F-value. More
eective ways to determine the parameters are bootstrapping or user interaction. For the
bootstrapping method, we can repeat the sampling and estimation process to determine
parameters; for the user interaction method, we can use the users feedback to reconstruct
the network dynamically using the best parameters that can maximize the F-value.

4 Social Network Extraction for Firms

We describe the extraction of a firm network as a case study of relation identification (mentioned in Section 3.2). Many relationships among firms are published in news
?

?

?
Table 1. Relation keywords extracted from the web using Jaccard coecient

tw

Capital alliance

Alliance relation
alliance AND corporate 1000 operation AND capital 1000 alliance AND business 1000
0878 capital AND manage 0553 alliance AND company 0475
alliance AND stock
alliance AND company 0704 capital AND company 0548 alliance AND operation 0459
0543 alliance AND develop 0437
alliance AND system
alliance AND business 0534 capital AND manage 0533 alliance AND company 0432

Business alliance

capital

0565

tw

tw

tw

Lawsuit relation
1.000 lawsuit AND accommodate 1.000
1.000
violate AND lawsuit
0.514
0.533 accommodate AND company 0.648
violate AND claim
0.490 sue AND technology 0.486 accommodate AND announce 0.646
violate AND judge
0.458
sue AND develop 0.483 accommodate AND develop 0.641
violate AND court
violate AND indemnify 0.444 sue AND relevance 0.469 accommodate AND product 0.640

Claim phase
violate AND sue
patent AND sue

Accommodation phase

tw

tw

articles and on news releases that are distributed on the web. In our work, we extract
alliance and lawsuit relations as respective representatives of positive and negative relations among firms. We further distinguish these relations into two detailed relations:
capital and business alliance relations, and claims and accommodation of lawsuit rela-
tions. A social network of 60 firms in Japan is extracted; it includes IT, communication,
broadcasting, and electronics firms. We will describe details of our system and experimental results.

4.1 System Flow

function RELAT ION EXT RACT ION (D, x, y, W)

scorexy  0
S  GetSentences(D, x, y)
for each s  S do

Our system has two major procedures: an online procedure and an oine procedure.
In the oine procedure, relation keywords for each relation are obtained beforehand
using the methods introduced in
Section 3.2. We gathered 456
pages and 165 pages for alliance and lawsuit relations, re-
spectively, from Nikkei Net and
IP News site2. As preprocess-
ing, we first eliminate all html
tags and scripts; then we extract
the body text of pages and apply
a part-of-speech tagger Chasen3
to choose nouns and verbs
(except
stop words). These
words are candidates of relation
keywords. We also use combinations of two words as candi-
dates. We measure the score of
2 Nikkei Net (http://release.nikkei.co.jp/) is a famous online business newspaper. IP News
(http://news.braina.com/judge.html) is an online news archive on intellectual property issues.

if s contains x and s contains y then
scores  wi(W) contained in s twi
if scores  scorexy then

done
if scorexy  scorethre then

Fig. 2. A procedure to extract relations by text processing

do set an edge between x and y in G

scorexy  scores

done

3 http://chasen.naist.jp/hiki/ChaSen/

Y. Jin, Y. Matsuo, and M. Ishizuka

each candidate word  phrase by calculating the Jaccard coecient with specific relation
4. Candidates with the highest scores are recognized as relation keywords.
keywords wc
Table 1 shows the top five relation keywords and their Jaccard scores denoted as tw

5.

In the online procedure, a list of firms and specific relation types is given as input; the
output is a social network of firms. Three steps exist: making queries, Google search, and
network construction. First, we make queries by adding relation keywords to each pair of
firms. We use top nq relation keywords from Table 1. Then, we put these queries into the
Google search engine to collect top-np web pages. (In this experiment, we set nq  2 and
np  5.) Lastly, for each downloaded document D, we conduct text processing to judge
whether or not the relation actually exists. A simple pattern-based heuristic (as described
in Fig. 2) is useful in our experience: We first pick up all sentences S that include the
two firm names (x and y), and assign each sentence the sum of relation keyword scores
tw in the sentence. The score of firms x and y is the maximum of the sentence scores. If
scorexy is greater than a certain threshold (in other words, if the two firms seem to have
the target relation with high reliability), an edge is invented between the two firms.

4.2 Results and Evaluation

The obtained network for 60 firms in Japan is shown in Fig. 1. Black lines represent
alliances (bold ones are capital alliances and thin ones are business alliances) and red
lines represent lawsuits (bold ones are in the claim phase and thin ones are in the accommodation phase).

Target relation
Alliance

Lawsuit

42.9% (921)

Table 2. Precision and Recall of the System

Precision
60.9% (70115) 62.0% (70113)

Recall

The precision and recall of
our system are shown in Table
2. For 60C2  1770 pairs of
firms, 113 pairs actually show
alliance relations. Our system
extracted 70 pairs correctly.
There were actually 21 and 100
pairs of capital and business al-
liances; our system extracted
9 and 60, respectively. Compared with alliances, the lawsuit relations have higher recall, probably because lawsuit relations are described in rather common formats using words such as judgment,
lawsuit, or accommodate.

capital alliance 75.0% (912)
business alliance 67.4% (6089) 60.0% (60100)

claim phase
accommodation 72.7% (811)

61.5% (1626) 100% (1616)
63.6% (1422) 87.5% (1416)

88.9% (89)

Although they are not comparable technically, we obtained alliance and lawsuit relations from Nikkei Net and IP News, and compared the precision and recall to our results.
The precision values at these sites are 100%, but the recall of alliance and lawsuit relations among 60 firms are low: 228% and 688%, respectively. This is true because
these sites deal little with information on small companies and corporations that are
capitalized with foreign capital (i.e. foreign companies).

4 We used alliance AND corporate as wc for alliance relations. Furthermore, we use the word
appearing in the first lines in Table 1 as wc for each relation: We determine these words through
preliminary experiments.

5 In our experiment, we mainly used web pages that had been composed in Japanese. For that

reason, relation keywords are translated from Japanese.
?

?

?
 0.8

 0.7

 0.6

 0.5

 0.8

 0.7

 0.6

 0.9

 0.8

 0.7

l
l

a
c
e

noW
W1
W2
W1+W2
W1+W2+noW

 0.5

 0.6

 0.9

 0.8

 0.7

noW
W1
W2
W1+W2
W1+W2+noW

 0.6
?

?

?
Number of top pages (k)
?

?

?
Number of top pages (k)

i

i

n
o
s
c
e
r

(a) Precision of retrieved pages

(b) Recall of relations

Fig. 3. Evaluation of relation keywords for lawsuit relations

Some detected relations are wrong: As one example, Hitachi and IBM are shown to
be embroiled in a lawsuit relation, but they actually are not. Our algorithm took the sentence Hitachi and HDD, a subsidiary of IBM have been sued a Chinese HDD maker
for patent violations as spurious proof of a lawsuit relation. Some relations are described using uncommon phrases (such as trouble and uproar) that do not appear often
in the training corpus. More sophisticated text processing might improve the results in
these cases.

4.3 Eectiveness of Relation Keywords

The eectiveness of relation keywords is shown in this section. We compared the information contained in retrieved pages merely by using a pair of names as a search query
to add relation keywords to the query. We compared the five methods described below:

noW: A firm pair (without relation keywords) is used as a query.
W1: A firm pair and the top-weighted relation keyword (w1) are used as a query.
W2: A firm pair and the second-weighted relation keyword (w2) are used as a query.
W1 W2: It generates two queries  W1 and W2.
W1W2noW: It generates three queries  W1, W2, and noW.

The noW is considered to be the existing method (i.e. Mika and Matsuos method).
The others are variations of the proposed method. In all cases, we downloaded the same
number of web pages. All other conditions are identical.

Figure 3 shows the results. Overall, the proposed methods perform better than the
existing method (noW) with respect to precision. The precision and recall are respectively 65.7% and 95.0% if we do not use any relation keywords. Relation keywords
improve the precision using the same number of downloaded documents. By integrating multiple queries (as W1W2noW case), we can achieve the highest precision as
71.9% while retaining high recall (92.5%).

5 Social Network Extraction for Artists

In this section, we describe the algorithm of threshold tuning (described in Section 3.3)
for extracting a social network of artists of contemporary art.

Y. Jin, Y. Matsuo, and M. Ishizuka

* First, we invent edges using two objective criteria: Tov and Tco. *. . . . . . . . . . . (step 1)
for each x  L and y  L

if (overlap(x, y) Tov AND cooc(x, y) Tco)

do set an edge between x and y in G

* Then, invent edges using two subjective criteria M1 and M2 ( M1). * . . . . . . (step 2)
for each x  L

do Yx  ConnectedNodes(x),
x  L 
 Yx
while Yx  M1 and  Yx  

 Yx  L 
 Yx,  Y 

y  argmax

overlap(x, y j),  Yx   Yx 
 y

* Yx are nodes set connected with x. *

* Yx is the number of nodes in Yx. *

if overlap(x, y) Tov OR cooc(x, y) Tco . . . . . . . . . . . . . . . . . . . . . . . . (step 2a)

do set an edge between x and y in G, Yx  Yx  y

done
while Yx  M2 and  Y 

x  

y  argmax

overlap(x, yk),  Y 

x   Y 

x 
 y

y j   Yx

yk   Y 
x

if overlap(x, y) 0 AND cooc(x, y) 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . (step 2b)

do set an edge between x and y in G, Yx  Yx  y

done

done

Fig. 4. Detailed Algorithm of threshold tuning used at the Yokohama Triennale 2005

5.1 System Flow

This system includes online and oine procedures. In the oine procedure, we tune
four parameters: Tov, Tco, M1, and M2. For them, Tov and Tco are thresholds to invent
edges by the overlap coecient and matching coecient, and M1 and M2 are the minimum numbers of edges for each node. We sample 1000 pairs of artists as training data:
146 positive examples and 854 negative examples. We change the values of parameters,
classify every pair of artists into positive and negative using the parameters, and find
the optimal values where the F-value is maximized: Tov  082, Tco  20, M1  5 and
M2  1. We try dierent settings for the four parameters; Tov is changed from 0 to 1 at
every 001, and Tco is changed from 0 to 60 in steps of 5, M1 and M2 are incremented
from 0 to 56.

For the online procedure, a list of artists names are given as input; the output is a
social network of artists. Three steps exist: making queries, Google search, and network
construction. First, we make queries for each pair of names. Then we put them into the
Google search engine to obtain the hit counts. Finally, we construct a social network
after tuning the parameters.

A detailed algorithm to generate a social network is shown in Fig. 4. Edges are
added using an objective criterion (in step 1): An edge is added between the nodes if

6 We might use more sophisticated algorithms such as hill-climbing searches. However, we do
not specifically examine the optimization method in this paper. For that reason, we employed
a simple (but reliable) approach.
?

?

?
Table 3. Maximized precision, recall, and F-value using the precedent approach

Cases Tov Tco Precision Recall F-value Extracted number* Correct number*
39 (39, 0, 0)
case (a)
146 (146, 0, 0)
case (b)
55 (55, 0, 0)
case (c)
: Numbers in brackets are numbers of edges invented in step 1, step 2a, and step 2b.

92.9% 26.7% 0.41
14.6% 100% 0.25
76.4% 37.7% 0.50

42 (42, 0, 0)
1000 (1000, 0, 0)
72 (72, 0, 0)

0.24 30

0.05 20

Table 4. Maximized precision, recall, and F-value using the proposed approach

Cases Tov Tco M1 M2 Precision Recall F-value Extracted number Correct number
277 (42, 227, 8)
95 (39, 54, 2)
1000 (1000, 0, 0) 146 (146, 0, 0)
72 (55, 17, 0)
249 (23, 212, 14) 108 (19, 84, 5)

34.4% 65.1% 0.45
14.6% 100% 0.25
55.4% 49.3% 0.52
43.4% 74.0% 0.55

case (a) 0.24 30 3

case (b)
case (c) 0.05 20 1
0.82 20 5
case (d)

130 (72, 58, 0)
?

?

?
 0.8

 0.6

 0.4

 0.2

Precision
Recall
F-value

 0.2
 0.8
Threshold of Overlap coefficient Tov (Tco=20)

 0.4

 0.6

 0.8

 0.8

 0.6

 0.6

 0.4

 0.4

 0.2

 0.2

Precision
Recall
F-value

 0.2

 0.8

Threshold of Overlap coefficient Tov (Tco=20,M1=5,M2=1)

 0.4

 0.6

 0.8

 0.6

 0.4

 0.2

(a) Existing algorithm

(b) Proposed algorithm

Fig. 5. Precision, recall and F-value for dierent Tov

the overlap coecient and the matching coecient are both over the thresholds. Then
subjective criteria are used to add edges (in step 2): If node x has less then M1 edges, we
choose nodes that have the strongest relations with node x. Node x is connected to the
other nodes until the number of edges reaches M1 (in step 2a). After that, if node x has
no M2 edges yet, we add edges in descending order of overlap coecient (in step b).

Although the algorithm is highly customized for dealing with web information, the
concept is simple. We use the objective criteria (using Tov and Tco) first, and the subjective criteria (using M1 and M2) subsequently. It is important to combine multiple criteria
to infer the relations among artists correctly from the available web information.

5.2 Evaluation

The existing approach by Mika and Matsuo generates a social network based on an
objective criterion with a predefined threshold. It corresponds to the case where M1  0
and M2  0 in our algorithm. To compare the existing method with our method, we
tune Tov and Tco so that precision, recall, and F-value are maximized, respectively. The
results are shown in Table 3. The maximal recall is 100% by setting Tov and Tco as zero
(which means the algorithm recognizes all the pairs having a relation), which yields
precision as low as 146%. Conversely, the maximal precision is 92.9% when the recall
is as low as 26.7%. The precision is 76.4% and the recall is 37.7% when the F-value is
maximized.

Y. Jin, Y. Matsuo, and M. Ishizuka

(a) The whole network.

(b) Centering artist Curatorman.

Fig. 6. System Interface for Yokohama Triennale 2005

Our algorithm can achieve better performance in either case. Table 4 shows results
of our algorithm using four parameters. Even if we set Tov and Tco as identical to those
in Table 3, we can achieve better results by adjusting M1 and M2. The most balanced
parameters achieve F-value of 055, which is more than 0.05 points better than the
proposed algorithm. Figure 5 shows a notable dierence: the proposed algorithm produces high recall while maintaining modest precision. It is useful when the purpose is
to promote navigation and communication using a social network.

In this section, we emphasize detection of relationships using only the hit number
of search engine. This is treated as a first step in the Yokohama Triennale system. As
second step, we further identify concrete relation types from web pages retrieved by
names of artists who are considered as related; we also filter out noisy edges to improve
the precision. Details about the relation type identification are available from [16].

5.3 Navigation Site for Yokohama Triennale

Our system was put into operation on the ocial support site for Yokohama Triennale
2005 (http://mknet.polypho.net/tricosup/) to provide an overview of the artists (133
artists with 71 projects) along with informational navigation for users. At exhibitions,
it is usual for participants to enjoy and evaluate each work separately. However, our
supposition was that if participants knew the background and relations of the artists,
they might enjoy the event more. For that purpose, the system provided relations of
artists and evidential web pages for users.

The system interface is shown in Fig. 6. It was implemented using Flash display
software to facilitate interactive navigation. The system provides a retrieval function.
Information about the artist is shown on the left side if a user clicks a node. In addition,
the edges from the nodes are highlighted in the right-side network. The user can proceed
to view the neighboring artists information sequentially, and can also jump to the web
pages that show evidence of the relation.
?

?

?
6 General Extraction of a Social Network Using a Search Engine

Based on the two case studies described in the preceding sections, this section presents
and explains an architecture to support general social network extraction from the web
using a search engine. The types of social networks depend on their purpose [22]. A
good social network should represent a target domain most appropriately.

We consider that social network extraction is generally written as

f (r(X Y) 
)  0 1

(1)

r (X Y) S (2)

where r(X Y) is an m-dimensional vector space (S (1)
(X Y))
to represent various measures for X and Y in relation r. For example, S (i)
r (X Y) can be
either nXY (matching coecient), nXY nXY (Jaccard coecient), or nXY min(nX nY)
(overlap coecient). It can possibly be a score function based on sentences including
both mentions of X and Y (as the algorithm in Section 4). The parameter 
 is an n-
dimensional vector space ((1) (2)     (n)). For example, 
 can be as a combination of
Tov, Tco, M1, and M2 as the algorithm in Section 5. The function f determines whether
an edge should be invented or not based on multiple measures and parameters.

r (X Y)     S (m)
r

A social network should represent the particular relations of entities depending on
purposes. Therefore, function f should not always be the same. We must have a method
to infer an appropriate function f , thus the algorithm inevitably consists of an oine
module and an online module. Function f is learned from the training examples and
provides good classification to other examples.

In the online phase, it is important to extract a social network from the web in an
ecient manner. We must consider how to use a search engine better and how to process web documents eciently and correctly. Generally, the procedure consists of three
steps:

Making queries. Two entities are used to generate a query. Basically, we put a query
X AND Y to a search engine. In this paper, we add relation keywords to extract
a particular type of relation eciently. A combination of multiple queries might
improve the result, as explained in Section 4. Entity disambiguation is another important issue that has already been addressed in several studies [3,4].

Google search. We put the queries into a search engine. Sometimes the counts are
used to infer relational strength. In other cases, we download some documents (or
snippets) and investigate the mentions of X and Y. A good combination of Google
counts and text analysis would make the search more ecient and scalable, as discussed in [16].

Network construction. We use Google counts and downloaded text as evidence to
construct a social network. The value of function f is calculated and the existence
of an edge is determined. Usually, the obtained social network is visualized and
reviewed. Sometimes we must change settings of the algorithm (or increase the
training data) and repeat the entire process to improve the quality.

Previous studies have emphasized how to calculate the strength of two names on the
Web in the Google search step, simply using X AND Y as query and construct networks
based on objective criteria. Our method, i.e., relation identification and threshold tuning

Y. Jin, Y. Matsuo, and M. Ishizuka

Table 5. Centrality of firms in the extracted social network

Matsushita

(a) Eigenvector centrality.
Rank
Value
0.366
?

?

?
0.351
0.289

0.275

0.263

0.257
?

?

?
Just System 0.241
0.208

9 Tokyo Electric 0.207
Seiko Epson 0.204

Hitachi

Fujitsu
Toshiba
Rakuten

Name

Name

(b) Betweenness centrality.
Rank
Value
1 Matsushita 168.981

IBM 149.192
144.675
?

?

?
136.978
Hitachi

Toshiba
113.239

109.887
Rakuten
?

?

?
Just System 77.175
74.141

64.558

56.081

Livedoor

Fujitsu

are proposed for Making queries and Network construction steps respectively for
complex and inhomogeneous communities. All of these methods are combined into our
architecture of general extraction of social networks for various entities.

The obtained network is useful for Semantic Web studies in several ways. For example (inspired by [2]), we can use a social network of artists for detecting COI among
artists when they make evaluations and comments on others work. We might find a
cluster of firms and characterize a firm by its cluster. Business experts often make such
inferences based on firm relations and firm groups, so the firm network might enhance
inferential abilities in the business domain. As a related work, F. Gandon et al. built
a Semantic Web server that maintains annotations about the industrial organization of
Telecom Valley to partnerships and collaboration [8].

We present a prototypical example of applications using a social network of firms.
We calculate the centrality, which is a measure of the structural importance of a node
in the network, for each firm on the extracted social network (on alliance relations).
Table 5(a) shows the top ten firms by eigenvector centrality. These firms have remained
large and reliable corporations in Japan for decades. Table 5(b) shows the top ten by
betweenness centrality. Interestingly, IBM, Livedoor, and Cisco are on the list. These
firms might bridge two or more clusters of firms: IBM and Cisco are United States firms
and form alliances with firms in multiple clusters; Livedoor is famous for its aggressive
M & A strategy in Japan. Such information can only be inferred after extracting a social
network. There seem to be many potential applications that can make use of social
networks in the Semantic Web.

7 Conclusion

This paper describes methods of extracting various social networks from the web. To
date, numerous studies have addressed the researcher domain to estimate extraction
methods. It is an important test-bed. Nevertheless, the next step must be taken to depart from the domain of researchers. This paper steps further to show that researcher
networks might be an easy domain for social network extraction from the web. Our
method, equipped with relation identification and threshold tuning that specifically
?

?

?
focus on complex and inhomogeneous communities respectively, can extract other types
of social networks: those of firms and artists. We show various evaluations of the methods along with discussions of the application of social network in the context of the
Semantic Web. The proposed architecture toward general extraction of social networks,
which bundles these dierent extraction methods, will enable us to extract various social networks from available information on the web.

In addition to some direct applications of social networks, we believe that a network point of view is important for knowledge integration and articulation and for
(lightweight) ontology emergence. The combination of social networks and ontology
emergence might prepare a fertile ground for Semantic Web research.

Acknowledgements. This research was supported by the New Energy and Industrial
Technology Development Organization (NEDO) as project ID 04A11502a.
