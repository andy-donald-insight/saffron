Web Semantics: Science, Services and Agents

on the World Wide Web 5 (2007) 5871

A survey of trust in computer science and the Semantic Web

Donovan Artz, Yolanda Gil


Information Sciences Institute, University of Southern California, 4677 Admiralty Way, Marina del Rey, CA 90292, United States

Received 9 February 2006; accepted 23 March 2007

Available online 31 March 2007

Abstract

Trust is an integral component in many kinds of human interaction, allowing people to act under uncertainty and with the risk of negative
consequences. For example, exchanging money for a service, giving access to your property, and choosing between conflicting sources of information
all may utilize some form of trust. In computer science, trust is a widely used term whose definition differs among researchers and application
areas. Trust is an essential component of the vision for the Semantic Web, where both new problems and new applications of trust are being studied.
This paper gives an overview of existing trust research in computer science and the Semantic Web.
 2007 Published by Elsevier B.V.

Keywords: Trust; Web of trust; Policies; Reputation

1. Introduction

Trust is a central component of the Semantic Web vision
[13]. The Semantic Web stack [3,4] has included all along a
trust layer to assimilate the ontology, rules, logic, and proof lay-
ers. Trust often refers to mechanisms to verify that the source of
information is really who the source claims to be. Signatures and
encryption mechanisms should allow any consumer of information to check the sources of that information. In addition, proofs
should provide a tractable way to verify that a claim is valid.
In this sense, any information provider should be able to supply
upon request a proof that can be easily checked that certifies the
origins of the information, rather than expect consumers to have
to generate those proofs themselves through a computationally
expensive process. The web motto Anyone can say anything
about anything makes the web a unique source of information,
but we need to be able to understand where we are placing our
trust.

Trust has another important role in the Semantic Web, as
agents and automated reasoners need to make trust judgements
when alternative sources of information are available. Computers will have the challenge to make judgements in light of the
varying quality and truth that these diverse open (unedited,
uncensored) sources offer. Today, web users make judgments


Corresponding author. Tel.: +1 310 822 1511; fax: +1 310 823 6714.
E-mail addresses: dono@isi.edu (D. Artz), gil@isi.edu (Y. Gil).

1570-8268/$  see front matter  2007 Published by Elsevier B.V.
doi:10.1016/j.websem.2007.03.002

routinely about which sources to rely on since there are often
numerous sources relevant to a given query, ranging from institutional to personal, from government to private citizen, from
objective report to editorial opinion, etc. These trust judgements
are made by humans based on their prior knowledge about a
sources perceived reputation, or past personal experience about
its quality relative to other alternative sources they may con-
sider. Humans also bring to bear vast amounts of knowledge
about the world they live in and the humans that populate the
web with information about it. In more formal settings, such
as e-commerce and e-science, similar judgments are also made
with respect to publicly available data and services. All of these
important trust judgments are currently in the hands of humans.
This will not be possible in the Semantic Web, where humans
will not be the only consumers of information. Agents will
need to automatically make trust judgments to choose a service or information source while performing a task. Reasoners
will need to judge which of the many information sources avail-
able, at times contradicting one another, are more adequate for
answering a question. In a Semantic Web where content will be
reflected in ontologies and axioms, how will a computer decide
what sources to trust when they offer contradictory information?
What mechanisms will enable agents and reasoners to make trust
judgments in the Semantic Web?

Trust is not a new research topic in computer science, spanning areas as diverse as security and access control in computer
networks, reliability in distributed systems, game theory and
agent systems, and policies for decision making under uncer-

tainty. The concept of trust in these different communities varies
in how it is represented, computed, and used. While trust in the
Semantic Web presents unique challenges, prior work in these
areas is relevant and should be the basis for future research.

This paper provides an overview of trust research in computer
science relevant to the Semantic Web. We focus on relating how
different areas define and use trust in a variety of contexts. The
paper begins with a general discussion and definitions of trust
from the literature. It describes reputation and policies as two
broad categories of research to model trust. It then discusses a
third category of trust research in designing general computational models of trust. The fourth and final category of research
surveyed is trust in information sources. Along the way, we discuss the relevance of the work presented to ongoing and future
Semantic Web research.

2. Modeling and reasoning about trust

Many have recognized the value of modeling and reasoning about trust computationally. A wide of variety of literature
now exists on trust, ranging from specific applications to general
models. However, as many authors in the field have noted, the
meaning of trust as used by each researcher differs across the
span of existing work. In order to give the reader a reference
point for understanding trust, we offer three general definitions
from existing research. The first definition, from Mui et al. [5],
refers to past encounters, and may be thought of by some as
reputation-based trust:

[Trust is] a subjective expectation an agent has about
anothers future behavior based on the history of their
encounters.

The next definition, from Grandison and Sloman [6], introduces context and is unique in referring to the competence to
act (instead of actions, themselves):

[Trust is] the firm belief in the competence of an entity to act
dependably, securely, and reliably within a specified context.

The third definition, from Olmedilla et al. [7], applies to many
cases in this survey, and it refers to actions and not competence
like the previous definition:

Trust of a party A to a party B for a service X is the measurable belief of A in that B behaves dependably for a specified
period within a specified context (in relation to service X).

A unifying theme is that trust is only worth modeling when
there is a possibility of deception, that is, when there is a chance
of a different outcome than what is expected or has been agreed
upon.

Two common ways of determining trust are through using
policies or reputation. We adopt these categories from Bonatti et al. [8], as they best describe the distinction we observe
between the hard evidence used in policies, and the estimation
of trust used in reputation systems. Policies describe the conditions necessary to obtain trust, and can also prescribe actions
and outcomes if certain conditions are met. Policies frequently
involve the exchange or verification of credentials, which are

information issued (and sometimes endorsed using a digital sig-
nature) by one entity, and may describe qualities or features of
another entity. For example, having the credential of a university
degree means its holder has been recognized by the issuing university as having a certain education level. This associates the
holder with the university and to those educated in his field. Credentials can be used when trust in the entity itself is unknown, but
there is existing trust in what is associated through the entitys
credentials.

Reputation is an assessment based on the history of interactions with or observations of an entity, either directly with the
evaluator (personal experience) or as reported by others (recom-
mendations or third party verification). How these histories are
combined can vary, and recursive problems of trust can occur
when using information from others (i.e., can I trust an entitys
recommendation about another entity?). At a basic level, both
credentials and reputation involve the transfer of trust from one
entity to another, but each approach has its own unique problems
which have motivated much of the existing work in trust.

Table 1 is a roadmap for this survey, and gives an overview
of research areas and references. We organize trust research in
four major areas:

(1) Policy-based trust. Using policies to establish trust, focused
on managing and exchanging credentials and enforcing access policies. Work in policy-based trust generally
assumes that trust is established simply by obtaining a sufficient amount of credentials pertaining to a specific party,
and applying the policies to grant that party certain access
rights. The recursive problem of trusting the credentials is
frequently solved by using a trusted third party to serve as
an authority for issuing and verifying credentials.

(2) Reputation-based trust. Using reputation to establish trust,
where past
interactions or performance for an entity
are combined to assess its future behavior. Research
in reputation-based trust uses the history of an entitys
actions/behavior to compute trust, and may use referralbased trust (information from others) in the absence of (or in
addition to) first-hand knowledge. In the latter case, work is
being done to compute trust over social networks (a graph
where vertices are people and edges denote a social relationship between people), or across paths of trust (where
two parties may not have direct trust information about each
other, and must rely on a third party). Recommendations are
trust decisions made by other users, and combining these
decisions to synthesize a new one, often personalized, is
another commonly addressed problem.

(3) General models of trust. There is a wealth of research on
modeling and defining trust, its prerequisites, conditions,
components, and consequences. Trust models are useful for
analyzing human and agentized trust decisions and for operationalizing computable models of trust. Work in modeling
trust describes values or factors that play a role in computing
trust, and leans more on work in psychology and sociology for a decomposition of what trust comprises. Modeling
research ranges from simple access control polices (which
specify who to trust to access data or resources) to anal-

D. Artz, Y. Gil / Web Semantics: Science, Services and Agents on the World Wide Web 5 (2007) 5871

Table 1
A categorization of major areas of trust research

Policy-based trust

Network security credentials

(Kohl and Neuman, 1993) [9]

Trust negotiation

(Yu et al., 2001) [10]
(Yu and Winslett, 2003) [11]
(Winslett et al., 2002) [13]
(Li et al., 2003) [14]
(Nejdl et al., 2004) [15]
(Bonatti and Olmedilla, 2005) [16]
(Gandon and Sadeh, 2004) [17]
(Winsborough et al., 2000) [12]
(Seigneur and Jensen, 2004) [18]
Security policies and trust languages

(Tonti et al., 2003) [19]
(Uszok et al., 2003) [20]
(Kagal et al., 2003) [21]
(Nielsen and Krukow, 2003) [22]
(Carbone et al., 2003) [23]
(EHR Policy, 2001) [24]
(XACML, 2005) [26]
(SAML, 2005) [27]
(WS-Trust, 2005) [28]
(Becker and Sewell, 2004) [25]
(Leithead et al., 2004) [29]
Distributed trust management

(Blaze et al., 1996) [31]
(Blaze et al., 1999) [32]
(Chu et al., 1997) [33]
(Kagal et al., 2002) [34]

Effect of credential type

(Zheng et al., 2002) [36]

Reputation-based trust

Decentralization and referral trust

(Abdul-Rahman and Hailes, 1997a) [37]
(Abdul-Rahman and Hailes, 1997b) [38]
(Yu and Singh, 2000) [39]
(Yu and Singh, 2002) [40]
(Yu and Singh, 2003) [41]
(Sabater and Sierra, 2002) [42]
(Beth et al., 1994) [43]
(Xiao and Benbasat, 2003) [44]
(ODonovan and Smyth, 2005) [45]

Trust metrics in a web of trust

(Goldbeck and Hendler, 2004a) [51]
(Goldbeck and Hendler, 2004b) [52]
(Stewart, 1999) [53]
(Stewart and Zhang, 2003) [54]
(Richardson et al., 2003) [55]
(Masa and Avesani, 2005) [56]
(Guha et al., 2004) [57]
(Advogato, 2000) [58]
(Chirita et al., 2004) [59]
(Ding et al., 2004) [60]

Trust in P2P networks and grids

(Kamvar et al., 2003) [47]
(Cornelli et al., 2002) [48]
(Aberer and Despotovic, 2001) [49]
(Damiani et al., 2002) [50]
(Olmedilla et al., 2005) [7]

Application-specific reputation

(Pirzada and McDonald, 2004) [61]
(Dash et al., 2004) [62]
(Josang and Ismail, 2002) [63]

Table 1 (Continued )

General models of trust

General characteristics of trust

(McKnight and Chervany, 1996) [64]
(Gefen, 2002) [65]
(Acrement, 2002) [66]
(Mui et al., 2002) [5]
(Staab et al., 2004) [67]

Computational and online trust models

(Marsh, 1994) [68]
(Ziegler and Lausen, 2005) [69]
(Resnick et al., 2000) [70]
(Friedman et al., 2000) [71]
(Falcone and Castelfranchi, 2004) [72]
(Jonker et al., 2004) [73]

Game theory and agents
(Buskens, 1998) [74]
(Brainov and Sandholm, 1999) [75]
(Ashri et al., 2005) [76]
(Ramchurn et al., 2003) [77]
(Huynh et al., 2004) [78]

Software engineering

(Viega et al., 2001) [82]

Trust in information resources

Trust concerns in the Web

(Khare and Rifkin, 1997) [83]
(Grandison and Sloman, 2000) [6]
Trust concerns in the Semantic Web

(Bizer and Oldakowski, 2004) [84]
(Berners-Lee, 1999) [1]
(OHara et al., 2004) [85]

Trust using hyperlinks

(Gyongy et al., 2004) [86]
(Massa and Hayes, 2005) [87]
(Brin and Page, 1998) [46]
(Kleinberg, 1999) [88]

Filtering information based on trust

(Ciolek, 1996) [89]
(Clarke et al., 2001) [90]
(Downey et al., 2005) [91]

Filtering the Semantic Web
(Bizer et al., 2005) [92]
(Ding et al., 2003) [93]
(Ding et al., 2005) [94]
(Ziegler, 2004) [95]

Subjectivity analysis

(Riloff et al., 2005) [97]
(Stoyanov et al., 2005) [98]
(Cardie et al., 2004) [99]

Provenance information

(McGuinness, 2005) [100]
(Golbeck, 2006) [101]
(Zhao et al., 2004) [102]
(Wong et al., 2005) [103]
(Kim et al., 2007) [104]

Content trust

(Gil and Ratnakar, 2002) [107,108]
(Chklovski et al., 2003) [109]
(Castelfranchi et al., 2003) [110]
(Gil and Artz, 2006) [111]
Site design and human factors
(Sillence et al., 2004) [112]
(Stephens, 2004) [113]
(Corritore et al., 2001) [114]

yses of competence, beliefs, risk, importance, utility, etc.
These subcomponents underlying trust help our understanding of the more subtle and complex aspects of composing,
capturing, and using trust in a computational setting.

(4) Trust in information resources. Trust is an increasingly common theme in Web related research regarding whether Web
resources and Web sites are reliable. Moreover, trust on
the Web has its own range of varying uses and meanings,
including capturing ratings from users about the quality
of information and services they have used, how web site
design influences trust on content and content providers,
propagating trust over links, etc. With the advent of the
Semantic Web, new work in trust is harnessing both the
potential gained from machine understanding, and addressing the problems of reliance on the content available in the
web so that agents in the Semantic Web can ultimately make
trust decisions autonomously. Provenance of information is
key to support trust decisions, as is automated detection of
opinions as distinct from objective information.

In the rest of the paper, we devote a section to each of the
categories in turn, and we provide a section each on related surveys and concluding remarks. We begin with policies, followed
by reputation, due to dependencies in some of the concepts
explained. Likewise, the section on general models uses concepts from both policies and reputation research. We cover
information sources and the Web last, as we believe research
in this area is best explained with knowledge of the previous
sections. In categorizing existing work, we do not focus on the
individual key contributions, but instead on how trust is used
and defined. Many papers may fit under multiple categories, but
we have organized references in a way we think is most useful
to readers.

3. Policy-based trust

This section summarizes work using policies to establish
trust. Policies allow the expression of when, for what, and even
how to determine trust in an entity.

3.1. Network security credentials

The application of a policy is performed by considering some
set of information about an entity with regard to trust, and this
information is commonly a credential. Although the word credential is frequently used to refer to signed statements about
an entity, it lacks a precise common definition across existing
work. Many policies rely on credentials, but in general they may
utilize a broader range of information that can be used to make
trust decisions. An illustrative example of a common alternative
to a signed credential occurs in the process of logging into a com-
puter. A valid user name with a correct password must be given
to gain access. According to the systems policy, this information
proves the user is trusted by the computers administrator. At
the same time, a user must keep his password secret, as revealing it to anything other than the computer system will allow
others to use the same credential. In more complex examples, it

may be undesirable to reveal credentials to another party. When
revealing a credential, an entity may sacrifice privacy and reveal
information that may be used by others to the entitys disad-
vantage. For example, most users implicitly trust the computer
they log into, but the need to establish trust in both directions
is essential for entities providing services on the Web. Evolving
work in policies highlight a more complex problem in trust: how
much to trust another entity to see your own credentials when
you wish to earn that entitys trust.

Credentials are sometimes implemented using security certificates with digital signatures. Typically in research, a security
certificate has the primary role of having one entity vouch for
the identity of another, but does not necessarily include credential information. A certificate can be used as a credential if it
includes properties about an entity.

The well-known Kerberos protocol [9] is used to exchange
credentials. The Kerberos system uses a third party to facilitate the exchange of credentials (digital signatures) between a
user and a computer. Kerberos does not determine access rights,
but instead enables two parties to securely exchange verifiable
credentials.

3.2. Trust negotiation

An important problem in establishing trust is that revealing
a credential may incur a loss of privacy or control of infor-
mation. Winslett and co-workers [1012] have focused on the
trade-off between privacy and earning trust. In this work, trust
in a particular context is earned by revealing a certain number
and type of credentials, and privacy of credential information
is lost as the credentials are revealed. An implemented architecture based on these principles is TrustBuilder [13], which
provides mechanisms for addressing this trade-off. This work
builds on a hard security view of trust, which means trust is
established using traditional security techniques (e.g., authenti-
cation, access control, encryption, etc.). In TrustBuilder, trust is
earned when sufficient credentials are revealed (but not too many
to sacrifice privacy). Making trust decisions requires understanding the risk of revealing a credential, and the benefit of
earning trust. Also in TrustBuilder, is the concept of a credential chain, where trust is transferred transitively through
credentials (e.g., if A trusts the credentials of B, and B trusts
the credentials of C, then A may have some trust in the credentials of C). The trust management language RT0 [14] is
designed explicitly to perform credential chaining, and allows
for an efficient distributed search to find such chains. Another
system is PeerTrust [15], a more recent policy and trust negotiation language that facilitates the automatic negotiation of a
credential exchange. Following PeerTrust, is PROTUNE [16],
a provisional trust negotiation framework. PROTUNE allows
policies with provisional predicates, where actions may be
specified that will satisfy (currently unsatisfied) conditions. In a
more specific view, Gandon and Sadeh [17] have proposed using
ontologies to enable context-aware applications on the Semantic
Web. Context-aware applications will only reveal credentials in
the correct context. Others working in this area have contributed
ideas on clientserver credential exchange [12], and protect-

D. Artz, Y. Gil / Web Semantics: Science, Services and Agents on the World Wide Web 5 (2007) 5871

ing privacy through generalizing or categorizing credentials
[18].

3.3. Security policies and trust languages

Security research is responsible for many of the first models
and descriptions of trust in computer science. Trust is frequently
motivated by work in security and policy representation, and
trust and security are related, interdependent concepts with
different purposes. In Tonti et al. [19], several current policy
languages, designed for use in the Semantic Web, are compared
and contrasted. A key point in this work is that policy specification for negotiating interactions is essential for building trust, as
the rules of negotiation determine how and if trust is achieved.
In most trust-related policy languages, the type of trust in mind
is typically related to access control. A notable system designed
originally for agents, Uszok et al. [20] describes the KAoS policy language and KAoS services used to enforce its policies.
The major drive for KAoS has been to enable the use of the
same policy in distributed heterogeneous environments and to
enable dynamic policy changes. In Kagal et al. [21], a policy language (subsequently known as Rei) is described which addresses
security and privacy issues in the Semantic Web, while allowing
each entity to specify their own policy. The Rei language uses
semantic representations to separate policy from implementa-
tion, and models speech acts (to programmatically discuss
a policy at runtime) as a means of negotiation and dynamic policy
manipulation.

Several recent efforts in creating security policies have considered how to represent and express trust. In Nielsen and
Krukow [22], the authors propose trust replaces key-based secu-
rity, based on the fact that we cannot ever know everything about
everyone. Trust in this work is comprised by observations of a
user, recommendations from others about that user, and references to other sources of trust on that user. Access control is
determined by a users level of trust, and this work provides a
formal policy language in which trust can be proved. In Carbone et al. [23], trust is decomposed into different types and
qualities, yielding a policy language that allows fine-tuned control over trust decisions using lattices of relative trust values.
One example of trust in policy form is the electronic health
records policy [24] generated for use with Cassandra [25].
This policy exemplifies Cassandras role-based access control
approach to trust. Keeping trust and security separate, some
policy languages, such as the OASIS extensible access control
markup language [26], still assume trust is established through
some external system. The OASIS security assertion markup
language [27], provides a means for authentication and autho-
rization, but is not able to represent or suggest trust. As a
consequence, SAML has the prerequisite that some external
system is trusted.

To facilitate the exchange of credentials, several standards
for representation of policies and credentials have been pro-
posed. WS-Trust [28], an extension of WS-Security, specifies
how trust is gained through proofs of identity, authorization, and
performance. This work literally views trust from a hard security perspective, issuing a security token when trust is earned.

WS-Trust does not address the trust negotiation process, only
its representation.

The Cassandra system [25] uses a policy specification language that enforces how trust may be earned through the
exchange of credentials. This work is inspired by role-based
access control, a context-based system for authorization. Leithead et al. [29] uses ontologies to flexibly represent trust
negotiation policies (rules used to negotiate trust). Ontologies
have more flexibility than set standards, they simplify policy
specification, and they enable more information to be specified
to control privacy during trust negotiation.

Olmedilla [30] provides a comprehensive overview and com-

parison of policy languages.

3.4. Distributed trust management

A problem in using credentials, is that they are also subject to trust decisions (i.e., can you believe a given credential
to be true?). A trusted third party may sign credentials if it has
verified or issued them, and in practice, certificate authorities
are used to verify signatures. Even with this limited capabil-
ity, it can be undesirable to have a single authority responsible
for deciding who and when someone is trusted. This problem
is broadly described as trust management. Early work on this
problem is found in PolicyMaker [31], which called for the separation of security and trust, recognizing the problems allowing
individual systems to have separate and different trust policies
separate from the common, global authentication and security
system. Following PolicyMaker, Blaze et al. [32] presents a
system called KeyNote, which provides a standard policy lan-
guage, which is independent of the programming language used.
KeyNote provides more application features than PolicyMaker,
and the authors compare their idea of trust management with
other existing systems at the time, including REFEREE [33].
However, as seen in more recent work [34], some researchers in
security still take a hard security approach to trust (i.e., trust is
completely present or absent). Trust in this work is defined as
what is earned after identity and authorization are verified, or
rather, after credentials and their claimed association is verified.
Ruohomaa and Kutvonen [35] provides a detailed survey and

discussion of alternative approaches for trust management.

3.5. Effect of credential type

Some types of credentials affect trust more than others in certain scenarios, and this phenomenon is examined by Zheng et al.
[36] for agents playing in a variation of the prisoners dilemma.
Trust is measured as the amount of cooperation between two
users, and the types of credentials include resumes, text-chats,
and pictures of players. The results of this study show that the
type of credential affects the amount of trust or distrust received.

4. Reputation-based trust

Reputation-based trust uses personal experience or the experiences of others, possibly combined, to make a trust decision
about an entity. This section explores work in reputation-

based trust, a well-defined area of trust research in computer
science.

4.1. Decentralization and referral trust

Just as in policy-based trust, one solution to obtaining trustworthy reputation information is to consult a central, trusted
third party that has had prior experience with the entity in
question and can provide an assessment of its reputation. The
majority of existing work avoids this solution, and most research
focuses explicitly on decentralization for reputation manage-
ment. Citing the problems with hard security in traditional
mechanisms, Abdul-Rahman and Hailes [37,38] focus on providing a system in which individuals are empowered to make
trust decisions rather than rely on a centralized process. The
main contribution of this work is to describe a system where
it can be acknowledged that malicious entities coexist with the
innocent, achieved through a decentralized trust decision pro-
cess. Yu and Singh [3941] describe a decentralized solution to
reputation management, which allows agents to actively determine trust using reputation information they receive from other
agents. Reputation management avoids a hard security approach
by distributing reputation information, allowing individuals to
make trust decisions instead of a single, centralized trust management system making the decisions for them. Singh and Yu
have provided approaches to using reputation information from
external sources, weighting it by the reputation of those sources
for providing good information. In this work, a peer that provides
trust information about another peer is referred to as a witness,
and this type of information is more commonly referred to as
referral trust. Sabater and Sierra [42] also give an approach on
how to combine reputation information from the individual and
from others while paying attention to context. This enables an
agent to specify both who can be trusted and for what they can
be trusted. The idea of using referral trust is presented early in
trust work in open networks by Beth et al. [43]. This work
provides methods for computing degrees of trust in the presence
of conflicting information, also departing from the view of hard
security. Other work with referral trust includes Xiao and Benbasat [44] and ODonovan and Smyth [45] for describing how
reputation is applied to and affects recommenders.

4.2. Trust in P2P networks and grids

A target application of reputation-based trust is to address
problems of data quality in peer-to-peer (P2P) networks. There
may be no barriers or requirements to publish a file in a P2P
network, thus allowing anyone to publish anything under any
name with any level (or lack) of quality. Moreover, the availability and reliability of any given node in the network is not
guaranteed, thus possibly precluding reliable transfer of data. In
the wake of the PageRank algorithm [46] for ranking Web sites
by authority, the EigenTrust algorithm [47] computes a global
reputation value (using PageRank) for each entity. Reputation in
this work is the quality of a peers uploads (e.g., did the file successfully upload?) within a peer-to-peer network. The P2PRep
system [48] gives protocols and an algorithm for sharing rep-

utation information with peers in a peer-to-peer network. This
work also uses the idea of referral trust in its approach.

Contrasting with the work of Singh and Yu, Aberer and
Despotovic [49] claim a more scalable approach, as other
reputation-based approaches require the maintenance of a growing performance history to maintain reputation information.
While still using reputation information, this approach uses statistical analysis to characterize trust and reputation so that the
computation remains scalable. Embracing the qualities of a peer-
to-peer network to provide a more robust method of reputation
management, Damiani et al. [50] present the XRep protocol,
which allows for an automatic vote using users feedback for
the best host for a given resource.

Olmedilla et al. [7] describes the requirements in supporting
trust in virtual organizations of shared resources, discusses the
limitations of existing work on trust in the context of grid com-
puting, and argues that Semantic representations can address the
requirements outlined.

4.3. Trust metrics in a web of trust

A trust decision can be a transitive process, where trusting
one piece of information or information source requires trusting another associated source. For example, one might trust a
book and its author because of the publisher, and the publisher
may be trusted only because of the recommendation of a friend.
Winsletts work in policy-based trust uses (or refers to) credential chains (the issuer of one credential is the subject of another),
the majority of transitive trust computation has been focused on
using reputation. A key recent example of this approach is Golbeck and Hendler [51,52], which describe how trust is computed
for the application TrustMail. Reputation is defined as a measure of trust, and each entity maintains reputation information on
other entities, thus creating a web, that is called a web of trust.
The work by Golbeck and Hendler uses ontologies to express
trust and reputation information, which then allows a quantification of trust for use in algorithms to make a trust decision about
any two entities. The quantification of this trust and associated
algorithms are called trust metrics.

Given an existing quantification of trust, approaches exist to
transfer that trust to other entities, which may not have been
evaluated for trust. One area of research assumes we are given
a web of trust, where a link between two entities mean a trust
decision has been made and the value of that trust is known. How
trust decisions are made do not matter, as long as the resulting
trust values can be quantified. If there is no link between a pair
of entities, it means no trust decision has yet been made. This is
the case in which trust transitivity can be applied, a simplified
example being if A trusts B and B trusts C, then A trusts C.
Building on work in reputation management (described earlier
as empowering individual agents to make trust decisions instead
of a single, central authority making decisions for them), multiple researchers are exploring ways to transfer trust within a
web of trust. In Stewart and Zhang [53,54], a set of hypotheses
and experiments are described for testing how trust is transferred
between hyperlinks on the web. Specifically, this work examines
how much trust (in the context of a consumer trusting a busi-

D. Artz, Y. Gil / Web Semantics: Science, Services and Agents on the World Wide Web 5 (2007) 5871

ness for purchasing a product) is transferred from a trusted Web
resource to an unevaluated one. The transfer is evaluated considering differing types of links, types of resources, and types
of trust in the known source. Other more recent work looks at
how to compute trust transitivity given actual quantities for trust
or distrust. A key work in this area is Richardson et al. [55],
whose goal is to provide a means of merging trust that is robust
to noise. Emphasizing personalized trust, as opposed to globally
computed values, this approach is described as a generalization
of PageRank [46] to the Semantic web. In contrast to the EigenTrust approach described earlier, Richardson et al. [55] avoids
computing global values by altering the algorithm to produce
personalized results for each entity. Likewise, EigenTrust uses
specifically computed reputation values, and not with an arbitrarily given quantification of trust. In Massa and Avesani [56],
the problem of controversial users (those who are both trusted
and distrusted) is addressed. This work shows that the globally computed trust value (in a web of trust) for a controversial
user may not be as accurate as a locally computed value due
to the global disagreement on trust for that user. Golbeck and
Hendlers TrustMail also performs a local computation of reputation within a web of trust. A difficult problem addressed in
Guha et al. [57] is the transitivity of distrust, the main problem
being if A distrusts B and B distrusts C, we cannot say if A
trusts C. This work also evaluates and ranks several methods
for propagating trust and distrust in a given web of trust. Evaluation is performed using data from Epinions.com, a common
data set used in trust research, where users have provided trust or
distrust information about each others ability to write reviews.
Another approach to computing trust transitivity is Advogato
[58], in which maximum network flow is computed over a web
of trust to find trust between any pair of entities. An advantage
to this approach, is that it is very robust to noise and even attacks
altering the given web of trust. In Chirita et al. [59], the authors
present a method that performs a global computation on reputation values (like EigenTrust) but considers the individuals input
to the evaluation as well. This approach uses personalized page
ranks to disseminate reputation information from individuals
while considering referral trust (like P2PRep).

All of these approaches to computation over a web of trust
do not consider context, and as a result do not differentiate
between topic specific trust and referral trust. In contrast,
Ding et al. [60], presents a method of computing within a web
of trust that also considers the domain of knowledge (context),
and does so separately from referral trust. This work enumerates several kinds of referral (trust in ability to recommend) and
associative (two agents being similar) trust as a result: domain
expert (trust in an agents domain knowledge), recommendation
expert (trust in an agents ability to refer other agents), similar trusting (two agents having similar trust in other agents),
and similar cited (two agents being similarly trusted by other
agents).

4.4. Application-specific reputation

Some applications allow for unique ways to harness or use
reputation. For the application of routing in ad hoc networks

where some nodes may be more trustworthy for routing packets
than others, Pirzada and McDonald [61] present a reputationbased system for deciding which nodes in a network to use for
routing traffic. Nodes in the network can indirectly monitor the
performance of other nodes nearby, and in this application, a
node will only ever need to select a nearby host to trust. This
is a good example of a case to apply local computation of
reputation. Another specific application is Dash et al. [62] for
allocating tasks to the best performing agent (instead of agent
with best specifications, noting the difference). Using statistics
to determine reputation from past performance history, Josang
and Ismail [63] present a method to combine reputation feedback
data using a beta probability distribution.

5. General models of trust

This section summarizes work that presents a broader view
on models of trust and the properties of trust. Work in multiple,
differing fields is presented, as it is relevant to and frequently
cited by computer scientists.

5.1. General considerations and properties of trust

Several papers in social sciences, similar to this survey, have
put forth an interpretation of existing research in trust. A frequently cited work is Mcknight and Chervany [64], which is
noted for its effort to integrate existing work and for its resulting
classification of types of trust. The goal of this work was to highlight and find common ground between the many different uses
of the word trust in social sciences research. Of key impor-
tance, are the four qualities that McKnight and Chervany identify
as being significant when making a trust decision: competence
(ability to give accurate information), benevolence (willingness
to expend the effort), integrity (adherence to honest behavior),
and predictability (evidence to support that the desired outcome
will occur). Alternatively cited, is Gefen [65], which simplifies
the trust decision to three of these qualities, leaving out predictability and keeping the others. Gefen stresses the importance
of these dimensions in different uses of trust online (e.g., how
vulnerable is the agent: is he just window-shopping, or is he a
serious buyer), citing a definition from relevant research in man-
agement: trust is a willingness to be vulnerable to the actions
of another person or people. In Acrement [66], seven qualities of trust are given from a business management perspective.
These qualities share predictability and integrity with McKnight and Chervanys set, and add five more new characteristics
specific to the management domain: congruity (actions match
claims), reliability, openness (do not keep secrets), acceptance
(equal respect among diversity), and sensitivity (pay attention
to individuals). An integrated account of trust and reputation
across disciplines is given in Mui et al. [5], which explicitly
focuses on deriving a computational model accounting for current work. A key concept used is reciprocity: be nice to others
who are nice to you. This work also differentiates trust and
reputation, describes how trust can be inferred from reputation,
and proposes a probabilistic mechanism for inferring trust given
reputation and reciprocity. Staab et al. [67] is an edited series

of short articles about different ways to represent, manage, and
manipulate the properties of trust.

results from this work suggest that trust does change with different experiences, and that distrust may be harder to overcome
than one would expect.

5.2. Computational and online trust models

The widely cited 1994 Ph.D. dissertation by Marsh [68] is
considered the first prominent, comprehensive, formal, computational model of trust. His intent was to address an imperfect
understanding, a plethora of definitions, and informal use in the
literature and in everyday life with regard to trust. Marsh proposed a set of (subjectively set) variables, and a way to combine
them to arrive at one continuous value of trust in the range [1,
1]. While the intuitive explanation of this range may be complete distrust to full trust, Marsh actually argues against these
meanings at the extremes, saying neither full trust or distrust is
actually possible. Marsh identified three types of trust: basic,
over all contexts; general, between two people and all their contexts occurring together; and situational, between two people in
a specific context. In addition to context, Marsh also identified
time as being relevant to each of the variables used to comprise
trust. Authors who cite Marsh frequently use a simplification of
his work (e.g., trust is a continuous value, and its composition is
not of concern) or do not follow his model due to the difficulty
of finding values for some variables used to compute trust (e.g.,
importance, utility, competence, risk, etc.).

Many researchers have endeavored to model and explain the
properties of trust and reputation in a computational setting.
Different trust metrics are compared against several features
in Ziegler and Lausen [69], where the concept of local group
trust computation is advocated (a compromise between local
and global trust computation). The authors make the claim that
trust is a subjective expectation. A method for performing
local group trust computation, Appleseed, is proposed, and the
authors also discuss the meaning and propagation of distrust.
Creating a clearer picture for reputation, Resnick et al. [70]
describes reputation as important for fostering trust among
strangers. This work outlines the qualities of reputation that
make it valuable for us on the Internet, and identifies issues
in applying reputation (e.g., what reputation does a new user
have?). In Friedman et al. [71], a general discussion of trust
on the Internet is given, outlining 10 characteristics of trust in
an online interaction. A key point presented is that simply performing a task is not the same as providing good service or
being high quality, which is a problem with automated reputation systems that fail to capture this subtle difference. Also
made prominent is the idea that people trust people, not tech-
nology, which itself earns (or loses) our trust as an extension of
trust in people. In Falcone and Castelfranchi [72], a key idea is
dealing with the dynamic nature of trust, and making the realization that an agent that knows hes trusted may act differently
from one who does not know his level of trust. As a result, this
work attempts to show that good reputation is useless without
knowledge of the context in which that reputation was earned
(e.g., was the agent behaving just to look good?). Looking at
another aspect of trust dynamics, Jonker et al. [73] reports on
human experiments showing how positive and negative experiences can change negative and positive trust, respectively. Key

5.3. Game theory and agents

Autonomous agents and multi-agent systems have several
uses for trust, and one perspective in related research is game
theory. In Buskens [74], the author is a sociologist using a game
theoretic approach to show that his proposed heuristics can measure a type of trust from the graph of a social network. Buskens
uses a variant of the Trust Game, which is analogous to the
prisoners dilemma, but set in a market scenario. Another work
using game theory is Brainov and Sandholm [75], which shows
that underestimating trust hurts all agents involved, and utility
is maximized if the level of trust is mutual. The game defined by
this work is again a market-based scenario, where the players
are a buyer and a seller. This is another work in which trust is
claimed to be a way to deal with uncertainty. Using relationships between agents, Ashri et al. [76] claims that rules of trust
can be determined from the context and the roles of interacting
agents. Specifically mentioned are the general relationships of
trade, dependency, competition, and collaboration. In this work,
trust exists when it is believed that one agent will not gain at the
disadvantage of another agent. Trust in Ramchurn et al. [77] is an
expectation of agents to exhibit a specific behavior in an interaction based on reputation from various sources. The main focus of
this work is combining the sources of reputation, and they refer
to direct experience as confidence. The FIRE model, presented
in Huynh et al. [78], is designed for combining multiple sources
of trust (reputation, context-based rules, and credentials) in an
agent system. A key part of this model is the use of references
(endorsements of trust from other agents), in cases where no
reputation or other sources of trust exist. This feature enables
FIRE to provide a trust metric in cases where other models fail
due to ignorance about an agent.

Sabater and Sierra [79] and Ramchurn et al. [80] both provide excellent in-depth surveys of trust in multi-agent systems.
Josang et al. [81] provides an overview of trust in web communities interacting through market-like systems and services.

5.4. Software engineering

In the domain of software engineering, Viega et al. [82]
declares that trust is a critical consideration citing the trust
assumptions (e.g., that a user will enter a certain input) commonly made when developing software. This work also notes
that trust is used to deal with uncertainty, when specific requirements are unknown, and the contribution is to describe where
requirements can fail to make trust explicit.

6. Trust in information resources

This section summarizes relevant work in web and document retrieval, information filtering, representing the sources of
information as its provenance trail, and other factors in trusting
content of information resources.

D. Artz, Y. Gil / Web Semantics: Science, Services and Agents on the World Wide Web 5 (2007) 5871

6.1. Trust concerns on the web

Trust on the Web may refer to several different problems,
and one perspective on this is given in Khare and Rifkin [83].
This work begins by noting a flawed assumption that cryptography provides trust, and continues to point out various
applications on the Web that require different kinds of trust.
The main contribution of Khare and Rifkin is identifying the
distinctions between types of agents, policies, and applications
with regard to trust management on the Web. Focusing on trust
in Internet applications, which exchange or display informa-
tion, Grandison and Sloman [6] give a provisional definition
and discussion of trust across a wide set of literature, and explore
solutions and applications of trust management (which in their
work essentially means the implementation of security policies).
The authors make an interesting deviation from the definition of
trust we offered in the introduction; they define trust as a belief
in an entitys ability and not directly a belief in how an entity
will perform.

6.2. Trust concerns on the Semantic Web

Declaring that there is more to trust than reputation, Bizer and
Oldakowski [84] make several claims with the Semantic Web
in mind. First, any statements contained in the Semantic Web
must be considered as claims rather than facts until trust can be
established. Second, this work makes the case that it is too much
of a burden to provide trust information that is current. Third,
context-based trust matters; in this case, context refers to the
circumstances and associations of the target of the trust decision.
An example of context is an agent providing a description for an
item, where the agent may be a vendor selling that item, or as a
consumer advocate reporting on that item. Fourth, it is possible
to use content-based trust, using common sense rules of the
world to make a trust decision (e.g., do no trust prices below
50% of the average price). Finally, Bizer and Oldakowski recall
Tim Berners-Lees Oh yeah? button [1], where he envisioned
functionality in Web browsers that when invoked would give
reasons why a Web page or service should be believed. Bizer
and Oldakowski build on this idea to provide the justification for
trust, which will be needed on the Semantic Web. Noting that
trust is at the heart of the Semantic Web vision, OHara et al.
[85] name five trust strategies for agents using the Semantic Web:
optimism, pessimism, centralized, investigation, and transitivity.
Optimism is to assume trust, pessimism is to assume distrust,
centralized is to trust through a single third party, investigation
is to collect trust information from others, and transitivity is
to use a web of trust. This work refers to trust generally as a
method of dealing with uncertainty.

6.3. Trust using hyperlinks

Work exists in learning users trust in Web sources using the
link structure of the Web to transfer trust. Given a small data
set of decisions made by users about whether or not Web sites
are spam, TrustRank [86] uses the link structure to other pages
to determine whether or not they are also spam. The decision

can be interpreted as a trust decision in the context of finding
true and accurate sources of information. Massa and Hayes [87]
address the problem of assuming that all Web links are positive endorsements (and indications of trust). Algorithms such as
Googles PageRank [46] make this assumption, which does not
always hold true. Massa and Hayes propose a minor addition to
HTML, enabling the author to specify whether a link is posi-
tive, negative, or neither. Kleinberg [88] makes the observation
that links encode a human judgment that one page is related to
another. Kleinberg describes the concepts of a hub and an author-
ity, the former being a page that points to many authorities, and
the latter being a page that is pointed to by many hubs. The
PageRank algorithm exploits Kleinbergs ideas of using links as
human encoded judgments of relevance and uses the concept of
authorities to compute a heuristic of popularity.

6.4. Filtering information based on trust

Work in information filtering has addressed some of the same
problems as work in trust. The concept of quality is a common
goal (i.e., high quality information), as quality often correlates with trust. Quality on the Web is discussed in detail in
Ciolek et al. [89], highlighting that massive amounts of Web
content are becoming outdated with the rapid pace of change
in the Web. More recently, the field of question answering is an
area of research that may use the Web as a source of answers for
given queries. In Clarke et al. [90], it is noted that many answers
may be returned for a given query, and one of these must be
selected as the answer. While trust is not mentioned, the problem can be characterized as determining which answer to trust.
The proposed solution is to assume the answer occurring most
frequently is correct. In the field of information extraction, the
goal is to extract information from unlabeled text (i.e., without
semantic markup). One question arising from this work is can
an automatically determined label be trusted?. A model given
in Downey et al. [91] shows that the magnitude of redundancy
of information (i.e., the frequency of occurrence) can be used as
metric for the accuracy (or trustworthiness) of a computed label.

6.5. Filtering the Semantic Web

Information filtering is becoming an increasingly significant
area of research as the amount of information available, specifically on the Web, continues to grow. After relevant information
is filtered, there is still a question of whether that information
can be trusted. In many cases, filtering still results in too much
relevant information, and the most trusted source or content is
desired. For the Semantic Web, Bizer et al. [92] have created
a browser, which filters content based on a user specified pol-
icy. These policies, written in the TriQL.P query language, allow
specification of requirements for the context, content, and source
of information. The implementation of this browser includes a
mechanism that displays to the user justification of why a Web
site should be trusted. In Ding et al. [93], agents are enabled to
use both context and reputation to determine what information
to trust in the Semantic Web. This work employs referral trust to
collect reputation, and it relies on the richness of the Semantic

Web to determine context. The result is the ability to ask another
agent which agent can I trust to get the weather?. In related
work, Ding et al. [94] provides a method for picking information sources using both provenance and computation over
a web of trust. Assuming that provenance can be determined, a
method is given for using this information to filter more trusted
sources. The work from this group also incorporates the concept of ignorance (i.e., not having any information about trust).
Recommender systems are common on the Web, and may filter
information based on recommendations and/or trust ratings. An
example considering the Semantic Web is Ziegler [95], where a
taxonomy is used to score the similarity between profiles of
users interests. Trust values, or recommendations, are computed
within a group of similar users, and the resulting information
is filtered accordingly.

6.6. Subjectivity analysis

Although information retrieval pioneered some of

the
approaches used now on the Web for locating relevant information sources, trust-based retrieval is a relatively recent focus in
that area of research. Trust in information retrieval is motivated
by the need for not just relevant documents, but high-quality
documents as well [96]. One approach to this is subjectivity
analysis, which aims at distinguishing true facts from subjective
opinions [97].

Trust is also an important area in question answering, since
contradictory answers can be obtained from diverse sources in
answer to a question. Sometimes opinions are often filtered out in
question answering tasks so that only objective facts are returned
as answers [98]. In other contexts, detecting opinions is useful
when no single ground truth can be provided in answer to a
question, and instead multiple perspectives are summarized as
the answer provided [99].

6.7. Provenance of information

The details regarding the sources and origins of information
(e.g., author, publisher, citations, etc.) are referred to as prove-
nance, and they serve as a means to evaluate trust. Provenance
representation and tracking has been studied in the context of
information sources. McGuinness [100] uses semantic annotations to represent the provenance of any results inferred by
reasoners, including explanations of reasoning steps and axioms
utilized, as well as descriptions of the original data sources. In
Golbeck [101], both provenance and the Semantic Web are used
to infer trust relationships. Provenance establishes a relationship
between people and information, and the Semantic Web contains
social network data used to compute trust between people.

Provenance has been studied in the context of scientific data
analysis, especially when generated by simulation and computa-
tion. Semantic Web technologies have shown to be effective in
representing application-relevant provenance information that
explains how results are obtained through workflows of computations [102104]. Simmhan et al. [105] and Moreau and
Ludaescher [106] provide overviews of provenance research in
scientific applications.

6.8. Content trust

Trust on the Web is needed to make decisions when information conflicts or is non-authoritative. In Gil and Ratnakar
[107109], a system called Trellis is introduced, which derives
consensus trust on information resources in a community of
users as they use or dismiss sources for information analysis
tasks. Also examining trust in information sources, Castelfranchi
et al. [110] proposes a model for making trust decisions about
sources, differentiating internal and external attributes affecting
trust in a source. The authors note that the composition of inputs
to a trust decision affects the outcome of the decision, and thus
the decision itself cannot be characterized by a final probabil-
ity. This observation might be restated that the inputs together
form part of the context in which trust is being determined. Also
acknowledged is that attribution of trust is a very complex task,
a problem that is exemplified on the Web, as the sources behind
information are not always clear or correct. Specifically for trust
in information sources, four types of inputs to a trust decision
are given: direct experience, categorization (generalization to
or from something known), reasoning (application of common
sense or rules to verify truth), and reputation. Gil and Artz [111]
differentiates trust in a given source from trust in a specific piece
of content provided by that source, where trust in one does not
always indicate trust in the other. For example, a trusted source
may inadvertently issue a patently false statement, or a typically
distrusted source may post information that is trustworthy. A
key focus of this work is content trust, how it may be derived,
and how it may be captured and used on the Semantic Web.

6.9. Site design and user interaction

The elements of a Web site considered by users when making
trust decisions on the Web is explored in Sillence et al. [112].
A key finding is that in spite of the personal risk and instructions to do otherwise, users in this study consistently examined
design factors when making a trust judgment. Focusing on small
hotels, Stephens [113] performs experiments to test a proposed
Integrated Trust Model for Web sites, which includes multiple
design elements (e.g., page layout, style, graphics, etc.). Several factors are shown to be more important for earning trust, at
least in the context of gaining customers (seeking a small hotel).
Related to Web design, researchers in humancomputer interaction have outlined in Corritore et al. [114] the importance of
establishing trust with users online. In this context, the authors
observe that trust is multi-dimensional, which is a cause of a current lack of agreement on trust issues, citing trust research across
multiple fields. The authors note that trust is used to decrease
complexity, and identify existing work in human factors that
points to trust as necessary for users to believe computers.

7. Discussion

Trust has been studied in social sciences, business and man-
agement, and psychology, before it became central to computer
science research. Considering the research we have reviewed,
there are several dimensions to describe trust:

D. Artz, Y. Gil / Web Semantics: Science, Services and Agents on the World Wide Web 5 (2007) 5871

(1) Target. The entity, which is being evaluated or given trust
varies with the perspective of the problem. Users are the target of trust in access control systems. Networks are trusted
by agents or users when using communication channels.
When seeking a reliable service, agents or services become
the target of trust. On the Web, we can trust agents providing content, on even make trust judgements on the content
itself.

(2) Representation. There are many ways that trust can be digitally encoded. Credentials include digital signatures and
tokens. Agents may carry histories of past interactions with
other agents. Users may employ social networks, or webs
of trust, to determine trust in an unknown correspondent.
Semantic web work includes detailed ontologies for trust
policies, trust negotiation, access control, and data prove-
nance.

(3) Method. Determining trust can be accomplished through
many methods. Hard security uses identification and authorization alone to decide complete trust in a user. Many
Internet applications use the exchange of credentials (i.e.,
digital signatures) to establish trust before engaging in a
transaction. Agents may use their histories of past interac-
tions, or other agents histories to determine trust through
reputation. In many applications, including information
retrieval, trust may be determined through transfer of trust
from associated entities.

(4) Management. The entity or entities that determine trust can
vary with the application. In many traditional systems, a
single service acts as a trusted third party to mediate the
establishment of trust between two unknown agents or users.
In more recent work, there is a push for decentralization
of control of the trust decision, including the enablement
of individual agents to make their own trust decisions. For
system-wide or global trust, voting mechanisms or other
forms of consensus may be used to collect individual trust
decisions.

(5) Computation. Trust may be quantified and computed in
many ways. Some approaches, including those harnessing the Semantic Web, choose discrete trust values (e.g.,
trust, distrust, or neutral), while others, especially when
computation is needed, choose a continuous numerical
range. Algorithms for how trust is transferred, combined,
or resolved can range from a simple average, to computing eigenvalues on graph adjacency matrices. Many
approaches compute trust assuming time is static, while others may account for the changes in trust over time. In cases
where trust information is large or always changing, several
approaches argue for local computation of trust, rather than
a globally consistent value.

(6) Purpose. The need for trust spans all aspects of computer
science, and each situation places different requirements on
trust. Human users, software agents, and increasingly, the
machines that provide services all need to be trusted in various applications or situations. The communication channels
between computers and users, and the content exchanged
between computers and users also require trust, in both
directions, for real world use. Trust can be used to protect

data, to find accurate information, to get the best quality
service, and even to bootstrap other trust evaluations.

Trust may be better seen as a motivating concept underlying
many problems and contexts rather than as a precise idea to be
studied under a uniform framework.

8. Conclusions

Trust research in the Semantic Web poses new challenges that
can be better met by building on the diverse but significant body
of work in modeling trust in computer science. In this paper, we
have identified four broad categories of existing work in trust
and given a brief overview of literature in each category. We
have discussed the relevance of each of these areas to important
aspects of ongoing and future Semantic Web research.

Acknowledgements

We would like to thank the anonymous reviewers for their
valuable comments and feedback on this work. We gratefully
acknowledge support from the US Air Force Office of Scientific
Research (AFOSR) with grant number FA9550-06-1-0031.
