An Approach for Named Entity Recognition in Poorly 

Structured Data 

Nuno Freire1,2, Jose Borbinha1, and Pavel Calado1 

1 INESC-ID/Instituto Superior Tecnico - Technical University of Lisbon, 

Av. Rovisco Pais, 1049-001 Lisboa, Portugal  

2 The European Library, National Library of the Netherlands, 
Willem-Alexanderhof 5, 2509 LK The Hague, Netherlands 
{nuno.freire,jlb,pavel.calado}@ist.utl.pt 

Abstract. This paper describes an approach for the task of named entity recognition in structured data containing free text as the values of its elements. We 
studied the recognition of the entity types of person, location and organization 
in bibliographic data sets from a concrete wide digital library initiative. Our approach is based on conditional random fields models, using features designed to 
perform named entity recognition in the absence of strong lexical evidence, and 
exploiting the semantic context given by the data structure. The evaluation results support that, with the specialized features, named entity recognition can be 
done  in  free  text  within  structured  data  with  an  acceptable  accuracy.  Our  approach was able to achieve a  maximum precision of 0.91 at 0.55 recall and a 
maximum  recall  of  0.82  at  0.77  precision.  The  achieved  results  were  always 
higher than those obtained with Stanford Named Entity Recognizer, which was 
developed for grammatically well-formed text. We believe this level of quality 
in named entity recognition allows the use of this approach to support a wide 
range of information extraction applications in structured data. 

Keywords:  named  entity  recognition,  structured  data,  metadata,  conditional 
random fields. 

Introduction 

A wide range of potentially usable business information exists in unstructured forms. 
Although that information is machine readable, it consists of natural language texts (it 
was estimated that 80% to 90% of business information may exist in those unstructured forms [1] [2]).  

As  businesses  become  more  data  oriented,  much  interest  has  arisen  in  these  unstructured  sources  of  information.  This  interest  gave  origin  to  the  research  field  of 
information extraction, which looks for automatic ways to create structured data from 
unstructured data sources [3]. An information extraction process can be characterized 
by an intention of selectively structure and combine data that is found in text, either 
explicitly stated or implied. The final output of the process will vary according to the 

E. Simperl et al. (Eds.): ESWC 2012, LNCS 7295, pp. 718732, 2012. 
 Springer-Verlag Berlin Heidelberg 2012 
?

?

?
purpose,  but  typically  it  consists  in  semantically  richer  data,  which  follows  a  structured data model, and on which more effective computation methods can be applied. 

Information  resources  in  digital  libraries  are  usually  described,  along  with  their 
context,  by  structured  data  records.  These  data,  which  is  commonly  referred  in  the 
digital library community as metadata, may serve many purposes, and the most relevant being resource discovery. Those records often contain unstructured data in natural language text, which might be useful to judge about the relevance of the resource. 
The  natural  hypothesis  is  if  that  information  can  be  represented  with  finer  grained 
semantics, then the quality of the system is expected to improve. 

This  paper  addresses  a  particular  task  of  information  extraction,  typically  called 
named entity recognition (NER),  which deals  with the textual references to entities, 
that  is,  when  they  are  referred  to  by  means  of  names  occurring  in  natural  language 
expressions, instead of structured data. This task deals with the particular problem of 
how to locate these references in the data set and how to classify them according their 
entity type [4]. 

We describe a NER approach, which we studied on the particular case of metadata 
from the cultural heritage domain, represented in the generic Dublin Core1 data mod-
el, which typically contains uncontrolled free text in the values of its data elements. 
We refer to this kind of data as poorly structured data. Typical examples of such data 
elements are the titles, subjects, and publishing information.  

NER has been extensively researched in grammatically well-formed text. In poorly 
structured data however, the text may not be grammatically well-formed, so our assumption is also that the data structure provides a semantic context which may support the NER task.  

This  paper  presents  an  analysis  of  the  NER  problem  poorly  structured  data,  describes a novel NER approach to address this kind of data, and presents an evaluation 
of the approach on a real set of data. The paper will follow  with an introduction to 
NER and related work in Section 2. The proposed approach is presented in Section 3, 
and  the  evaluation  procedure  and  results  are  presented  in  Section  4.  Section  5  concludes and presents future work. 

Problem and Related Work 

The  NER  task  refers  to  locating  atomic  elements  in  text  and  classifying  them  into 
predefined categories such as the names of persons, organizations, locations, expressions of time, quantities, etc. [4].  

Initial approaches were based on manually constructed finite state patterns and/or 
collections of entity names [4]. However, named entity recognition soon was considered as a typical scenario for the application of machine learning algorithms, because 
of  the  potential  availability  of  many  types  of  evidence,  which  form  the  algorithms 
input  variables [5]. Current solutions can reach an  F-measure accuracy around 90% 
[4] in grammatically well-formed text, thus a near-human performance. 

                                                           
1 http://dublincore.org/ 

N. Freire, J. Borbinha, and P. Calado 

However,  previous  work  suggested  that  current  NER  techniques  underperform 
when  applied  to  texts  existing  within  structured  digital  library  records  [6]  [7]  [8]. 
Most research on NER has focused mainly on natural language processing, involving 
text tokenization, part-of-speech classification, word sequence analysis, etc. Recognition with these techniques is therefore language specific and dependent of the lexical 
evidence given by the natural language text. 

The most similar scenarios we are aware of have researched information extraction 
within poorly structured data, and with a different focus than us. Research described 
in [9] proposes the use of information extraction techniques within relational database 
management systems, in order to exploit existing unstructured data within databases. 
This approach also was followed in [10], which addresses information extraction in a 
similar  type  of  data  as  we  do,  but  applies  simultaneously  named  entity  recognition 
and entity resolution (the recognized names are resolved in a data set of known enti-
ties). The contribution of this work for advancing in NER techniques in this type of 
data is somewhat limited, since it only addressed the recognition of entities that are 
present in the source data set. Similarly to our experience, this work also reports difficulties with the NER solutions for natural language text (although only one tool was 
evaluated  [11]).  However,  this  approach  differs  significantly  from  ours.  In  order  to 
improve the NER results, this approach was based on the evidence provided by structured data about the entities to be recognized, and the recognition model is based on 
manually crafted parsing rules created by a domain expert. 

Although not addressing the same type of data as we do, we can find approaches 
used  in  other  contexts  that  also  perform  NER  in  text  containing  little  or  no  lexical 
evidence. In [12], an approach is described for performing information extraction on a 
particular  kind  of  unstructured  and  ungrammatical  text  posted  on  the  World  Wide 
Web, such as item auction posts or online classifieds. The aim of this approach however  is  to  extract  a  structured  data  record  from  each  post,  assuming  that  each  post 
contains multiple attributes values of one entity, making the approach not applicable 
to our scenario.    

Other works, addressing NER in text without lexical evidence, focused on search 
engine queries [13][14]. In this work the problem is defined assuming the existence of 
one main entity per query, and adopted a specific technique for such cases, based on 
query logs [13] or user sessions [14] and topic models. We find the topic model approach to be not generally applicable for NER in to the data we are studying, since it 
assumes the existence of only one main entity per data element value.   

Approach 

We aimed at developing a general NER approach which could be systematically applied to any poorly structured data set. This section starts by presenting our analysis 
of the NER problem in structured data, and the general design decisions behind our 
approach. The description of the approach follows, and finalizes with the description 
of relevant implementation details. 
?

?

?
3.1  Analysis 

From our analysis of named entities found in structured data sets, we can highlight the 
following points:  

  Availability of lexical evidence varies in many cases. In some data elements 
we  found  grammatically  well-structured  text,  in  other  elements  we  found 
short sentences, containing very limited lexical evidence, or plain expression 
with practically non-existing lexical evidence. We also observed that in some 
cases, analysis of the same field across several records, revealed a mix of all 
cases. 
Instead of lexical evidence, we observed that, in some cases, textual patterns 
are often available and could be explored as evidence for NER. For example, 
punctuation  marks play an important role, but its  use  may differ from  how 
they are used in natural language text. 

 

  These  data  elements  are  typically  modeled  with  general  semantics.  The 
semantics associated with each element influences the type of named entities 
found  in  the  actual  records.  Therefore,  we  observed  different  probability 
distributions for each entity type across data elements. 

  One of the major sources of evidence is the actual name of the entities. Each 
entity  type  presents  names  with  different  words  and  lengths,  and  also  with 
different degrees of ambiguity with other words and entity types. 

From this analysis we believe that a generic approach must be highly adaptable, not 
only to the data set under consideration but also to each data element. Text found in 
each element across the whole data set is likely to be associated with particular patterns and degrees of available lexical evidence.   

On a  more  generic level, the  approach should  have a  strong  focus on the disambiguation of the names between the supported entity types, and be able to disambiguate between entity names and other nouns/words. 

3.2  Entity Types 

We studied the three entity types on which most NER research has been focused, and 
which  are  commonly  known  as  enamex  [15]:  person,  location  and  organization.  In 
addressing  these  three  entity  types,  we  wanted  to  design  an  approach  that  was  not 
limited to a set of known entity names, but could recognize any named entity of the 
supported entity types, as usually done in NER in grammatically well-structured text. 
As mentioned in the previous section, in structured data the characteristics of the 
names of persons, organizations, and places are a strong evidence for recognizing the 
named entities and determining their entity type. Therefore, in order to allow the predictive model to use the likelihood of a token being part of a named entity, we have 
collected  name  usage  statistics  from  comprehensive  data  sets  of  persons,  organizations and locations.  

Person and organization name statistics were extracted from VIAF - Virtual International Authority File [16]. VIAF is a joint effort of several national libraries from 

N. Freire, J. Borbinha, and P. Calado 

all continents towards a consolidated data set gathered for many years about the creators of the bibliographic resources held at these libraries. 

Location name statistics were extracted from Geonames [17], a geographic ontolo-

gy that covers all countries and contains over eight million locations. 

A description of how the statistics were extracted, and used in the predictive mod-

el, is presented in Section 3.4. 

3.3 

Predictive Model 

Our analysis suggested that a flexible approach with the capacity to adapt to the data 
set  would  be  necessary  for  performing  NER  in  structured  data.  This  suggested  the 
application  of  a  machine  learned  model,  an  option  also  supported  by  the  literature 
review of state of the art NER approaches. 

The NER problem can be formulated as follows. Given a text string x and a set of 
entity types Y, where x consists of a sequence of tokens x1 . . . xn, and each token is a 
word or a punctuation mark, the entity recognition task consists in segmenting x into a 
sequence s of non-overlapping segments s1 . . . sp where each segment sj is associated 

with  a  yj Y,  and  a  start  position  tj  ,  and  an  end  position  uj  (for  notation  readability 

purposes we assume Y to also contain a non_entity type). All segments of s are nonoverlapping  and  fully  encompass  all  tokens  of  x,  therefore  for  all  xi  exists  one  and 
only one sj that satisfies stj<= i and suj>=i. 

We use as a basis the conditional models of conditional random fields (CRF) [18]. 
CRFs define a conditional probability p(y|x) over label sequences given a particular 
observation sequence x. These models allow the labelling of an arbitrary sequence x 
by choosing the label sequence y that maximizes the conditional probability p(y|x). 
The  conditional  nature  of  these  models  allows  arbitrary  characteristics  of  the  sequences to be captured by the model, without requiring previous knowledge, by the 
modeller, about how these characteristics are related [19].  

In order to find the sequence s that correctly recognizes the entity names from the 
observation sequence x, evidence is extracted or calculated. This evidence consists in 
a set of features which capture those characteristics of the empirical distribution of the 
data that support the recognition of names. Many different methods have been used to 
calculate  and  use  features  in  a  combined  manner.  Features  may  be  calculated  from 
natural language processing of the source text, by rules defined by domain experts, by 
lookups in lists of entity names and ontologies, from syntactical characteristics of the 
tokens, etc. The following section presents the set of features that we defined for our 
particular predictive model. 

3.4 

Features 

Several  features  were defined to give the predictive  model the capability to capture 
distinct  aspects  of  the  text,  such  as  locating  potential  names,  disambiguate  between 
entity types and other words, or detecting textual patterns from syntactical and lexical 
evidence. This section presents the definition of these features. 
?

?

?
A set of features were defined to provide the predictive model with some evidence 
for locating potential names of entities in the text. These features were created based 
on data or statistics taken from the comprehensive listings of names described in Section 3.2. Each entity type has different characteristics in the way entities are named, 
so we defined the features in different ways for each entity type. 

The features for person names explore how frequent a word was found in person 
names, making a distinction between first names, surnames and names that appear in 
lowercase. Let F denote a bag built from all first names found in VIAF, and let S denote a bag built from all surnames found in VIAF, and let C be a bag built from all 
names found non-capitalized in VIAF. We define the following real valued features: 

 

(cid:1868)(cid:1857)(cid:1870)(cid:1871)(cid:1867)(cid:1866)(cid:1832)(cid:1861)(cid:1870)(cid:1871)(cid:1872)(cid:1840)(cid:1853)(cid:1865)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)log(cid:1737)(cid:1736)(cid:1735)1(cid:3397)(cid:1832)#(cid:3051)(cid:3284)(cid:4678)
(cid:1740)(cid:1739)(cid:1738)
(cid:1832)#(cid:3037)
(cid:3417)
#(cid:3007)(cid:3037)(cid:2880)(cid:2868)#(cid:1832) (cid:4679)
(cid:1740)(cid:1739)(cid:1738)
(cid:1868)(cid:1857)(cid:1870)(cid:1871)(cid:1867)(cid:1866)(cid:1845)(cid:1873)(cid:1870)(cid:1866)(cid:1853)(cid:1865)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)log(cid:1737)(cid:1736)(cid:1735)1(cid:3397)(cid:1845)#(cid:3051)(cid:3284)(cid:4678)
(cid:1845)#(cid:3037)
(cid:3417)
(cid:4679)
#(cid:3020)(cid:3037)(cid:2880)(cid:2868)#(cid:1845)
(cid:1868)(cid:1857)(cid:1870)(cid:1871)(cid:1867)(cid:1866)(cid:1840)(cid:1867)(cid:1829)(cid:1853)(cid:1868)(cid:1861)(cid:1872)(cid:1853)(cid:1864)(cid:1871)(cid:1840)(cid:1853)(cid:1865)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)log (cid:1737)(cid:1736)(cid:1735)1(cid:3397)(cid:1829)#(cid:3051)(cid:3284)(cid:4678)
(cid:1829)#(cid:3037)
(cid:3417)
#(cid:3004)(cid:3037)(cid:2880)(cid:2868)#(cid:1829) (cid:4679)
(cid:1740)(cid:1739)(cid:1738)
(cid:1867)(cid:1870)(cid:1859)(cid:1853)(cid:1866)(cid:1861)(cid:1878)(cid:1853)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:1840)(cid:1853)(cid:1865)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)log (cid:1737)(cid:1736)(cid:1735)1(cid:3397)(cid:1829)#(cid:3051)(cid:3284)(cid:4678)
(cid:1829)#(cid:3037)
(cid:3417)
#(cid:3004)(cid:3037)(cid:2880)(cid:2868)#(cid:1829) (cid:4679)

 

For organizations, only one feature was defined. Let C be a bag built from all words 
and punctuation  marks found in the names of organizations in VIAF,  we define the 
following real valued feature: 

(cid:1740)(cid:1739)(cid:1738)

 

 

For places, the diversity of the names  makes the frequency of use of the  words not 
effective,  so  one  feature  was  defined,  using  the  type  of  geographic  entity  and  the 
highest population known for a place on whose name the word appears in. Let C denote a bag built from all tokens found in the names of continents and countries. Similarly let D, E, F and G denote bags built from all tokens found in the names of cities, 
administrative divisions or islands, natural geographic entities, and other geographic 

features, respectively. Also let (cid:1868)(cid:1867)(cid:1868)(cid:1873)(cid:1864)(cid:1853)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:4666)(cid:1872)(cid:4667)(cid:3645)(cid:1331) denote a function that returns the 

maximum population found in a location name with token t. We defined the following 
real valued feature: 

N. Freire, J. Borbinha, and P. Calado 

(cid:1864)(cid:1867)(cid:1855)(cid:1853)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:1840)(cid:1853)(cid:1865)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)

1,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1829)
min (cid:4666)100000,population(cid:4666)(cid:1876)(cid:3036)(cid:4667)
100000
0.7,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1831)
0.6,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1832)
0.1,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1833)
0,(cid:1867)(cid:1872)(cid:1860)(cid:1857)(cid:1870)(cid:1875)(cid:1861)(cid:1871)(cid:1857)

(cid:1749)(cid:1750)(cid:1750)(cid:1748)(cid:1750)(cid:1750)(cid:1747)

,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1830)

 

We also use the Wordnet to capture the possible part-of-speech of some tokens. We 

 

Some  features  are  based  on  data  extracted  from  the  WordNet  [20]  of  the  language 
matching the language of the source text, which in the case we studied was English. 
These features provide evidence to disambiguate between named entities of the target 
types and other words. 

With the aim to disambiguate between proper nouns referring to other entity types, 
and  proper  nouns  referring  to  persons,  locations  and  organizations,  we  define  the 

have a part-of-speech value of proper noun, and let G, H, I, J, K, L denote the sets of 
variants in synsets which are hyponyms, either directly or transitively, of one of the 
synsets2  geographic  area#noun#1,  landmass#noun#1,  district#noun#1,  body  of  wa-
ter#noun#1,  organization#noun#5,  and  person#noun#1,  respectively.  The  feature  is 

0,(cid:1867)(cid:1872)(cid:1860)(cid:1857)(cid:1870)(cid:1875)(cid:1861)(cid:1871)(cid:1857)
(cid:1868)(cid:1867)(cid:1871)(cid:1840)(cid:1867)(cid:1873)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404) (cid:3420)1,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1827)
0,(cid:1867)(cid:1872)(cid:1860)(cid:1857)(cid:1870)(cid:1875)(cid:1861)(cid:1871)(cid:1857) 

feature (cid:1868)(cid:1870)(cid:1867)(cid:1868)(cid:1857)(cid:1870)(cid:1840)(cid:1867)(cid:1873)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669). Let P denote the set of all variants in synsets which 
defined as: (cid:1868)(cid:1870)(cid:1867)(cid:1868)(cid:1857)(cid:1870)(cid:1840)(cid:1867)(cid:1873)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404) (cid:3420)1,(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1842)\(cid:4666)(cid:1833)(cid:1515)(cid:1834)(cid:1515)(cid:1835)(cid:1515)(cid:1836)(cid:1515)(cid:1837)(cid:1515)(cid:1838)(cid:4667) 
defined  the  feature (cid:1868)(cid:1867)(cid:1871)(cid:1840)(cid:1867)(cid:1873)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669),  which  indicates  if  the  token  exists  in  a 
Similar  features  were  defined  for  other  parts-of-speech: (cid:1868)(cid:1867)(cid:1871)(cid:1848)(cid:1857)(cid:1870)(cid:1854)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669), 
(cid:1868)(cid:1867)(cid:1871)(cid:1827)(cid:1856)(cid:1862)(cid:1857)(cid:1855)(cid:1872)(cid:1861)(cid:1874)(cid:1857)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669), (cid:1868)(cid:1867)(cid:1871)(cid:1827)(cid:1856)(cid:1874)(cid:1857)(cid:1870)(cid:1854)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669), and (cid:1868)(cid:1867)(cid:1871)(cid:1842)(cid:1870)(cid:1857)(cid:1868)(cid:1867)(cid:1871)(cid:1861)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669). 
tokens.  The  features (cid:1871)(cid:1872)(cid:1853)(cid:1870)(cid:1872)(cid:1841)(cid:1858)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669)  and (cid:1857)(cid:1866)(cid:1856)(cid:1841)(cid:1858)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669) 
case  of  the  token  is  captured  through  the  features (cid:1861)(cid:1871)(cid:1829)(cid:1853)(cid:1868)(cid:1861)(cid:1872)(cid:1853)(cid:1864)(cid:1861)(cid:1878)(cid:1857)(cid:1856)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669)  and 
(cid:1861)(cid:1871)(cid:1832)(cid:1873)(cid:1864)(cid:1864)(cid:1829)(cid:1853)(cid:1868)(cid:1871)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:4668)0,1(cid:4669),  which indicate if the token  is a  word and contains the first 
length is captured by the feature (cid:1872)(cid:1867)(cid:1863)(cid:1857)(cid:1866)(cid:1838)(cid:1857)(cid:1866)(cid:1859)(cid:1872)(cid:1860)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:1331). 
The tokens are also used in a nominal feature (cid:1872)(cid:1867)(cid:1863)(cid:1857)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:1846), where T denotes the 

synset  with  part-of-speech  noun.  Let  A  denote  the  set  of  variants  in  synsets  which 
have a part-of-speech value of noun, we define the feature as: 

We also defined features  to capture syntactical characteristics of  the text and the 

indicate if token xi is at the start or at the end of the value of the data element. The 

letter  in  uppercase,  or  all  letters  in  uppercase,  respectively.  The  tokens  character 

set of tokens built from the three preceding tokens, and the two following tokens, of 
every named entity found in the training data: 

                                                           
2 To refer to Princeton WordNet synsets, we use the notation w#p#i where i corresponds to the 

i-th sense of a literal w with part of speech p. 
?

?

?
(cid:1872)(cid:1867)(cid:1863)(cid:1857)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)(cid:3420)(cid:1876)(cid:3036),(cid:1861)(cid:1858) (cid:1876)(cid:3036) (cid:1846)
(cid:1486),(cid:1867)(cid:1872)(cid:1860)(cid:1857)(cid:1870)(cid:1875)(cid:1861)(cid:1871)(cid:1857) 
(cid:1855)(cid:1853)(cid:1868)(cid:1861)(cid:1872)(cid:1853)(cid:1864)(cid:1861)(cid:1878)(cid:1857)(cid:1856)(cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3404)log(cid:4678)

(cid:1829)#(cid:3051)(cid:3284)
(cid:4666)1(cid:3397)(cid:1830)#(cid:3051)(cid:3284)(cid:4667)(cid:4679) 

Capitalization statistics of  words in the data  set are extracted and used in a  feature.  
Let C denote the bag of capitalized words in the data set, and let D denote a bag of the 
non-capitalized words in data set, we define the following real valued feature:  

fined  the  feature (cid:1856)(cid:1853)(cid:1872)(cid:1853)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667)(cid:3645)(cid:1830),  where  D  denotes  the  set  of  data  element 

Since  typically  each  data  element  will  have  values  with  different  characteristics,  a 
feature is necessary to capture the data element where the text is contained. We de-

identifiers of the data model (for example, in data encoded in XML, these identifiers 
consist of the xml elements namespace and elements name). 

Additional features are defined in similar way, but they refer to the three previous 

tokens and the two following tokens, instead of the current one.  

3.5 

Implementation Details 

In  this  section  we  provide  some  relevant  details  of  the  implementation  of  our  ap-
proach,  in  particular  we  address  text  tokenization  and  the  CRF  implementation  and 
configuration.  

Tokenization of the text inside the data elements is performed only at word level. 
No  sentence  or  paragraph  tokenization  is  performed,  since  in  many  cases  wellstructured sentences are not present in the data and the results of sentence and paragraph tokenization could invalidate the detection of patterns in the data.  

Word  tokenization  is  performed  in  a  language  independent  way.  We  also  justify 
this option to avoid the breaking of patterns in the data, in particular in cases where 
punctuation is used in the data with different meanings than it is has in natural language text. We have applied the word breaking rules of UNICODE [22]. 

The  CRF  implementation  used  was  provided  by  the  Java  implementation  in  the 
MALLET - Machine Learning for Language Toolkit [21]. The CRF was configured to 
use the three previous states in the sequence in the labelling of the sequence, and was 
trained using an objective function for CRFs that consists in the label likelihood plus 
a Gaussian prior on parameters. 

Evaluation 

The  evaluation  of  our  approach  was  performed  in  the  data  sets  from  Europeana3, 
which consist in descriptions of digital objects of cultural interest. This data set follows a data model using mainly Dublin Core elements, and named entities appear in 
                                                           
3 http://www.europeana.eu/ 

N. Freire, J. Borbinha, and P. Calado 

data elements for titles, textual descriptions, tables of contents, subjects, authors and 
publication.  

The data set contains records originating from several European providers from the 
cultural sector, such as libraries, museums and archives. Several European languages 
are  present,  even  within  the  description  of  the  same  object,  for  example  when  the 
object being described is of a different language than the  one used to create its de-
scription.  

Providers from where this data originates follow different practices for describing 
the digital objects, which causes the existence of highly heterogeneous data. Lexical 
evidence is very limited in this data set, so it provides a good scenario for the evaluation of the evidence made available by the structure and textual patterns of the data. 

This section describes the experimental setup and its results. It will follow with the 
description of the data set used for evaluation, and then describe the evaluation proce-
dure. Results of the evaluation are presented afterwards, and it finalizes with the results of the evaluation of individual features. 

4.1  Evaluation Data Set 

An  evaluation  of  our  approach  was  performed  on  a  selected  collection  of  metadata 
records from Europeana. This collection was created by randomly selecting records in 
the English language. The selection process was done in two steps: first, all records in 
the English language were selected from all Europeana data providers; and second, a 
random selection of records was performed, balancing the number of records chosen 
across different providers. 

In total, the evaluation data set4 consisted in 120 records containing in its elements 584 

references to persons, 457 to locations and 153 to organizations, as shown in Table 1.  

Table 1. Data elements studied in the data set and total annotated named entities 

Data element 

 Element definition5 

Pers. Locat.  Organiz. 

Title 
creator / contributor 
Subject 
Coverage 
?

?

?
A name given to the resource. 
An  entity  primarily  responsible  for  making  the  resource  /  An 
entity responsible for making contributions to the resource.  
The topic of the resource. 
The spatial or temporal topic of the resource, the spatial applicability  of  the  resource,  or  the  jurisdiction  under  which  the 
resource is relevant. 
An account of the resource. 
A list of subunits of the resource. 
?

?

?
Description 
table of contents 
Publisher 
 
                                                           
4 The data set is available for research use at http://web.ist.utl.pt/~nuno.freire/ner/ 
5 Element definitions were taken from the Dublin Core Metadata Terms. 

An entity responsible for making the resource available. 
?

?

?
Total:  584 
?

?

?
An Approach for Named Entity Recognition in Poorly Structured Data 

The evaluation data set was manually annotated. In very few cases, the manual annotation was uncertain, because the data records may not contain enough information 
to  support  a  correct  annotation.  For  example,  some  sentences  with  named  entities 
were too small and no other information was available in the record to support a decision  on  the  classification  of  the  named  entities  to  their  entity  type.  Named  entities 
were  annotated  with  their  enamex  type.  If  the  annotator  was  unsure  of  the  enamex 
type of a named entity, he would annotate it as unknown. These annotations were not 
considered for the evaluation of the results, and any recognition made in these entities 
was discarded. 

4.2  Evaluation Procedure 

The accuracy of the results of our approach was compared with that of other two ap-
proaches: one was the implementation of a conditional maximum entropy model [25], 
taken from the OpenNLP package; the other was based on conditional random fields 
[26], from the Stanford Named Entity Recognizer (Stanford NER). For both cases, we 
used  the  respective  predictive  models  trained  on  the  CoNLL  2003  English  training 
data  [27].  However,  since  in  all  tests  the  Stanford  NER  performed  better  than 
OpenNlp, for readability, we only present the results of Stanford NER as our baseline 
for comparison. 

Since our predictive model was trained on the evaluation data set, all the measurements 
were obtained using cross-validation tests, which has been widely accepted as a reliable 
method for calculating generalization accuracy [24]. Cross-validation involves partitioning 
the evaluation data set into complementary subsets, testing on one subset, while training 
on  the  remaining  subset.  Ten-fold  cross-validation  was  performed  using  different  parti-
tions, and the validation results were averaged over the ten runs. 

As the NER evaluation method, we have used the exact-match method. This method 
has been used in several named entity recognition evaluation tasks [23] [27]. In the exactmatch method, an entity is only considered correctly recognized if it is exactly located as 
in the manual annotation. Recognition of only part of the name, or with words that are not 
part of the name, is not considered correct. In combination with the exact-match method, 
we used the metrics of precision6, recall7 and F1-measure8.  

To evaluate on the balance between results in precision and recall, we have taken 
measures at several minimum confidence thresholds. For both our approach and the 
baseline,  we  only  consider  a  named  entity  recognized  if  the  joint  probability  of  the 
corresponding segment is equal or above the minimum confidence threshold.  

4.3  Results 

The overall results of the evaluation of all entity types are presented in Fig. 2, and the 
results of each entity type are presented in Fig. 1. The results of our approach were 

                                                           
6 The percentage of correctly identified named entities in all named entities found. 
7 The percentage of named entities found compared to all existing named entities. 
8 The weighted harmonic mean of precision and recall (equal weights for recall  and precision). 

N. Freire, J. Borbinha

a, and P. Calado 

higher for all entity types, m
approach and the baseline w
ments except for the entity
we obtained P>0.01 on the 

metrics and confidence levels. The differences between 
were statistically significant with P>0.001 for all measu
y type location  where, in the lowest confidence thresho
three metrics. 

our 
ure-
old, 

 

 

 

 

Fig. 1. Precision, recall and F1
tion data set 

1 results of the three enamex entity types measured on the eva

alua-

Both Stanford NER and 
tation of CRFs used was n
used,  we  believe  that  the  d
due to the different feature
semantic context of the dat

our approach are based on CRFs. Although the implem
ot the same, and other differences exist on how CRFs 
difference  in  the  results  obtained  with  both  approache
es used, therefore supporting our initial hypothesis that 
ta structure, and non-lexical features, could support NE

men-
are 
es  is 
the 
ER. 
?

?

?
for Named Entity Recognition in Poorly Structured Data 

An interesting result can be
entity location, where Stanf
probability given by the CR
of  the  recognized  named  e
evidence had a major impac
Results of both approach
cision. In our approach ove
ranged from 0.77 to 0.91. G
entities, as show in the nex
by names that had no prese
able to empirically support 
Our approach was able t
recall of 0.82 at 0.77 precis
support a wide range of info
requirements for recall or p

e observed at the lowest confidence threshold result for 
ford NER was actually able to achieve a F1 of 0.68, but 
RF predictive model was close to zero for more than 7
entities.  This  observation  suggests  that  the  lack  of  lex
ct in its results. 
hes generally showed lowest values for recall than for p
erall recall ranged from 0.55 to 0.82 while overall precis
Given the importance of the features based on the name
t section, we believe that the lower recall is mainly cau
ence in the entity names data sets. However, we were 
this conclusion. 
to achieve a high precision of 0.91 at 0.55 recall, or reac
ion. We believe these values reached levels high enough
ormation extraction applications, which may have differ
recision. 

the 
the 
70% 
ical 

pre-
sion 
s of 
used 
not 

ch a 
h to 
rent 

 

 

Fig. 2. Precision, recall and F

F1 results of all entity types measured on the evaluation data s

set 

4.4 

Feature Evaluation
n 

In order to evaluate the con
we have performed a featur
[28]. This method employs
to estimate the accuracy of 
To conduct this evaluatio
tures related  with part-of-s
haustive evaluation for all c
In total, we formed 10 g
Each  feature  combination 
best performing feature com
 Table 2 summarizes the
the best performing combin
individually. Since the  feat

ntribution of each feature for the quality of the NER resu
re selection evaluation based on the wrapper methodolo
 cross-validation using the actual target learning algorit
subsets of features. 
on, we have grouped related features (for example, all f
peech  were considered one group), and performed an 
combinations of groups of features.  
groups of features and tested all combinations of 7 grou
was  evaluated  by  a  10-fold  cross-validation  test  and 
mbination, measured by the F1, of each fold was noted.  
e results, by showing how often each feature was presen
nation of the 10 folds, for all entity types, and for each t
tures related  with the names of  the entities  were essen

ups. 
the 

nt in 
type 
ntial 

ults, 
ogy 
thm 

fea-
ex-

N. Freire, J. Borbinha, and P. Calado 

for  the  overall  results,  on  the  evaluation  on  the  individual  entity  types,  we  always 
used combinations including these three groups of features, so that the results could 
be more easily compared and analyzed.  

Table 2. Results of the evaluation of the features 

and 

only in 10% or 20% of the folds in the overall results for locations and organizations, 
but for persons it was used in 60% of the folds. This indicates that the textual patterns 
where persons are referenced were distinct across data elements, while for the other 
entity  types  the  patterns  were  more  uniform  across  data  elements.  Our  analysis 

100% 
100% 

Feature groups 

100% 
100% 

100% 
100% 

100% 

100% 
100% 

90% 

50% 

70% 

80% 

50% 

100% 

100% 

100% 

100% 

Included in best combination 

all types 

persons 

locations 

organizations 

(cid:2198)(cid:2187)(cid:2200)(cid:2201)(cid:2197)(cid:2196)(cid:2162)(cid:2191)(cid:2200)(cid:2201)(cid:2202)(cid:2170)(cid:2183)(cid:2195)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2187)(cid:2200)(cid:2201)(cid:2197)(cid:2196)(cid:2175)(cid:2203)(cid:2200)(cid:2196)(cid:2183)(cid:2195)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2187)(cid:2200)(cid:2201)(cid:2197)(cid:2196)(cid:2170)(cid:2197)(cid:2159)(cid:2183)(cid:2198)(cid:2191)(cid:2202)(cid:2183)(cid:2194)(cid:2201)(cid:2170)(cid:2183)(cid:2195)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2197)(cid:2200)(cid:2189)(cid:2183)(cid:2196)(cid:2191)(cid:2208)(cid:2183)(cid:2202)(cid:2191)(cid:2197)(cid:2196)(cid:2170)(cid:2183)(cid:2195)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2194)(cid:2197)(cid:2185)(cid:2183)(cid:2202)(cid:2191)(cid:2197)(cid:2196)(cid:2170)(cid:2183)(cid:2195)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2202)(cid:2197)(cid:2193)(cid:2187)(cid:2196)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2201)(cid:2202)(cid:2183)(cid:2200)(cid:2202)(cid:2171)(cid:2188)(cid:2161)(cid:2194)(cid:2187)(cid:2195)(cid:2187)(cid:2196)(cid:2202)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2187)(cid:2196)(cid:2186)(cid:2171)(cid:2188)(cid:2161)(cid:2194)(cid:2187)(cid:2195)(cid:2187)(cid:2196)(cid:2202)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2191)(cid:2201)(cid:2159)(cid:2183)(cid:2198)(cid:2191)(cid:2202)(cid:2183)(cid:2194)(cid:2191)(cid:2208)(cid:2187)(cid:2186)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2191)(cid:2201)(cid:2177)(cid:2198)(cid:2198)(cid:2187)(cid:2200)(cid:2185)(cid:2183)(cid:2201)(cid:2187)(cid:2186)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2170)(cid:2197)(cid:2203)(cid:2196)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2178)(cid:2187)(cid:2200)(cid:2184)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2157)(cid:2186)(cid:2192)(cid:2187)(cid:2185)(cid:2202)(cid:2191)(cid:2204)(cid:2187)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2157)(cid:2186)(cid:2204)(cid:2187)(cid:2200)(cid:2184)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2172)(cid:2200)(cid:2197)(cid:2198)(cid:2187)(cid:2200)(cid:2170)(cid:2197)(cid:2203)(cid:2196)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2197)(cid:2201)(cid:2172)(cid:2200)(cid:2187)(cid:2198)(cid:2197)(cid:2201)(cid:2191)(cid:2202)(cid:2191)(cid:2197)(cid:2196)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2198)(cid:2200)(cid:2197)(cid:2198)(cid:2187)(cid:2200)(cid:2170)(cid:2197)(cid:2203)(cid:2196)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2202)(cid:2197)(cid:2193)(cid:2187)(cid:2196)(cid:2168)(cid:2187)(cid:2196)(cid:2189)(cid:2202)(cid:2190)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2186)(cid:2183)(cid:2202)(cid:2183)(cid:2161)(cid:2194)(cid:2187)(cid:2195)(cid:2187)(cid:2196)(cid:2202)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
(cid:2185)(cid:2183)(cid:2198)(cid:2191)(cid:2202)(cid:2183)(cid:2194)(cid:2191)(cid:2208)(cid:2187)(cid:2186)(cid:2162)(cid:2200)(cid:2187)(cid:2199)(cid:2203)(cid:2187)(cid:2196)(cid:2185)(cid:2207)(cid:4666)(cid:2206),(cid:2191)(cid:4667) 
the  best  overall  results, (cid:1856)(cid:1853)(cid:1872)(cid:1853)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667)  and (cid:1855)(cid:1853)(cid:1868)(cid:1861)(cid:1872)(cid:1853)(cid:1864)(cid:1861)(cid:1878)(cid:1857)(cid:1856)(cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:4666)(cid:1876),(cid:1861)(cid:4667),  were 
always  used  in  the  overall  results.  And,  in  addition,  the  features (cid:1872)(cid:1867)(cid:1863)(cid:1857)(cid:1866)(cid:4666)(cid:1876),(cid:1861)(cid:4667), 
(cid:1871)(cid:1872)(cid:1853)(cid:1870)(cid:1872)(cid:1841)(cid:1858)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667), 
(cid:1861)(cid:1871)(cid:1847)(cid:1868)(cid:1868)(cid:1857)(cid:1870)(cid:1855)(cid:1853)(cid:1871)(cid:1857)(cid:1856)(cid:4666)(cid:1876),(cid:1861)(cid:4667) were used very often. This seems to indicate that textual patterns 
In the results of the feature (cid:1856)(cid:1853)(cid:1872)(cid:1853)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667), it is worth noting that it was used 

often used when evaluated on the results of the individual entity types. Therefore we 
believe that all features should be used when applying this approach to other data sets. 
We can also observe that the features that detected the names of the entities were 

All features contributed to the best performing combination, for all entity types, in 
at least two of the cross-validation folds. The features which were used the least for 

(cid:1857)(cid:1866)(cid:1856)(cid:1841)(cid:1858)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:4666)(cid:1876),(cid:1861)(cid:4667), 

(cid:1861)(cid:1871)(cid:1829)(cid:1853)(cid:1868)(cid:1861)(cid:1872)(cid:1853)(cid:1864)(cid:1861)(cid:1878)(cid:1857)(cid:1856)(cid:4666)(cid:1876),(cid:1861)(cid:4667), 

were very relevant for providing evidence for NER. 

60% 
60% 
20% 
20% 

70% 
60% 
60% 
50% 

70% 

60% 

70% 

50% 
30% 
10% 
70% 

70% 

60% 

50% 

50% 
50% 
20% 
80% 
?

?

?
pointed that, in the data elements for creators and contributors, the names for persons 
often  appeared  in  inverse  order  (that  is,  surname,  first_names),  while  in  the  other 
elements  they  appeared  in  direct  order  (that  is,  first_names  surname).  We  therefore 
conclude  that  the  semantic  context  given  by  the  data  structure  is  generally  not  required to allow the recognition of the entities, but in some cases, it can provide importance evidence for the predictive model. 

Conclusion and Future Work 

We presented an approach for the task of named entity recognition in structured data 
containing free text as the values of its elements. This approach is based on the extraction of features from the text, which allows the predictive model to operate with 
more  independent  of  lexical  evidence  than  named  entity  recognition  systems  developed for grammatically well-formed text. 

Our approach was able to achieve a maximum precision of 0.91 at 0.55 recall, and 
a maximum recall of 0.82 at 0.77 precision. The achieved results were significantly 
higher than those obtained with the baseline. We believe this level of quality in named 
entity recognition allows the use of this approach to support a wide range of information extraction applications in digital library metadata. 

Although we have specifically studied metadata from the cultural heritage sector, 
we believe our approach has general applicability to any poorly structured data model. 
In  future  work  we  will  explore  the  use  of  ontologies  for  creating  features  to  improve  the  recognition  of  named  entities.  We  will  also  address  the  resolution  of  the 
recognized named entities in linked data contexts and ontologies.  
