Feedback--based Daata Seet Reecommendation for Buuilding 

Liinked Daata Appplications

Helio Rodrigues de Oliveira

Center for Informatics -

Alberto Trindade Tavares

Center for Informatics -

Bernadette Farias Loscio

Center for Informatics -

Federal University of Pernambuco

Av. Jornalista Anibal Fernandes, s/n -

Cidade Universitaria, 50.740-560 -

Federal University of Pernambuco

Av. Jornalista Anibal Fernandes, s/n -

Cidade Universitaria, 50.740-560 -

Federal University of Pernambuco

Av. Jornalista Anibal Fernandes, s/n -

Cidade Universitaria, 50.740-560 -

Recife  PE, Brazil
+55 81 2126.8430
hro@cin.ufpe.br

Recife  PE, Brazil
+55 81 2126.8430
att@cin.ufpe.br

Recife  PE, Brazil
+55 81 2126.8430
bfl@cin.ufpe.br

ABSTRACT
The  huge  and  growing  volume  of  linked  data  is  increasing  the 
interest in developing applications on top of such data. One of the 
distinguishing features of linked data applications is that the data 
could  come  from  any  RDF  data  set  available  on  the  Web. 
Different from conventional applications, where the data sources 
are under control of the application's owner or developer, linked 
data applications follow the Semantic Web vision of a world full 
of reusable data. Considering a potentially large number of data 
sets,  one  of  the  primary  challenges  facing  the  development  of 
such  solutions  is  the  identification  of  suitable  data  sources,  i.e., 
data sets that could give a good contribution to the answer of user 
queries. In this paper, we discuss this problem and we present a 
feedback-based approach to incrementally identify new data sets 
for domain-specific linked data application.
Categories and Subject Descriptors
H.4 [Information System Applications]: Miscellaneous;
H.2 [Database Management]:  Miscellaneous
General Terms
Algorithms, Management, Measurement, Experimentation.
Keywords
Semantic Web, Linked Data, Feedback.
1. INTRODUCTION
Applications  built  on  top  of  linked  data  may  offer  generic 
functionalities  as,  for  example,  linked  data  browsers  and  search 
engines  or  may  offer  more  domain-specific  functionalities  by 
accessing and integrating data from various linked data sets [3].
One of the primary challenges facing the development of domainspecific  applications  is  the  identification  of  relevant  data  sets. 
Considering  a  potentially  large  number  of  datasets,  to  manually 
identify  suitable  ones for  a  given  application  may  become  an 
unfeasible task. 
In this paper we restrict our attention to applications whose aim is 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee.
I-SEMANTICS  2012,  8th  Int.  Conf.  on  Semantic  Systems,  Sept.  5-7, 
2012, Graz, Austria
Copyright 2012 ACM 978-1-4503-1112-0 ...$10.00.

combining  data  from  different  data  sets  to  cover  the  needs  of 
specific user communities. We are interested on applications that 
follow a query federation pattern, where complex queries may be 
posed directly to a fixed set of data sources without creating local 
data  sources  replicas [3].  One  of  the  main  challenges  of  such
approach is that performance problems may arise if the number of 
data  sets  becomes  too  large.  Therefore,  the  number  of  data  sets 
that an application intends to use must be controlled in such a way 
that just the more relevant data sets should be considered.
In this paper, we propose a user feedback-based approach to assist 
developers  to  find  proper  data  sources  while  they  are  building 
domain  specific  linked  data  applications. Our  approach  involves 
two  main  tasks:  i)  to  find  a  subset  of  data  sets that answers  the
application queries and ii) to choose the most important data sets 
in this subset. The first phase focuses on filtering the huge volume 
of  linked  data  sets  available  on  the  Web,  while  the  second  one 
ranks the candidate data sets obtained in the first phase in order to 
identify the best ones. One distinguishing issue of our approach is 
that  instead  of  using  keyword  search  to  identify  candidate  data 
sets we consider the semantics of SPARQL queries to be posed or 
already  submitted  to  the  application. Moreover user  feedback  is 
used as a way to assess the relevance of the candidate data sets. 
The  remainder  of  this  paper  is  organized  as  follows.  Section  2 
presents  some  examples  to  motivate  the  proposed  approach. 
Section  3  introduces  some  preliminary  definitions  used  as  the 
basis  for  our  proposal.  Section  4  describes  our  approach  while 
Section 5 discusses some initial experiments we performed to test 
our  approach.  Section  6  shows  the  related  works.  Section  7 
presents conclusions and future work.
2. MOTIVATION
One  of  the  main  problems  with  building  applications  that  make 
use  of  data  sources  available  on  the  Web  consists  in  finding 
relevant  data  sources.  In  a  general  way, a  data  source  is 
considered relevant when contributes for answering queries posed 
to the application [5]. However, it may happen that a data set may 
contribute  for  answering  an  application  query  but  the  obtained 
answer  does  not  meet  the  user  requirements. This  may  occur
because the data source has generic data and the user wants more 
specific data, for example, or the data set has data of poor quality, 
i.e., the data may be outdated or incorrect. In such cases, it is not 
enough  just  finding  data  sets  that  can  answer  the  application 
queries it is also necessary to check if the available data meet the
user requirements.

49information  about  medicine 

Suppose,  for  example,  an  application,  which  aims  at  publishing 
information  about  academic  researchers  in  Computer  Science. 
One  relevant  query  for  such  application  could  be:  Return  all 
researchers who published papers in 2012. Considering that the 
application  is  specific  for  Computer  Science,  it  is  not  worth 
mentioning  the  researchers  area  when  formulating  the  query. 
Therefore,  in  this  case,  any  data  set  that  has  bibliographic 
information  about  scientific  papers  could  be  considered  relevant 
for its answer. This situation happens because just considering the 
application query, as criteria for selecting relevant data sets, is not 
enough, once that application queries may be generic and do not 
reflect  precisely  the  user  requirements. In  this  case,  considering 
application  queries  as  the  only  criteria for  finding  relevant  data 
sets  will  lead  to  generic  information  too.  Specifically,  the 
application  is  interested  in  data  sets  as  DBLP  RKBExplorer1 or
DBLP  L3S2 that  stores  information  about  Computer  Science 
bibliography. However,  data sets like DBPedia3 and Geonames4
will also be considered relevant once that they have bibliographic 
information about the most famous researchers. In a similar way, 
PubMed5 and  RAE6
are  considered  relevant  because  they  have 
bibliographic 
researchers  and 
researchers working in UK institutions, respectively.
Suppose another scenario where an application offers information 
about songs in general, which are obtained from linked data sets 
available  on  the  Web.  Considering  that  this  is  a  very  popular 
domain  then  we  suppose  the  existence  of a  lot of  data  sets  and, 
consequently, there is a big probability of finding data sets of poor 
quality.  In  this  context,  even  when  application  queries  are  more 
specific it is possible finding data sets that are able of answering 
the application queries but the retrieved data is not relevant from 
the user perspective.
In  both  cases  we  can  conclude  that  considering  just  application 
queries  is  not  sufficient  to  identify  relevant  data  sources  for  a 
given application. Information about the contents of the data set is 
also necessary in order to assure that the data set meets the user 
requirements and,  therefore,  should be  considered  as  a  relevant 
one. In our approach we consider both criteria: i) a data set filter
criteria:  given  the  increasing  number  of  data  sets  becoming 
available  on  the  Web,  there  is  a  need  for  mechanisms  to  help 
filtering data sets according to a given domain and a specific set 
of  data  requirements,  and  ii)  a data  set relevance  criteria:
considering that the data sets may have poor quality, i.e., the data 
can be incorrect, incomplete or outdated, or may be very general 
(or  very  specific)  for  a  given  application,  the  need  arises  for 
solutions that help to identify relevant data sets according to the 
user requirements.
In our approach, we use application queries for data sets filtering 
and user feedback as way of capturing the relevance of a data set.  
Application  queries  are  a  good  start  pointing  for  identifying 
information that users want or need while user feedback given on 
the  results  of  application  queries  may  help  to  identify  more 
precisely  if  the  information  provided  by  a  given  data  set  is 
relevant  or  not.  In  general  terms,  if  the  user  considers  that  the 
query  results  are  good  then  the  data  set  may  be  considered 
relevant.  

1http://dblp.rkbexplorer.com/
2http://dblp.l3s.de/d2r/
3http://dbpedia.org/
4 http://www.geonames.org/
5http://pubmed.bio2rdf.org
6http://rae2001.rkbexplorer.com/

3. PRELIMINARY DEFINITIONS
In  this  section,  we  present  some  preliminary  definitions  and 
notations that we use throughout the paper.
3.1 SPARQL Query
The main parts of a SPARQL query are [10]: the pattern matching
part,  which  is  composed  by  features  of  pattern  matching  of 
graphs; the solution modifiers, which allow modifying the output
of the pattern and the output, which specifies the result form of 
the query.
In  this  work,  we  are  interested  on  SELECT  SPARQL  queries 
whose final form of the result is a table. The columns of the table 
correspond  to  the  values  obtained  from  the  matching  of  the 
variables  specified  in  the  SELECT  clause  against  the  graphs 
considered  when  computing  the  answer  and  according  to  the 
pattern described in the WHERE clause.
As an example, let us consider the SPARQL query given in Figure 
1 that  extracts  information  from  DBPedia about titles  of  The 
Beatles songs. We  can  identify  in  such  a  query:  i) the  Query 
Result Form SELECT ?title, ii) the Basic Graph Pattern (or BGP)
contained  within  WHERE  {?bandWorkdbpprop:artist  ?band.}
{?bandWorkfoaf:name 

dbpedia:The_Beatles)} section  that  describes the  patterns  the 
resulting triples should all match.
Query 1. Return the titles of all The Beatles songs.

FILTER 

?title.}

(?band 

SELECT ?title WHERE

{ ?bandWorkdbpprop:artist ?band. }
{ ?bandWorkfoaf:name ?title . }
FILTER (?band = dbpedia:The_Beatles)

Figure 1. SPARQL query example

3.2 User Feedback
Users may annotate SELECT SPARQL queries results to specify 
if a given tuple was expected in the answer table of a query (cid:1869) or 
not. We call feedback of a query (cid:1869)

(cid:1873)(cid:1858)((cid:1869)) = {((cid:1872)1, (cid:1874)1), ((cid:1872)2, (cid:1874)2), ... , ((cid:1872)(cid:1863) , (cid:1874)(cid:1863) )}

(3.1)

the set of annotations over the results of (cid:1869), such that an annotation 
may be defined by the pair: ((cid:1872), (cid:1874)), where (cid:1872) is a tuple in the answer 
table of  (cid:1869) and  (cid:1874) is the feedback value, which can be one of the 
following  terms  [2]: i)  true  positive  (tp):    a  given  instance was 
expected in  the answer;  ii)  false  positive (fp):  a  certain instance
was  not  expected  in  the  answer  and  iii)  false  negative (fn): an 
expected instance was not retrieved.

Table 1. User Feedback of Query 1

Title

Feedback

"Rock and Roll Music"

"Sgt. Pepper's Lonely Hearts Club Band"

"What Goes On"
"Hello, Goodbye"

"Hey Jude"

(cid:1872)(cid:1868)
(cid:1872)(cid:1868)
(cid:1872)(cid:1868)
(cid:1858)(cid:1868)
(cid:1858)(cid:1866)

Id
(cid:1872)1
(cid:1872)2
(cid:1872)3
(cid:1872)4
(cid:1872)5

As an example assume that the evaluation results of Query 1 are 
displayed  as  shown  in Table  1. The  user  examines  such  results 
and  provides  feedback  specifying  whether 
the 

they  meet 

50requirements. For example, the feedback instance (cid:1873)(cid:1858) given below 
specifies that the tuple (cid:1872)1 is a true positive, i.e., meets the users 
expectations, while tuple (cid:1872)4 is a false negative, i.e., the result was 
expected by the user and it was not returned.

(cid:1873)(cid:1858)((cid:1843)(cid:1873)(cid:1857)(cid:1870)(cid:1877) 1) = { ("Rock and Roll Music", (cid:1872)(cid:1868)),

("Sgt.Pepper's Lonely Hearts Club Band", (cid:1872)(cid:1868)),

("What Goes On", (cid:1872)(cid:1868)), ("Hello, Goodbye", (cid:1858)(cid:1866)), ("Hey Jude", (cid:1858)(cid:1866))}
Given the feedback annotations, it is possible to obtain values for 
precision and recall of the results of a given query (cid:1869). Specifically,
(cid:1868)(cid:1870)(cid:1857)(cid:1855)(cid:1861)(cid:1871)(cid:1861)(cid:1867)(cid:1866) is defined as the ratio of the number of true positives to 
the sum of true positives and false positives of (cid:1873)(cid:1858)((cid:1869)). Similarly, 
(cid:1870)(cid:1857)(cid:1855)(cid:1853)(cid:1864)(cid:1864) is the ratio of the number of true positives to the sum of 
true positives and false negatives of (cid:1873)(cid:1858)((cid:1869)). From these we may 
calculate  (cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) value, which is  defined  as  the  harmonic  mean 
of  the  precision  and  recall.  We  will  use  these  values  for 
calculating the relevance analysis proposed in this paper.

(cid:1842)((cid:1869)) =

(cid:1872)(cid:1868)((cid:1869))

(cid:1872)(cid:1868)((cid:1869)) + (cid:1858)(cid:1868)((cid:1869))

(cid:1844)((cid:1869)) =

(cid:1872)(cid:1868)((cid:1869))

(cid:1872)(cid:1868)((cid:1869)) + (cid:1858)(cid:1866)((cid:1869))

(cid:1832)((cid:1869)) =

2 (cid:1499) (cid:1842)((cid:1869)) (cid:1499) (cid:1844)((cid:1869))

(cid:1842)((cid:1869)) + (cid:1844)((cid:1869))

(3.2)

(3.3)

(3.4)

the  user  gives  feedback  on 

4. PROPOSED APPROACH
In this work we propose an approach for recommending data sets 
for domain-specific linked data applications. The recommendation 
is based on a set of queries that reflect the user requirements and 
on  the  user  feedback  given  on  the  results  of  such  queries. We 
assume  that  during  application  design,  an  initial set  (cid:1843) of 
significant  application  queries is defined and  a  data  set  (cid:1856)(cid:1871) is 
chosen  to  be  the  initial  one.  In  addition,  queries  from  (cid:1843) are 
executed  over  (cid:1856)(cid:1871) and 
the 
corresponding  result.  It  is  important  to  note  that  the  proposed 
approach is domain-independent, i.e., it will work with data sets 
of any domain. 
In this section we present the proposed approach, which consists 
of  two  main phases.  The  first  one searches  the  Web  of  Data  to 
discover  new  data  sets, whereas  the  second  phase  ranks  the 
discovered data sets to identify the most relevant ones.
4.1 Data Sets Filtering
The  increasing  number  of  data  sets  becoming  available  on  the 
Web  of  Data  arises  the  need  for  mechanisms  that  help  filtering 
data  sets  according  to  some  criteria.  In  our  approach,  we  are 
interested  in  filtering  data  sets  according  to  application  queries, 
i.e. we are interested in finding data sets that may help improving 
the results of the most frequently queries posed to the system. The 
filtering  process  is  performed  by  the  algorithm  presented  in 
Algorithm 1 and described as follows. 
The  algorithm  receives  a  set  of  application  queries  as  input  and 
gives  a  list  of  candidate  relevant  data  sets  as  output  (a  set  of 
SPARQL  endpoints).  The  first  step  of  the  algorithm  consists  in 
using the function ExtractRelevantResources in order to identify 
the set of relevant query resources, which will guide the search for 
candidate data sets. A query resource may be a subject or object 
of a triple and consists of a URI. The ExtractRelevantResources
function (Algorithm 2) receives as input the set  (cid:1843) and returns a 
list of the most frequent resources of (cid:1843). Specifically, the resources 

extraction  consists  in  retrieving  the  BGP  for  each  one  of  the 
queries (cid:1869) of (cid:1843) and for each triple pattern (triplePattern) of a given 
BGP, their elements (subject, predicate and object) are visited in 
order to build a list of resources and their respective occurrence. 
At the end of this process, the top-k resources will be selected as 
the most relevant ones. 
Algorithm DatasetsFiltering
Input    Q: A set of queries

k:  Number  of  relevant  resources  to  be  considered  during 
the crawling

Output  DE: A set of SPARQL endpoints of fetched datasets
Begin
1.
2.
3.
4.
5.

(cid:1844)(cid:1844) (cid:1370) (cid:2161)(cid:2206)(cid:2202)(cid:2200)(cid:2183)(cid:2185)(cid:2202)(cid:2174)(cid:2187)(cid:2194)(cid:2187)(cid:2204)(cid:2183)(cid:2196)(cid:2202)(cid:2174)(cid:2187)(cid:2201)(cid:2197)(cid:2203)(cid:2200)(cid:2185)(cid:2187)(cid:2201)((cid:1843))
(cid:1845)(cid:1857)(cid:1857)(cid:1856)(cid:1871) (cid:1370) (cid:1845)(cid:1857)(cid:1864)(cid:1857)(cid:1855)(cid:1872)(cid:1844)(cid:1857)(cid:1871)(cid:1867)(cid:1873)(cid:1870)(cid:1855)(cid:1857)(cid:1871)((cid:1844)(cid:1844), (cid:1863))
(cid:1842)(cid:1870)(cid:1857)(cid:1856)(cid:1861)(cid:1855)(cid:1853)(cid:1872)(cid:1857)(cid:1871) (cid:1370) {(cid:1871)(cid:1853)(cid:1865)(cid:1857)(cid:1827)(cid:1871), (cid:1871)(cid:1857)(cid:1857)(cid:1827)(cid:1864)(cid:1871)(cid:1867), (cid:1857)(cid:1869)(cid:1873)(cid:1861)(cid:1874)(cid:1853)(cid:1864)(cid:1857)(cid:1866)(cid:1872)(cid:1829)(cid:1864)(cid:1853)(cid:1871)(cid:1871)}
(cid:1832)(cid:1857)(cid:1872)(cid:1855)(cid:1860)(cid:1857)(cid:1856)(cid:1846)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1871) (cid:1370) (cid:1831)(cid:1876)(cid:1857)(cid:1855)(cid:1873)(cid:1872)(cid:1857)(cid:1829)(cid:1870)(cid:1853)(cid:1875)(cid:1864)(cid:1861)(cid:1866)(cid:1859)((cid:1845)(cid:1857)(cid:1857)(cid:1856)(cid:1871), (cid:1842)(cid:1870)(cid:1857)(cid:1856)(cid:1861)(cid:1855)(cid:1853)(cid:1872)(cid:1857)(cid:1871))
(cid:1842)(cid:1870)(cid:1867)(cid:1874)(cid:1857)(cid:1866)(cid:1853)(cid:1866)(cid:1855)(cid:1857)(cid:1846)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1871) (cid:1370)
(cid:1831)(cid:1876)(cid:1872)(cid:1870)(cid:1853)(cid:1855)(cid:1872)(cid:1842)(cid:1870)(cid:1867)(cid:1874)(cid:1857)(cid:1866)(cid:1853)(cid:1866)(cid:1855)(cid:1857)((cid:1832)(cid:1857)(cid:1872)(cid:1855)(cid:1860)(cid:1857)(cid:1856)(cid:1846)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1871))
(cid:1830)(cid:1831) (cid:1370) (cid:1486)
(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1868) (cid:1488) (cid:1842)(cid:1870)(cid:1867)(cid:1874)(cid:1857)(cid:1866)(cid:1853)(cid:1866)(cid:1855)(cid:1857)(cid:1846)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1871) (cid:2186)(cid:2197)

(cid:1830)(cid:1831) (cid:1370) (cid:1830)(cid:1831) (cid:1515) (cid:1844)(cid:1857)(cid:1872)(cid:1870)(cid:1861)(cid:1857)(cid:1874)(cid:1857)(cid:1845)(cid:1868)(cid:1853)(cid:1870)(cid:1869)(cid:1864)(cid:1831)(cid:1866)(cid:1856)(cid:1868)(cid:1867)(cid:1861)(cid:1866)(cid:1872)((cid:1868))

6.
7.
8.
9.
10. (cid:2174)(cid:2187)(cid:2202)(cid:2203)(cid:2200)(cid:2196) (cid:1830)(cid:1831)
EEnd

(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)

Algorithm 1. Data sets Filtering

(cid:1828)(cid:1833)(cid:1842) (cid:1370) (cid:1831)(cid:1876)(cid:1872)(cid:1870)(cid:1853)(cid:1855)(cid:1872)(cid:1828)(cid:1833)(cid:1842)((cid:1869))

(cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:1838)(cid:1861)(cid:1871)(cid:1872) (cid:1370) (cid:1486)
(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1869) (cid:1488) (cid:1843) (cid:2186)(cid:2197)

Algorithm ExtractRelevantResources
Input      Q: A set of queries
Output   RR: A sorted list by frequency of query resources
Begin
1.
2.
3.
4.
5.
6.
7.
8.
9.
10. (cid:2174)(cid:2187)(cid:2202)(cid:2203)(cid:2200)(cid:2196) (cid:1844)(cid:1844)
EEnd

(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)
(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)
(cid:1844)(cid:1844) (cid:1370) (cid:1830)(cid:1857)(cid:1855)(cid:1870)(cid:1857)(cid:1853)(cid:1871)(cid:1861)(cid:1866)(cid:1859)(cid:1831)(cid:1864)(cid:1857)(cid:1865)(cid:1857)(cid:1866)(cid:1872)(cid:1871)(cid:1838)(cid:1861)(cid:1871)(cid:1872)((cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:1838)(cid:1861)(cid:1871)(cid:1872))

(cid:1844)(cid:1857)(cid:1871)(cid:1867)(cid:1873)(cid:1870)(cid:1855)(cid:1857)(cid:1871) (cid:1370) (cid:1848)(cid:1861)(cid:1871)(cid:1861)(cid:1872)(cid:1846)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1842)(cid:1853)(cid:1872)(cid:1872)(cid:1857)(cid:1870)(cid:1866)((cid:1872)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1842)(cid:1853)(cid:1872)(cid:1872)(cid:1857)(cid:1870)(cid:1866))
(cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:1838)(cid:1861)(cid:1871)(cid:1872) (cid:1370) (cid:1832)(cid:1870)(cid:1857)(cid:1869)(cid:1873)(cid:1857)(cid:1866)(cid:1855)(cid:1877)(cid:1838)(cid:1861)(cid:1871)(cid:1872) (cid:1515) (cid:1844)(cid:1857)(cid:1871)(cid:1867)(cid:1873)(cid:1870)(cid:1855)(cid:1857)(cid:1871)

(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1872)(cid:1870)(cid:1861)(cid:1868)(cid:1864)(cid:1857)(cid:1842)(cid:1853)(cid:1872)(cid:1872)(cid:1857)(cid:1870)(cid:1866) (cid:1488) (cid:1828)(cid:1833)(cid:1842) (cid:2186)(cid:2197)

Algorithm 2. Extraction of Relevant Resources

owl:sameAs

(rdfs:seeAlso,

set  of  predicates 

Once  the  most  relevant  resources  were  identified,  the  next  step 
consists of crawling the Web of Data in order to find the set of 
candidate data sets. The crawling process considers as seeds the
top-k  resources  of  the  list RR  (Relevant  Resources),  which  is 
composed by URIs that represent relevant resources according to 
(cid:1843).  A 
and
owl:equivalentClass) is used  during  the  crawling  to  obtain  new 
resources that are similar to the ones of  seeds. At the end of the 
crawling, a list of new resources is obtained. The next step of the 
filtering process consists in building the list of relevant candidate 
sets. For this, it is extracted the provenance of the triples obtained 
during  the  crawling  process.  Using  the  provenance  URI  of  each 
triple stored, it is used the function RetrieveSparqlEndpoint which 
extracts  the  data  sets  from  which  the  resources  of  the  list  were 
obtained.
4.2 Data Sets Relevance Analysis
Our proposal for evaluating the relevance of a data set considers 
as input a set  (cid:1843) = {(cid:1869)1, ... ,(cid:1869) (cid:1866) } of SPARQL queries and a set of 
user feedback annotations (cid:1847)(cid:1832) = {(cid:1873)(cid:1858)((cid:1869)1), ... ,(cid:1873)(cid:1858) ((cid:1869)(cid:1866) )} given over 
the  results  of  such  queries. Queries  from  (cid:1843) reflect  the  main 
application  data  requirements, while 
the  set  of  feedback 
annotations helps to identify the information that users really want 
or need. 

51In  order  to  refine  the  relevance  analysis,  for  each  query  (cid:1869) is 
assigned  a  weight  value.  The  (cid:1875)(cid:1857)(cid:1861)(cid:1859)(cid:1860)(cid:1872) of  a  query  (cid:1869),  denoted  by 
(cid:1875)((cid:1869)),  helps  to  identify  the  most  important  queries  for  the 
application. Consider two queries (cid:1869)1 and (cid:1869)2, with weights (cid:1875)((cid:1869)1)
and(cid:1875)((cid:1869)2), respectively, if (cid:1875)((cid:1869)1) >(cid:1875) ((cid:1869)2) then query (cid:1869)1 is more 
important than (cid:1869)2. In our approach, the weight (cid:1875)((cid:1869))is defined by 
the execution frequency of (cid:1869).
In this context, given a set of SPARQL queries (cid:1843) = {(cid:1869)1, ... , (cid:1869)(cid:1866) },
a  set  of  weights  (cid:1849) = {(cid:1875)((cid:1869)1), ... (cid:1875)((cid:1869)(cid:1866) )},  a  set  of  feedback 
instances  (cid:1847)(cid:1832) = {(cid:1873)(cid:1858)((cid:1869)1), ... ,(cid:1873)(cid:1858) ((cid:1869)(cid:1866) )} and a candidate data set  (cid:1856),
the  relevance calculus of the data set may be defined as follows:

(cid:1844)((cid:1856)) =

(cid:963) (cid:1828)((cid:1856), (cid:1869)(cid:1861)) (cid:1499) (cid:1875)((cid:1869)(cid:1861))

|(cid:1843)|
(cid:1861)=1

(cid:963) (cid:1875)((cid:1869)(cid:1861))

|(cid:1843)|
(cid:1861)=1

(4.1)

Where  (cid:1828)(cid:1857)(cid:1866)(cid:1857)(cid:1858)(cid:1861)(cid:1872) (cid:1828)((cid:1856), (cid:1869)(cid:1861)) determines  the  level  of  influence  of  a 
data set (cid:1856) for a given query (cid:1869)(cid:1861). Benefit (cid:1828)((cid:1856), (cid:1869)) may be calculated 
as defined below:

(cid:1828)((cid:1856), (cid:1869)) =

(cid:1832)(cid:1314)
(cid:1832)

(3.5)

Where (cid:1832) is the value of (cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) obtained from (cid:1873)(cid:1858)((cid:1869)), such that 
(cid:1873)(cid:1858)((cid:1869)) is  the  set  of  annotations  over  the  results  of  (cid:1869) when  (cid:1869) is 
evaluated  over  a  set  of  data  sets  that does  not  include  the 
candidate data set (cid:1856); and (cid:1832) is the value of (cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) obtained from 
(cid:1873)(cid:1858)((cid:1869)), where (cid:1873)(cid:1858)((cid:1869)) is the set of annotations over the results of 
(cid:1869) when  (cid:1869) is  evaluated  over  a  set  of  data  sets  that includes  the 
candidate data set (cid:1856).
In a general way, we can say that a data set (cid:1856) has a good influence 
over  a  query  (cid:1869) if  richer  query  answers  are  obtained  when  (cid:1869) is 
evaluated  over a  set  of  data  sets,  which  includes  the  data  set  (cid:1856).
Considering the  user  feedback previously defined,  (cid:1856) has a  good 
influence  over  (cid:1869) if  (cid:1872)(cid:1868)((cid:1869)) augments  or  if  (cid:1858)(cid:1868)((cid:1869)) reduces.  The 
algorithm  for  computing  the  relevance  of  a  given  data  set  is 
presented in Algorithm 3 and described below.
Initially, for each one of the queries  (cid:1869) of  (cid:1843), the following steps 
are  performed. Using  the  function  CalculateF_measure the 
algorithm computes the values of precision, recall and f-measure 
based on annotations provided by the user feedback (line 2). The 
next  step  consists  of  rewriting  the  query  (cid:1869) in  order  to  allow  its 
execution  over  (cid:1856) (lines  3-4).  The  rewriting  of  query  (cid:1869) becomes 
necessary because (i) the query  (cid:1869) was initially submitted over a 
data set different from (cid:1856) and (ii) the calculus of the (cid:1854)(cid:1857)(cid:1866)(cid:1857)(cid:1858)(cid:1861)(cid:1872) of (cid:1856)
depends  on  the  feedback  given  over  the  results  of  (cid:1869) when  (cid:1869) is 
evaluated  considering  a  set  of  data  sets  that  includes  (cid:1856).  It  is 
important  to  note  that  the  query  rewriting  process  is  out  of  the 
scope of this work. More details about this task can be found in 
other works proposed in the literature [1,4,6,8].
Once the rewritten query (cid:1869) is obtained and executed over (cid:1856), the 
function  InferFeedback (Algorithm  4) is  used  to  infer  feedback 
annotations for the results of the rewritten query. This is done in 
order to avoid that the user has to give a new feedback every time 
that a new data set is being evaluated. The InferFeedback function 
infers feedback annotations for the results of the rewritten query 
(cid:1869). Such inference is based on the feedback annotations previously 
given over the results of (cid:1869), in such a way that when a tuple (cid:1872) in 
the answer table of (cid:1869) is equivalent to a tuple (cid:1872) in the answer table 
of (cid:1869), we have two cases: (i) the feedback value of (cid:1872) is true positive 
or false positive: in this case, the new feedback value of (cid:1872) is the 
same of (cid:1872) (true positive or false positive, respectively) and (ii) the 

feedback  value  of  (cid:1872) is  false  negative:  in  this  case the  feedback 
value of (cid:1872) receives false negative. However, if there is no tuple (cid:1872)(cid:1314)
that is equivalent to (cid:1872) and the feedback value of (cid:1872) is false negative 
then  a  new  annotation  ((cid:1872), (cid:1858)(cid:1866)) should  be  included  in  the  set  of 
inferred  feedback  annotations. Next,  new  values  of  precision, 
recall and f-measure are calculated (line 6) and a relevance value 
(line 7) is computed based on the values of (cid:1832), (cid:1832) and the (cid:1875)(cid:1857)(cid:1861)(cid:1859)(cid:1860)(cid:1872)
of (cid:1869). After doing this for each query (cid:1869), the relevance value of the 
data set (cid:1856) is calculated and its value (line 11) is returned. 
The RelevanceAnalysis algorithm has to be executed for each one 
of the data sets identified during the filtering phase. As a result, a
ranking list is created and the top-k most candidate relevant data 
sets will be recommended to be included in the application.
Algorithm RelevanceAnalysis
Inputs      Q: A set of queries

UF: A set of feedback instances
W: A set of weights
d: a candidate data set

(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1869) (cid:1488) (cid:1843) (cid:2186)(cid:2197)

Output     Relevance: A new relevance value
Begin
1.
2.
3.
4.
5.
6.
7.
8.
9.
10. (cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1874)(cid:1853)(cid:1866)(cid:1855)(cid:1857) (cid:1370) (cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1874)(cid:1853)(cid:1866)(cid:1855)(cid:1857) / (cid:1845)(cid:1873)(cid:1865)
11. (cid:2174)(cid:2187)(cid:2202)(cid:2203)(cid:2200)(cid:2196) (cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1874)(cid:1853)(cid:1866)(cid:1855)(cid:1857)
EEnd

(cid:1847)(cid:1832) (cid:1370) (cid:1829)(cid:1853)(cid:1864)(cid:1855)(cid:1873)(cid:1864)(cid:1853)(cid:1872)(cid:1857)(cid:1832)_(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857)(cid:3435)(cid:1873)(cid:1858)((cid:1869))(cid:3439)
(cid:1869)(cid:1314) (cid:1370) (cid:1844)(cid:1857)(cid:1875)(cid:1870)(cid:1861)(cid:1872)(cid:1857)(cid:1843)(cid:1873)(cid:1857)(cid:1870)(cid:1877)((cid:1869), (cid:1856))
(cid:1844)(cid:1857)(cid:1871)(cid:1873)(cid:1864)(cid:1872) (cid:1370) (cid:1831)(cid:1876)(cid:1857)(cid:1855)(cid:1873)(cid:1872)(cid:1857)(cid:1843)(cid:1873)(cid:1857)(cid:1870)(cid:1877)((cid:1869)(cid:1314))
(cid:1873)(cid:1858)((cid:1869)(cid:1314)) (cid:1370) (cid:2165)(cid:2196)(cid:2188)(cid:2187)(cid:2200)(cid:2162)(cid:2187)(cid:2187)(cid:2186)(cid:2184)(cid:2183)(cid:2185)(cid:2193)((cid:1873)(cid:1858)((cid:1869)), (cid:1844)(cid:1857)(cid:1871)(cid:1873)(cid:1864)(cid:1872)))
(cid:1847)(cid:1832)(cid:1314) (cid:1370) (cid:1829)(cid:1853)(cid:1864)(cid:1855)(cid:1873)(cid:1864)(cid:1853)(cid:1872)(cid:1857)(cid:1832)_(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857)( (cid:1873)(cid:1858)((cid:1869)(cid:1314)))
(cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1874)(cid:1853)(cid:1866)(cid:1855)(cid:1857) (cid:1370) (cid:1844)(cid:1857)(cid:1864)(cid:1857)(cid:1874)(cid:1853)(cid:1866)(cid:1855)(cid:1857) + (cid:3435)(cid:1832)(cid:1314) (cid:1832)(cid:932) (cid:1499) (cid:1875)((cid:1869))(cid:3439)
(cid:1845)(cid:1873)(cid:1865) (cid:1370) (cid:1845)(cid:1873)(cid:1865) + (cid:1875)((cid:1869))

(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)

Algorithm 3. Relevance Analysis

Algorithm InferFeedback
Inputs   uf(q): A user feedback of query q

Result(q):  A  set  of  tuples  obtained  over  execution  of  the
query (cid:1869)

Output  uf(q): A inferred feedback of query (cid:1869)
Begin

1.
2.
3.
4.

(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1853)(cid:1866)(cid:1866)(cid:1867)(cid:1872)(cid:1853)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:1853) (cid:1488) (cid:1873)(cid:1858) (cid:2186)(cid:2197)
(cid:2162)(cid:2197)(cid:2200) (cid:2187)(cid:2183)(cid:2185)(cid:2190) (cid:1870)(cid:1857)(cid:1871)(cid:1873)(cid:1864)(cid:1872) (cid:1870) (cid:1488) (cid:1844)(cid:1857)(cid:1871)(cid:1873)(cid:1864)(cid:1872) (cid:2186)(cid:2197)
(cid:2165)(cid:2188) ((cid:1872)((cid:1853)) = (cid:1870)) (cid:2202)(cid:2190)(cid:2187)(cid:2196)
(cid:2165)(cid:2188) ((cid:1874)((cid:1853)) = (cid:1846)(cid:1870)(cid:1873)(cid:1857) (cid:1842)(cid:1867)(cid:1871)(cid:1861)(cid:1872)(cid:1861)(cid:1874)(cid:1857) (cid:2171)(cid:2174) (cid:1874)((cid:1853)) =
(cid:1832)(cid:1853)(cid:1864)(cid:1871)(cid:1857) (cid:1840)(cid:1857)(cid:1859)(cid:1853)(cid:1872)(cid:1861)(cid:1874)(cid:1857)) (cid:2202)(cid:2190)(cid:2187)(cid:2196)

(cid:1874)(cid:1314)((cid:1853)) (cid:1370) (cid:1846)(cid:1870)(cid:1873)(cid:1857) (cid:1842)(cid:1867)(cid:1871)(cid:1861)(cid:1872)(cid:1861)(cid:1874)(cid:1857)

(cid:2161)(cid:2194)(cid:2201)(cid:2187) (cid:1874)(cid:1314) ((cid:1853)) (cid:1370) (cid:1832)(cid:1853)(cid:1864)(cid:1871)(cid:1857) (cid:1842)(cid:1867)(cid:1871)(cid:1861)(cid:1872)(cid:1861)(cid:1874)(cid:1857)
(cid:1858)(cid:1861)(cid:1866)(cid:1858) (cid:1857)(cid:1870)(cid:1870)(cid:1857)(cid:1856) (cid:1370) (cid:1858)(cid:1861)(cid:1866)(cid:1858)(cid:1857)(cid:1870)(cid:1870)(cid:1857)(cid:1856) + {((cid:1872)((cid:1853)), (cid:1874)(cid:1314)((cid:1853)))}
(cid:2161)(cid:2196)(cid:2186) (cid:2165)(cid:2188)
(cid:2161)(cid:2194)(cid:2201)(cid:2187) (cid:2165)(cid:2188) ((cid:1874)((cid:1853)) = (cid:1832)(cid:1853)(cid:1864)(cid:1871)(cid:1857)(cid:1840)(cid:1857)(cid:1859)(cid:1853)(cid:1872)(cid:1861)(cid:1874)(cid:1857)) (cid:2202)(cid:2190)(cid:2187)(cid:2196)
(cid:1874)(cid:1314) ((cid:1853)) (cid:1370) (cid:1832)(cid:1853)(cid:1864)(cid:1871)(cid:1857) (cid:1840)(cid:1857)(cid:1859)(cid:1853)(cid:1872)(cid:1861)(cid:1874)(cid:1857)
(cid:1858)(cid:1861)(cid:1866)(cid:1858)(cid:1857)(cid:1870)(cid:1870)(cid:1857)(cid:1856) (cid:1370) (cid:1858)(cid:1861)(cid:1866)(cid:1858)(cid:1857)(cid:1870)(cid:1870)(cid:1857)(cid:1856) + {((cid:1872)((cid:1853)), (cid:1874)(cid:1314)((cid:1853)))}
(cid:2161)(cid:2196)(cid:2186) (cid:2165)(cid:2188)
(cid:2161)(cid:2196)(cid:2186) (cid:2165)(cid:2188)

5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16. (cid:2174)(cid:2187)(cid:2202)(cid:2203)(cid:2200)(cid:2196) (cid:1858)(cid:1861)(cid:1866)(cid:1858)(cid:1857)(cid:1870)(cid:1870)(cid:1857)(cid:1856)
End

(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)

(cid:2161)(cid:2196)(cid:2186) (cid:2188)(cid:2197)(cid:2200)

Algorithm 4. Feedback Inference

4.3 An example
To  illustrate  the  data  sets  recommendation  process  consider  the 
following  example.    Suppose  an  application  that  plans  to  offer 
information about The Beatles. Let (cid:1843) = {(cid:1869)1, (cid:1869)2, (cid:1869)3}, presented in 
Figure  2,  be  the  set  of  initial  queries  defined  during  the 

52application design and DBpedia the data set chosen to be the base
data source. 
q1: Return the titles of all The Beatles songs
SELECT ?title WHERE{

{ ?bandWorkdbpprop:artist ?band. }
{ ?bandWorkfoaf:name ?title . }
FILTER (?band = dbpedia:The_Beatles)

q2: Return the member names and his spouse name
SELECT ?memberName ?memberSpouseName WHERE{

{ dbpedia:The_Beatlesdbpedia-owl:bandMember ?member . }
{ ?member foaf:name ?memberName . }
{ ?memberSpousedbpedia-owl:spouse ?member . }
{ ?memberSpousefoaf:name ?memberSpouseName . }

q3: Return the websites of the band The Beatles
SELECT ?memberName ?memberPage WHERE{

{dbpedia:The_Beatlesdbpedia-owl:bandMember  ?member 

. }

{ ?member foaf:name ?memberName . }
{ ?member foaf:page ?memberPage . }

Figure 2. Example queries

Initially,  queries  from  (cid:1843) are  executed  over  DBpedia  and  the 
corresponding  results  are  annotated  by  one  of  the  application 
users.  Figure  3 presents  some  of 
the  provided  feedback 
annotations.

(cid:1873)(cid:1858)((cid:1869)1) = { ("Rock and Roll Music", tp),

("Sgt.Pepper's Lonely Hearts Club Band", tp),

query (cid:1869)1, which is executed over the data set BBCMusic. Then, 
tuples in the corresponding answer are annotated according to the 
inferred feedback (Figure 4) and new values for precision, recall 
and f-measure are calculated for query (cid:1869)1. As a result, the values 
(cid:1858)(cid:1314)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)1) = 0,8 and  (cid:1828)(BBC, (cid:1869)1) = 1,14
are obtained. 
Considering such values and the weight of (cid:1869)1 then it is calculated 
the relevance of BBCMusic with respect to (cid:1869)1.
Table 2. List of resources identified during crawling the Web 

of Data

http://dbpedia.org/resource/The_Beatles
http://en.wikipedia.org/wiki/Ringo_Starr
http://dbpedia.org/resource/John_Lennon
http://dbpedia.org/resource/Ringo_Starr
http://en.wikipedia.org/wiki/George_Harrison

http://dbpedia.org/resource/Paul_McCartney

(cid:1873)(cid:1858)((cid:1869)1(cid:1314))
= { ("Rock and Roll Music", tp), ("What Goes On", tp),

("Hello, Goodbye", fn),("Hey Jude", tp)}

(cid:1873)(cid:1858)((cid:1869)2(cid:1314)) = { ("John Lennon", "Yoko Ono," tp),

("Ringo Starr", "Barbara Bach," tp),

("Paul McCartney", "Linda McCartney" , tp)}

(cid:1873)(cid:1858)((cid:1869)3(cid:1314)) = {

("What Goes On", tp),("Hello, Goodbye", fn),("Hey Jude", fn)}

("Ringo Starr", http://dbpedia.org/resource/Ringo_Starr ,tp),

(cid:1873)(cid:1858)((cid:1869)2) = { ("John Lennon", "Yoko Ono", tp),

("Ringo Starr", "Barbara Bach", fn),

("Ringo Starr", "Maureen Cox", fp),

("Paul McCartney", "Linda McCartney", tp)  }

(cid:1873)(cid:1858)((cid:1869)3)
= { ("Ringo Starr", http://dbpedia.org/resource/Ringo_Starr ,

tp), ("MBE", http://dbpedia.org/resource/PaulMcCartney, fp ),

("McCartney", http://dbpedia.org/resource/McCartney, fn )}
Figure 3. Feedback annotations for tuples obtained from 

DBpedia

Next, based on queries from (cid:1843) is executed the data sets filtering 
phase. Initially, the most relevant resources are extracted from the 
BGP of queries  (cid:1869)1, (cid:1869)2 and  (cid:1869)3. Once the most relevant resources 
were  identified,
the  crawling  process  starts  with  the  goal  of 
finding new data sets capable of providing resources similar to the 
ones  obtained  from  the  BGP.  Table  2  presents  a  list  of  similar 
resources identified during the crawling. The provenance of these 
resources  is  analyzed  and  then  the  data  sets  BBCMusic7 and 
MusicBrainz8
The next step consists of analyzing the relevance of the candidate 
data  sets  with  respect  to  the  user  requirements gathered  through 
the user feedback. Consider, for example, the data set BBCMusic. 
Initially, values for precision, recall and f-measure are calculated 
for  query  (cid:1869)1 considering  the  feedback  annotations  given  on  the 
answer obtained from DBpedia (Figure 3). As a result, the value 
(cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)1) = 0,7 is obtained. Next, query (cid:1869)1 is rewritten into a 

are identified as the candidate relevant data sets.

7www.bbc.co.uk/music
8musicbrainz.org/

("McCartney", http://dbpedia.org/resource/McCartney, fn )}
Figure 4. Inferred feedback annotations for tuples obtained 

from BBCMusic

The same procedure is performed for queries (cid:1869)2 and (cid:1869)3. Finally, 
based on relevance values of BBCMusic with respect to(cid:1869)1, (cid:1869)2 and 
(cid:1869)3,  it  is  calculated  its relevance  with  respect  to  (cid:1843).  The  final 
relevance  value  is  (cid:1844)((cid:1828)(cid:1828)(cid:1829)(cid:1839)(cid:1873)(cid:1871)(cid:1861)(cid:1855)) = 1,08.  Table  3 summarizes 
the  values  of  f-measure,  f-measure  and  benefit  used  in  this 
calculus. 

Table 3. Benefit values for the data set BBCMusic

(cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)1) = 0,7

(cid:1858)(cid:1314)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)1) = 0,8

(cid:1828)((cid:1828)(cid:1828)(cid:1829), (cid:1869)1) = 1,14

(cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)2) = 0,67

(cid:1858)(cid:1314)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)2) = 0,75

(cid:1828)((cid:1828)(cid:1828)(cid:1829), (cid:1869)2) = 1,12

(cid:1858)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)3) = 0,85

(cid:1858)(cid:1314)(cid:1865)(cid:1857)(cid:1853)(cid:1871)(cid:1873)(cid:1870)(cid:1857) ((cid:1869)3) = 0,9

(cid:1828)((cid:1828)(cid:1828)(cid:1829), (cid:1869)3) = 1,05

In  our  approach  the  relevance  value  (cid:1844)((cid:1856)) (cid:1488) [1, (cid:955)).  When 
0 < (cid:1844) (cid:3409) 1, we can say that the data set (cid:1856) has low or no influence 
over the application queries, i.e. data set  (cid:1856) will be less relevant. 
However, when (cid:1844) increases, the influence of the data set over the 
application queries also increases. Then the higher the value of (cid:1844),
the better will be the data set for the application considering both 
the application queries and the user feedback.
The  relevance  analysis  is  also  performed  for  the  data  set 
MusicBrainz. Then the user may choose which data set should be 
included in the application.

535. IMPLEMENTATION ISSUES AND 
EXPERIMENTS
To  validate  our  approach,  a  prototype  has  been  developed  and 
some  experiments on  the domain  of  bibliographic  data were 
performed. Specifically, we considered that users were interested 
on  information  about  Computer  Science  bibliography.  Our 
scenario  was  composed  of  five  SPARQL  queries  and  DBLP 
RKBExplorer was chosen as the initial data set.
Initially,  as  the  result  of the  data  set  filtering  phase,  three 
candidate data sets were returned: ACM9, Citeseer10 and Roma11

For  each  one  of  the  candidate data  sets,  the  relevance  analysis 
process  was  performed. Figure  5 presents  the  benefit  values  for 
each data set with respect to each one of the five queries. 

Benefit values for each candidate data set


CiteSeer

Roma

q1

q2

q3

q4

q5

Figure 5. Benefit values for candidate data sets

It  is  important to note  that for some  queries,  a  data  set  may  be
more relevant than the others. For example, for query (cid:1869)4 Citeseer
has  a  benefit  value  better  than  ACM,  while  in  query  (cid:1869)2 is  the 
opposite. Based  on  these  benefit  values  a  relevance  value  was
obtained for each one of the three data sets. 
The Citeseer data set had the best relevance value ((cid:1844) = 1,30). In 
other words, this data set has contributed more than the others as 
it could  contribute  for  the  answering  of the  great  majority  of 
queries.  In  the  ACM data  set,  we  had  a  smaller  contribution
((cid:1844) = 1,14) because only  few queries could  be affected  by  the 
inclusion of this source. The last one, the Roma data set had the 
worst result ((cid:1844) = 0) because almost no query would be affected 
by the addition of this source.
Considering  these  relevance  values,  we  conclude that  ACM  and 
Citeseer data  sets  are  good  sources  and 
then  should  be 
recommended  as  new  data  sets  for  the  application  because  they 
may  improve  the  quality of  queries results.  However,  the  Roma 
data  set  is  not  good.  Its  relevance  value  is  low,  i.e.,  it  did  not 
contribute to improve the query results.
We can also observe that the proposed approach selects a data set 
as a relevant one based on its relevance for the application instead 
of the quality of the data. In this setting, it is possible to avoid the 
selection  of  those data  sets  with  bad  quality.  However, it  is  not
possible to assure that the data sets with good data quality will be 
selected.

9http://acm.rkbexplorer.com/
10http://citeseer.rkbexplorer.com/
11http://roma.rkbexplorer.com/

During our experiments, we faced some difficulties regarding the 
availability  of  data  sets  because  some  of  them do  not  have  an 
active and online SPARQL endpoint. Therefore, if the endpoint is 
not  available  then  it  is  not  possible  to  execute  queries  and 
consequently the data set couldnt be evaluated. Because of this, 
several good data sets couldnt be recommended. Other difficulty 
regards  the  blocking  of  the  crawler  access. In  our  approach, 
crawlers  are  used  to  obtain  relevant  resources  based  on  the 
application queries in order to help to find out new candidate data 
sets.  During  our  experiments,  we  found  out  that  many  data  sets 
block the access of these crawlers, making it impossible to access 
its data.
Moreover,  it  should  be  noted  that  the  number  of  queries  is 
relevant to the proposed approach, however the quality of the user 
feedback is even more important. As a consequence, there will be 
situations  where  a  small  number  of  queries  with  feedback 
annotations of good quality will give better results than a larger 
set of queries with inconsistent feedback annotations, for example.
6. RELATED WORK
In this section we briefly present some of the research literature 
related to our work. The work presented in [9], for example, has 
the  goal  of  identifying  relevant  sources  for  data  linking.  They 
propose an approach, which utilizes keyword-based search to find 
initial candidate sources for data linking, and ontology matching 
technologies as a way to assess the relevance of these candidates. 
Their  approach  has  two  main  steps:  (i)  the  searching  for 
potentially entities in external data sources and (ii) the filtering of 
these sources using ontology matching techniques to filter out the 
irrelevant  ones.  They  also  apply  a  similarity  measure  between 
classes of the different sources in order to filter out the ones with 
low scores. One drawback of this proposal is that, in the filtering 
stage, only classes with stronger degree of semantic similarity are 
confirmed. In other words, many relevant classes may be filtered 
out because they are not considered as exact matches. Our work 
differs  from  this  one  in  the  sense  that  their  approach  focus  on 
finding relevant data sets for linking instead of querying.
In the paper [12] the authors propose to find dirty sources using 
functional dependencies with probabilities (pFD) in the context of
pay-as-you-go data integration systems. During the addition of a 
new  data  source,  it  is  possible  to  decide  if  the  source  is  good 
enough  for  the  system based  on  the  quality  of  the  functional 
dependencies.  It  is  important  to  mention  that  this  approach  just 
considers  the  relational  model;  in  addition  it  also  supposes  the 
presence of a mediated schema.
In the work [11] is presented an approach to guide the addition of 
new  sources  in  keyword  search-based  data  integration  systems. 
This  process  builds  a  search  graph  from  the  sources  and  its 
relationships.  The  search  is  performed  over  the  graph  and  the 
results are returned in a top-k view with the most relevant answers 
to the user. The graph maintenance is made incrementally through 
user  feedback  and  when  new  data  sources  are  discovered  the 
graph is realigned.
The work proposed in [7] uses the user feedback to rank mappings 
in pay-as-you-go systems. The approach uses the concept of VPI 
(value of perfect information) as a metric to rank. VPI provides a 
means  of  estimating  the  benefit  to  the  pay-as-you-go system  in 
such  a  way  that  it  is  possible  to  evaluate  the  correctness  of  a 
candidate  matching  based  on  the  user  feedback. This  concept  is 
based on the utility function that quantifies the quality of querys 
results. Similarly to VPI, in our approach we generate a relevance 

54value.  However,  we  are  interested  in  rank  data  sets  instead  of 
ranking mappings.
7. CONCLUSION
Given  the  high  heterogeneity  and  dynamicity  of  the  data  sets 
available on the Web of Data, it is possible to have data sets of 
poor  quality,  i.e.,  the  data  can  be  incorrect,  incomplete  or 
outdated. Therefore,  having  solutions  that  may  help  to  identify 
good data sets to be used as input for a given application becomes 
crucial.  In  this  paper,  we  presented  an  approach  to  recommend 
new  data  sets  for  domain-specific  linked  data  applications. 
Among  the  main  distinguishing  issues  of  our  approach,  we 
highlight  the  use  of  application  queries  and  feedback  given  by 
users, where application queries help to filter the Web of Data in 
order to find candidate relevant data sets, while the user feedback 
helps  to  analyze  the  relevance  of such  candidates.  Considering 
both criteria, it is possible to find out data sets that really meet the 
user  requirements.  It  is  important  to  note  that  the  proposed 
approach may be used in any application that needs to add new 
data sources regardless of the data model in which the application 
was designed.
To  validate  our  approach,  it  was  developed  a  prototype  that 
performs both data sets filtering and data sets relevance analysis. 
The  prototype  also  allows  users  to  provide  feedback  through  a 
GUI interface. Some experiments were performed to validate the 
behavior  of  the  relevance  value  calculated  according  to  the 
approach proposed in this paper.
As future work, we would highlight some directions: 
(i) The  improvement  of  techniques  for  the  data  sets  filtering 
phase: we intend to adopt new data sets filtering techniques 
in  order  to  minimize  some  of  the  difficulties  previously 
described. For example, candidate data sets may be identified 
through  the  use  of  semantic  indexes  or  a  repository  that 
stores information about the data sets may also be built. We 
will also investigate the use of VoiD descriptions in order to 
help or improve relevant data sets identification.

(ii) The improvement of the user feedback gathering: currently, 
we assume that a single user gives the feedback and if there 
are  two  annotations  for  the  same  tuple  just  the  last  one  is 
considered  the  latest.  We  intend  to  apply  more  efficient 
techniques  for  user  feedback  gathering  as  well  as  for 
managing the user feedback evolution. 

(iii) The  improvement  of  the  feedback  inference  algorithm: 
considering that feedback inference was not the main focus 
of our work, we applied just a simple technique for obtaining 
new  feedback  from  the  one  already  given  by  the  user.  We 
also intend to improve the quality of the inferred feedback, 
and thus improve the quality of the data set recommendation.
(iv) Case  study  on  government  data integration:  we  intend  to 
build a case study considering open data from the Brazilian
government.  We  plan  to  build  a  service  to provide  support 
for the development of applications that integrates data sets 
about  the  Brazilian  government  available  on  the  Web, 
through the recommendation of relevant data sets.
