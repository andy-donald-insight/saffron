Semantic Analysis of Human Movements in Videos

Sawsan Saad

Department of Computer

Science

Faculty of Engineering

Mons, Belgium

Science

Faculty of Engineering

Mons, Belgium

Said Mahmoudi

Department of Computer

Pierre Manneback
Department of Computer

Science

Faculty of Engineering

Mons, Belgium

sawsan.saad@umons.ac.be

said.mahmoudi@umons.ac.be

pierre.manneback@umons.ac.be

ABSTRACT
Segmentation and representation of movements in videos
play an important role in different applications such as search
engines, video recommender systems, and video summariz-
ers. In this paper, we present a system for semantic annotation of movements in video. This system is based on the
temporal segmentation method that extracts the movement
objects in still scenes, and on the high-level movement concepts to bridge the semantic gap between such concepts and
the low-level video features. We propose a knowledge-based
Model of movements in videos by using the OWL ontology
and SWRL rules. Our Video Movement Ontology (VMO)
considers different concepts related to the relevant movement features, which is based on the semantic of the Benesh
Movement Notation (BMN). BMN can describe any form of
dance or human movement. Rules in description logic are
defined to describe how low-level features and mapping process between those features and ontologys concepts should
be applied according to different perception of video content
analysis. This system can improve the quality of annotation
of movements in the videos and can discover the hidden
information by reasoning video knowledge and movements
features.

Keywords
Semantic Gap, Semantic Web , Ontology, Description Log-
ics, SPARQL, SWRL, Movement notation, Benesh movement notation, Video Segmentation

1.

INTRODUCTION

The capacity to detect high-level semantic concepts from
multimedia databases and to bridge the gap between such
concepts and the low-level features has received a large attention from the research community in the recent years
([18], [10]). In fact, the segmentation and the representation of the objects, or movements in the videos could improve the performance of many applications such as search
engines, video recommenders, and video summarizers.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
I-SEMANTICS 2012, 8th Int. Conf. on Semantic Systems, Sept. 5-7, 2012,
Graz, Austria
Copyright 2012 ACM 978-1-4503-1112-0 ...$10.00.

In current video systems,one can find a wide gap between
human interpretations of image and video data. This disIn this work1,
crepancy is called as the semantic gap[4].
we propose to use video motion capture technology that apply the temporal segmentation method (background sub-
traction) [19]. This method extracts the movement objects
in still scenes. Then, a particular performance are recorded
to analyze the movements of the video features expressed
in XML (eXtensible Markup Language) based on Benesh
Movement Notation (BMN) using the ontology solution.

BMN was proposed by Joan and Rudolf Benesh as a notation system for analyzing and recording human movement
by using symbols [6]. BMN is written from left to right on
a five lines horizontal stave (similar to music notation), to
form a suitable basis or matrix for the human figure [12].
Notice that other notation systems have been proposed to
depict movements such as Labanotation [11],
Eshkol-Wachman Movement Notation (EWMN)2 and Sutton dance writing 3. In [15], the authors proposed representing and archiving dance choreographies by building Dance
Ontology in OWL based Labanotation.

In a previous paper [16], we have presented an ontology for
representing the human movements in video based on Benesh notation. In this paper, a novel model called Video Semantic Movement Analysis Architecture will be presented.
In addition, we will detail a Video Movement Knowledge
Model (VMKM) for automatic annotation of human movements in videos. Ontology Web Language (OWL), proposed
by the World Wide Web Consortium (W3C), is used for the
ontology description. Rules in description logic are defined,
which how should be applied the mapping process between
low-level features and ontology concepts according to different perception video contents analysis.

This paper is organized as follows:

in section 2, we discuss the related work in the area of semantic video analysis
using both ontologies and segmentation approaches. In section 3, We illustrate the different layers of video semantic
movement analysis. Then, we present the video movement
knowledge model based BMN, Section 4. We detail some
aspects of the knowledge-based realization and implementation in Section 5. Finally, conclusion and future works are
drawn .

1supported by the OLIMP Project, FWB, Belgium,
ARC-AUWB-2008-08/12-FPMS11 and the Numediart Programme of Excellence, Wallony, Belgium, 716631
2http://www.movementnotation.com/
3http://www.dancewriting.org

1412. RELATED WORK

Ontology is an explicit and formal specification of a conceptualization of a domain of interest [8]. In this definition,
domain ontology [13] expresses conceptualizations that are
specific for a particular domain (such as medicine, multi-
media, transport, etc.). Ontologies have been developed in
many domains and studies, thanks to their capacity for representing the knowledge bases, and for facilitating knowledge
sharing. We can also find ontology studies in the domain of
multimedia with different goals (annotation, retrieval, etc.).
An example of multimedia ontology is COMM (Core Ontology for Multi-Media),[3]. It is based on MPEG-7, which
describes multi-media content in a standard way by creating
complex and comprehensive metadata descriptions by using
XML schema.

In the last years, different approaches have been proposed
for the use of ontologies for video annotation. Many proposals have been raised to bridge the semantic gap in several systems by designing visual concept recognition models.
For instance, in [17], the authors proposed an ontology for
representing the knowledge of video event analysis. They
integrated the different types of knowledge in their ontology to detect the objects and events in a video scene by
using a video analysis framework. Other authors have been
exploited the use of rule-based reasoning over objects and
events in different domains for obtaining richer annotations.
Hollink et al.
[9], proposed a semi-automatic annotation
method applied on the images of pancreatic cell by creating
a new ontology with new vocabularies. They also used a
Rules-By-Example (RBE) method in order to support the
annotation of other cell domain. Zhang et al [7], proposed
a driven context-aware based on ontology for improving the
reliability to infer user compliance to a healthy lifestyle , and
to supply appropriate feedback and reminder delivery.

For realizing an automatic extraction of semantic concept
(general description, emotion, objects, etc.), three tasks can
be identified according to the literature: (1) learning based
and automatic annotation, (2) ontology based approaches
and (3) relevance feedback methods.
In our proposal, we
aim to automating the annotation of detecting movements
in video by using ontology based approach, which is based
on semantic concepts and rule-based techniques. So, we
use video movement ontology approach based BMN to detect the movement/concept, which can help in bridging the
semantic gap issues, by relying on the multimedia domain
knowledge. According to the authors knowledge, this is the
first time an ontology derived from BMN is used to annotate
the human movement in videos.

3. VIDEO SEMANTIC MOVEMENT ANAL-

YSIS ARCHITECTURE

In this section, we illustrate the general view of our system
and describe the different layers of the proposed video semantic movement analysis architecture(VSMAA). VSMAA
is organized in three layers: segmentation layer, mapping
layer, and semantic layer, as mentioned in Figure 1 :
3.1 Segmentation Layer

The segmentation or the information extraction is a compulsory stage for videos understanding, pattern recognition

Figure 1: Video Semantic Movement Analysis Ar-
chitecture

142and computer vision. For a human being, it seems easy to
distinguish the objects and afterword been able to interpret
them. Unfortunately, for the computerized systems it is not
the case. To distinguish animated objects, the systems have
to effectively extract the relevant information. So, all operations of video content analysis, including features extraction algorithm and generating BMN-based XML features
file, must be carried out in the segmentation layer.

There are several segmentation methods, some are coming from the image processing where the temporality notion
is less or even non-existent. Other processing were developed by taking into account this temporality. In this work,
we propose a preliminary segmentation method based upon
the use of video motion capture technology associated to a
spatio-temporal segmentation method ( i.e.background subtraction [19]). Background subtraction technique is used to
extract moving objects or human beings in videos. Indeed,
each video frame is compared with a reference image (back-
ground). Pixels in frames which present a significant difference compared to the background are considered as moving
objects. This comparison is performed by applying a simple
subtraction of current frames with the reference image.

In our work, we subtract two adjacent frames, the result
yields a frame with two silhouettes, and only one is desired.
To avoid this issue, we subtract a frame with a reference
frame, which we determine statistically by taking the average of the 10 first frames. We consider a key pose image
when there are no movements from the human beings. optical flow will indicate the presence of movement in the video.
Key poses are then compared with some pre-established patterns which provide visual features.

Thereafter, these features are translated to a hierarchy
that is easily represented by XML. Indeed, we record a particular performance by defining a standard format for integrating the extracted movements features from videos. This
format use knowledge structure in the form of BMN-baesd
XML file .

3.2 Mapping Layer

The mapping layer is based on building a Decision Tree
(DT) based on Description Logics (DLs) to associate the
low-level video features with video movement ontology (VMO)
concepts.

DT learning is one of the most popular classification data
algorithms.
It has gained its popularity from its efficient
performance by applying the interpretable representation on
large size of data sets. The DT has three types of nodes:
a root node that has no incoming edges and zero or more
outgoing edges. Non-leaf nodes that represent the input at-
tributes, and the leaf nodes that represent output decisions
[14]. For generating the DT, given a set of training data described by a fixed set of input attributes and a known output
for each sample, the training data instances can by classified
in the DT by starting at the root node. Thus, testing attributes starts at the root and moves down the tree branch
corresponding to the value of the attribute. This process is
repeated until arrive to a leaf node (a decision). Indeed, the
path from root of DT to the leaves is formulated by a set
of simple decision rules (if-then). Thus, DT is obtained by
recursively splitting the training data into different subsets
according to the possible values of the selected attributes
until data samples of each subset belong to the same class.
In order to use the various information of ontology for

decision tree, we propose a split condition of DT based on
description logics (DLs). It is a logic-based knowledge representation formalism for modelling a domain in terms of
concepts (classes), roles (properties and relations) and individuals (instances of classes), [5].

In our system, we use the induction of decision trees by
mapping the visual video features to intermediate-concepts
(interConcept). Those interConcepts are simple key words
based on Benesh notation such as ( plane, head line, hand,
foot, etc.). Furthermore, the induced decision tree is classified according to the relevant individuals of our ontology
(e.g. hand raise) by using DT based description logics,
where an inductive instance is adopted for classifying indi-
viduals. Therefore, The input attributes of the DT are the
low-level video features, which contain terminological concept descriptions (corresponding to OWL-DL classes) and
the output is one of the video movement ontology concepts.
3.3 Semantic Layer

In order to realize a semantic analysis of the human movements in the video based ontology, once the segmentation
step is achieved, we can extracted the annotation data from
the video features. According to the available knowledge,
the main components of our semantic layer, called Video
Movement Knowledge Model (VMKM), are: video movement ontology (VMO), and the semantic rules that will be
used to map the interConcepts of each video sequence to
the corresponding ontology concepts. The semantic rules
are automatically derived from mapping layer based DT and
description logics, and constructed as a tree structure. The
build-up of VMKM will be described in detail in the next
section.

4. VIDEO MOVEMENT KNOWLEDGE

MODEL (VMKM)

4.1 Video Movement Ontology (VMO)

Ontologies can appear to be useful means for structuring
descriptions of video semantic content. So, all elements in
the video ,such as content, features, images, sounds, or other
video objects, must be described clearly in VMO. The main
role of VMO is to capture knowledge about the videos movements in the domain of video content analysis to achieve an
efficient semantic annotation of movement in videos.

The development of the proposed ontology is starting by
conceptualization step, which is used to define the VMO
concepts and their interrelations, as illustrated in Figure
2. In our VMO, the classes defined above are expressed in
OWL and represented as UML class diagram, for the sake
of simplicity the concept properties were not shown.

 Video Concepts:

- Class Video: we segmented video input into se-
quences. Most visual and audio features (move-
ment, speech, and text) are associated to description of each sequence.

- Class Sequence: each sequence consists of several
key poses. Each sequence instance is related to
temporal feature instance from class Duration, by
the hasDuration property.

- Class KeyPose: to record a KeyPose based on Benesh notation. We have noted the exact locations

143Figure 2: Part of ontology related to movement representation

occupied by the four extremities, the hands and
feet, in relation to the Body of the person on the
plane, as we will explain in the follow.

- Class Duration: Denotes the temporal description
of each corresponding sequence. The Duration
concept is used for modelling keyPoses.

 BMN Concepts:

- Class Plane: used to describe the anatomic position
It has three values: Sagittal,

of the silhouette.
Coronal, and Transverse.

- Class StiveLines: represents the position of extremity within the stave. And it denotes, for each Key-
Pose, the position of the person hands and feet
in relations to their anthropometrics [12]. This
class is formatted in five height positions represented by the Five Lines of the Benesh notation
stave:
the Floor(height is zero), Knee Height,
Waist Height, Shoulder Height, and the Head Height.

- Class BasicSigne: different sets of signs are used
to add more information to the two-dimensional
information provided by the position of a sign on
the stave. These sets of signs are: Front, Level or
Behind the body.
 Movement Concepts:

- Class Movement: a movement is defined in the lit-
erature, [6], as a way of representing body parts,
which can move together in order to form a higherlevel movement. In his turn, body movements tell
us what a body part should do at a specific point
in time.

- Class Body Part:

is the basic class for all movement classes. All knowledge about body parts
have been detected through features extraction
process at the video segmentation layer.

Based on those knowledge, several kinds of body movements can be presented for providing rich movement
annotation. We organized them in a taxonomy of
movements.
In this way a movement is classified as
in Figure 3.

- Class Simple Movement: is based on the movement
of one or more parts of the body. SimpleMovement can be divided into three kinds of move-
ments: HandM ovement, when the body part of
a movement uses only Lef tHand or RightHand
elements. And FootMovement when it uses only
feet elements. For HandF eetM ovement,
it is
based on the movement of hands and feet elements together.

- Class Complex Movement: It is a series of simple
movement and actions take place over a period of
time.

- Class Actions: We also used the semantic concepts
to describe actions ( ex. move, raise, etc.) within
formalism of knowledge movement representation.

The OWL code of our VMO can founded at the URL: (http:
//www.ig.fpms.ac.be/sites/default/files/DanceOntology.
owl)
4.2 Video Semantic Rules in Description Log-

ics

Visual information of the video which are extracted in
mapping phase are passed to VMO. Then, we infer rich semantic information about movement in videos either about
simple body movement( e.g. raise hand in front of the body,
etc.), or about complex movement( e.g. jumping, walking,
etc.) from VMO and domain ontology.

In order to discover the hidden knowledge in VMO, several
rules were defined for the movements semantic analysis in
video. To describe the VMO, we have used the OWL-DL
formalism. Thus, the rules, which are used for movements

144Figure 3: Part of the ontology related the different kinds of movement

detection in the video, are presented in description logics as
follows:

 A Video V is subclass of Media and has sequences

(S1,S2,..., Sn):

V ideo (cid:118) M edia (cid:117) hasSequence.(S1, ..., Sn)

 A Sequence S consists of KeyPoses (Kp1,Kp2,..., Kpn)

and it has duration D:

Sequence (cid:118) V ideo (cid:117)

(hasKeyP ose.(Kp1, Kp2, ..., Kpn) (cid:117)

hasDuration.Duration)

 Each KeyPose KP has movement (M1,M2,..., Mn):

KeyP ose (cid:118) Sequence (cid:117)

 hasM ovement.(M1, M2, ..., Mn)

 Movement M has Action Ac applied on one or more of
a BodyPart and it has (plane, stave line, basic singe).

M ovement (cid:118)

hasAction.Action (cid:117) hasBodyP art.BodyP art
hasP lane.P lane (cid:117) hasBasicSigne.BasicSigne

(cid:117)hasStiveLine.StiveLine

Where Plane, StiveLines,and BasicSigne, are defined
as follows:

P lane  Coronal (cid:116) Sagittal (cid:116) T ransverse

StiveLines 

(HeadLine (cid:116) ShoulderLine(cid:116)

W aistLine (cid:116) KneesLine (cid:116) F eetLine)

BasicSigne  Behind (cid:116) Inf orntOf (cid:116) Level

 BodyPart Bp is a part P which has a side Bs. Part

can be hand or foot. The side can be left or right.
BodyP art (cid:118) hasP art.P art (cid:117) hasSide.Side

P art  {Hand, F oot}
Side  {Lef t, Right}

Figure 4: Semantic Rule based Description logics

4.3 Reasoning

One of the key features of ontologies is that they can be
processed by a Reasoner which supports the decision- making process and derive new knowledge by applying the inference rules to existing knowledge in a specific domain. On the
other side, using an ontology and an appropriate Reasoner
allows to automatically infer the ontology class hierarchy
and it is easier for consistency checking.

In our system, we have used SWRL (Semantic Web Rule
Language) 4, which is based on a combination of the OWLDL and OWL-Lite sub-languages of OWL for defining the
different movement semantic rules. Currently, various rule
engines for OWL reasoning have been proposed ([2], [1]). We
use Jess (Java Expert System Shell) rule engine 5, which
is integrated in Protege, for reasoning and executing the
SWRL rules.

Formal expressions were defined to classify each kind of
movement. Those rules are defined by expressing how to

4http://protegewiki.stanford.edu/wiki/SWRLTab
5http://www.jessrules.com

145is about 2 minutes per video. As mention in section 2, we
have used background subtraction technique to extract moving objects in videos. So, we consider a keyPose image,(see
Figure 6), when there are no movements from the human be-
ings. The optical flow will indicate the presence of movement
in the video. KeyPoses are then compared with some preestablished patterns which provide features. Notice that, we
have used the OpenCV version (2.3.1) 6 image processing li-
brary, in all our implementation steps.

Extracted video features are recorded in XML file for producing BMN. The keyPose is generated automatically involving text information. This test is based on the keyPose number, the time element (Duration), The information (Part, Side), and BMN information (Plane, Position,
and Distance). Figure 5, on the upper left side, shows the
XML code that corresponds to the keyPose image on the
upper right side and its SWRL rule appears in the bottom
side.

We have edited our ontology by using Protege-OWL 7,
a knowledge tool that easies the description of concepts.
During ontology instantiation, we verified that all concepts
were used and all the information required for supporting
the movement annotation based BMN was represented. In
fact, a number of SWRL rules were created, as shown in
the Figure 6. Each SWRL rule was constructed by using
SWRLtab8, which is a Protege plug-in. The key idea for detecting the semantic inference rules is based on the mappings
between the visual features of video movement presented in
XML fragments, and ontology concepts and rules.

We annotate all our movements by defining the composition of movement concepts, properties, and thier interrela-
tions. Then, we specify the inference rules that let us draw
conclusions from what we recognize in the data (XML file).
All this information is exploited to create several instances in
the VMO. Afterwards, the system runs inference engine from
Protege. As a result the inferred movements are proposed
for annotating video human movement. Figure 6 illustrates
the use of the VMO and the movement rules. In order to
facilitate the definition of those rules, several instances of
the movement concept have been created in our ontology.
Preliminary results have shown a good adequacy between
our approach for semantic annotation and human interpre-
tation. Where, we had 20 KeyPoses and we could annotate
about 80 % , we have still to conduct larger experiments to
quantify our method.

6. CONCLUSION AND FUTURE WORK

In this paper, we proposed a Video Semantic Movement
Analysis Architecture that is composed of segmentation layer,
mapping layer, and semantic layer , called Video Movement
Knowledge Model (VMKM). VMKM defined from the analysis of the main concepts of the multimedia domain considering and BMN. We aimed to using this ontology to automate
video movement annotation.

In order to create domain ontology for video movement
annotation based BMN, OWL is used for ontology concepts
specification. Rules in description logics are defined to describe how low level video features can be mapped to ontology concepts based BMN. Finally, we described also how

Figure 5: XML and SWRL movement representa-
tion.

infer that a specific movement is considered of as a specific
type.
Indeed, rules automatically are derived from mapping layer based DT and description logics and are constructed as a tree structure. An example of such rules is
presented in Figure 4. In this example, the rule states that
unknown movement, named a (Movement (?a)), is classified as a simple movement (RightHand-Moved-Horizontally-
FrontalPlane).
In this rule, the concepts are (Movement, Actions, Hand,
Side, Plane(Cornal), StiveLine(ShoulderLine),
BasicSigne(InFrontOf), and Description (Horizontal)).
The relations between concepts are (hasAction, hasPart,
hasSide, hasPlane, hasStiveLine,hasBasicSigne, and hasDescription ).
RightHand  M oved  Horizontally  F rontalP lane 

M ovement (cid:117) hasAction.M ove

(cid:117)hasBodyP art.(hasP art.Hand (cid:117) hasSide.Right)
hasP lane.Coronal (cid:117) hasBasicSigne.Inf rontOf

(cid:117)hasStiveLine.ShoulderLine(cid:117)hasDescription.Horizontal

5. EXPERIMENTAL RESULTS

To validate our VMO, we create several instances based
on real video of human movements. For training and eval-
uation, 10 videos were used, and the duration of the data

6http://sourceforge.net/projects/opencvlibrary
7http://protege.stanford.edu/
8http://protegewiki.stanford.edu/wiki/SWRLTab

146Figure 6: SWRL representation and Inferred movements by using Jess engine

147Gesture for Expressive Characters (AISB04) (2004),
pp. 8697.

[13] Noy, N., and McGuinness, D. L. Ontology

development 101: A guide to creating your first
ontology. Tech. rep., Stanford Knowledge Systems
Laboratory and Stanford Medical Informatics, 2001.

[14] Quinlan, J. R. Induction of decision trees. Mach.

Learn. 1, 1 (1986), 81106.

[15] Raheb, K. E., and Ioannidis, Y. A labanotation
based ontology for representing dance movement. in
Proceedings of Gesture Workshop (2011).

[16] Saad, S., Beul, D., Mahmoudi, S., and

Manneback, P. An ontology for video human
movement representation based on benesh notation. in
Proceedings IEEE of The 3rd International Conference
on Multimedia Computing and Systems
(ICMCS12),Tangier, Morocco (May 10-12 2012,).
[17] SanMiguel, Carlos, J., Martinez, M., J., and
 Alvaro, G. An ontology for event detection and its
application in surveillance video. In Proceedings of the
2009 Sixth IEEE International Conference on
Advanced Video and Signal Based Surveillance
(Washington, DC, USA, 2009), AVSS 09, IEEE
Computer Society, pp. 220225.

[18] Snoek, C., and Worring, M. Multimodal video

indexing: A review of the state-of-the-art. Multimedia
Tools and Applications 25 (2003), 535.

[19] Zhang, Y. J. An overview of image and video

segmentation in the last 40 years. Advances in Image
and Video Segmentation (2006), 115.

this ontology can be used to support video annotation and
retrieval by the inference of the axioms defined in the ontol-
ogy. We have been collected the instances of the concepts
from low-level video data, which have been recorded in XML
by using Benesh Movement Notation (BMN)file based.

We are now working to applying our proposal to develop
a query system that automatically captures the instances
from different XML files, and to analyze the different possibilities of complex movements representations that we can
get by using this ontology. Future works include also the
evolution of the VMO with other kind of movements types
(Dance, Gymnasium, Physical Therapist, Sport, etc.). Fi-
nally, we would like to corporate other existing ontologies as
( MPEG 7, or COMM ontology) to represent the temporaland spacial decomposition of video content.
