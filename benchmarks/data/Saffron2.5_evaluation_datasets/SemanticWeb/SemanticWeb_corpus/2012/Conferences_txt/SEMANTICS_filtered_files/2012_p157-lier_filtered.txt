Facilitating Research Cooperation through Linking and

Sharing of Heterogenous Research Artefacts

Cross Platform Linking Of Semantically Enriched Research Artefacts

Florian Lier

Center of Excellence
Cognitive Interaction
Technology (CITEC)

Universitatsstrae 21-23

Bielefeld, Germany
flier@cit-ec.uni-

bielefeld.de

Sebastian Wrede

CoR-Lab Research Institute
for Cognition and Robotics

Universitatsstrae 25
Bielefeld, Germany

swrede@cor-lab.uni-

bielefeld.de

Frederic Siepmann
Center of Excellence
Cognitive Interaction
Technology (CITEC)

Universitatsstrae 21-23

Bielefeld, Germany

fsiepman@cit-ec.uni-

bielefeld.de

Ingo Lutkebohle
Bielefeld University

Germany

iluetkeb@techfak.uni-

bielefeld.de

Thilo Paul-Stueve
Center of Excellence
Cognitive Interaction
Technology (CITEC)

Universitatsstrae 21-23

Bielefeld, Germany

tpaulstu@cit-ec.uni-

bielefeld.de

Sven Wachsmuth
Center of Excellence
Cognitive Interaction
Technology (CITEC)

Universitatsstrae 21-23

Bielefeld, Germany

swachsmu@cit-ec.uni-

bielefeld.de

ABSTRACT
Researchers and other knowledge workers frequently produce and use diverse research artefacts such as papers, data
sets, experiment specifications, software, etc. In this, they
are often faced with unclear relationships (e.g., which version
of a software was in use for a particular paper), creating unnecessary work and potentially errors. Semantic web technologies can provide metadata as well as explicit, specific
links between the artefacts. However, data acquisition and
perceived utility are potential stumbling blocks for adoption.
Therefore, we propose a system which is focused on integrating and augmenting existing data (thus protecting the
existing investment), and examine it using an interactionoriented perspective, on users without semantic web experi-
ence. Specifically, we first study requirements of the target
group and then present an exploratory study of managing
research artefacts related to software-centric projects. The
results confirm that diverse data sources are in common
use, that re-using existing repositories is perceived as efficient (e.g., more convenient, shorter cycle time), and that
the experimented aggregates are perceived as functionally
relevant. Furthermore, the integration of quality assurance
mechanisms, such as continuous integration, is perceived as
beneficial, despite some added effort.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. I-SEMANTICS 2012, 8th Int. Conf. on Semantic
Systems, Sept. 5-7, 2012, Graz, Austria
Copyright 2012 ACM 978-1-4503-1112-0 ...$10.00.

Categories and Subject Descriptors
H.5 [INFORMATION STORAGE AND RETRIEVAL]:
Online Information Services

General Terms
Experimentation, Design, Measurement

Keywords
Research Artefacts, Linking, Aggregates, Semantic Web

1.

INTRODUCTION

Much current research is enabled by software-intensive
systems, and for replication or understanding of research results (represented by papers), it is often beneficial to know
which software components, in which versions, have been
run on which data-sets or in which experimental setup. Fur-
thermore, for user-oriented research, questionnaires and similar instruments need to be recorded. However, an open issue
for data acquisition in such situations is that many of these
artefacts cannot be linked automatically, because they are
of a different, unrelated class. In particular, there is often no
or only highly ambiguous overlapping information, so standard instance matching methods do not apply. For example,
many papers do not specify which software has been used,
and even when they do, configuration information is often
omitted or incomplete. Furthermore, each research discipline has many practices that are internally well-known, and
thus often go un-recorded, which can cause confusion in interdisciplinary endeavors. Thus, to provide a coherent view
using semantic web formalisms, some additional data entry,
and particularly linking, is required. Fortunately, repositories often already exist for the different artefact classes
(e.g., for papers, or software), and can be imported once

157linking information is provided. So far, this is a classic use
case. However, to make this and similar systems work well
for users, we consider it important to adopt an interactioncentric view, rather than a technology-centric one. Firstly,
we should check that the systems model matches the users
work-flow, and we have examined this based on a questionnaire of current practices. Secondly, it is obviously better
and important for adoption that the whole process does not
take too much time, and to identify potential trouble-spots
we have studied this based on time taken for the various
sub-tasks. Last, but not least, the quality of the resulting
data is crucial to achieve the goals of the system. Thus,
our prototype system uniquely integrates with a continuous integration system. This system determines software
artefact version availability and also provides metadata on
dependency and versions, which saves data-entry. However,
it also requires some additional configuration on software
builds, which may cause additional effort for users, and we
have therefore also examined users experience with this in-
tegration. Thus, in this paper, we explore user interactions
(N=15) with an experiment repository built around such
an import-and-link procedure, to determine its practicality
and identify remaining open issues. For background, we have
also collected some information on how researchers currently
cope with such issues.

2. RELATED WORK

Knowledge workers, such as researchers, nowadays already
facilitate various tools and repositories1 to share information
about their research (context), and their papers or reports
(publications)2. In fact most universities and other research
facilities provide an institutional repository to archive their
results and publications. This might serve as an indicator
that the first step, making research results and publications
available as, e.g., proposed by the Open Access Initiative 3
which states that the results of publicly funded research
should be publicly available, is becoming reality. The impact of the availability has been discussed, e.g., by Gutam et
al. [5]. An overview of available repositories is, e.g., provided
by the OpenDOAR4 project. There are organizations, such
as the euroCRIS 5, that are dedicated to the development
of Current Research Information Systems (CRIS) and their
interoperability. These semantic web technologies have content available and in the succeeding step led to Linked Data,
which refers to methods of publishing interlinked structured
data on the web [6]. This data has not only made the search
for related work for many researchers easier, it has produced
new ways for scientists to communicate their work [7] and
initiate research collaboration [2].

This availability led to questions on how to store these
information. Many repositories represent bibliographic information facilitating the 15 Dublin Core (DC) elements as
specified by the Open Archive Initiative (OAI)6 Protocol
for Metadata Harvesting (PMH) [8]. One approach on what
to store was, e.g., the Academic Information Domain (AD)
introduced by Razum [11], which differentiates between per-

http://data.3tu.nl/repository
http://www.openaire.eu/
http://open-access.net/
http://www.opendoar.org/
http://www.eurocris.org/
http://www.openarchives.org/

sonnel information, financial information and academic in-
formation. More recently, researchers tried to enhance online information by adding data assets [14], allowing to aggregate academic data with other information (e.g., photos)
available online.

To better facilitate semantic web technologies for various
areas of research, building ontologies often is considered as a
first step. However, for interdisciplinary research these technologies face additional challenges. A common application
here is improving the access to large systems as, e.g., proposed by Damljanovic et al. [4] to reduce the learning curve
for developers/maintainers. Another common use-case is the
retrieval of appropriate software components that are avail-
able, e.g., from open source repositories as proposed by Yao
et al. [15] or Sugumaran et al. [13]. Moreover, the development of open source software to annotate, verify and upload
experimental metadata as well as data sets to public repositories in a standardized way, has been realized by, e.g, Rocca
et al. [12] in the recent past. Passant et al. [10] introduced
to an Interlinked Semantic Wiki Farm where users are able
to easily create and link data between independent Wikis,
even if they dont share hyperlinks. The content created
in different Wikis is also represented as RDF graphs, based
on a set of ontologies defined by administrators, to provide
ontology-based knowledge representation and to enable machine interpretable harvesting.

In this paper, we will focus on managing and linking diverse research artefacts and on how to efficiently acquire the
necessary data, without having experience in semantic web
technologies. While our work shares similar goals, e.g., to
[10], in terms of hiding the complexity of semantic web technologies through the utilization of intuitive web-forms, and
to avoid semantic heterogeneity, it differs in the following
aspects. First of all, in contrast to [12], our approach includes diverse research artifacts of different classes, such as
software descriptions, software releases (binaries), publica-
tions, data sets and so on. Secondly, our concept is based on
an import-and-link procedure, where content is directly imported from (native) repositories, which are often different
and distributed platforms providing varying data represen-
tations. Thirdly, in contrast to [10], users are able to group
research artifacts of different kinds together in multiple ag-
gregates, matching their working sets. Last but not least,
we introduce to an automated quality assurance mechanism
(the continuous integration server) for software related research artifacts linked into the above mentioned aggregates.

3.

INFRASTRUCTURE AND CONCEPT

This section presents the underlying infrastructure and
concept of our prototype system. In particular we will introduce to our infrastructure with its diverse data sources,
the data architecture to cope with these sources (see Fig.
1(b)), and an exemplary system component in detail (see
Fig. 1(a)).

In order to promote interdisciplinary, collaborative, and
open research at CITEC we maintain a research infrastructure of different online platforms and services together forming the Collaborative Research Environment. A Open Research project management platform7 hosts public available
Open Data and Open Source projects, while the Synergy

https://openresearch.cit-ec.de/

158project management platform8 hosts development projects
with 3rd party involvement; a third platform hosts internal development projects. The project management platforms provide issue management, documentation facilities,
and distributed version control to hosted projects. A Continuous Integration Service (CIS)9 recurrently builds, tests
and deploys software artefacts from the different projects
into a repository service10 providing access to snapshot and
release builds. Additionally, the publication data service
11 (PUB) of the Bielefeld University stores reference information of research publications. In general, similar to our
infrastructure, highly distributed data sources or repositories often already exist for different research artefacts, e.g.,
for papers, software, or data sets. However, cooperative research requires that artefacts and their relevant relationships
are known, available, and have a shared identifier (see Sec.
1). Semantic technologies are already tackling such situa-
tions, but especially data acquisition and aggregation, with
respect to the requirements of researchers who provide and
make use of that data, is still a challenge.

Based on these issues, we derived the following concept
and design goals: common research artefacts are represented
as typed entities, in terms of an object oriented approach,
to provide a basis for a sufficient data model. The entities
have to reflect basic attributes of their data source, e.g.,
version, source code location, programming language and
developer with regard to software artefacts. Researchers
should be able to easily create and publish entities in a
web-based platform, primarily re-using (importing) existing
sources. As a main added value, researchers should be able
to link diverse but interrelated entities, e.g., a publication
and the corresponding data set, into functionally relevant,
publicly available, aggregates. If applicable, stand-alone entities and aggregates (e.g., software releases or systems) can
be versioned, which enables researchers to keep track of their
history and to preserve functional states at a certain point of
time. Automated quality assurance, with regard to software
artefacts, should also be provided. Furthermore, entities
and aggregates should be semantically enriched to provide
greater machine interpretability and to allow further processing of semantically structured knowledge.

Figure 1(a) exemplary depicts a software related aggre-
gate, based on a so-called component. First of all, a standalone component can be seen as an entity, reflecting accumulated information (wiki URL, scm location) about, e.g,
a specific software framework. The main benefit emerges if
related entities, such as release builds, related publications
or data sets are linked to the component, thus forming a
functionally relevant aggregate. While information about
a component mostly needs to be added manually (copy and
paste), publications, data sets and software builds should be
imported utilizing existing repositories, e.g., from the PUB
(see Fig. 1(a)).

Overall, the concept includes components, data sets, pub-
lications, releases, data types, data type versions, systems
and system versions as depicted in Figure 1(b). In our concept we also modeled cardinalities, as an example, a system
can be linked to n publications n data sets and can have n

system versions (forming another aggregate). In contrast, a
system version can only be linked to 1 system. The same
principle applies to a software release, which can only be
linked to a single component  vice versa a component features multiple releases (see Fig. 1(b)).

4. SOFTWARE & IMPLEMENTATION

In this section we present the software used for the prototype implementation of our concept as described in the
previous section. The prototype is publicly available via
https://toolkit.cit-ec.uni-bielefeld.de. Initially, we looked at
a number of existing frameworks and tools which may serve
as a basis to realize our concept. Besides Jena12, which is an
open source framework, written in Java, providing an API
to store and write information as RDF triples in directed
graphs, we also considered Sesame13 as a useful candidate.
However, we decided to utilize Drupal14 (version 7.14) for
the following reasons: Drupal already implements an entity concept, entities (in Drupal) are used to store and display data. Additionally, Drupal features helper functions to
add fields to existing entities such as nodes. Secondly, as
Corlosquet et al. [3] have shown, Drupal already provides
modules able to generate RDF descriptions15 in addition
to (X)HTML webpages including RDFa attribute sets for
embedding rich metadata. Furthermore, these modules implement a SPARQL16 endpoint which provides an interface
to directly query an RDF repository.

As addressed above, Drupal already implements an entity concept. Hence, we were able to represent our entity
relationship model as Drupal nodes. A Drupal node holds
diverse attributes, named fields. Fields contain the actual
content of a node, e.g., title, author and body. Custom
node and field types can be easily added, e.g., to provide a
link to other nodes. Exemplarily we will describe the implementation of the release entity (see Fig. 1(b)), due to the
fact that the implementation of the residual entities is sim-
ilar. We created a custom node type for the release entity
and added typed fields (type::name): title::title, feed::feeds,
link::artifact, link::scm-location, taxonomy::operating-system,
taxonomy::build-on, node-reference::required-release.
If a user references a certain data source, e.g., a URL to
our CIS, the resource is fetched, the imported content is
parsed (via XPath matching) and mapped on existing fields
(as mentioned above) within a node.
In this example we
utilized the feeds field to import XML data provided by our
continuous integration server. Furthermore, to ease the content creation process, we implemented several Drupal form
templates for our entity types. The main advantage over
standard forms is to hide unnecessary information, where
applicable, and to preprocess filled in data, such as to redi-
rect/rewrite malformed URLs or to concatenate several field
values.

In order to realize the automated enrichment of entities
and aggregates, we set up entity fields to RDF vocabulary
mappings using the RDFx module. Upon installation of
the module, Drupal already includes DC, FOAF and SIOC

https://synergy.cit-ec.de
https://ci.cor-lab.de/
https://repo.cit-ec.uni-bielefeld.de
http://pub.uni-bielefeld.de/(en)/index.html

http://incubator.apache.org/jena/
http://www.openrdf.org/
http://drupal.org/
http://drupal.org/project/rdfx
http://drupal.org/project/sparql

159(a) Aggregate

(b) ERM

Figure 1: (a) Exemplary aggregate consisting of a software component, publications, data sets and a study.
(b) Entity relationship model of the presented concept.

vocabularies. Additionally, we imported the DOAP17 and
BIBO 18 vocabularies. As an example we mapped the following fields (vocabulary = field):
doap:Repository = scm-location, doap:file-release = artifact,
doap:os = build-on, doap:name = title.
Subsequently, we repeated this implementation step for the
residual entities of our system.
In summary: we implemented a web-based platform, which enables users to easily create and link heterogenous, semantically enriched entities to aggregates, through a convenient import-and-link
based process. Last but not least, an aggregate or entity
can be requested in different (rich) representations such as:
(X)HTML+RDFa, XML, pure RDF and JSON.

5. SYSTEM EVALUATION

While the presented concept is based on experience with
the working practices of the target population, only an empirical study can determine whether the interaction with the
system is as expected. In particular, we evaluated i) which,
and how many, of the available artefact types were actually
used to determine the match between the concept and the
users requirements, ii) how long adding and importing data
took, to determine whether the user interface for referencing
other repositories actually delivers the potential benefits of
re-using data, and iii) the subjective satisfaction with the
system.

Furthermore, while we knew that researchers use various
information sources in their work, we did not have information available on the relative importance of these, and did
take this opportunity to gather some more information on
such aspects, too, as input for future development.

This variety of questions is typical of the prototype stage,
and thus we have chosen an exploratory usability study as a
suitable instrument for the current stage of system concep-
tualization. It essentially consists of some instruction, then

http://usefulinc.com/ns/doap#
http://bibliontology.com/specification

users carry out a typical task, during which they and their
computer screen are captured on video, and finally they answer a questionnaire.
5.1 STUDY SETUP

All attendees were instructed on the procedure of the experiment as follows: As a preparatory step, the participants
were asked to integrate their (current) software projects into
our continuous integration server  if not already existent.
All subjects watched an 18 minutes tutorial video, where
all features of the system were introduced and explained in-
depth. We decided to present a tutorial video instead of
a personal interactive tutorial to achieve a uniform baselevel knowledge throughout all attendees. Thirdly, the subjects were asked to add and link as many artefacts to our
system, as it appeared necessary to them, to reflect their
research. All subjects were allowed to access the tutorial
video at any time during the experiment. Finally, all participants we asked to answer a questionnaire based on their
experience with the system.
5.2 PARTICIPANTS

Fifteen test subjects were recruited from our institutes
software engineering interest group. They had no prior knowledge about the concept or implementation of our prototype
system. Participants were aged between 25 and 30, had a
scientific background and assessed themselves highly experienced in software development. The majority of all participants were, at the time of the study, involved in at least
one software development project which produced multiple
artefacts, e.g., source code, multiple binary releases, system
descriptions and publications.

6. RESULTS

This section presents the results of our user study concerning information retrieval strategies of the users and we will
describe what kind of artefacts have been added to the system and how long it took to add and link them. This section

160also includes results pertaining to the users tool chain and
system assessment in terms of added benefit, convenience
and functional relevance.
6.1 INFORMATION SOURCES IN USE

To gain insight into the participants strategy for information retrieval, related to their research, we asked them if
they utilize common data sources such as: social networks,
source code repositories, project specific web sites and
contacting the developer directly. Furthermore we asked
them to rank the sources from most important to less im-
portant. The results show that, 40% of all attendees are
utilizing social networks, 73% email lists, 100% source code
repositories, 93% project specific web sites and 93% are contacting the developer directly to obtain information. We
are especially interested in web-based information sources,
which are already integrated into our system (see Fig. 1(a)
(metainfo)). The presented ranking (see Table 1) clearly
shows that project specific websites and public source code
repositories play the most important role for information
retrieval. In contrast, social networks, e.g., Facebook19 or
Google+,20 are currently less important to our subjects.

Type
Project specific websites
Public source code repositories
Email lists
Contact the developer directly
Social networks

Rank Percent


53%
60%
53%
50%
80%

Table 1: Information source ranking.

6.2 TYPES & NUMBERS OF ARTEFACTS
During the experiment 14 components, 18 releases and 9
publications have been added. Interestingly, no system, system version, data type or data type version, and data set
has been created. Pertaining to systems, respectively, system versions, this may have been due to the fact that adding
a system/-version requires the creation of multiple components and releases in the first place. Furthermore, the participants need a profound knowledge of all components and
their dependencies, integrated into a single system. This is
often difficult to keep track of by a single developer. How-
ever, as we will introduce in later parts, the required effort
for creating multiple entities does not seem to be an issue
(see Sec. 6.5.1). These numbers basically indicate, that all
attendees were able to successfully operate our system (see
Sec. 3) and integrate at least one research artefact related
to their current work. No participant failed, such as she/he
was not able to add an entity at all. Furthermore, the creation of a basic component (see Sec. 3) including a release
is perceived as most functionally relevant to our subjects.
6.3 ADDING DATA

As described in Sec. 3 one of our main goals is to reduce
the effort for creating entities by importing, thus re-using
existing sources, e.g., from the PUB and our continuous integration server. Hence, we looked at the time spent to add
artefacts as an indicator for the usability as, e.g., proposed

http://www.facebook.com
http://plus.google.com

by Bevan et al. [1]. The given times are median values, for
minimum, maximum and average values please see Fig. 2.

For components no information is available for import
and the time for creation is 363 seconds seconds (about
six minutes). Qualitative investigation of the video recordings showed that the majority of this time was taken up by
the developers in collecting the various items of information
(e.g., source code repositories, documentation URLs, bugtracking systems, etc.). The time taken for entering this
information was negligible in comparison.

Artefacts where information can be imported are much
faster to add, e.g., a release takes 124 seconds (roughly
two minutes), while adding a publication took just under
a minute. This confirms our assumption and indicates an
efficient and convenient way of editing according to, e.g.,
Morita et al. [9].


Components 
Release 
Publication 

average 

median 

min 

max 

Figure 2: Time required to add artifacts in seconds.

6.4 LINKING DATA

Besides the data acquisition, the main added value of our
system is to link data from existing sources into experimentally relevant aggregates (see Sec. 3). Based on the added
entities described above, we investigated link characteristics,
e.g., component linked to release and measured the time
that was required to create an aggregate. During the experiment 71 links have been created. The distribution is shown
in Table 2.

Link type
Component linked to required component
Release linked to required release
Component linked to developer
Component linked to maintainer
Publication linked to component

Link count


Table 2: Number of links created between different
entities.

The distribution indicates that the participants were especially interested in referencing dependencies of their software
components (22 links) and releases (16 links).
In fact 14
components have been added but 22 were aggregated. Considering the time required to create links between diverse
entities, thus creating aggregates, we would like to point
out the small amount of time. Overall, the required time
(median) didnt exceed 5 seconds, also the absolute maxi-

161mum amounts to 27.1 seconds as depicted in Fig. 3.


	

	

	

	

	

	

	

AVG
MED
MIN
MAX

Component + 
Maintainer

Component + 
Developer 

Component + Req. 

Release + Req. 

Component

Release

Publication + 
Component

Figure 3: Time required to link artefacts in seconds.

6.5 SUBJECTIVE JUDGMENTS
6.5.1 RE-USE
The results shown in Table 3 demonstrate that utilizing
a continuous integration server for the provision of software
artefacts, import of release information, and to assure quality measurements is perceived as highly useful (statement
A-C). Despite the added benefit, a continuous integrationenabled tool chain also introduces an additional expense
(statement D). Importing publication data from the Universitys PUB Service is also noticed as useful but, in contrast
to software artefacts, doesnt produce an additional effort
(statement E,F). The majority of all attendees (66%) doesnt
want to add publications manually, nevertheless they would
also like to import publications from other sources, e.g.,
Google Scholar21 or Mendeley22 (57%) (statement G,H).
However, we assume that re-using existing repositories is
assessed as convenient and efficient.
6.5.2 EFFORT & FUNCTIONAL RELEVANCE
As shown in Table 4 the overall required effort for includ-
ing, respectively adding, artefacts is considered low (ques-
tion A). Based on this result we surmise that the additional
effort, introduced by utilizing a continuous integration server
(see Sec. 6.5.1), is negligible. The cost of linking research
artefacts is perceived as very low (question B). We consider
this excellent rating as a result based on the small amount of
time to link artefacts, presented in Sec. 6.4. Moreover, 73%
of all subjects determine the artefacts as almost completely
reflecting relevant information about their research (ques-
tion B). Additionally no participant perceived the added
artefacts as not useful at all.

We also looked at the functional relevance of our system
based on the participants assessment. First of all, 80% of
all subjects believe, that they would accomplish their tasks
more quickly, if others were to publish their data similar to
our system (statement A). Secondly, this way of publishing would also improve their performance (60%)(statement
B). Thirdly, the majority (60%) of all subjects assume, by
providing their research artefacts in our system, they can

scholar.google.com
www.mendeley.com

0% 0% 7% 0% 20% 13% 60%

0% 0% 0% 0% 5% 27% 67%

0% 0% 0% 13% 7% 20% 60%

20% 40% 20% 20% 0% 0% 0%

0% 0% 0% 4% 0% 33% 60%

60% 20% 6% 7% 0% 7% 0%

53% 13% 0% 7% 0% 20% 7%

13% 13% 0% 27% 27% 7% 13%

Statement
A) Using
a CIS
to provide software
artefacts is useful.
B) Importing data
from build jobs to
add information to a
component release is
useful.
C) Using a CIS to
assure quality measures of software is
useful.
D) Using a CIS as
the basis for component releases produces to much over-
head.
E) Importing pub-
lications
from the
PUB is useful.
F) Importing pub-
lications
from the
PUB produces too
much overhead.
G) I would like to
add my publications
manually,
e.g., by
filling in forms.
I would like
H)
import
publi-
to
cations from other
sources
as
Google Scholar or
Mendeley.

such

Table 3: Tool chain ratings of all participants in per-
cent. 1 = Strongly disagree, 7 = Highly agree
CIS = Continuous Integration Server

help other researchers to accomplish their individual goals
(statement C). Moreover, the attendees find the system easy
to learn and use (statement E,F). This is especially impor-
tant, because a low acceptance threshold is essential if also
non experts are going to utilize our system. Surprisingly,
no participant is aware of a system or platform basically
providing the same features as the prototype presented here
(statement D).
6.6 TRANSPARENCY

As a another added value of our system, user generated
entities and aggregates are automatically semantically en-
riched, as described in Sec. 4. We propose that this process
should be transparent to the user. She/he should be able to
focus on just creating content and does not need to become
an expert in semantic technologies. To further back up our
proposal we exemplary questioned the subjects about their
expertise with RDF. First of all, 53% of the subjects never
heard of RDF before.

In addition 33% assessed their expertise with RDF as low
(see Fig. 4). Not surprisingly, only 20% assume that adding
RDF information to research artefacts is useful. Subsequently 60% were not able to answer the question, hence
we assume because they havent heard of, or just have little expertise with RDF. Last, but not least, only 14% of
the subjects would like to add RDF information manually,
however 73% could not answer the question, probably for

1622

13% 53% 20% 7% 7% 0% 0%

80% 6% 7% 0% 0% 7% 0%

0% 0% 0% 27% 33% 13% 27%

overall

Question
A) How do you rate
the overall effort of
including your
re-
search artefacts?
B) How do you rate
the
effort
linking research
of
artefacts
each
other?
C) Referring to your
artefacts,
do you
think they do reflect
relevant information
about your related
research?

to

Table 4: Effort ratings of all participants in percent.
1 = Very low (Not at all), 7 = Very high (Com-
pletely)

No answer 

53%

1 Very Low  

7 Very High 

No answer 

33%

7%

7%

Figure 4: Participants expertise with RDF.

the same reasons as assumed above. We surmise these results confirm, that it is reasonable to make the enrichment
process transparent to the user.
6.7 SUMMARY

One of the main goals of this study was to find out how
users interact with our prototype system and how they use
the import-and-link procedure to determine its practicality.
As foundation for the information retrieval of the prototype
system we started from existing repositories such as our collaboration environment. Component developers can revert
to, e.g., svn repositories and Wikis that are already provided there. Additionally to what we have said in Sec. 6.4
we would like to point out that we did not measure the time
spent on single steps of the procedure or how users where
adding content (copy & paste vs. manually filling in). Due
to the lack of appropriate studies on time spent for comparable tasks (import-and-link), we consider the measured
time as low, which is also backed by Table 4 A). Hence,
our assumption that it is possible to import and link data
in a reasonably short period of time is valid. This study,
since it is the first study conducted, does not include measures for the effort necessary to maintain the information.
However, the system does already provide functionality to
automatically update information. This needs to be investigated in a future long term study. The results found in Sec.
6.6 correspond with our expectation, users do not want to

Statement
A) Using the system in my research
would
enable me
to accomplish tasks
more quickly if others were to publish
their data like that.
B) Using the system in my research
would improve my
performance if others were to publish
their data like that.
C) By
providing
and publishing my
artifacts
research
system I
in the
other
can
help
researchers
to ac-
complish
their
individual goals.
D)
I know other
platforms which basically provide the
same features as this
system.
E) Learning to operate the system was
easy.
F) I find the system
easy to use.

0% 0% 0% 7% 33% 40% 20%

0% 7% 0% 0% 33% 40% 20%

0% 6% 0% 7% 27% 33% 27%

100% 0% 0% 0% 0% 0% 0%

0% 0% 0% 0% 13% 40% 47%

0% 0% 0% 0% 27% 27% 46%

Table 5: Functional relevance and convenience ratings of all participants in percent.
1 = Strongly disagree, 7 = Highly agree

add semantic information by hand. As mentioned before no
system was added during the study. This is manly due to
the fact that the user group participating was very homo-
geneous, consisting mainly of component developers that do
work with systems but had no complete overview over all
necessary components to include them in the system.

7. CONCLUSION

Scientific research involves the interaction, and thus the
production of diverse kinds of research artefacts such as pa-
pers, data sets, software, and others. Nowadays, research is
not conducted by oneself, but collaboratively, spatially dis-
tributed, and by utilizing software-intensive systems. For
replication and understanding of such research results, it
is essential to know which software components, in which
versions and configurations have been run. Unfortunately,
these informations are often omitted or incomplete. How-
ever, an open issue for data acquisition in such situations is
that many of these artefacts cannot be linked automatically,
because they are of a different, unrelated class. To provide
a coherent view using semantic web formalisms, some additional data entry, and particularly linking, is required. To
tackle this issue we have presented our concept (see Sec.
3) which proposes that researchers should be able to easily
publish and link diverse but interrelated research artefacts
into semantically enriched aggregates, with respect to their
requirements and work-flow. To further investigate these
requirements we have built a prototype system, which has
been introduced in Sec. 4. We subsequently conducted an

163exploratory user study (see Sec. 5) to determine whether
the interaction with the system is as expected.

The results presented in Sec. 6.4 show the match between
our concept and the users requirements such as all subjects
were able to integrate at least one research artefact related
to their current work. Additionally, the participants created 71 links between diverse artefacts and were especially
interested in aggregating software components and their related releases (see Sec. 6.4). Moreover, we could demonstrate that our approach for referencing other repositories
actually delivers the potential benefits of re-using data and
is perceived as efficient. As depicted in Sec. 6.3, the median time to create a release takes 124 seconds (roughly two
minutes), adding a publication took just under a minute.
More promising results, with regard to our concept, were
shown in Sec. 6.4 where the overall time to link diverse artifacts was measured  it didnt exceed 5 seconds. Since this
has been an exploratory study, and we couldnt find any
base-line metrics comparable to such an import-and-link
process, we assess the results as convenient, especially because of the results presented in Sec. 6.5.2. These results
indicate that the overall required effort for importing, respectively adding artefacts, is considered low. The cost of
linking research artefacts is assessed very low. Furthermore,
our system is perceived as useful to accomplish individual
goals more quickly, and to help other researchers to accomplish their goals by providing information in this manner.

Besides these positive results, there are also a few caveats.
Surprisingly, no system or system version has been added.
We assume this may have been due to the fact that adding
a system/-version requires the creation of multiple components and releases in the first place. Furthermore, the participants need a profound knowledge of all components and
their dependencies, integrated into a single system. This is
often difficult to accomplish by a single developer. Pertaining to the total number of participants, we are aware of the
fact that 15 subjects are often not sufficient. Since we have
conducted an exploratory study of a prototype system, similar to typical applications in this domain, we are optimistic
to be able transfer these results to larger studies.

The next step for us is the release of additional entities
in our system. We are currently implementing entities for
open hardware and user study design descriptions. Based
on these new entities we can further investigate the requirements of researchers who, on the one hand work with
physical artefacts, and on the other hand work in a predominantly empirical manner. Additionally, we are interested
in data preservation and, more importantly, data maintenance regarding to our system. Last but not least, we will
provide our implementation in the form of Drupal modules,
thus contributing to the community.

8. ACKNOWLEDGMENTS

This work has been partially supported by the German
Aerospace Center (DLR) with funds from the Federal Ministry of Economics and Technology (BMBF) due to resolution 50RA1023 of the German Bundestag.
