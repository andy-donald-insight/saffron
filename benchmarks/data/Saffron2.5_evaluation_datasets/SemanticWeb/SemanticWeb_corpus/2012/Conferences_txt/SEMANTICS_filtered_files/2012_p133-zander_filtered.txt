A Computational Model for the Integration of Linked Data

in Mobile Augmented Reality Applications

Research Group Multimedia

Research Group Multimedia

Stefan Zander

Information Systems
University of Vienna

Chris Chiu

Information Systems
University of Vienna

Gerhard Sageder

Research Group Multimedia

Information Systems
University of Vienna

stefan.zander@univie.ac.at

chris.chiu@univie.ac.at

gerhard.sageder@univie.ac.at

ABSTRACT
Linked Data, Augmented Reality (AR), and technical advancements in mobile information technology lead to an increasing desire to exploit Linked Data for the integration and
visualization in mobile AR applications. However, current
approaches are either bound to existing client-server-based
infrastructures or use closed data sources and proprietary
data formats. Moreover, a number of related approaches
are built upon content-based recognition algorithms that are
both memory and processing-intensive, require a permanent
connection to a host, and thus are inappropriate for a direct
deployment onto mobile devices. In this work, we present a
computational model that builds on a sensor-based tracking
approach and maps proactively replicated Linked Data sets
to a virtual representation of the users vicinity computed
by a mathematical model. We demonstrate the applicability
of our approach through a proof-of-concept AR application
that retrieves and aggregates mountain-specific data from a
set of different sources and displays such data in a live-view
interface. In consequence, our approach is resource-efficient,
does not require a permanent network connection, is independent from existing server-based infrastructures, and allows to process Linked Data directly on a mobile device.

Categories and Subject Descriptors
H.5.1 [Information Interfaces and Presentation]: Multimedia Information SystemsArtificial, augmented, and virtual realities; I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and RealismVirtual reality

General Terms
Human Factors, Algorithms, Experimentation

Keywords
linked data, mobile information systems, augmented reality,
semantic web, visualization

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
I-SEMANTICS 2012, 8th Int. Conf. on Semantic Systems, Sept. 5-7, 2012,
Graz, Austria
Copyright 2012 ACM 978-1-4503-1112-0 ...$10.00.

1.

INTRODUCTION

The concept of Augmented Reality (AR) is well-known
since the early 90s; it complements physical objects with
computer-generated content in real time and allows users
to interact with the real world through computer-generated
interfaces [2, 7, 15]. This definition comprises the main aspects an AR system exhibits: (i) it combines real and virtual
objects in a real environment; (ii) it runs interactively and
in real time; (iii) it registers (aligns) real and virtual objects
with each other [3]. The omnipresence of mobile devices together with continuous advancements in mobile information
technology make the implementation of mobile AR systems
attractive for a broad number of users and use cases  as
indicated by the increasing number of conferences and research projects related to that topic [31].

One domain that is well-suited for the exploitation of data
to be integrated in AR systems is that of Linked Data [5, 13]
where the potential of such a synthesis was recently acknowledged (cf. [22]). By fetching and combining Linked Data
from different data sets, such data can be complemented
with location-based information retrieved from geographic
services such as GeoNames 1 or Earthtools 2 that provide the
necessary information to determine the latitude and longitude coordinates of non-information resources [14] as well as
their altitude and elevation. Existing approaches host such
data in isolated data silos or employ proprietary data for-
mats. Moreover, vision-based or hybrid AR systems, which
are discussed in Section 2, exhibit computationally complex
matching and classification algorithms and require extensive sets of reference images wherefore their deployment in
mobile AR scenarios is limited.

In this work, we present a computational model that builds
on a mathematical model and combines a sensor-based tracking approach with a mobile RDF processing and management framework for the provision and complementation of
Linked Data with related geographical information to enable an utilization by mobile AR applications. It proactively
replicates Linked Data to a mobile device, aggregates and
consolidates such data (e.g., filter identical resources as indicated by the owl:sameAs property or identify and merge
identical resources using declaratively described heuristics),
and can be deployed in a wide variety of application scenar-
ios. In consequence, our approach is resource-efficient compared to vision-based tracking approaches, it does not require a permanent network connection as data can be replicated proactively, it operates independently from specific

http://www.geonames.org/
http://www.earthtools.org/

133data sources and host infrastructures, and it is capable to
process Linked Data directly on a mobile device.

In the remainder of this work, we introduce different AR
tracking methods and discuss relevant works w.r.t. employed
tracking methods and data utilization methodologies in Section 2. Details of the proposed computational and mathematical model together with main conceptual constituents
are presented on a formal basis in Section 3. We demonstrate the practical applicability of our approach through a
prototypical proof-of-concept application that retrieves and
federates mountain-specific data from various Linked Data
sources and displays them real-time in an AR interface in
Section 4. Runtime-specific aspects of the proof-of-concept
prototype together with data quality aspects are evaluated
in Section 5, and in Section 6 we summarize our findings
and discuss possible future directions of Linked Data AR
application development.

2. BACKGROUND AND RELATED WORK
In general, AR tracking methods can be categorized into
marker-based and marker-less. Marker-less tracking is further divided into vision-based tracking, sensor-based track-
ing, and a hybrid form of both methods (cf. [31]). As markerbased tracking requires a controlled environment and is less
suited for outdoor mobile AR systems, we exclusively discuss
works that employ marker-less tracking techniques.
2.1 Vision-based Tracking

Generally, vision-based tracking systems are classified according to the position of the optical sensor relative to the
environment and the objects it tracks [10]. As we specifically focus on tracking methods for mobile AR systems, we
only consider works that are built on the assumption of a
stationary scene and a flexible optical sensor.

Vision-based systems generally provide accurate results
but require manual initialization and a permanent connection to a host infrastructure for computing object recognition and feature classification algorithms [11, 21]. Such an
architectural approach is particularly suited for mobile AR
systems as handheld devices usually lack processing power
and memory capacity, orin case of entry-level devicesthe
deployment of powerful floating point units (FPU), which requires a transformation into fixed point integer-based data
structures [11, 18]. Moreover, vision-based tracking depends
on the quantity and quality of reference images and might
be influenced by lighting conditions [23]. Another problem
is scalability as the computational effort increases with the
amount of objects to be recognized and tracked as well as
with the quantity of reference images [9].

Vision-based tracking methods are applied for both indoor and outdoor localization and are complemented with
different technologies for classification and recognition such
as Fourier transformation-based matching techniques [23],
machine-learning techniques to improve tracking approximations (e.g. [8]), scale invariant feature transformation and
classification [29], annotation features [27], or adaptive land-
mark-based feature extraction and clustering to increase efficiency and robustness [24]. Other works (e.g. [18, 19]) utilize vision-based tracking methods for indoor localization
where the virtual representation of physical objects is complemented with computer-generated 3D models that are projected over a recognized object.

2.2 Sensor-based Tracking

Sensor-based tracking techniques make use of data streams
acquired from built-in sensors such as accelerometer, gy-
roscope, GPS sensor, or digital compass to determine the
physical location and the orientation of a device. The accurateness of sensor-based tracking systems depends on the
quality and precision of acquired sensor data as well as on
the users current location. It requires an augmentation of
objects with geographic information to determine their position relative to the orientation and field-of-view of a camera.
A computational model calculates the projections of the augmented content displayed in an AR interface in relation to
the real world coordinates of the pertaining physical objects
and keeps track of camera movements.

Existing works can be classified into AR browsers and
domain-specific AR applications; Mixare 3, Wikitude 4, and
the Layar Reality Browser 5 are recently published AR browsers that employ sensor-based tracking techniques. While
Wikitude is built upon the Augmented Reality Markup Language (ARML)6 for mapping and visualizing geo-referenced
objects such as POIs and their meta data, Mixare and Layar
use hidden and proprietary data structures encapsulated in
developer APIs. All works rely on a server-based infrastructure and enable the integration of user-generated content
through specific infrastructure-dependent Web services.

A sensor-based AR application for the visualization of
user-generated content in an AR interface is introduced by [4]
that analyzes the EXIF metadata annotations of multimedia objects and places pictures over the corresponding POIs.
Sekai Camera 7 and LibreGeoSocial 8 introduce social-sharing
features and allow users to post and discover user-generated
multimedia content in a physical space. While Sekai Camera exhibits a closed architecture, LibreGeoSocial and [4]
operate independently from specific content providers and
can be adapted to specific application needs.

Two works that make use of Linked Data are the SmartReality 9 project and the AR browser presented in [25] in which
Linked Data is combined with domain-specific content for
the exploration and visualization of cultural heritage con-
tent. A backend server handles aggregation, consolidation,
and complementation tasks and fetches Linked Data resources
from DBpedia, LinkedGeoData, and Geonames. The SmartReality framework combines AR technology with Web services and semantic technologies for the exploration of information about Things of Interest (TOIs) located in the users
immediate vicinity. Reasoning, aggregation, classification,
and filtering tasks are handled by a server-side platform that
preprocesses acquired data for client-side visualization.
2.3 Hybrid Tracking

Hybrid tracking approaches combine vision- and sensorbased tracking techniques; while sensor-based tracking is
fast, computationally less expensive, and does not require a
host infrastructure, vision-based tracking methods provide
more accurate recognition results at the cost of complex host

http://www.mixare.org/
http://www.wikitude.com/
http://www.layar.com/
http://www.openarml.org/wikitude4.html
http://www.tonchidot.com/en/
http://www.libregeosocial.org/
http://www.smartreality.at/

134infrastructures.
In hybrid approaches, sensor-based tracking methods usually serve as initialization means for visionbased recognition and matching methods and allow for a
more precise and robust estimation of the camera pose in
uncontrolled environments [17].

Hybrid tracking techniques are mainly deployed in outdoor localization system for urban environments and combine vision-based tracking methods with GPS data to minimize the necessity of re-initialization inputs and reduce GPS
signal inaccuracies, which are caused by atmospheric signal
delays and multi-path signals [21]. Other works (e.g. [9]) employ sophisticated patch-retrieval and image clustering algorithms for real-time camera pose recovery or employ multilayered tracking methods (e.g. [17]) where sensor-based tracking is complemented with edge detection and feature point
tracking techniques. Some hybrid approaches use statistical
models or Kalman filters (e.g. [1, 16, 20]) for the fusion of
sensor and vision-based mensurations.
2.4 Summary

In summary, most works exhibit client/server-based infrastructures for hosting location-based information and are
developed for specific domains or are centered around specific data sources (e.g. Wikipedia). They rely on proprietary
data formats and specific APIs for the integration of usergenerated content. Only two projects make use of Linked
Data repositories and discuss aspects related to the iden-
tification, integration, consolidation, and complementation
of Linked Data with domain-specific content. Nevertheless,
standardization efforts towards a common markup format
for AR-specific content are currently ongoing.

3. APPROACH

Our approach builds on sensor-based tracking technology
and maps Linked Data to a virtual representation of the
users vicinity computed by a mathematical model that is
described in detail in Section 3.1. The computational model
underlying our approach is capable to process RDF data
directly on a mobile device, performs consolidation and reasoning heuristics, and displays relevant information of both
information and non-information resources in AR interfaces
at run-time. We presuppose the existence of a local RDF
processing and management infrastructure which can be realized with a mobile RDF framework such as Androjena 10.
In contrast to other mobile RDF frameworks, Androjena
revealed to be most mature and is able to process comparatively large data sets containing around 100,000 triples on
current state-of-the-art devices in reasonable time [30].

The computational model presented as approach in this
work consists of six phases and is depicted in Figure 1. It
relies on the assumption that both position and orientation
sensors are deployed on a mobile device. Current mobile operating systems provide well-defined APIs and interfaces for
requesting data from locally deployed sensors and use such
data for further processing as indicated by the Acquisition
of Sensor Data phase. Position data serve as initialization
parameters for Linked Data replication and the mathematical model, whereas orientation data, which are continuously
acquired, are used as runtime parameters for the real-time
calculation of the corresponding screen coordinates updates.
In the Linked Data Replication phase, acquired position

data are analyzed and used for initiating requests to Linked
Data sources; depending on offered interfaces, such data
can be integrated in GeoSPARQL queries 11 or incorporated
in dedicated API requests (e.g., the current location coordinates plus a particular radius). Retrieved data sets are
stored in a local triple store on the mobile device as named
graphs and are prepared for further processing steps.

The Consolidation phase comprises the identification and
removal of identical resources as well as the aggregation of
different descriptions pertaining to a resource.
In order
to discover links between resources retrieved from different LOD sources and to perform data-level consolidations
on aggregated and federated data, our approach builds on
the inclusion of a link discovery framework such as Silk [6]
that allows for a declarative specification of linking type se-
mantics, conditions, and (aggregation) operations for establishing RDF links between federated resources. Moreover,
Silk considers sub graph topologies of RDF resources and
allows to define reverse/inverse properties on the basis of
extended graph-based edge navigation and filter-based node
selection [6]. These aspects allow for a declarative definition
of complex consolidation heuristics that serve as means for
computing similarities between resources hosted in different domains and identify owl:sameAs relationships. For in-
stance, the similarity of descriptions can be computed on different levels and assigned different weights to infer whether
they describe the same resource. We demonstrate the consolidation of two such descriptions as part of the proof-of-
concept by means of a simple example in Section 4.

In order to display Linked Data in AR interfaces, the descriptions of replicated resources need to be complemented
with geographical data that allow for determining the realworld position (3D coordinates) of an objects representa-
tion; for instance, the latitude and longitude property values
of a resource are complemented with altitude information
acquired from secondary data sources in case a resources
description does not exhibit this information originally. In
the Complementation phase, all resources hosted in the local
tripe store are therefore analyzed and become augmented in
case such information is missing. Missing information can
either be acquired from the description of another resource
identified as identical or by invoking requests to geo-location
services such as GeoNames or Earthtools. However, depending on the amount of replicated resources hosted in the local
triple store, the augmentation may cause multiple concurrent asynchronous requests.

The consolidated and augmented resource descriptions are
then processed by the Mathematical Model to calculate the
corresponding screen coordinates on the basis of the acquired sensor data. The mathematical model is a central
aspect of our approach and described in detail in Section 3.1.
It allows for creating a virtual representation of resources located in the surrounding environment on the basis of their
real-world coordinates. For performance reasons and to differentiate run-time-specific aspects from static resource de-
scriptions, we host the calculated screen coordinates of the
pertaining resources in a separate internal data structure.
However, to make calculated data available to external ap-
plications, we suggest to represent them as RDF and exhibit
interfaces for their utilization by external applications such
as mobile AR browsers (cf. Section 2) or to map them to AR

http://code.google.com/p/androjena/

http://geosparql.org/

135Recalculation of Screen Coordinates

Initiation of new Replication Process

Acquisition of 
Sensor Data

Linked Data 
Replication

Consolidation

Complementation 
with AR-relev. Data

Calculation 

Mathematical Model

Visualization

Storage

Merging and 
Identification

Augmentation

Calculation of 

Screen Coordinates

Local Triple Store

Visualization of POIs that are in the current view area

Figure 1: Conceptual workflow of the computational model

markup languages such as ARML 2.012. We consider that
future work as it requires the existence of efficient RDF re-
trieval, persistence, and query implementations for mobile
platforms.

After transforming real-world 3D coordinates of replicated
resources to 2D screen coordinates, they are displayed in an
AR user interface where the mathematical model calculates
the corresponding screen coordinates relative to the users
current position and the orientation of a device in realtime.
3.1 Mathematical Model

Both the cameras position as well as the positions of
replicated resources13 are provided in geographic coordinates , ,  (cf. Table 1). The cameras geographic position is obtained by using the values provided by the mobile
devices GPS sensor. If the mobile devices sensors do not
provide an accurate sea level altitude value, we make use of
geo information services to obtain elevation data for given
longitude/latitude values.

In order to convert geographic coordinates into camera
view space, the distance of each POI relative to the cameras position is calculated using the Vincentys inverse formula [26]. It approximates the earths shape as an ellipsoid
with a shorter polar axis than equatorial diameter (oblate
spheroid) as opposed to other calculation methods such as
the haversine formula, which assumes the earths shape to
be perfectly spherical (cf. [12]) causing less accurate results
near the polar caps. The formula is iterative and loops until
the required degree of accuracy is reached. Thus, the higher
computational costs of the Vincenty formula compared to
less accurate methods such as the haversine formula can be
partially mitigated by setting conservative accuracy require-
ments. As a result, we obtain the final bearing angle 
from the cameras position to the POIs position and the
distance z. We also calculate the vertical inclination angle 
between the camera eye point and and the specific POI (Fig-
ure 2) by considering the altitude of the camera and the POI
using basic trigonometry.

We introduce definitions for each part of our mathematical
model: Definition 1 introduces bearing, distance, and inclination as functions. Definition 2 introduces camera angle-
of-view borders, and Definition 3 shows the combined mathematical model our approach uses.

http://www.opengeospatial.org/projects/groups/arml2.0swg

13We refer to resources as points of interest (POIs)

Table 1: Mathematical symbols

Symbol
poi, cam
poi, cam
poi, cam







Haov
Vaov

xdeg, ydeg

xscreen, yscreen

w, h


Description

Longitude of POI/camera
Latitude of POI/camera
Altitude of POI/camera
Range radius within which POIs are queried
Recorded azimuth of device
Recorded pitch of device
Bearing from device to POI
Ellipsoidal distance between device and POI
Inclination angle from camera to POI
Horizontal angle of view
Vertical angle of view
Horizontal/vertical angle mapped to cameras
horizontal/vertical angle of view
2D screen space position in pixels
Device screen dimensions
Set of POI geo-locations
Set of camera parameters
Vector field of angle-based screen positions

Definition 1. Let P be a set of POI geographic locations
p = (poi, poi, poi), and C be a set of camera parameters
c = (, , Haov, Vaov, cam, cam, cam). We define a func-
tion

vbearing : P  C  [0, 2)

that calculates the bearing angle  from a camera point
c  C to a geographic location p  P using the Vincenty
Inverse Formula. Furthermore, we define a function

vdistance : P  C  [0, r]

that calculates the distance from c to p, also using Vincentys
Inverse Formula. Lastly, we define an inclination function
fincl : P  C  [, +] as

 poi  cam


fincl(p, c) = arctan

vdistance(p, c)

with p  P, c  C

that calculates the inclination angle  from a camera point
c to a POI p.

The cameras azimuth  and pitch  can be obtained from
the mobile devices orientation sensors, and we also retrieve
the focal length/field of view of the camera lens through the
device API. We can calculate the horizontal and vertical angles of view from the cameras field of view. In combination

136with the cameras location (cam, cam, cam) obtained by
the GPS sensor we then have a complete definition of the
devices position and rotation in space.

Definition 2. Let c = (, , Haov, Vaov, cam, cam, cam) 
C be a tuple of camera parameters. We define two horizontal
angle-of-view border functions

lborder(c) = (  Haov/2) mod 2
rborder(c) = ( + Haov/2) mod 2

yielding the bearing angles of the left and right border of the
cameras horizontal field of view respectively. Analogous we
define two vertical angle-of-view border functions

tborder(c) = ( + Vaov/2)
bborder(c) = (  Vaov/2)

for defining the angles of the top and bottom border of the
cameras vertical field of view.

The bearing  can now be mapped to the horizontal angle
of view, and the inclination angle  to the vertical angle of
view by considering both the devices azimuth , pitch 
and horizontal/vertical angles of view. We only consider
POIs inside the cameras view frustum, and calculate the
horizontal and vertical angles (xdeg, ydeg) inside the cameras
horizontal and vertical angles of view (Figure 2).

Figure 2: Mapping the POIs bearing  and inclination  to (xdeg, ydeg)

4. PROOF OF CONCEPT

We demonstrate the practical applicability of our approach
in form of a prototype running on the Android mobile operating system that proactively replicates mountain-specific
Linked Data to the mobile device and visualizes such data
real-time in an AR interface depending on the users current
position and the orientation of the device.

Definition 3. Let S be a vector field of POI positions
within the cameras field of view s = (xdeg, ydeg, z) with
xdeg  [0, Haov], ydeg  [0, Vaov] (see Figure 2) and the
camera-POI-distance z  [0, r]. We define a function faov : P
C  S that transforms POI locations to angle-of-view coordinates as


tx(vbearing(p, c), c)

ty(fincl(p, c), c)
vdistance(p, c)




faov(p, c) =


where tx is a horizontal transformation function tx : [0, 2)
C  [0, 1] defined as

tx(, c) =

  lborder(c)
  (2  lborder(c))

if   lborder(c),
otherwise

and ty is a vertical transformation function ty : [, ]C 
[0, 1] defined as

ty(, c) = tborder(c)  

tx and ty result in a coordinate xdeg  [0, Haov] andy deg 
[0, Vaov] defining the position within the cameras field of
view. To get final device screen coordinates, we use the
screen dimensions w and h to calculate

xscreen =

yscreen =

xdeg
Haov
ydeg
Vaov

(1)

(2)

We obtain the coordinates (xscreen, yscreen, z) for each
POI which can be used as center point for drawing each
POI information overlay on screen. The distance value z
can be used for visualizing how far each POI is situated.

We assume the following scenario to illustrate the steps
described in Figure 1: the GPS sensor acquires device location information and provides them to the Linked Data
replication, which in turn retrieves mountain-specific data
located in the vicinity of the device determined by a specific range radius. For example, a camera location might be
cam = 47.689254
, cam = 1, 969 m,
and we look for POIs within the range radius around the
camera using Linked Data.

, cam = 15.688808





4.1 Data Integration

We use multiple Linked Data sources that have to be repli-
cated, aggregated, and consolidated as well. Since there are
real-world entities described by different schemes and on-
tologies, there is a necessity to define rules how to integrate
these data sets respectively to identify identical or highly
similar resources. Exemplary responses from GeoNames and
LinkedGeoData are shown in Listing 1 and 2. These two descriptions represent a well-known peak in the Styrian Rax
Alps - the Dreimarkstein. We will show how these two representations can be mapped using both textual and geographic
information, even if the retrieved data do not coincide ex-
actly. Thus, we define a composite metric constructed of a
geospatial and text criterion using the Silk Link Specification Language [6] in order to determine objects considered
as equivalent. Listing 3 illustrates a basic LinkageRule and
therewith how resources of GeoNames (x) and LinkedGeoData (y) are matched using a measure of length as threshold
for the geographic vertical and horizontal distance. In ad-
dition, gn:name respectively rdfs:label must correspond
according to the Jaro distance metric for string similarity.
If the weighted and aggregated similarities fall below a certain threshold (<Filter threshold="0.85" limit="1"/>),
a link is being created using the owl:sameAs property.

137Listing 1: Response from GeoNames

< http :// sws . geonames . org /6940124/ > gn : Feature [

rdfs : i s D e f i n e d B y

< http :// sws . g e o n a m e s . org / 6 9 4 0 1 2 4 / about . rdf > ;

gn : name " D r e i m a r k s t e i n " ;
gn : f e a t u r e C l a s s gn : T ;
gn : f e a t u r e C o d e gn : T . PK ;
gn : c o u n t r y C o d e " AT " ;
w g s 8 4 _ p o s : lat " 4 7 . 7 0 8 6 1 " ;
w g s 8 4 _ p o s : long " 1 5 . 7 1 5 8 3 " ;
w g s 8 4 _ p o s : alt " 1 9 4 8 " ;
gn : p a r e n t C o u n t r y < http :// sws . geonames . org /2782113/ > .

] .

Listing 2: Response from LinkedGeoData
< http :// l i n k e d g e o d a t a . org / t r i p l i f y / no d e 4 2 8 8 6 6 4 7 5 >

= < http :// sws . g eonames . org /6940124/ > ;
lgdo : ele " 1 9 6 0 " ^ ^ xsd : float ;
rdfs : label " D r e i m a r k s t e i n " ;
a < http :// l i n k e d g e o d a t a . org / o n t o l o g y / Peak > ;
georss : point " 4 7 . 7 0 8 7 3 1 1 5 . 7 1 5 4 6 3 7 " ;
geo : lat " 4 7 . 7 0 8 7 3 1 " ^ ^ xsd : d e ci ma l ;
geo : long " 1 5 . 7 1 5 4 6 3 7 " ^ ^ xsd : d ec i ma l .

Listing 3: Silk LSL LinkageRule

< L i n k a g e R u l e >

< A g g r e g a t e type = " a v e r a g e " >

< C ompare metric = " jaro " weight = " 2 " >

< Input path = " ? x / g n: na m e " / >
< Input path = " ? y / r d f s : l a b e l " / >

</ Compare >
< Co m p a r e m e t r i c = " n u m S i m i l a r i t y " we i g h t = " 1 " >

< Input path = " ? x / w g s 8 4 _ p o s : l a t " / >
< Input path = " ? y / g eo :l a t " / >

</ Compare >
< Co m p a r e m e t r i c = " n u m S i m i l a r i t y " we i g h t = " 1 " >

< Input path = " ? x / w g s 8 4 _ p o s : l o n g " / >
< Input path = " ? y / g e o : l o n g " / >

</ Compare >
< Co m p a r e m e t r i c = " n u m S i m i l a r i t y " we i g h t = " 1 " >

< Input path = " ? x / w g s 8 4 _ p o s : a l t " / >
< Input path = " ? y / l g d o : e l e " / >

</ Compare >
</ A g g r e g a t e >
</ L i n k a g e R u l e >

4.2 Calculation of Screen Coordinates

Continuing our example scenario, the Dreimarkstein POI

has the geographic location poi = 47.70861
and the altitude poi = 1, 948 m. As orientation and field-

of-view data14, we assume a camera azimuth of  = 45

a camera pitch of  = 5
, a horizontal angle of view of
Haov = 45

, and a vertical angle of view of Vaov = 30

, poi = 15.71583













, tborder = 20

Using Definition 1, we calculate a Vincenty distance of

z = 2, 957.228 m, a final bearing angle of  = 43.3129
(rounded), and an inclination angle of  = 0.4069
be-
tween camera and POI. We use Definition 2 to determine
lborder = 22.5
. Subsequently, Definition 3
yields xdeg = 20.8129
(both rounded).
Finally, using Equation 1 and 2 and assuming a screen resolution of 800x480, we obtain the 2D coordinate xscreen =
370, yscreen = 327 in pixels (rounded to integers) which is
used as the center point for the POI information overlay on
the screen for visualization.

and ydeg = 20.4069







Our interface implements the magic lens metaphor [28]

14We use degrees instead of radians in this example

which utilizes the mobile device as a see-through interface
into the AR view, combining footage from the real world via
the devices camera with virtual information overlays correlating to the current real-world view. Compared to other AR
interface approaches such as wearable glasses, this approach
reduces both the amount and the cost of hardware the user
needs to carry along and makes it suitable for mobile use in
countryside and mountain environments. The mathematical model serves as connecting link between the real-time
camera footage and the virtual information overlays.

5. EVALUATION

We have conducted several experiments in oder to evaluate the proposed approach in terms of performance and data
quality of the used LOD data sources. Since GPS-related
performance measures are tremendously varying from device
to device, we spare location gathering issues for this evaluation and realize our performance measuring by determining
the amount of time needed for addressing, querying and aggregating the utilized data graphs of 3 LOD sources, the
computational costs of the calculation of screen coordinates
from raw LOD triples as well as the expense for visualizing
the pre-processed POIs in the field of view represented by
the Augmented Reality application. Moreover we analyzed 3
LOD data sources and evaluated the availability of resources
in general as well as the respective ontology related type def-
initions. We provide semantically identical concepts/classes
as a proposition for reasonable consolidation of LOD triples
that are potentially useful for being utilized as POIs.
5.1 Data Sources

Even though LOD sources have a non-proprietary and
heterogeneous nature and in general cannot be deemed to
comply with certain quality aspects such as correctness or
consistency we however examined 3 LOD sources, the data
sets as well as the underlying ontologies, with the intention
to determine similar semantic concepts that describe a summit entity. We define a summit as a topologically highest
elevation or ridge atop a mountain in the country and try
to identify the most appropriate equivalence as shown in
Table 2.



The LinkedGeoData ontology (prefix: lgdo) covers 1,293
concepts that describe various spatial aspects which have
been derived from OpenStreetMap. There is only one class
lgdo:Peak that matches our definition of a summit and fits
perfectly. DBpedia (prefix: dbpo) offers 27,895 instances of
the class dbpo:Mountain which is considered to be equivalent
to a summit as defined, thus lgdo:Peak matches dbpo:Mountain
in our scenario. GeoNames (prefix: gn) does not offer a public SPARQL endpoint that can be used to retrieve POI de-
scriptions. However, there is a variety of webservices that
fulfill our requirements even if the data is served as XML
and needs further processing steps for consolidation with
LinkedGeoData and DBpedia. The nodes in GeoNames are
identified by the type gn:Feature having gn:featureClass
and gn:featureCode as properties. According to the GeoNames taxonomy, the feature class gn:T covers mountain, hill,
and rock concepts. We use the feature codes gn:T.PK and
gn:T.PKS having the SKOS definition pointed elevation(s)
atop a mountain, ridge, or other hypsographic features.
5.2 Performance

In order to evaluate the performance of our approach,

138Table 2: Equivalent LOD Concepts

Data Source

Concept

Instances

LinkedGeoData
DBpedia
GeoNames

lgdo:Peak
dbpo:Mountain

gn:T.PK  gn:T.PKS


we defined 4 time deltas tq, tm, tc, and tq that
are obtained directly through our implemented prototype.
We analyzed the runtime behavior of the main constituents
while covering the main aspects of the data flow processes as
shown in Table 3 depending on the number of POIs NP OI
respectively the range radius r which defines the maximum
POI distance to be considered.

Table 3: Time Deltas

Query process

tq
tm Triple merge, aggregation, and consolidation
tc
tv

Geometrical transformation of POI coordinates
POI Visualization

We conducted test runs for six different locations, which
have been chosen representatively for topographic characteristics in Austria by classifying them according to the terms
urban, rural, and alpine. For these places, the range radius
has been varied from 1 km to 75 km in order to evaluate
the scalability and the temporal costs. While tq and tm
strongly depend on the camera location, the local area characteristics and thus the chosen range radius, tc as well as
tv are evaluated merely in relation to the number of POIs
by performing 100 test runs for each POI number. All tests
have been performed on the current reference device Samsung Galaxy Nexus running Android OS version 4.0.2.

Table 4: Test Locations & Characteristics
alpine

urban rural

Location

48.231077, 16.294781
47.391435, 13.048575
47.359699, 13.639694
47.073542, 15.437586
47.688706, 15.799706
47.272154, 11.396935


Figure 3 illustrates the coherence between the range radius and the number of retrieved POIs as well as the amount
of time needed for querying LOD sources. tm as well as
tv are shown in Figure 4. The consolidation of resource
descriptions takes a maximum of 772.46 ms for a range radius of 75 km and the mean of 802 POIs, which is acceptable
for a practical case study. Furthermore, it illustrates that
there exists an almost linear correlation between the number of POIs and the respective visualization time. Whereas
the number of concurrently visualized POI markers is technically limited by display resolution and camera character-
istics, the visualization process scales rather assertive.

The calculation respectively the transformation from the
gathered POI location information into the mathematical
model is given by tc, which amounts to a maximum calculation time of 75.23 ms, a mean of 0.69 ms, and a minimum
of 0.03 ms.


POIs per Range Radius

LinkedGeoData
GeoNames

Range (km)


Query process

LinkedGeoData
GeoNames

Range (km)

Figure 3: Number of POIs NP OI and Query time tq


Consolidation

POIs (#)


Visualization

POIs (#)

Figure 4: Consolidation tm and Visualization tv

6. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented a computational model
that exploits locally gathered sensor data in combination
with geographic information provided by multiple Linked
Data sources in order to enable the visualization of points
of interests in Augmented Reality interfaces. Linked Data
graphs are replicated, aggregated, and consolidated directly
on mobile devices, whereby it simplifies the geometric transformation from POI locations in a 3-dimensional space to
normalized 2D screen coordinates, which can be easily used
for the visualization. Our experiments demonstrate the applicability for real-world use cases and the effectiveness of
our approach.

As future work, we plan to extend and generalize the computational model to consider and interpolate on movement
and rotation patterns, thus supporting camera roll and predictive POI fetching. On a technical basis, the graphics
processing units (GPUs) of mobile devices can be utilized to
accelerate calculation and transformation processes in order
to increase the amount of POIs that can be simultaneously
processed and visualized on the screen. We further plan to
implement intelligent caching and replication strategies that
reduce replication and consolidation overheads. Moreover,
we intend to explore possibilities of enhancing the accuracy
of tracking techniques by using content-based features.
Acknowledgements
We thank Silvia F urst for implementing parts of the proof-
of-concept prototype.
