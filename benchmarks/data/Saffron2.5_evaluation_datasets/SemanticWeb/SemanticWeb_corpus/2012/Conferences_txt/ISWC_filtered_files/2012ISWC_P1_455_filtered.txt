Discovering Concept Coverings in Ontologies

of Linked Data Sources

Rahul Parundekar, Craig A. Knoblock, and Jos e Luis Ambite

Information Sciences Institute and Department of Computer Science

University of Southern California

4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292

{parundek,knoblock,ambite}@usc.edu

Abstract. Despite the increase in the number of linked instances in the
Linked Data Cloud in recent times, the absence of links at the concept
level has resulted in heterogenous schemas, challenging the interoperability goal of the Semantic Web. In this paper, we address this problem by
finding alignments between concepts from multiple Linked Data sources.
Instead of only considering the existing concepts present in each ontol-
ogy, we hypothesize new composite concepts defined as disjunctions of
conjunctions of (RDF) types and value restrictions, which we call restriction classes, and generate alignments between these composite concepts.
This extended concept language enables us to find more complete definitions and to even align sources that have rudimentary ontologies, such
as those that are simple renderings of relational databases. Our concept
alignment approach is based on analyzing the extensions of these concepts
and their linked instances. Having explored the alignment of conjunctive
concepts in our previous work, in this paper, we focus on concept coverings (disjunctions of restriction classes). We present an evaluation of this
new algorithm to Geospatial, Biological Classification, and Genetics do-
mains. The resulting alignments are useful for refining existing ontologies
and determining the alignments between concepts in the ontologies, thus
increasing the interoperability in the Linked Open Data Cloud.

Introduction

The Web of Linked Data has grown significantly in the past few years  31.6
billion triples as of September 2011. This includes a wide range of data sources
from the government (42%), geographic (19.4%), life sciences (9.6%) and other
domains.1 A common way that the instances in these sources are linked to others is through the owl:sameAs property. Though the size of Linked Data Cloud
is increasing steadily (10% over the 28.5 billion triples in 2010), inspection of
the sources at the ontology level reveals that only a few of them (15 out of the
190 sources) include mappings between their ontologies. Since interoperability
is crucial to the success of the Semantic Web, it is essential that these heterogenous schemas, the result of a de-centralized approach to the generation of data

http://www4.wiwiss.fu-berlin.de/lodcloud/state/

P. Cudr e-Mauroux et al. (Eds.): ISWC 2012, Part I, LNCS 7649, pp. 427443, 2012.
c Springer-Verlag Berlin Heidelberg 2012

R. Parundekar, C.A. Knoblock, and J.L. Ambite

and ontologies, also be linked. The problem of schema linking, such as schema
matching in databases and ontology alignment in the Semantic Web, has been
well researched [5,1,4]. As in Instance-based Matching [4,3,7], we follow an extensional approach to generating the alignments. The novelty of our approach
consists of generating new concept hypotheses beyond the concepts originally
present in the ontologies, and aligning these extended concepts by exploiting the
linked instances in the Linked Data Cloud.

The problem of finding alignments in ontologies of Linked Data sources is
non-trivial, since there might not be one-to-one concept equivalences. In some
sources the ontology is extremely rudimentary, for example GeoNames has only
one class - geonames:Feature, and the alignment of such an ontology with a well
defined one, such as DBpedia, is not particularly useful. In order to be successful
in linking ontologies, we need to generate more expressive concepts. The necessary information to do this is often present in the properties and values of the
instances in the sources. For example, in GeoNames the values of the featureCode and featureClass properties provide useful concept constructors, which can
be aligned with existing concepts in DBpedia, so that we have that the concept
geonames:featureCode=P.PPL (populated place) aligns to dbpedia:City. There-
fore, our approach explores the space of concepts defined by value restrictions,
which we will call restriction classes in the reminder of the paper. A value restriction is a concept constructor present in expressive description logics, such as
OWL-DL (SHOIN (D)) [6]. We consider class assertions (rdf:type) and value restrictions on both object and data properties, which we will represent uniformly
as {p = v}, where either p is an object property and v is a resource (including
rdf:type=Class), or p is a data property and v is a literal. We consider two restriction classes equal if their respective instance sets can be identified as equal
after following the owl:sameAs links.

In our previous work [10], we explored conjunctive restriction classes. In this
paper, we explore disjunctive restriction classes. Specifically, we focus on concept coverings where a larger concept from one source can be explained by (i.e.,
is extensionally equivalent to) the union of multiple smaller classes in the other
source. Our approach finds alignments based on the extensions of the concepts,
that is, the sets of instances satisfying the definitions of the restriction classes.
We believe that this is an important feature of our approach in that it allows
one to understand the relationships in the actual linked data and their corresponding ontologies. The alignments generated can readily be used for modeling
and understanding the sources since we are modeling what the sources actually
contain as opposed as to what an ontology disassociated from the data appears
to contain based on the class name or description.

This paper is organized as follows. First, we describe the Linked Open Data
sources that we align. Second, we present the alignment algorithm that consists of
two steps: finding initial equivalence and subset relations, and then discovering
concept coverings using disjunctions of restriction classes. Third, we describe
representative alignments discovered by our approach and present an evaluation of the results. An interesting outcome of our algorithm is that it identifies
?

?

?
inconsistencies and possible errors in the linked data, and provides a method
for automatically curating the Linked Data Cloud. Finally, we compare against
related work, and discuss our contributions and future work.2

2 Sources Used for Alignments

In the Linked Open Data Cloud, sources often conform to different, but related,
ontologies that can also be meaningfully linked [2,9,10]. In this section we describe some of these sources from different domains that we align, instances in
which are linked using an equivalence property like owl:sameAs.
Linking GeoNames with Places in DBpedia: DBpedia (dbpedia.org) is a
knowledge base that covers multiple domains including around 526,000 places
and other geographical features from the Geospatial domain. We align concepts
in DBpedia with GeoNames (geonames.org), which is a geographic source with
about 7.8 million geographical features. GeoNames uses a rudimentary flat-file
like ontology, where all instances belong to a single concept of Feature, with the
type data (e.g. mountains, lakes, etc.) encoded in the featureClass and featureCode properties.
Linking LinkedGeoData with Places in DBpedia: We also find alignments
between the ontologies behind LinkedGeoData (linkedgeodata.org) and DBpe-
dia. LinkedGeoData is derived from the Open Street Map initiative with around
101,000 instances linked to DBpedia using the owl:sameAs property.
Linking Species from Geospecies with DBpedia: The Geospecies knowledge base (lod.geospecies.org) contains a taxonomic classification of living organisms linked to species in DBpedia using the skos:closeMatch property. Since
these sources have many species in common, they are ideal for finding alignments
between the vocabularies.
Linking Genes from GeneID with MGI : The Bio2RDF (bio2rdf.org) project
contains inter-linked life sciences data extracted from multiple data-sets that
cover genes, chemicals, enzymes, etc. We consider two sources from the Genetics domain from Bio2RDF, GeneID (extracted from the National Center
for Biotechnology Information database) and MGI (extracted from the Mouse
Genome Informatics project), where the genes are marked equivalent.

In Section 4 we provide results for the four alignment experiments described
above. In the rest of this paper we explain our methodology, which is source
independent, by using the alignment of GeoNames with DBpedia as an example.

3 Finding Concept Coverings across Ontologies

We use a two step approach to find concept coverings. First, we extract atomic
equivalent and subset alignments from the two sources where the restriction

2 This paper is an extended version of our workshop paper [11]. We have added more

formal descriptions, explanations of the algorithms and detailed evaluation.

R. Parundekar, C.A. Knoblock, and J.L. Ambite

class on each side contains a single property-value pair and no conjunction or
disjunctions. These are the simplest alignments that can be defined. We then
use these to find concept coverings by describing a larger concept with a union
of smaller ones using set containment.

We also discuss how our alignment approach detects outliers, which often
indicate missing or incorrect links, and provides a powerful tool to curate the
Linked Data cloud.

3.1 Finding Alignments with Atomic Restriction Classes

As a precursor to finding concept coverings between the two sources, our algorithm first finds alignments where the restriction classes on each side of the
alignment are atomic - i.e. have one property-value pair each. In our previous
work [10], we used a similar approach to find alignments between the ontologies
where a conjunction of restriction classes was aligned with its equivalent concept
in the other source. In this paper we focus on atomic restriction classes. Since
we do not need to find alignments of conjunctive restriction classes, the search
space is polynomial rather than combinatorial.

The sources are first prepared for exploration by performing an inner join on
the equivalence property (e.g., owl:sameAs) and optimized by removing inversefunctional properties. Then, the following algorithm is used to find the alignments between atomic restriction classes.

for all p1 in Source1 and distinct v1 associated with p1 do

r1  restriction class {p1 = v1} containing all instances where p1 = v1
Img(r1)  Find all corresponding instances from Source2 to those in r1,

linked by owl:sameAs

|r2|

|r1|

for all p2 in Source2 and distinct v2 associated with p2 do

, R  |Img(r1)r2|

r2  restriction class {p2 = v2} containing all instances where p2 = v2
P  |Img(r1)r2|
if P   then alignment(r1, r2)  r1  r2
if R   then alignment(r1, r2)  r2  r1
if P   and R   then alignment(r1, r2)  r1  r2

end if

end if

end if

end for

end for

Fig. 1 illustrates the set comparison operations of our algorithm. In order to
allow a certain margin of error induced by the data set, we use P   and R  
(instead of P = 1 and R = 1, which would hold if there were no error or missing
links) in our score function. In our experiments we used a threshold  = 0.9,
which was determined empirically, but can be changed as desired. For example,
consider the alignment between restriction classes {geonames:countryCode=ES}
?

?

?
= 0.9997 and P
?

?

?
from GeoNames and {dbpedia:country = dbpedia:Spain} from DBpedia. Based on
the extension sets, our algorithm finds |Img(r1)| = 3198, |r2| = 4143, |Img(r1)
r2| = 3917, R

= 0.9454. Thus, the algorithm considers the alignment as equivalent in an extensional sense. Some alignments that do not qualify
as equivalent, but with the smaller concept contained in the larger concept, qualify as subset relations. For example, we find that each of {geonames:featureCode
= S.SCH}, {geonames:featureCode = S.SCHC} and {geonames:featureCode =
S.UNIV} (i.e. Schools, Colleges and Universities from GeoNames) are subsets
of {dbpedia:EducationalInstitution}.
?

?

?
Fig. 1. Comparing the linked instances from two ontologies

Similar to our previous work [10], we also use certain optimization strategies
for faster computation. For example, if we explore the properties lexicograph-
ically, the search space is reduced to half because of symmetry. To qualify as
a concept, the intersection of the restriction classes needs to have a minimum
support, which we set experimentally to ten instances.

3.2 Identifying Concept Coverings

In step two, we use the subclasses and equivalent alignments generated by the
previous step to try and align a larger concept from one ontology with a union
of smaller subsumed concepts in the other ontology. To define a larger concept,
we group its subclasses from the other source that have a common property and
check whether they are able to cover the larger concept. By keeping the larger
restriction class atomic and by grouping the smaller restriction classes with
a common property, we are able to find intuitive definitions while keeping the
problem tractable. The disjunction operator that groups the smaller restriction
classes is defined such that i) the concept formed by the disjunction of the
classes represents the union of their set of instances, ii) the property for all the
property-value pairs of the smaller aggregated classes is the same. We then try to
detect the alignment between the larger concept and the union restriction class
by using an extensional approach similar to the previous step. The algorithm for
generating the hypotheses and the alignments is as follows:

R. Parundekar, C.A. Knoblock, and J.L. Ambite

for all alignments found in the previous step, with larger concepts from one
source with multiple subclasses from the other source do

UL  larger restriction class{pL = vL}, and corresponding instances.
for all smaller concepts grouped by a common property (pS) do

US  the union restriction class, and the corresponding instances of
all the smaller restriction classes{pS = {v1, v2, ...}}
UA  Img(UL)  US, PU  |UA|
|US| , RU  |UA|
|UL|
if RU   then alignment(r1, r2)  UL  US

end if

end for

end for

Since all smaller classes are subsets of the larger restriction class, PU  

by construction. We used  = 0.9 in our experiments. The smaller restriction
classes that were omitted in the first step because of insufficient support size of
their intersections, were included in constructing US for completeness.

such as

that alignments

subsets of {rdf:type = dbpedia:EducationalInstitution}. As

Figure 2 provides an example of the approach. The first step is able
{geonames:featureCode = S.SCH},
to detect
{geonames:featureCode = S.SCHC}, {geonames:featureCode = S.UNIV}
are
can be
Img({rdf:type = dbpeseen in the Venn diagram in Figure 2, UL is
dia:EducationalInstitution}), US is
{geonames:featureCode = S.SCH} 
{geonames:featureCode = S.SCHC}  {geonames:featureCode = S.UNIV}, and

UA is the intersection of the two. Upon calculation we find that R
U for the alignment of dbpedia:EducationalInstitution to {geonames:featureCode= {S.SCH,
S.SCHC, S.UNIV}} is 0.98. We can thus confirm the hypothesis and consider UL
& US equivalent. Section 4 describes these calculations and additional examples
of concept coverings.
?

?

?
Fig. 2. Concept covering of Educational Institutions from DBpedia
?

?

?
3.3 Curating the Linked Data Cloud

It turns out that the outliers, the instances of the restriction classes that do not
satisfy subset relations despite the error margins, are often due to incorrect and
missing links or assertions. We are able to detect these, thus providing a novel
method to curate the Web of Linked Data.
In the alignment of {rdf:type = dbpedia:EducationalInstitution} to {geonames:
featureCode = {S.SCH, S.SCHC, S.UNIV}} we find 8 outliers (Table 6, row 1).
For {rdf:type = dbpedia:EducationalInstitution}, 396 instances out of the 404 Educational Institutions were accounted for as having their geonames:featureCode
as one of S.SCH, S.SCHC or S.UNIV. From the 8 outliers, 1 does not have a
geonames:featureCode property asserted. The other 7 have their feature codes
as either S.BLDG (3 buildings), S.EST (1 establishment), S.HSP (1 hospital),
S.LIBR (1 library) or S.MUS (1 museum). This case requires more sophisticated
curation and the outliers may indicate a case for multiple inheritance. For ex-
ample, the hospital instance in geonames may be a medical college that could
be classified as a university.
In the {dbpedia:country = Spain}  {geonames:countryCode = {ES}} alignment (Table 6, row 2), one outlier instance was identified as having the country code IT (Italy) in GeoNames, suggesting an incorrect link/assertion. The
algorithm was able to flag this situation as a possible error, since there is overwhelming support for ES being the country code of Spain. Our union alignment
algorithm is able to detect similar other outliers and provides a powerful tool to
quickly focus on links that require human curation, or that could be automatically flagged as problematic, and provides evidence for the error.

4 Experimental Results

The results of our concept covering algorithm over the four pairs of sources we
consider appear in Table 1. The first step of our algorithm was able to generate about 180k equivalence and subset alignments. After running the covering
algorithm, 77966 subset alignments were explained by 7069 coverings, for a compression ratio of about 11:1.

Table 1. Concept Coverings Found in the 4 Source Pairs

Source1

Source2 O1-O2: Coverings

O2-O1 Coverings

Total

(Subset Alignments) (Subset Alignments) Coverings

GeoNames DBpedia
LinkedGeoData DBpedia
Geospecies DBpedia

GeneID

434 (2197)

2746 (12572)

191 (1226)

6 (29)

318 (7942)

3097 (48345)

255 (2569)
22 (3086)
?

?

?
R. Parundekar, C.A. Knoblock, and J.L. Ambite

4.1 Representative Examples of the Concept Coverings Found

|Img(r1)r2|

|r2|

|UL| ) followed by |UA| and |UL| in
|UA|

Some representative examples of the concept coverings found are shown in Tables 6, 7 and 8. In the tables, for each concept covering, column 2 describes
the large restriction class from ontology1 and column 3 describes the union of the
(smaller) classes on ontology2 with the corresponding property and value set. The
score of the union is noted in column 4 (RU =
columns 5 and 6. Column 7 describes the outliers, i.e. values v2 of property p2 that
form restriction classes that are not direct subsets of the larger restriction class.
Each of these outliers also has a fraction with the number of instances that belong to the intersection over the the number of instances of the smaller restriction
class (or
). One can see that the fraction is less than our relaxed subset
score. If the value of this fraction was greater than the relaxed subset score (i.e.
 = 0.9), the set would have been included in column 3 instead. The last column
mentions how many of the total UL instances were we able to explain using UA
and the outliers. For example, the concept covering #1 of Table 6 is the Educational Institution example described before. It shows how educational institutions
from DBpedia can be explained by schools, colleges and universities in GeoNames.
Column 4, 5 and 6 explain the alignment score RU (0.98), the size UA (396) and
the size of UL (404). Outliers (S.BLDG, S.EST, S.LIBR, S.MUS, S.HSP) along
with their P
fractions appear in column 7. Thus, 403 of the total 404 instances
were identified as either part of the covering or the outliers (see column 8). The
remaining instance did not have a geonames:featureCode property asserted.
?

?

?
In some of the concept coverings discovered, the alignments found were intuitive because of an underlying hierarchical nature of the concepts involved, especially in case of alignments of administrative divisions in geospatial sources and
alignments in the biological classification taxonomy. For example, #3 highlights
alignments that reflect the containment properties of administrative divisions.
Other interesting types of alignment were also found. For example #7 tries to
map two non-similar concepts. It explains the license plate codes found in the
state (bundesland) of Saarland. For space, we explain the other concept coverings inside Tables 6, 7 and 8. The complete set of alignments discovered by our
algorithm is available online.3

Outliers. In alignments, we also found inconsistencies, identified by three main
reasons: (i)Incorrect instance alignments - outliers arising out of possible erroneous equivalence link between instances (e.g., in #4, a hill is linked to an
airport, etc.), (ii) Incorrect values for properties - outliers arising out of possible erroneous assertion for a property (e.g. #5, #6, Flags of countries appear
as values for the country property). In the tables, we also mention the classes
that these inconsistencies belong to along with their support. We are unable to
detect correct alignments if there is insufficient support for coverage due to missing links between instances or missing instances (e.g. in #9 we find a complete
coverage with all instances, but it is incomplete  the state of New Jersey has
21 counties).

http://www.isi.edu/integration/data/UnionAlignments
?

?

?
4.2 Evaluation

We present an evaluation of a random set of 642 discovered alignments across
the tested source pairs to describe the precision of our approach. We checked the
correctness of each of the 642 alignments manually, after verifying the completeness of concept coverings on websites with the relevant information. Precision
was calculated as the ratio of alignments marked correct to the size of the random set. Establishing recall is difficult as finding the ground truth of all possible
concept coverings is infeasible due to the large size of the sources and the combinatorial nature of the disjuntions. We do, however, provide an evaluation of
the country alignments found in terms of precision and recall as an example.

Linking GeoNames with places in DBpedia: As shown in Table 2, out of
the 752 (i.e. 434 + 328) alignments found between GeoNames and DBpedia, we
evaluated 236 (i.e. 185 + 51) alignments. 152 (i.e. 127 + 25) of them were found
to be correct after resolving redirects (synonyms in DBpedia), giving a precision
of 64.40%, while 84 alignments were found to be incorrect. These 84 alignments
were found to suffer common patterns of error. There are 40 alignments that
had incorrect assertions of their properties. For example, in many instances in
DBpedia, the county property assertion was misspelled as country (especially
for places in UK & Ukraine), or the .svg file of the flag of a country appeared
dbpedia:country value. The corresponding alignments, which we counted as in-
correct, could have been properly detected if the data was cleaner. We detected
only partial alignments for 14 others, where the smaller concepts left out were
incorrectly classified as outliers due to insufficient support (R < 0.9). There
were 7 partial alignments that were incorrectly detected as complete (R > 0.9),
similar to the New Jersey example mentioned earlier. Another 14 alignments
suffered from a mismatch to a parent, because of insufficient links/instances.
The remaining 9 alignments had an assortment of problems in the values of
properties. For example, regions inside a country (Andean Region of Colombia)
appeared as value for the country property (Colombia).

Precision, recall and f-measure of Country Alignments: Since manually establishing ground truth for all possible concept coverings in the four sources is infea-
sible, we decided to find the precision and recall of only the country alignments
we found, as an illustration. These are alignments having a common pattern,
aligning a restriction class with a dbpedia:country property with other restriction classes featuring geonames:countryCode property or vice-versa. A ground
truth was established by manually checking what possible country alignments
were present in the two sources. Even then, establishing the ground truth needed
some insight. For example, Scotland, England, Wales, Northern Ireland & the
United Kingdom are all marked as countries in DBpedia, while in GeoNames,
the only corresponding country is the United Kingdom. In cases like these, we
decided to relax the evaluation constraint of having and alignment with a country from either of these, as correct. Another similar difficulty was in cases where
militarily occupied territories were marked as countries (e.g. Golan Heights occupied by Israel is marked as dbpedia:country).

R. Parundekar, C.A. Knoblock, and J.L. Ambite

Table 2. Linking GeoNames with places in DBpedia

Description of Pattern Observed

Total # Alignments

# Alignments Evaluated

Correct

(after resolving redirects)

Unidentified due to mislabelling
the Country property as County

Unidentified due to .svg file

of the flag as value for the country

Partially found with remaining as outliers

Partially found without outliers
Misaligned with a parent concept

Other problems

Alignments w/ larger Alignments w/ larger
class from GeoNames class from DBpedia
?

?

?
Out of the 63 country alignments detected, 26 were correct. 27 other alignments
had a .svg file appearing as value of the country property in DBpedia. We would
have detected such concept coverings, had such assertions for the country property been correct. Since this is a problem with the data and not our algorithm, we
consider these 27 as correct for this particular evaluation. We thus get a precision
of 84.13% ((26+27) out of 63). The two sources contained around 169 possible
country alignments between them, including countries with a .svg value for the
country property. There were many alignments in the ground truth that were not
found because the system did not have enough support (R < 0.9) to pass our
threshold. Accordingly, the recall was 31.36% and the F-measure was 45.69%.

Linking LinkedGeoData with places in DBpedia: We evaluated 200 alignments found between LinkedGeoData and DBpedia, out of which 157 were found
to be correct, giving a precision of 78.2%. Common patterns of alignments include alignments of an area identified by its OpenGeoDb location id with its
name or license plate codes from DBpedia. We were not able to detect 14 alignments correctly, where there were multiple spellings for the same entity (e.g.
LinkedGeoData uses both Hof Oberfranken and Landkreis Hof Oberfranken
in its values for its linkedgeodata:is in property). Another 20 alignments evaluated were partial (e.g. out of the 88 counties in Ohio, the algorithm produced
a covering including only 54). There were some other errors as well (e.g. Places
with license plate code GR in DBpedia were aligned with instances having license
code GR, NOL & ZI in LinkedGeoData).

Linking Species from Geospecies with DBpedia: In aligning Geospecies
with DBpedia, out of the 178 alignments that we evaluated, we found 109 correct
alignments for a precision of 61.24%. For 25 of the results, due to the presence of
multiple names/lexical values for the same item (e.g. bothDecapoda@en and
dbpedia:Decapoda values exist for dbpedia:ordo property). In 28 of the evaluated
alignments, we were only able to find partial concept coverings, mostly because of
insufficient instances and property assertions. For 16 other alignments, however,
?

?

?
Table 3. Linking LinkedGeoData with DBpedia

Description of Pattern Observed

Alignments w/ larger Alignments w/ larger

class from LinkedGeoData class from DBpedia

Total # Alignments

# Alignments Evaluated

Correct

Unidentified due to multiple spellings

Partially found

Other
?

?

?
Table 4. Linking Geospecies with DBpedia

Description of Pattern Observed Alignments w/ larger Alignments w/ larger

class from Geospecies class from DBpedia

Total # Alignments

# Alignments Evaluated

Correct

Unidentified due to multiple spellings

Partially found

Other
?

?

?
there were some interesting reasons. In some cases, the biological classes were no
longer in use (Urticales, Homoptera, etc.). There were some alignments that we
were not able to guess correctly because the species were marked as belonging
to different classification systems. There were also a few mismatches to a class
at a different level in the hierarchy.

Linking Genes from GeneID with MGI : In the 28 alignments found between GeneID and MGI, 24 were found to be correct for a precision of 85.71%.
Most (20) of these were alignments linking a gene start position from MGI with
possible locations from GeneID. In theory, these are numeric distances in centimorgans and can actually be an infinite set. In the data however we find all
possible distances occurring as text. The other 4 alignments were partial because
of insufficient data.

Table 5. Linking GeneID with MGI

Description of Pattern Observed Alignments w/ larger Alignments w/ larger

class from GeneID

class from MGI

Total # Alignments

# Alignments Evaluated

Correct

Partially found
?

?

?
R. Parundekar, C.A. Knoblock, and J.L. Ambite

5 Related Work

Ontology alignment and schema matching have been a well explored area of research since the early days of ontologies [5,1] and received renewed interest in
recent years with the rise of the Semantic Web and Linked Data. In the Web of
Linked Data, even though most work done is on linking instances across different
sources, an increasing number of authors have looked into aligning the source
ontologies in the past couple of years. Jain et al. [8] describe the BLOOMS ap-
proach, which uses a central forest of concepts derived from topics in Wikipedia.
An update to this is the BLOOMS+ approach [9] that aligns Linked Open Data
ontologies with an upper-level ontology called Proton. BLOOMS is unable to
find alignments because of the single Feature class in GeoNames. BLOOMS+,
which uses contextual information, finds some alignments between GeoNames &
Proton (precision of 0.5%) and DBpedia & Proton (90%). Cruz et al. [2] describe
a dynamic ontology mapping approach called AgreementMaker that uses similarity measures along with a mediator ontology to find mappings using the labels
of the classes. From the subset and equivalent alignment between GeoNames (10
concepts) and DBpedia (257 concepts), AgreementMaker achieves a precision of
26% and a recall of 68%. In comparison, for GeoNames and DBpedia, we achieve
a precision of 64.4%. But this comparison does not reflect that we find concept
coverings in addition to one-to-one alignments, while the other systems only find
one-to-one alignments. The advantage of our approach over these is that our use
of restriction classes is able to find a large set of alignments in cases like aligning
GeoNames with DBpedia even in the presence of a rudimentary ontology. We
believe that since other approaches do not consider concept descriptions beyond
those in the original ontology (like concept coverings), they would not have been
able to find alignments like the Educational Institutions example (#1) by using
only the labels and the structure of the ontology.

Extensional techniques and concept coverings have also been studied in the
past [7]. V olker et al. [13] describe an approach, similar to our work, that uses
statistical methods for finding alignments. This work induces schemas for RDF
data sources by generating OWL-2 axioms using an intermediate associativity
table of instances and concepts (called transaction datasets) and mining associativity rules from it. The GLUE [3] system is a instance-based matching
algorithm, which first predicts the concept in the other source that instances
belong to using machine learning. GLUE then hypothesizes alignments based on
the probability distributions obtained from the classifications. Our approach, in
contrast, depends on the existing links (in Linked Open Data Cloud), and hence
reflects the nature of the source alignments in practice. CS R [12] is a similar
work to ours that tries to align a concept from one ontology to a union of concepts from the other ontology using the similarity of properties as features in
predicting the subsumption relationships. It differs from our approach in that it
uses a statistical machine learning approach for detection of subsets rather than
the extensional approach.
?

?

?
a
i
d
e
p
?

?

?
-
a
t
a

o
e

d
e
k
n
i

a
i
d
e
p
?

?

?
-
s
e
m
a

o
e

m
o
r
f

s
t
n
e
m
n
g
i
l
a

e
l
p
m
a
x

.

e
l

b
a

d
e
n
i
a
l
p
x

#

s
e
c
n
a
t
s
n

s
r
e
i
l
t
u

|
?

?

?
|

|
?

?

?
|

|

|
?

?

?
|

|

=

 U

}

v
{
=

p

r

#

a
i
d
e
p
?

?

?
n

i

s
n
o
i
t
u
t
i
t
s
n

l
a
n
o
i
t
a
c
u
d

e
k
a
m

s
e
m
a

o
e

n

i

s
e
i
t
i
s
r
e
v
i

n

d
n
a

s
e
g
e
l
l
o

,
s
l
o
o
h
c

,

n
o
i
t
c
e

n
i

d
e
b
i
r
c
s
e
d

s

)
r
e
l
l
a
m

s
(

s
e
m
a

o
e

-

)
r
e
g
r
a
l
(

a
i
d
e
p
?

?

?
,
)
?

?

?
/

(
?

?

?
.

,
)

/

(
?

?

?
.

)
?

?

?
/

(
?

?

?
.
?

?

?
,
)
?

?

?
/

(
?

?

?
.

,
)
?

?

?
/

(
?

?

?
.
?

?

?
.
?

?

?
)
?

?

?
/

(
?

?

?
.

.
k
n

i
l

s
u
o
e
n
o
r
r
e

n
a

,
y
l
a
t

s
a

y
r
t
n
u
o
c

s
t
i

s
a
h

r
e
i
l
t
u
o

y
l

n
o

e
h
}
?

?

?
{
=

.
s
e
c
r
u
o
s

e
d
o

y
r
t
n
u
o
c
:
s
e
m
a
n
o
e
g

n
i
a
p

:
a
i
d
e
p
b
d
=

y
r
t
n
u
o
c
:
a
i
d
e
p
b
d

h
t
o
b

n

i

l
a
u
q
e

e
r
a

n
i
a
p

y
r
t
n
u
o
c

e
h
t

r
o
f

s
t
p
e
c
n
o
c

e
h

=

}
?

?

?
.

e
d
o

e
r
u
t
a
e
f
:
s
e
m
a
n
o
e
g

,
?

?

?
.

,
?

?

?
.

{

n
o
i
t
u
t
i
t
s
n

l

a
n
o
i
t
a
c
u
d

:
a
i
d
e
p
b
d

=
e
p
y
t
:
f
d
r

.
s
l
e
v
e
l

t
n
e
r
e
ff
d

i

o
w
t

t
a

s
t
i

n
u

e
v
i
t
a
r
t
s
i

i

n
m
d
a

n
e
e
w
t
e
b

s
t
n
e
m
n
g
i
l
a

h
t
i
w
s
n
o
i
s
i
v
i

d

e
v
i
t
a
r
t
s
i

i

n
m
d
a

f
o

e
r
u
t
a
n

l
a
c
i
h
c
r
a
r
e
i
h

e
h
t

m
r
fi
n
o
c

e
?

?

?
.

=

,
?

?

?
:
s
e
m
a
n
o
e
g
{
?

?

?
t
n
e
r
a
p
:
s
e
m
a
n
o
e
g

,
?

?

?
:
s
e
m
a
n
o
e
g

}
?

?

?
:
s
e
m
a
n
o
e
g

e
i
d
n
a
m
r
o

-
e
s
s
a

:
a
i
d
e
p
b
d

=
n
o
i
g
e
r
:
a
i
d
e
p
b
d

)

/

(
?

?

?
.

,
)
?

?

?
/

(
?

?

?
.

)
?

?

?
/

(
?

?

?
.

,
)
?

?

?
/

(
?

?

?
.
?

?

?
,
)

/

(
?

?

?
.

,
)
?

?

?
/

(
?

?

?
.
?

?

?
.

.

=
}
?

?

?
e
d
o

e
r
u
t
a
e

f
:

s
e
m

a

n
{
o
e
g

.

,

t
r
o
p
r
i

:
a
i
d
e
p
b
d

=
e
p
y
t
:
f
d
r

.
t
r
o
p
p
u
s

e
c
n
a
t
s
n

i

h
g
u
o
n
e

t
o
n

s
a
w
e
r
e
h
t

,
r
e
v
e
w
o

.
t
r
o
p
r
i
a

n
a

n
a

n
e
e
b

e
v
a
h

d
l
u
o
h
s

d
l
e
fi
r
i
a

n
a

,
s
t
r
o
p
r
i
a

g
n
i
n
g
i
l
a

n

s
a

h
c
u
s

,
s
e
m
a
n

s
a
i
l
a

e
l

b
i
s
s
o
p

e
v
a
h

e
w
r
e
v
e
w
o

.

#

s
a

d
r
a
w
r
o
f
t
h
g
i
a
r
t
s

s
a

n
e
e
b

e
v
a
h

d
l
u
o
h
s

s
d
n
a
l
r
e
h
t
e

r
o
f

t
n
e
m
n
g
i
l

e
h

g
v
s
.
s
d
n
a

l
r
e
h
t
e

e
h
t

f
o

g
a

l

o
t

r
o
r
r
e

e
g
a
k
n

i
l

e
l

b
i
s
s
o
p

a

l
l
e
w
s
a

,
s
d
n
a

l
r
e
h
t
e

f
o
m
o
d
g
n
i

d
n
a

s
d
n
a

l
r
e
h
t
e

e
h

)
r
e
l
l
a
m

s
(

a
i
d
e
p
?

?

?
-

)
r
e
g
r
a
l
(

s
e
m
a

o
e
?

?

?
f
o
m
o
d
g
n

:
a
i

i

d
e
p
b
d
?

?

?
.

=

y
r
t
n
u
o
c
:
a
i
d
e
p
b
d
?

?

?
=

e
d
o

y
r
t
n
u
o
c
:
s
e
m
a
n
o
e
g

)

/

(

s
d
n
a
l
r
e
h
t
e

e
h
t

,
s
d
n
a
l
r
e
h
t
e

e
h

:
a
i

d
e
p
b
d
{

.

n
a
d
r
o

f
o

y
r
t
n
u
o
c

e
h
t

r
o
f

t
n
e
m
n
g
i
l
a

s
i

h
t

m
o
r
f

n
e
e
s

e
b

n
a
c

s
a

,
y
l
l
a
c
i
t
a
m
e
t
s
y
s

t
a
e
p
e
r

o
t

s

m
e
e
s

#
n
i

n
r
e
t
t
a
p

r
o
r
r
e

e
h

e
h
t

f
o

g
a
l

:
a
i

d
e
p
b
d

}
s
d
n
a
l
r
e
h
t
e

:
a
i

,
g
v
s
.
s
d
n
a
l
r
e
h
t
e

d
e
p
b
d
?

?

?
.

=

y
r
t
n
u
o
c
:
a
i
d
e
p
b
d
?

?

?
=

e
d
o

y
r
t
n
u
o
c
:
s
e
m
a
n
o
e
g

}
g
v
s
.

,

n
a
d
r
o

:
a
i

d
e
p
b
d
{

n
a
d
r
o

f
o

g
a
l

:
a
i

d
e
p
b
d

)
r
e
l
l
a
m

s
(

a
t
a

o
e

d
e
k
n
i

-

)
r
e
g
r
a
l
(

a
i
d
e
p
?

?

?
.

,
?

?

?
}
?

?

?
,

l

,
?

?

?
-
e
t
a

e
s
n
e
c
i
?

?

?
o
e
{

=
n
e
r
p
e

b
m
:
d
u
g
l

,
?

?

?
,
?

?

?
,
?

?

?
,
?

?

?
d
n
a

l
r
a
a

=
d
n
a

l
s
e
d
n
u
b
:
a
i
d
e
p
b
d

s
e
t
a
l
p

e
s
n
e
c
i
l
?

?

?
e
h
t

f
o

d
n
fi

e
w

,
e
s
a
c

s
i

h
t

n

.
s
e
i
t
r
e
p
o
r
p

t
n
e
r
e
ff
d

i

n
e
e
w
t
e
b

s
t
n
e
m
n
g
i
l
a

g
n
i
t
s
e
r
e
t
n
i

s
e
c
u
d
o
r
p

o
s
l
a
m
h
t
i
r
o
g
l
a

r
u

g
n
i
s
s
i

m
e
r
e
w
s
e
t
a
l

p

e
s
n
e
c
i
l

g
n

i

n
i
a
m
e
r

e
h
t

g
n

i
t
r
o
p
p
u
s

s
e
c
n
a
t
s
n

.
d
n
a
l
r
a
a

f
o

e
t
a
t
s

e
h
t

n
i

R. Parundekar, C.A. Knoblock, and J.L. Ambite

a
i
d
e
p
?

?

?
-
s
e
i
c
e
p
s
o
e

,
a
i
d
e
p
?

?

?
-
a
t
a

o
e

d
e
k
n
i

m
o
r
f

s
t
n
e
m
n
g
i
l
a

e
l
p
m
a
x

.

e
l

b
a

d
e
n
i
a
l
p
x

#

s
e
c
n
a
t
s
n

s
r
e
i
l
t
u

|
?

?

?
|

|
?

?

?
|

|

|
?

?

?
|

|

=

 U

}

v
{
=

p

r

#

a
t
a

o
e

d
e
k
n
i

m
o
r
f

s
l
o
o
h
c

d
n
a
?

?

?
s
e
p
y
t

h
t
i
w
d
e
n
i
a
l

p
x
e

e
b

n
a
c

a
i
d
e
p
?

?

?
n

i

s
l
o
o
h
c
?

?

?
d
e
s
o
p
m
o
c

s
i

y
e
s
r
e

w
e

f
o

e
t
a
t
s

e
h
t

t
a
h
t

s

m
i
a
l
c

y
l
t
c
e
r
r
o
c
n
i

g
n
i
r
e
v
o
c

t
p
e
c
n
o
c

s
i

h
t

,
s
k
n

i
l

e
c
n
a
t
s
n

i

g
n
i
s
s
i

m
o
t

e
u

)
r
e
l
l
a
m

s
(

a
i
d
e
p
?

?

?
-

)
r
e
g
r
a
l
(

a
t
a

o
e

d
e
k
n
i

.
?

?

?
s
a
h

t
i

y
l
l
a
u
t
c
a

e
l
i

h
w
s
e
i
t
n
u
o
c

f
o
?

?

?
.

}
?

?

?
:
d
g
l

,
l
o
o
h
c

:
d
g
l
{

=
e
p
y
t
:
f
d
r

l
o
o
h
c

:
a
i
d
e
p
b
d

=
e
p
y
t
:
f
d
r

,
n
o
t
g
n
i
l
r
u

,
c
i
t
n
a
l
t

{
?

?

?
.

=
e
m
a

n
o
i
s
i
v
i
d
b
u
s
:
a
i
d
e
p
b
d
?

?

?
=
a
h
p
l
a
?

?

?
s
i
n
g
:
d
g
l

}
c
i
a
s
s
a

,
n
o
s
d
u

,
y
a

e
p
a

,
h
t
o
m
n
o

,
n
o
d
r
e
t
n
u

,
n
a
e
c

,
y
e
s
r
e

w
e
?

?

?
)
?

?

?
/

(
e
c
a
l

:
a
i
d
e
p
b
d
?

?

?
.

r
e
v
i

:
a
i
d
e
p
b
d
{

}
m
a
e
r
t

:
a
i
d
e
p
b
d

=
e
p
y
t
:
f
d
r

y
a
w
r
e
t
a

:
d
g
l

=
e
p
y
t
:
f
d
r

a
i
d
e
p
?

?

?
m
o
r
f

s
r
e
v
i
r

d
n
a

s

m
a
e
r
t
s

f
o

n
o
i
n
u

e
h
t

o
t

l
a
u
q
e

s
i

a
t
a

o
e

d
e
k
n
i

n

i

s
y
a
w
r
e
t
a
?

?

?
.

)
e
n

i

d
u
t
s
e

(

e
l
t
r
u

a

s
a

d
e
fi
i
s
s
a
l
c

s
a
w
n
a
i
b
i
h
p
m
a

e
n
o

.
g
.
e

,
s
e
c
n
a
t
s
n
i

d
e
n
g
i
l
a
s
i

m
o
t

e
u
d

s
e
i
c
n
e
t
s
i
s
n
o
c
n

i

d
n
fi

o
s
l
a

e

s
n
a
i
b
i
h
p
m

l
l
a

e
r
a

a
i
n
o
i
h
p
o
n
m
y

&
a
t
a
d
u
a

,
a
r
u
n

s
e
m
a
n

r
e
d
r
o

e
h
t

h
t
i
w

s
e
i
c
e
p
s
o
e

m
o
r
f

s
e
i
c
e
p
?

?

?
)
r
e
l
l
a
m

s
(

s
e
i
c
e
p
s
o
e

-

)
r
e
g
r
a
l
(

a
i
d
e
p
?

?

?
)

/

(

s
e
n
i
d
u
t
s
e
?

?

?
.

=

e
m
a

r
e
d
r

s
a
h
:
s
e
i
c
e
p
s
o
e
g

,
a
t
a
d
u
a

}
a
i
n
o
i
h
p
o
n
m
y

,
a
r
u
n

{

n
a
i
b
i
h
p
m

:
a
i
d
e
p
b
d

=
e
p
y
t
:
f
d
r

)

/

(

s
e
n
i
d
u
t
s
e
?

?

?
.

=

e
m
a

}
r
a
e
t
d
a
r
d

u
s
a
a

h
:
{
s
e
i
c
e
p
s
o
e
g

r
e
d
n
a
m
a
l
a

:
a
i
d
e
p
b
d
=
e
p
y
t
:
f
d
r

)
r
e
l
l
a
m

s
(

a
i
d
e
p
?

?

?
-

)
r
e
g
r
a
l
(

s
e
i
c
e
p
s
o
e

r
e
d
n
a
m
a
l
a

a

s
i

t
i
r
p
l
u
c

e
h
t

t
a
h
t

d
n
fi

e
w

,
?

?

?
#

f
o

n
o
i
t
c
e
p
s
n

i

r
e
h
t
r
u
f

n
o
p
?

?

?
.
s
u
g
n
u
f

a

e
b

o
t

s
n
e
p
p
a
h

e
c
n
a
t
s
n
i

t
n
e
t
s
i
s
n
o
c
n
i

y
l
n
o

e
h

.
y
l
t
c
e
f
r
e
p

s
e
h
c
t
a
m

t
s
o
m
l
a

,
s
e
c
r
u
o
s

h
t
o
b
m
o
r
f

,
e
a
t
n
a
l

m
o
d
g
n

e
h
?

?

?
i

)

/

(
?

?

?
c

/
s
m
o
d
g
n
i
k
:
s
e
i
c
e
p
s
o
e
g
?

?

?
.

}
b
=

/
m
s
m
o
d
o
g
d
n
g
i

n
i
n
k
i
:
:
s
s
e
e
i
c
i
c
e
e
p
p
s
s
o
o
e
e
g
g
{

t
n
a
l

:
a
i
d
e
p
b
d
=
e
p
y
t
:
f
d
r
?

?

?
-
?

?

?
e
n
e

d
n
a
a
i
d
e
p
?

?

?
-
s
e
i
c
e
p
s
o
e

m
o
r
f

s
t
n
e
m
n
g
i
l
a

e
l
p
m
a
x

.

e
l

b
a

d
e
n
i
a
l
p
x

#

s
e
c
n
a
t
s
n

s
r
e
i
l
t
u

|
?

?

?
|

|
?

?

?
|

|

|
?

?

?
|

|

=

 U

}

v
{
=

p

r

#

.
e
r
o
v
i

n
r
a

d
n
a

a
r
o
v
i

n
r
a

h
t
o
b

h
t
i
w
d
e
n
g
i
l
a

e
r
a

s
e
i
c
e
p
s
o
e

m
o
r
f

s
e
r
o
v
i

n
r
a

-

n
e
e
s

e
b

o
s
l
a

n
a
c

s
e
u
l
a
v

t
c
e
j
b
o

e
h
t

n

i

s
e
i
c
n
e
t
s
i
s
n
o
c
n
?

?

?
.

n
a
e
l
c

t
o
n

s
i

t
n
e
m
n
g
i
l
a

e
h
t

,

n
e
@
a
t
p
o
r
i

h



l
a
r
e
t
i
l

e
h
t

g
n
i
e
b

y
t
r
e
p
o
r
p

e
h
t

f
o

s
e
u
l
a
v

o
t

e
u
d

,
y
l
e
t
a
n
u
t
r
o
f
n

.
s
t
a

f
o

r
e
d
r
o

e
h
t

o
t

g
n
o
l
e
b

y
l
t
c
e
r
r
o
c

a
r
e
t
p
o
r
i

h

r
e
d
r
o

h
t
i
w
s
e
i
c
e
p
s

t
a
h
t

t
c
e
t
e
d

n
a
c

e
?

?

?
,

n
}
e
t
@
a
a

r
:
e
a
t
i
p
o
r
i

d
e
p
b
d

=
o
d
r
o
:
a
i
d
e
p
b
d

h

{

=

e
m
a

r
e
d
r

s
a
h
:
s
e
i
c
e
p
s
o
e
g

a
r
e
t
p
o
r
i
h
?

?

?
.

,
a
r
o
v
i

}
e
r
o
v
i

=
o
d
r
o
:
a
i
d
e
p
b
d

n
r
a

:
a
i

d
e
p
b
d
{

n
r
a

:
a
i

d
e
p
b
d

a

t
j
/
s
r
e
d
r
o
:
s
e
i
c
e
p
s
o
e
g

=

r
e
d
r

n
i
:
s
e
i
c
e
p
s
o
e
g
?

?

?
)
?

?

?
/
?

?

?
(

e
n
e
?

?

?
.

=
}
e
e
p
n
y
e

g
o
b
d
u
s
u
:
e
f
s
d

r

{
o
i
b

=
e
p
y

b
u
s
:
f
d
r

o
i
b

o
d
u
e
s
p

)
r
e
l
l
a
m

s
(
?

?

?
-

)
r
e
g
r
a
l
(
?

?

?
e
n
e

.

n
g
i
l
a

s
e
n
e
g
o
d
u
e
s

r
o
f

s
e
s
s
a
l
c

e
h
?

?

?
.
s
e
n
e
g
o
d
u
e
s

d
n
a

s
e
n
e

,
s
t
n
e
m
g
e
s
?

?

?
,
s
r
e
t
s
u
l
c

x
e
l

p
m
o
c

f
o

d
e
s
o
p
m
o
c

s
i

e
m
o
n
e
g

)
e
s
u
o
m
e
s
u
o
h
(

s
u

l

u
c
s
u

s
u

e
h
?

?

?
)
?

?

?
/
?

?

?
(

g
n

i

d
o
c
-
n
i
e
t
o
r
p

)
?

?

?
/
?

?

?
(
n
w
o
n
k
n
u
?

?

?
)
?

?

?
/

(

r
e
h
t
o
?

?

?
.
g
n

i

h
t

e
m
a
s

e
h
t

e
t
o
n
e
d

o
t

d
e
s
u

e
r
a
?

?

?
.

d
n
a

e
n
e
g
o
d
u
e
}
s
o

d
u
e
s
p
{
=
e
p
y

b
u
s
:
f
d
r

o
i
b

o
d
u
e
s
p

s
e
u
l
a
v

e
h
t

s
a

t
n
e
d

i
v
e

o
s
l
a

e
r
a

s
e
i
c
n
e
t
s
i
s
n
o
c
n
?

?

?
=
e
p
y

b
u
s
:
f
d
r

o
i
b

e
n
e
g
o
d
u
e
s

,
e
n
e

,
t
n
e
m
g
e
?

?

?
)
r
e
l
l
a
m

s
(
?

?

?
e
n
e

-

)
r
e
g
r
a
l
(
?

?

?
}
e
n
e
g
o
d
u
e
s

,

n
o
i
g
e

/
r
e
t
s
u

x
e
l

l

=
e
p
y

b
u
s
:
f
d
r

o
i
b

p
m
o

{

=
n
o
x
a

x
:
f
d
r

o
i
b
?

?

?
:
n
o
x
a
t

.
g
n
i
r
t
s

y
t
p
m
e

n
a

s
i

e
u
l
a
v

e
h
t

e
r
e
h
w

,
?

?

?
#
n

i

d
n
a

d
e
n
g
i
l
a
s
i

m

s
i

h
t
i
w
s
t
r
a
t
s

t
a
h
t

e
n
e
g

a
?

?

?
#
n

i

.
g
.
e

,

n
e
e
s

o
s
l
a

e
r
a

s
e
i
c
n
e
t
s
i
s
n
o
c
n

.
x
fi
e
r
p

a

s
a

e
m
o
s
o
m
o
r
h
c

e
h
t

n
i
a
t
n
o
c
?

?

?
e
n
e

n

i

)
s
n
a
g
r
o
m

i
t
n
e
c

n

i

s
e
c
n
a
t
s
i

d
(

s
n
o
i
t
a
c
o
l

e
h
t

f
o

s
e
u
l
a
v

e
h
t

,

n
e
e
s

e
b

n
a
c

s
?

?

?
e
n
e

n

i

n
o
i
t
a
c
o
l

e
h
t

h
t
i
w
?

?

?
n

i

)
e
m
o
s
o
m
o
r
h
c

e
h
t

h
t
i
w
(

t
r
a
t
s

e
n
e
g

e
h
t

n
g
i
l
a

h
c
i

h
w

,
?

?

?
#
&
?

?

?
#
e
k
i
l

s
t
n
e
m
n
g
i
l
a

d
n
fi

e
?

?

?
)
?

?

?
/
?

?

?
(


?

?

?
.

)
?

?

?
/
?

?

?
(



)
?

?

?
/

(
?

?

?
.

=
n
o
i
t
a
c
o

}
.
.
.

}
.
.
.

,

c

.

l
:
d
,
i

e
{
n
e
g

=

t
r
a
t

e
m
o
n
e
g
:
i
g
m

,

c

.
?

?

?
,

c

.

=
n
o
i
t
a
c
o

,

c

.
?

?

?
,

l
:
d
i

e
n
{
e
g

,

c

.
?

?

?
,

c

.
?

?

?
=

t
r
a
t

e
m
o
n
e
g
:
i
g
m
?

?

?
R. Parundekar, C.A. Knoblock, and J.L. Ambite

6 Conclusions and Future Work

We described an approach to identifying concept coverings in Linked Data sources
from the Geospatial, Biological Classification and Genetics domains. By introducing the definition of restriction classes with the disjunction operator, we are
able to find alignments of union concepts from one source to larger concepts
from the other source. Our approach produces coverings where concepts at different levels in the ontologies of two sources can be mapped even when there
is no direct equivalence or only rudimentary ontologies exist. Our algorithm is
also able to find outliers that help identify erroneous links or inconsistencies in
the linked instances. Our results provide a deeper insight into the nature of the
alignments of Linked Data.

In future work we want to find more complete descriptions for the sources.
Our preliminary findings show that the results of this paper can be used to find
patterns in the properties. For example, the countryCode property in GeoNames
is closely associated with the country property in DBpedia, though their ranges
are not exactly equal. By mining rules from the generated alignments, we will
be closer to the interoperability vision of the Semantic Web. A second direction
of future work is to use the outliers to feed the corrections back to the sources,
particularly DBpedia, and to the RDF data quality watchdog group pedantic-
web.org. To achieve this satisfactorily, we not only need to point out the instances
that have errors, but suggest why those errors occurred, that is, whether it was
due to incorrect assertions or missing links.

Acknowledgements. This research is based upon work supported in part by
the National Science Foundation under award number IIS-1117913. The views
and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the official policies or endorsements, either
expressed or implied, of NSF or any person connected with them.
