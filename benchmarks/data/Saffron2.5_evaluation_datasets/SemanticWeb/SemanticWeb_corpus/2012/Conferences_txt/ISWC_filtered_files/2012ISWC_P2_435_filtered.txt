Very Large Scale OWL Reasoning
through Distributed Computation

Raghava Mutharaju

Kno.e.sis Center,

Wright State University, Dayton, Ohio

raghava@knoesis.org

Abstract. Due to recent developments in reasoning algorithms of the
various OWL profiles, the classification time for an ontology has come
down drastically. For all of the popular reasoners, in order to process
an ontology, an implicit assumption is that the ontology should fit in
primary memory. The memory requirements for a reasoner are already
quite high, and considering the ever increasing size of the data to be
processed and the goal of making reasoning Web scale, this assumption
becomes overly restrictive. In our work, we study several distributed
classification approaches for the description logic EL+ (a fragment of
OWL 2 EL profile). We present the lessons learned from each approach,
our current results, and plans for future work.

Introduction

Over the years, the efficiency of classification algorithms for the description logic
EL+has constantly improved [3,10,11], so much so that, ELK reasoner [11] can
classify SNOMED CT1, one of the largest biomedical ontologies in 5 seconds. But
the improvement has been only in runtime and not space. In a recent study on
the performance of reasoners [8], it was noted that, in tableau-based reasoners,
memory exhaustion is a known problem. So, in this scenario, performing inmemory computations on a single machine would be problematic for ontologies
larger than SNOMED CT.

The amount of available data is always on the rise. We would not be off the
mark in saying that there would be ontologies bigger than SNOMED CT very
soon. In fact, there is a biomedical ontology named LinkBase, which is thrice
the size of SNOMED CT [26,17]. There could be even more bigger ontologies,
especially, ontologies with large ABoxes. Even if we consider that the RAM
prices are cheap and that might solve the issue, in order to really perform OWL
reasoning at Web scale, the current infrastructure that the reasoners are based
on, is not sufficient. In this scenario, there is a good possibility of falling short
on both memory and computation power. This is where our work on distributed
OWL reasoning algorithms is expected to bridge the gap.

 Supervisor: Pascal Hitzler.
1 Can be obtained from http://ihtsdo.org

P. Cudr e-Mauroux et al. (Eds.): ISWC 2012, Part II, LNCS 7650, pp. 407414, 2012.
c Springer-Verlag Berlin Heidelberg 2012

R. Mutharaju

Normal Form

A1    An  B R1 If A1, . . . , An  S(X), A1     An  B  O, and B  S(X)

A  r.B
r.A  B
r  s
r  s  t

Completion Rule
then S(X) := S(X)  {B}
then R(r) := R(r)  {(X, B)}
then S(X) := S(X)  {B}
then R(s) := R(s)  {(X, Y )}
then R(t) := R(t)  {(X, Z)}

R2 If A  S(X), A  r.B  O, and (X, B)  R(r)
R3 If (X, Y )  R(r), A  S(Y ), r.A  B  O, and B  S(x)
R4 If (X, Y )  R(r), r  s  O, and (X, Y )  R(s)
R5 If (X, Y )  R(r), (Y, Z)  R(s), r  s  t  Os, (x, Z)  R(t)

Fig. 1. Normal forms and Completion rules in CEL

The remainder of the paper is organized as follows. Section 2 contains some
preliminaries. In Section 3, we mention the previous work and how our work
differs from it. In Section 4, we present our approaches that we have taken
towards tackling this problem and in Section 5, we present our preliminary results
followed by our planned work for the future.

2 Preliminaries
Concepts in description logic EL+are formed according to the grammer

C ::= A |  | C  D | r.C,

where A ranges over concept names, r over role names, and C, D over (possibly
complex) concepts. Ontology in EL+is a finite set of general concept inclusions
(GCIs) C  D and role inclusions (RIs) r1rn  r, where C, D are concepts,
n is a positive integer and r, r1, . . . , rn are role names. For the semantics of
EL+please refer [2].

Classification of an ontology is the computation of the complete subsumption
hierarchy between all concept names occurring in the ontology. Classification is
one of the standard reasoning tasks. Among others, CEL algorithm [4] performs
classification of an EL+ontology. It uses the completion rules in Figure 1. It
requires the ontology to be in normal form, where all the axioms should be in
one of the forms shown in the left part of Figure 1.

3 Related Work

In order to make reasoning Web scale, algorithms should be scalable. To that
extent, various parallel and distributed approaches for classification of OWL fragments and closure of RDFS have been explored. Harmelen et al. use MapReduce
?

?

?
and peer-to-peer network for large scale RDFS reasoning [18,24]. They extend
their work to OWL Horst fragment in [7]. Many of their optimization techniques
from their work on RDFS reasoning could not be carried over to OWL Horst due
to the increased complexity of rules in OWL Horst. As the expressivity increases,
the rules as well as the pre-conditions in the rules would be increasingly complex.
An embarrassingly parallel algorithm is used in [27] for computing the RDFS
closure. In [9], distributed hash tables were used for the computation of RDFS
closure. Soma et al. [23] investigate two partitioning approaches for parallel
inferencing in OWL Horst. In [25], backward chaining is used to scale up to a
billion triples in the OWL Horst fragment. Distributed reasoning of fuzzy OWL
Horst has also been investigated in [15].
Stuckenschmidt et al. have used resolution techniques in distributed settings
to achieve scalability of various OWL fragments such as ALC [20] and ALCHIQ
[21]. There have been attempts at achieving distributed reasoning on EL+profile
in [16] and [22], but they do not provide any experimental results. Distribution
of OWL EL ontologies over a peer-to-peer network and algorithms based on
distributed hash table have been attempted in [5], but they do not provide any
evaluation results.

There have also been some successful attempts at making use of the multiple
cores on a single machine in order to speed up the classification of ontologies.
Haarslev et al. have worked on parallel tbox classification [1] and parallel tableau
based description logic reasoner for ALC [28]. In [13], the authors parallelized
the non-deterministic choices inherent in tableau algorithms. Parallelization of
tableau algorithm, for SHIQ has also been attempted in [14], but they havent
provided any evaluation results. In [11], the authors use multi-threading and
consequence-based procedure to achieve highly optimized classification runtime.
The authors of [19] extend the approach in [11] to parallel ABox reasoning. They
were able to compute all ABox entailments for an ontology having 1 million individuals in 3 minutes. But, for this, they require an unreasonably high memory
of 60GB on an 8 core processor. With concurrent approaches, it would be possible to improve the efficiency of the classification algorithm, but it would not
be possible to achieve scalability. For Web scale reasoning and for very large on-
tologies, these approaches would suffer from the same memory constraints that
were highlighted in Section 1.
Compared to the above approaches, the authors of [6] take a different route.
They focus on using secondary memory for classification of ELH ontologies and
were able to classify SNOMED CT in 20 minutes and the RAM used for computations is only 32MB. But this approach lacks the parallelism demonstrated
in other approaches. Please note that many of the fragments mentioned here are
different from the one that we are interested in, which is EL+. But this section
highlights some of the existing scalable reasoning approaches. For reasonably
expressive OWL profiles, we wish to explore the distribution of axioms of the
ontology across the cluster and perform parallel computations. We explain our
approach further in the next section.

R. Mutharaju

4 Research Problem and Approaches

4.1 Research Problem

Our research problem can be broken down into the following two questions

1. What are the approaches for distributed reasoning of OWL reasoning algo-

rithms; specifically, for profiles EL+and higher?

2. Demonstrate the need and the validity of the approach for distributed rea-

soning on a real world use case.

4.2 Research Plan

Our research plan for the above two questions is as follows
Step 1. Start with a relatively less expressive description logic such as EL+.

Explore distributed reasoning approaches for this profile.
Step 2. Choose the distributed reasoning approach which is most appropriate and extend it to more expressive profiles such as EL++[2] and
SROELV n(,) [12]. Note that this step might not be a straightforward

extension of step 1. It might require additional optimizations and further
research.

Step 3. There is an ongoing work in our research center where the Semantic
Web Journal website2 is being upgraded to Drupal 7. The purpose of the
upgrade is to have access to the Semantic Web extensions of Drupal 7. If not
already present, we plan on developing an OWL reasoner module for Drupal
and integrate the distributed reasoning work into it. The Semantic Web
Journal website would be backed by an ontology and website content would
be annotated appropriately. The website has a constant flow of submissions
and by having a reasoner support, we plan on providing semantic search,
semantic browsing and semantic content creation. Apart from the journal
website content, the reasoner would also access appropriate datasets from
Linked Open Data (LOD) cloud. The number of submissions for the journal
website as well as the size of LOD cloud keep increasing. So we believe that
this would be a very good application to demonstrate the need of having a
distributed reasoner.

4.3 Approaches
All the approaches presented are for description logic EL+. Approaches can be
categorised into distributed memory and shared memory.

http://www.semantic-web-journal.net
?

?

?
4.3.1 Distributed Memory

MapReduce. Our first attempt was to use the popular distributed framework,
MapReduce, for computing classification of EL+ontologies [16]. We revised
the CEL algorithm [4] to suit the key-value format of the data required for
MapReduce. In the Map phase, preconditions of the rules are checked and
in the Reduce phase, conclusion of the rules are computed. Pros and cons of
this approach are given below.
Pros

 Parallelization can be achieved easily.
 Fault tolerance is handled by the framework.

Cons

 In each iteration, duplicates are generated. This makes termination de-

tection hard.

 MapReduce is not suitable if there are dependencies between the data
chunks. In CEL completion rules, some of the rules are interdependent.
 It is difficult to filter data in subsequent iterations. For example, ideally,
in the next iteration, the algorithm needs to run only on the newly generated data (compared to last iteration).

Distributed Queue. In MapReduce, nodes in the cluster cannot talk to each
other. Since there are dependencies among the data chunks, there would be
a need for the nodes to talk to each other. Due to this, we replaced map and
reduce methods with our custom methods which can talk to other nodes,
when required. We also replaced HDFS with a distributed key-value data
store. CEL algorithm implementation makes use of a queue mechanism [4]
to trigger rule execution. In distributed queue approach, the idea is to take
this queue implementation and spread the load across the cluster. Axioms
are distributed across the cluster and the queue implementation runs on
each node of the cluster. So each node acts as a stand-alone reasoner, which
talks to other nodes when required. This approach was not as efficient as we
expected it to be due to the following reasons.

 There was a lot of cross communication among the nodes.
 Large ontologies like SNOMED CT generate many R(r)s which makes
rule R3 in [4] very slow. This rule slows down the entire operation across
the cluster.

Distributed Completion Rules. Instead of distributing the axioms and the
queues randomly, we distributed the axioms based on their type. Based on
the normal form type [4], each axiom in an ontology can be placed under one
of the five types. Now, each node is dedicated to only one type of normal form
and runs an appropriate rule on the axioms. Compared to the distributed
queue approach, this approach has the advantage of isolating the slowest rule
and not letting it affect the processing of other rules. Furthermore, we have
split rule R3 into two rules, R3-1 and R3-2 as mentioned in [16]. These two
rules run in parallel on separate nodes of the cluster. In order to reduce the

R. Mutharaju

cross communication, we use fixpoint iteration instead of the queue algorithm
to process the completion rules. This makes termination detection harder,
because, we need to be able to detect that there is no new output across the
cluster. This approach was efficient compared to our previous approaches.
Some preliminary results are given in Section 5.

4.3.2 Shared Memory

Multi-threaded graph. Apart from the distributed approaches mentioned be-
fore, we have also tried shared memory approach. The idea here is to represent all the axioms as a graph and perform parallel traversals3. Concepts
are represented as nodes and the relationship between concepts as edge. Unlabelled edges represent subclass relation and labelled edges represent role
name. This work was done on Cray XMT4, a massively parallel supercomputer with shared memory architecture.

After representing axioms as graphs, classification would be reduced to
the problem of computing transitive closure in the graph with respect to the
subclass relation. Cray XMT compiler generates parallelizable version of the
code based on the hints that the programmer places in the code. Although
Cray XMT provides huge computing power, if there are data dependencies in
the code, it is difficult to parallelize that part of the code. We were unable to
parallelize the compute intensive parts of the code due to these dependencies.
Apart from this, issues like synchronization, deadlocks, hot spots need to be
handled by the programmer. Overall, Cray XMT has a steep learning curve
and resolving data dependencies is not straightforward. Due to this, the vast
computing power of the supercomputer could not be utilized properly.

5 Results and Future Work

Except distributed completion rule approach, none of the other approaches work
well on a large ontology like SNOMED CT. We were able to classify SNOMED
CT in approximately 50 minutes using a 5 node cluster with the distributed
completion rule approach. These are just preliminary results and they can be
improved in a variety of ways like making more nodes work on the slowest rule,
improving the termination algorithm etc. After further evaluation and optimiza-
tions, we plan to publish our results (with complete details) along with rest of
the approaches. Then, we would be moving on to Steps 2 and 3 mentioned in
section 4.2.

Acknowledgements. This work was supported by the National Science Foundation under award 1017225 III: Small: TROn - Tractable Reasoning with On-
tologies. Any opinions, ndings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily reect the views
of the National Science Foundation.
3 Internship work at Clark & Parsia LLC.

http://www.cray.com/products/XMT.aspx
?

?

