Linked Stream Data Processing Engines:

Facts and Figures

Danh Le-Phuoc1, Minh Dao-Tran2, Minh-Duc Pham3,

Peter Boncz3, Thomas Eiter2, and Michael Fink2

1 Digital Enterprise Research Institute, National University of Ireland, Galway

danh.lephuoc@deri.org

2 Institut f ur Informationssysteme, Technische Universit at Wien

{dao,eiter,fink}@kr.tuwien.ac.at
3 Centrum Wiskunde & Informatica, Amsterdam

{p.minh.duc,p.boncz}@cwi.nl

Abstract. Linked Stream Data, i.e., the RDF data model extended for representing stream data generated from sensors social network applications, is gaining
popularity. This has motivated considerable work on developing corresponding
data models associated with processing engines. However, current implemented
engines have not been thoroughly evaluated to assess their capabilities. For reasonable systematic evaluations, in this work we propose a novel, customizable
evaluation framework and a corresponding methodology for realistic data genera-
tion, system testing, and result analysis. Based on this evaluation environment, extensive experiments have been conducted in order to compare the state-of-the-art
LSD engines wrt. qualitative and quantitative properties, taking into account the
underlying principles of stream processing. Consequently, we provide a detailed
analysis of the experimental outcomes that reveal useful findings for improving
current and future engines.

1 Introduction

Linked Stream Data [18] (LSD), that is, the RDF data model extended for representing stream data generated from sensors and social network applications, is gaining
popularity with systems such as Semantic System S [8], Semantic Sensor Web [19]
and BOTTARI [10]. Several platforms have been proposed as processing engines for
LSD, including Streaming SPARQL [7], C-SPARQL [5], EP-SPARQL [2] on top of
ETALIS, SPARQLstream [9], and CQELS [14]. By extending SPARQL to allow continuous queries over LSD and Linked Data, these engines bring a whole new range of
interesting applications that integrate stream data with the Linked Data Cloud.

All above platforms except Streaming SPARQL provide publicly accessible imple-
mentations. As processing LSD has gained considerable interest, it is desirable to have
a comparative view of those implementations by evaluating them on common criteria

 This

research has been supported by Science Foundation Ireland under Grant
No. SFI/08/CE/I1380 (Lion-II), by Marie Curie action IRSES under Grant No. 24761
(Net2), by the Austrian Science Fund (FWF) project P20841, and by the European Commission under contract number FP720117287661 (GAMBAS) and FP72007257943 (LOD2).

P. Cudr e-Mauroux et al. (Eds.): ISWC 2012, Part II, LNCS 7650, pp. 300312, 2012.
c Springer-Verlag Berlin Heidelberg 2012
?

?

?
in an open benchmarking framework. Such a framework is also valuable in positioning new engines against existing ones, and in serving as an evaluation environment for
application developers to choose appropriate engines by judging them on criteria of
interest.

Unfortunately, no benchmarking systems for LSD processing exist. Close to one is
Linear Road Benchmark [4], which however is designed for relational stream processing systems and thus not suitable to evaluate graph-based queries on LSD processing
engines. Furthermore, [14] provides a simplistic comparison of CQELS, CSPARQL,
and ETALIS, which is based on a fixed dataset with a simple data schema, simple query
patterns, and just considers average query execution time as the single aspect to measure
the performance. With further experience and studies on theoretical/technical foundations of LSD processing engines [15], we observed that the following evaluation-related
characteristics of these engines are critically important.
 The difference in semantics has to be respected, as the engines introduce their own
languages based on SPARQL and similar features from Continuous Query Language [3];
 The execution mechanisms are also different. CSPARQL uses periodical execution,
i.e., the system is scheduled to execute periodically (time-driven) independent of the
arrival of data and its incoming rate. On the other hand, CQELS and ETALIS follow
the eager execution strategy, i.e., the execution is triggered as soon as data is fed to the
system (data-driven). Based on opposite philosophies, the two strategies have a large
impact on the difference of output results.
 For a single engine, any change in the running environment and experiment parameters can lead to different outputs for a single test.

All these characteristics make a meaningful comparison of stream engines a nontrivial
task. To address this problem, we propose methods and a framework to facilitate such
meaningful comparisons of LSD processing engines wrt. various aspects. Our major
contribution is a framework coming with several customizable tools for simulating realistic data, running engines, and analyzing the output. Exploiting this framework, we
carry out an extensive set of experiments on existing engines and report the findings.

The outline of the paper is as follows. Section 2 describes our evaluation framework,
including the data generator for customizable data from realistic scenarios. In Section 3
we present experimentation methodology and results, including design and strategies
(Section 3.1), as well as several testing aspects and outcomes ranging from functionality,
correctness, to performance (Sections 3.23.4). Section 4 reports general findings and
provides a discussion, while Section 5 considers related benchmarking systems. Finally,
Section 6 concludes the paper with an outlook to future work.

2 Evaluation Framework

Social networks (SNs) provide rich resources of interesting stream data, such as sequences of social discussions, photo uploading, etc. Viewed as highly-connected graphs
of users, SNs is an ideal evaluation scenario to create interesting test cases on graphbased data streams. Therefore, our evaluation environment provides a data generator to
realistically simulate such data of SNs. Next, the graph-based stream data schema and
the data generator are described.

D. Le-Phuoc et al.

2.1 Graph-Based Stream Data Schema

The data schema is illustrated by a data snapshot in Figure 1. This snapshot has two
layers for stream data and static data corresponding to what users continuously generate from their social network activities and user metadata including user profiles, social
network relationships, etc. The details of these two layers are described below.

hGwGAEAA

hGWZIsuG

ZsA

hG

usUG

SAYYGu

hG

/AY

sYGLJ

hG

GAZ ZI

usUG

AUGE

GAO

ZYAsYG ZI

'W^

WZ

uZYO

uA

uZYO

ZwwGY





WSZZ

uA

<Adu





dzEEAG





Fig. 1. Logical schema of the stream data in a social network

that is updated or arrives frequently, is shown in the bottom layer. It

Stream Data.
contains various sources of streaming data:
 GPS stream (Sgps): inspired by the use case in Live Social Semantics [1], we assume
that each user has a GPS tracking device to send updated information about her current
location to the SN frequently. This information contains latitude/longitude of the users
position, e.g., the location of an event where user is attending, and the sending time.
 Posts and comments stream (Spc): there is a huge stream of posts and comments
in the SN as users start or join discussions. Similar to the availability of the wall
for each Facebook user or the Tweet timeline for each Twitter, every user in our
generated SN has her own forum for writing posts. People who subscribe to this forum
(or follow the forum moderator as in Twitter) can read and reply to the posts and
comments created in the forum. Each forum is used as a channel for the posting stream
of a user. In this stream, we are particularly interested in the stream of likes (i.e.,
people who show their interest in a post), denoted by Spclike, the stream of tags (i.e., set
of words representing the content of the discussion), denoted by Stags.
 Photos stream (Sfo): Uploaded photos and their associated attributes provide interesting information for discovering user habits, friend relationships, etc. In this stream,
we focus on exploiting useful information from the stream of user tagged in a photo,
Sfotags, and the stream of likes per photo, denoted by Sfolike.
Static Data. Udata, that is not frequently changed or updated, is shown in the upper
layer. It contains user profile information (name, date of birth, relationship status, etc.),
the relationships between users and the channel where they write posts, comments.
?

?

?
2.2 Data Generator

To the best of our knowledge, there exists no stream data generator that can realistically simulate the stream data in SNs. We propose a novel data generator for
LSD, called Stream Social network data Generator (S2Gen). It generates data according to the schema in Section 2.1 in consideration of the continuous query semantics [3,7,5,9,2,14] and various realistic data distributions such as the skewed distributions of posts/comments. As window operators are primitive operators in a continuous
query, the correlations of simulated data have to affect on data windows over streams. To
meet this requirement, S2Gen uses the window sliding approach from the structurecorrelated social graph generator S3G2 [16]. As such, to generate the stream data, it
slides a window of users along all users in the social graph and creates social activities for each user (writing a post/comment, uploading photos, sending GPS tracking
information). For creating a particular stream data, e.g., Spc, S2Gen extracts all the
posts/comments created for all the users, then sorts them according to their timestamps,
and finally serializes these data to a file. A stream player is created in order to push the
stream data from this file into a streaming engine. Similarly, Sfo, Spclike, Sfolike, and
Sgps are created.

For static data, S2Gen generates the user profiles and the friendship information of
all the users in order to form the static data, i.e., Udata. The details of this step and how
to simulate the data correlations in the static data are the same as in S3G2. Note that all
the generated stream data is correlated with non-stream data, e.g., user tags in the photo
stream are correlated with friendship information. Various realistic situations are also
simulated while generating stream data, e.g., for GPS stream data, around a specific
time, the latitude and longtitude sent by those people attending the same event are close
to each other and that of the events location.

For flexibility, S2Gen offers a range of parameters. Some main parameters used in

following experiments are:
 Generating period: the period in which the social activities are generated, e.g., 10
days, one month, etc. By varying this parameter, one can create streams with different
sizes for testing scalability.
 Maximum number of posts/comments/photos for each user per week: each of these parameters can be adjusted in order to change the amount of data that arrives in a window
of time. It thus can increase/decrease the input rate (e.g., number of triples/seconds) as
the stream player pushes the data according to a window of time. Besides, it also varies
the total amount of generated streaming data for a fixed generating period.
 Correlation probabilities: there are various parameters for the data correlations between graph data and the graph structure, e.g., the probability that users will be connected if they are living in the same area. They can be customized to specify how data
is skewed according to each data attribute. The tested systems need to recognize these
correlation properties in order to optimize their query plan.

D. Le-Phuoc et al.

3 Experimentation

3.1 Evaluation Design and Strategies

The setup to evaluate an engine E with a stream query Q is as follows. Suppose that
Q requires as input a non-empty set of finite streams SQ = {S1, . . . , Sm}, (m  1),
and possibly static data. Let R = {r1, . . . , rm} be a set of rates (elements/sec) such
that each Si is fed into E at rate ri. We expect as the output a sequence of elements
O(E, Q, R) = o1, . . . , on, abbreviated by OE when Q and R are clear from the context.
Our evaluation design is general enough to capture different stream engines. For
examples, E can be CQELS, CSPARQL, or EP-SPARQL1 for testing LSD engines,
where the static data is a set of RDF graphs, each Si is a Link Data stream, i.e., a
sequence of RDF triples, and the output is a sequence of triples or SPARQL results.

Note that EP-SPARQL is just a wrapper of the Prolog-based stream engine ETALIS.
When testing EP-SPARQL, we observed that it suffered considerably heavy loads from
parsing big RDF graphs. Moreover, it does not support multi-threading to easily control
the rates of input streams. Recently, JTALIS has been developed as a Java wrapper for
ETALIS. It does not exhibit the above parsing problems, as it works with facts and
Prolog rules. Furthermore, using Java makes it very convenient to control input rates
via multi-threading. Hence we decided to evaluate JTALIS. Here the static data is a set
of ground facts, each Si as well as the output is a sequence of ground facts,2 and queries
can be formalized as sets of Prolog rules. We thus compare CQELS, CSPARQL, and
JTALIS with the following plan:3
 Functionality tests: we introduce a set of stream queries with increasing complexities
and check which can be successfully processed by each engine;
 Correctness tests: for the executable queries, we run the engines under the same
conditions to see whether they agree on the outputs. In case of big disagreement, we
introduce notions of mismatch to measure output comparability. When the engines are
incomparable, we thoroughly explain the reasons to our best understanding of them.
 Performance tests: for queries and settings where comparability applies, we execute
the engines under their maximum input rates and see how well they can handle the limit.
Performance is also tested wrt. different aspects scalability, namely the volume of static
data size and the number of simultaneous queries.

3.2 Functionality Tests

We use the SN scenario generated by the tool in Section 2.2. Here, the static data is
Udata, and the incoming streams are Sgps, Spc, Spclike, Sfo, Sfolike (cf. Section 2.1).
The data generator considers many parameters to produce plausible input data, but for
our experimental purpose, we are interested in the size (number of triples/facts) of the
static data and input streams, i.e., |Udata|, |Spc|, etc.
1 SPARQLstream implementation has not supported native RDF data (confirmed by the main

developer).

2 We normalize the outputs to compare sets of facts and SPARQL result sets.
3 All experiments are reproducible and recorded, details are available at

http://code.google.com/p/lsbench/
?

?

?
Table 1. Queries classification

Patterns covered


F J A E N U T



 



S NP NS






?

?

?
Engines
  
CQ CS JT
  
  
  
   Q11
   Q12

Q7
Q8
Q9
Q10

Q1
Q2
Q3
Q4
Q5
Q6

Patterns covered

 
F J A E N U T

 



 

S NP NS


  







Engines
  
CQ CS JT

2   

E 
  

2   

1   
?

?

?
F: filter J: join E: nested query N: negation T: top k U: union A: aggregation S: uses static data
NP : number of patterns, NS: number of streams, : syntax error, E: error, : return no answer, : not supported
CQ: CQELS, CS: C-SPARQL, JT: JTALIS

The engines are run on a Debian squeeze amd64 2x E5606 Intel Quad-Core Xeon
2.13GHz with 16GB RAM, running OpenJDK Runtime Environment (IcedTea6 1.8.10)
OpenJDK 64-Bit Server VM, and SWI-Prolog 5.10.1.

To evaluate the engines by query expressiveness, we propose 12 queries which cover
different desired features and aspects of stream queries. Table 1 reports the query patterns in detail and our observation of which queries can be successfully executed by
which engines. It shows that a number of desired features are not yet satisfactorily
covered. CQELS supports neither negation nor nested aggregation. CSPARQL reports
syntax errors on Q7(complicated numeric filter), Q8(negation), and encounters runtime
error on Q9(most complicated query). Regarding JTALIS, patterns in Q5-Q9 are theoretically supported, but the run produces no output. Also, there is no support for explicit
representations of aggregations that works with timing windows; with kind help from
the JTALIS team, we encoded the simple counting aggregation in Q10 by recursive
rules, but left out the more complicated aggregations in Q11, Q12.

3.3 Correctness Tests

Based on the supported functionalities from all engines, we first evaluate them on Q1
Q4 and Q10 with a static data of 1000 user profiles, and a stream of posts during one
month: |Udata| = 219825 and |Spc| = 102955, at two rates of 100 and 1000 input
elements per second,4 which are considered slow and fast, respectively. Then, we check
the agreement on the results between the engines by simply comparing whether they
return the same number of output elements. Here we adopt the soundness assumption,
i.e., every output produced by the engines are correct. The results are reported in the
Output size columns of Table 2. It turned out that:

(i) CSPARQL disagrees with the rest wrt. the total number of output triples. It returns
duplicates for simple queries while for complicated ones, it misses certain outputs;
(ii) CQELS and JTALIS agree on the total number of output triples on most of the

case, except for Q4.

4 Each element is a triple (resp., fact) for CQELS, CSPARQL (resp. JTALIS).

D. Le-Phuoc et al.

Explaining these mismatches, regarding (i), CSPARQL follows the periodical execution
strategy, i.e., the query processor is triggered periodically no matter how fast or slow
the inputs are streamed into the system. When the execution is faster than the update
rate, the engine will re-operate on the same input before the next update, hence outputs
replicated results between two updates. In contrast, when the execution is slower than
the update rate, certain inputs are ignored between two consecutive executions. Thus,
for complicated queries, one expects that CSPARQL misses certain outputs.

CQELS and JTALIS, on the other hand, follow the eager execution strategy, and trigger the computation incrementally as soon as new input arrives; or, in case data arrives
during computation, they queue the input and consume it once the engine finishes with
the previous input. Therefore, eager ones do not produce overlapping outputs.

For the differences between CQELS and JTALIS that lead to observation (ii), the execution speed and windows play an important role. For simple queries Q1Q3,5 the two
engines perform on more or less the same speed, hence the results are identical. For the
more complex query Q4, the inputs contained in the time-based windows determine the
outputs of each execution. The slower the execution rate, the less input in the windows,
as more of the input is already expired when the new input is processed. Consequently,
output for Q4 produced by JTALIS (which is slower) is smaller than that produced by
CQELS. To circumvent this problem, one can use triple-based windows. Unfortunately,
this type of windows is not yet supported by JTALIS.

The total number of outputs is not ideal to cross-compare the engines. We next propose a more fine-grained criterion by tolerating duplication and checking for mismatch.

Comparing by Mismatch. We now propose a function to compute the mismatch
mm(E1, E2, Q, R) between two output sequences OE1 = a1, . . . , an1 and OE2 = b1,
. . . , bn2 produced by engines E1, E2 running query Q at rates R, i.e., we would like to
see how much of the output from E1 is not produced by E2.

An output-partition of OE1 is defined as a sequence of blocks A1, . . . , Am where
each Ai is a subsequence ai1, . . . , aii of OE1 such that the sequence a11, . . . , a11 , a21,
. . . , a22, . . . , am1, . . . , amm is exactly OE1. There are multiple output-partitions of
OE1, but we are only interested in two special ones:
 the periodical output-partition: each Ai is corresponding to the result of an execution
of E1, when E1 is a periodical engine;
 the eager output-partition: Ai = ai, i.e., a sequence of a single output element.
A slice of OE2 from j, 1  j  n is the sequence OE2
j iff for every aik  Ai, there exists bt  OE2
is covered by a slice OE2
aik = bt. In case of non-coverage, the maximal remainder of Ai wrt. OE2
i = Ai  OE2
by P j
there exists bt  OE2
elements in Ai that also appear in OE2
of Ai which is covered by OE2

j = bj, . . . , bn2. A block Ai
j such that
j is defined
j = air1, . . . , airki such that for 1  s  ki, airs  Ai and
j such that airs = bt. Intuitively, P j
i is constructed by keeping
j; in other words, P j
i is the maximal sub-block

j.

5 Reason for identical total number of output tuples for Q10 will be made clear in explaining the

mismatch.
?

?

?
Table 2. Output Mismatch, |Udata| = 219825, |Spc| = 102955

Rate: 100 (input elements/sec)

Rate: 1000 (input elements/sec)

Output size

Mismatch (%)

Output size

Mismatch (%)

CQCS CQJT

CSJT

CQCS CQJT

CSJT
?

?

?
Q CQ
?

?

?
68 1.47 0.00 0.00 0.00 0.00 1.47
68 1.47 0.00 0.00 0.00 0.00 1.47
533 0.00 0.00 0.00 0.00 0.00 0.00

68 1.47 0.00 0.00 0.00 0.00 1.47
68 1.47 0.00 0.00 0.00 0.00 1.47
533 0.00 0.00 0.00 0.00 0.00 0.00

4 11948 125910 1442 1.69 1.10 87.93 0.00 78.91 0.07 11945 127026 4462 1.54 1.12 62.65 0.00 52.79 0.02
10 28021 205986 28021 14.96 0.04 87.66 0.00 44.67 0.00 28021 209916 28021 14.70 0.04 86.30 0.00 43.25 0.00
?

?

?
Table 3. (Comparable) Maximum Execution Throughput

C-SPARQL
JTALIS

Q1 Q2 Q3 Q4 Q5 Q6 Q10
24122 8462 9828 1304 7459 3491 2326
10 1.72 1.71

99   87

10 1.68 1.63
3790 3857 1062

If P j

i is an empty sequence, we define match(P j

i ) = j. Otherwise, for each element
i , let match(airs ) be the smallest index t, where j  t  n2, such that

airs  P j
airs = bt, and let match(P j

i ) = min{match(airs ) | airs  P j

i

}.

We now can define the maximal remainder sequence of OE1 that is covered by OE2
= Ai 
as T1, . . . , Tm, where T1 = P 1
match(Ti1) for 1 < i  m. Intuitively, we progressively compute the maximal
OE2
1 from the beginning of OE2.
remainder of each block, starting with the slice OE2
When finishing with one block, we move on to the next one and shift the slice to the
minimal match of the last block.

1 and Ti = P match(Ti1)

1 = A1  OE2

i

i=1(|Ai|  |Ti|)
m
m
i=1

|Ai|

 100%

The mismatch is mm(E1, E2, Q, R) =
Table 2 reports the mismatches between the engines on Q1-Q4 and Q10. For a column labeled with E1E2 where E1 = E2  {CQ, CS, JT}, the left sub-column
presents mm(E1, E2, Q, R) and the right one shows mm(E2, E1, Q, R), respectively.
For simple queries Q1-Q3, the mismatches are very small, meaning that CSPARQL
computes many duplicates but almost of all its output is covered by CQELS and JTALIS.
When the query complexity increases in Q4 and Q10, CSPARQL misses more answers of CQELS as mm(CQ, CS, Q4, 100) = 1.69% and mm(CQ, CS, Q10, 100) =
14.96%. On the other hand, JTALIS produces far less output than the other two for Q4,
due to the reasons in explanation (ii) above. The big mismatches here (from 52.79% to
87.93%) result from the different execution speeds of JTALIS and the other engines.

Interestingly, CQELS and JTALIS output the same number of tuples for Q10, but the
contents are very different: mm(CQ, JT, Q10, 100) = 87.66% and mm(CQ, JT, Q10,
1000) = 86.30%. This is because Q10 is a simple aggregation, which gives one answer
for every input; hence we have the same number of output tuples between CQELS
and JTALIS, which follow the same execution strategy (eager). However, again the
difference in execution speed causes the mismatch in the output contents. For all queries

D. Le-Phuoc et al.

that JTALIS can run, mm(JT, CQ, Q, R) = 0 where Q  {Q1, . . . , Q4, Q10} and
R  {100, 1000}, meaning that the output of JTALIS is covered by the one of CQELS.
In concluding this section, we formalize the notion of comparability by mismatch as
follows. Given a set of engines E, a query Q, and rates R, the engines in E are said to
be comparable with a prespecified mismatch tolerance 	, denoted by comp(E, Q, R, 	),
iff for every E1, E2  E, it holds that mm(E1, E2, Q, R)  	.

3.4 Performance Tests

This section defines execution throughput, and then reports on this measure when the
engines run on a basic setting as well as when different input aspects scale.

Execution Throughput. Besides comparability, one also would like to see how fast
the engines are in general. We therefore conduct experiments to compare the engines
wrt. performance. The most intuitive measure to show the performance of a stream
processing engine is throughput, which is normally defined as the average number
of input elements that a system can process in a unit of time. However, as mentioned
above, systems like C-SPARQL using the periodical execution mechanism can skip
data coming in at high stream rate. Therefore, a maximum streaming rate is not appropriate to measure throughput. Moreover, as shown in previous sections, there are
several reasons that make the output from such engines incomparable. We thus propose
comparable maximum execution throughput as a measure for the performance test.
According to this measure, we first need to make sure that the engines produce
comparable outputs at some (slow) rate, e.g., comp({CQELS , CSPARQL, JTALIS},
Q2, 100, 0.147) holds. When this is settled, we modify all periodical engines such that
the input and execution rates are synchronized. Interestingly, confirmed by our tests
on all executable queries on C-SPARQL, there are only marginal changes in execution rates when varying input rates. In this particular evaluation, thanks to the APIs
from CSPARQL that notify when an execution finishes, we can achieve this modification by immediately streaming new inputs after receiving such notifications. Note that
CSPARQL schedules the next execution right after the current one unless explicitly
specified.
Then, given the size NE of the input streamed into an engine E  E and the total
running time TE that E needs to be processed by a query Q, assume that E immediately
reads the new input once finishing with the previous one, the number of executions is
NE as E does one computation per input. When comp(E, Q, R, 	) holds, the comparable maximum execution throughput of E is defined as cmet (E,E, Q, R, 	) = NE/TE.
Table 3 reports the value of cmet for Q1Q6 and Q10. For Qi, the comparable
test is fulfilled by comp({CQELS , CSPARQL, JTALIS}, Qi, 100, 	i) where 	i =
max{mm(E1, E2, Qi, 100) | E1 = E2  {CQ, CS, JT}} with the mismatch values
taken from Table 2. It shows that CQELS and JTALIS have higher cmet than CSPARQL
by some orders of magnitude.

Scalability Tests. Next, we investigate how the systems behave wrt. to two aspects
of scalability, namely (i) static data size, and (ii) the number of queries. Regarding the
former, we gradually increment the static data by generating test cases with increasing
?

?

?
 100000

 0.1

l

e
a
c
s
g
o
l
 
-
 
)
c
e
s
/
e
x
e
(
 
t
e
m
c

Q2

C-SPARQL
JTALIS

 100000

 0.1

l

e
a
c
s
g
o
l
 
-
 
)
c
e
s
/
e
x
e
(
 
t
e
m
c

Q3

C-SPARQL
JTALIS

 100000

 0.1

l

e
a
c
s
g
o
l
 
-
 
)
c
e
s
/
e
x
e
(
 
t
e
m
c

C-SPARQL

Q5

 0.01

1k/103k

2k/246k

4k/526k

8k/1.1M 16k/2.2M 

Number of users/triples

 0.01

1k/103k

2k/246k

4k/526k

8k/1.1M 16k/2.2M 

Number of users/triples

 0.01

1k/103k

2k/246k

4k/526k

8k/1.1M 16k/2.2M 

Number of users/triples

Fig. 2. Comparable max. execution throughput for varying size of static data

l

e
a
c
s
g
o

l
 
-
 
)
c
e
s
/

e
x
e
(
 
t

e
m
c

 0.1

 0.01

 0.001

C-SPARQL
JTALIS

Q3

128 256 512

l

e
a
c
s
g
o

l
 
-
 
)
c
e
s
/

e
x
e
(
 
t

e
m
c

 0.1

 0.01

C-SPARQL
JTALIS
?

?

?
Number of queries

Q4

128 256 512

l

e
a
c
s
g
o

l
 
-
 
)
c
e
s
/

e
x
e
(
 
t

e
m
c

 0.1

 0.01

C-SPARQL
?

?

?
Number of queries
?

?

?
Number of queries

Q10

128 256 512

Fig. 3. Comparable max. execution throughput running multiple query instances

number of users. For the latter, we can use the same query template but augmenting the
number of registered query instances. From queries generated from the same pattern,
we expect better scalability effects on the engines that support multiple query optimization [12]. Figures 2 and 3 report cmet when running the engines on those settings. As
seen in Figure 2, C-SPARQLs cmet dramatically decreases when the size of static data
increases; JTALIS performs better than C-SPARQL. On the other hand, CQELSs performance only slightly degrades for complicated queries like Q5. In Figure 3, CQELS
still outperforms to C-SPARQL and JTALIS but the speed of throughput deterioration
is still linear like those of the counterparts. Here, the performance gains mainly come
from the performance of single queries.

4 Findings and Discussion

As most of the considered systems are scientific prototypes and work in progress, unsurprisingly they do not support all features and query patterns. Moreover, the comparability tests in the Section 3.2 clearly exhibit that even for queries with almost identical
meaning resp. semantics, the outputs sometimes are significantly different due to the
differences in implementation. The differences in outputs are also contributed by intrinsic technical issues of handling streaming data , e.g., time management, potentially
fluctuate execution environment [12,15]. Therefore, any reasonable cross-system comparison must take comparability criteria akin to those we considered into account. In
addition, comparability tests with tolerant criteria are useful for testing the correctness
of a query engine if there is an equivalent baseline system, i.e, given that the baseline
system has standard features that a new system should conform to.

The performance and scalability tests show that throughout C-SPARQL yields considerably lower throughput compared to JTALIS and CQELS. This provides further

D. Le-Phuoc et al.

evidence for the argument that the recurrent execution may waste significant computing resources. Recurrent results can be useful for some applications, such as answering
return the last 10 tweets of someone. However, re-execution is unnecessary unless
there are updates between consecutive executions, and thus incremental computing is
mainly recommended in the literature [12,15]. There, outputs are incrementally computed as a stream, and recurrences can be extracted by a sliding window. In this fashion,
the output of eager incremental computing as by CQELS and JTALIS can be used to
answer recurrent queries. As the incremental execution by CQELS and JTALIS outperforms the periodic computation of C-SPARQL by an order of magnitude, the conjecture
is that they would also answer recurrent queries as described above more efficiently.

Comparing CQELS and JTALIS, the former performs better mainly because it uses
a native and adaptive approach (cf. [14]). Note that the performance of C-SPARQL
and JTALIS heavily depends on underlying systems, viz. a relational stream processing engine and Prolog engine, respectively. Their performance can thus benefit from
optimizations of the (or use of the) underlying engines. Similarly, CQELS would profit
from more sophisticated, optimized algorithms compared to the current one [14].

The scalability tests show that all engines have not yet been optimized for scalable
processing. Apparently, C-SPARQL already exhibits performance problems on simple
queries involving static data beyond 1 million triples. JTALIS, while capable of handling these settings, struggles on more complicated queries with static datasets larger
than 1 million triples. CQELS is the only system that precomputes and indexes intermediate results from sub-queries over the static data [14], and therefore scales well with
increasing static data size. Clearly, this technique is not restricted to CQELS and applicable to C-SPARQL, JTALIS, and any other LSD engine to improve on scalability
wrt. static data. However, CQELS does also not scale well when increasing the number
of queries (sharing similar patterns and data windows). The same holds for C-SPARQL
and JTALIS, which clearly testifies that none of the systems employ multiple query
optimization techniques [12,15] , e.g., to avoid redundant computations among queries
sharing partial computing blocks and memory.

5 Related Work

We already mentioned characteristics of LSD engines that are critically relevant to this
paper in Section 1. In-depth theoretical/technical foundations of LSD processing can be
found in [15]. We next briefly review existing related benchmarking systems.

Linear Road Benchmark [4] is the only published benchmarking system for relational
data stream processing engines so far. However, it focuses on ad-hoc scenarios to evaluate outputs. On the other hand, our work treats the processing engines and queries
as black boxes. Furthermore, while Linear Road Benchmark only focuses on a single
quantitative metric scale factor, our evaluation studies several aspects of the systems
as shown above. Concerning stream data generator, NEXMark6 can be used to test relational data stream systems. However, not only is its data schema quite simple but the
simulated data is unrealistically random.

6 http://datalab.cs.pdx.edu/niagara/NEXMark/
?

?

?
Triple Storage Benchmarks. With increasing availability of RDF triple stores,7 a number of RDF benchmarks have been developed to evaluate the performance of these
systems.8 However, most of the popular benchmarks such as BSBM [6], LUBM [13]
and SP2Bench [17] are either limited in representing real datasets or mostly relationallike [11]. On top of that, none of them focus on time-varying data and continuous query.
By extending recently developed Social Network Interlligence Benchmark (SIB)9, our
evaluation framework is natively designed not only to support continuous queries over
LSD but also to simulate realistic graph-based stream data.

6 Conclusion

In this work, we propose the firstto the best of our knowledgecustomizable framework with a toolset for cross-evaluating Linked Stream Data (LSD) engines. Along with
the framework we developed a methodology and measures to deal with conceptual and
technical differences of LSD engine implementations. Powered by this environment,
another main contribution in this paper is a systematic and extensive experimental anal-
ysis, revealing interesting functional facts and quantitative results for state-of-the-art
LSD engines (see Section 3). Our findings from this analysis identify performance shortcomings of these engines that need to be addressed in further developments of these, but
also by future LSD processing engines.

It is often the case that linked stream data engines are rated negatively when compared with relational stream engines. Therefore, for further work, we will provide a
baseline test set [4] with corresponding relational data schema [6] to compare LSD engines with relational ones. On top that, we plan to extend the evaluation framework in
order to support LSD engines that enable continuous approximation queries and feature
load shedding [12]. Also, distributed LSD engines are expected to be in place soon, and
evaluating them is another challenging and interesting topic of research to pursue.
