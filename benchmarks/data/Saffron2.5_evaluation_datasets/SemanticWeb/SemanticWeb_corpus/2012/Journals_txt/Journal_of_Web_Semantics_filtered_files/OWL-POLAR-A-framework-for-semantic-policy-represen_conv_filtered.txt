Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

OWL-POLAR: A framework for semantic policy representation and reasoning
Murat Sensoy a,

, Timothy J. Norman a, Wamberto W. Vasconcelos a, Katia Sycara b

a Department of Computing Science, University of Aberdeen, AB24 3UE Aberdeen, UK
b Carnegie Mellon University, Robotics Institute, Pittsburgh, PA 15213, USA

a r t i c l e

i n f o

a b s t r a c t

Article history:
Available online 27 November 2011

Keywords:
Semantic Web
Policies
Norms
Conflict resolution
Multi-agent systems

In a distributed system, the actions of one component may lead to severe failures in the system as a
whole. To govern such systems, constraints are placed on the behaviour of components to avoid such
undesirable actions. Policies or norms are declarations of soft constraints regulating what is prohibited,
permitted or obliged within a distributed system. These constraints provide systems-level means to mitigate against failures. A few machine-processable representations for policies have been proposed, but
they tend to be either limited in the types of policies that can be expressed or are limited by the complexity of associated reasoning mechanisms. In this paper, we present a language that sufficiently expresses
the types of policies essential in practical systems, and which enables both policy-governed decisionmaking and policy analysis within the bounds of decidability. We then propose an OWL-based representation of policies that meets these criteria and reasoning mechanisms that use a novel combination of
ontology consistency checking and query answering. The proposed policy representation and reasoning
mechanisms allow development of distributed agent-based systems that operate flexibly and effectively
in policy-constrained environments.

O 2011 Elsevier B.V. All rights reserved.

1. Introduction

Multi-agent systems are distributed systems whose components are intelligent software agents [37]. Each agent has a set of
goals, capabilities and resources. Using their capabilities and re-
sources, agents within a multi-agent system execute actions to
achieve their goals. Individual actions, however, may result in
undesirable consequences within the system.
It is, therefore,
important to regulate the actions of agents to reduce the risk of
these undesirable consequences [9].

Authorities may enforce policies to regulate actions of agents
within a specific context. Policies are soft constraints determining
in which situations a certain action is obliged, permitted, or prohibited [35]. Authoring polices for a specific domain requires the
ability to imagine all implications of policies within that domain.
This challenge may lead to policies that are inconsistent or incomplete and, possibly, policies that do not fully capture the intentions
of the authors. Furthermore, different authorities with different
goals may enforce different policies in the same context. For in-
stance, governments enforce policies to promote health and safety,
while companies create their own policies to promote productivity
and consumer satisfaction. Policies of a single authority may be as-

 Corresponding author. Tel.: +44 7522474621.

E-mail addresses: m.sensoy@abdn.ac.uk (M. Sensoy),

t.j.norman@abdn.ac.uk
(T.J. Norman), w.w.vasconcelos@abdn.ac.uk (W.W. Vasconcelos), katia@cs.cmu.edu
(K. Sycara).

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.11.005

sumed to be consistent, however policies of different authorities
may conflict in a specific context. With mechanisms to reason with
and about policies, one is able to anticipate contexts in which conflicts may arise; these mechanisms should enable the resolution of
conflicts at design time, thus providing guarantees to systems before they are executed. Without such reasoning mechanisms, it
would be difficult for policy authors to identify the contexts in
which their policies conflict. The identification and resolution of
policy conflicts are left to the agents operating in those contexts,
with potentially undesirable run-time effects.

In this paper, we present a novel and powerful OWL 2.0 [15]
knowledge representation and reasoning mechanism for policies:
OWL-POLAR (an acronym for OWL-based POlicy Language for
Agent Reasoning). Policies, which are also known as norms, are
system-level principles of ideal activity that are binding upon the
components of that system. Depending on the nature of the system
itself, policies may serve to control, regulate or simply guide the
activities of components. In systems security, for instance, the
aim is typically to control behaviour such that the system complies
with the policies [31]. In real socio-technical systems, however,
there are important limits to this and the aim is to develop effective sets of policies along with incentives to regulate behaviour
[2]. In systems of autonomous agents, the term norm is most
prevalent, but the concept and issues remain the same [7]; for
example, norms are used to regulate the behaviour of agents representing disparate interests in electronic institutions [13]. The
objective of this research is to fulfil the essential requirements of

policy representation, reasoning, and analysis. In meeting this
objective three key requirements must be met:

(1) System/institutional policies must be machine understand-

able and underpinned by a clear interpretation.

(2) The representation must be sufficiently expressive to cap-

ture the notion of a policy across domains.

(3) Policies must be able to be effectively shared/interpreted at

run-time.

The choice of OWL 2.0 as an underlying language addresses the
first requirement, but in meeting the second two, we must clearly
outline what is required of a policy language and what reasoning
should be supported by it. The desiderata of a model of policies
that motivates the language OWL-POLAR are as follows:

 Representational adequacy. Policies (or norms) must capture the
distinction between activities that are required (obliged),
restricted (prohibited) and, in some way, authorised but not
necessarily expected (permitted) by some representational
entity within the environment. It is essential to capture the
authority from which the policy/norm comes, the subject
(agent) to whom it applies, the object (activity) to which the
policy/norm refers, and the circumstances within which it
applies.

 Supporting decisions. Any reasoning mechanism that is driven/
guided by policies must support both the determination of what
policies/norms apply in a given situation, and what activities
are warranted by the normative state of the agent if it were to
comply with these policies.

 Supporting analysis. Any reasoning mechanism that is driven/
guided by normative/policy constraints must support
the
assessment of policies in terms of: (i) whether a policy/norm
is meaningful and (ii) whether norms conflict, and in what circumstances they do conflict.

We believe that this desideratum of a model of policies can be
met within the confines of OWL-DL. If this claim can be shown
to be valid (as we aim to do within this paper), we believe that
OWL-POLAR provides, for the first time, a sufficiently expressive
policy language for which the key reasoning mechanisms required
of such a language are decidable. These mechanisms allow intelligent software agents and policy authors to reason within context.
Inconsistent and unfounded policies are automatically identified.
Heterogeneous knowledge across different authority domains
could be better integrated by revealing the context in which the
policies they enforce may be in conflict. Furthermore, the proposed
mechanisms equip agents with the capability to resolve such
conflicts.

The paper is organised as follows: in Section 2 we formally
specify the OWL-POLAR language within OWL-DL; in Section 3
we describe how a set of active policies may be computed, and
how decisions about what activities are warranted by some set
of policies may be made; then in Section 4 we present in detail
the reasoning mechanisms that support the analysis of policies
and conflict detection between policies; in Section 5 we present
conflict resolution mechanisms for policies. Section 6 discusses
the computational complexity of the proposed reasoning mecha-
nisms. OWL-POLAR is then compared to existing languages for policies in Section 7, and we present our conclusions in Section 8.

2. Semantic representation of policies

The proposed language for semantic representation of policies
is based on OWL-DL [15]. An OWL-DL ontology o = (TBoxo, ABoxo)

consists of a set of axioms defining the classes and relations (TBoxo)
as well as a set of assertional axioms about the individuals in the
domain (ABoxo). Concept axioms have the form C v D where C
and D are concept descriptions, and relation axioms are expressions of the form R v S, where R and S are relation descriptions.
The ABox contains concept assertions of the form C(a) where C is
a concept and a is an individual name, and relation assertions of
the form R(a,b), where R is a relation and a and b are individual
names.

~v 14

Conjunctive semantic formulas are used to express policies. A

conjunctive semantic formula Fo
i140/i over an ontology o is a
conjunction of atomic assertions /i, where ~v 14 h?x0; . . . ; ?xni repre-

sents a vector of variables used in these assertions. For the sake of
i140/i  f/1; . . . /ng in order to consider a

convenience, we assume
conjunctive formula as a set of atomic assertions. Based on this, Fo
~v
~v [ Co
can be considered as To
~v is a set of type assertions using the concepts from o, e.g., fstudent?xi; nurse?xjg; Ro
~v
is set of relation assertions using the relations from o, e.g.,
fmarriedTo?xi; ?xjg; Co
~v is a set of constraint assertions on vari-
ables. Each constraint assertion is of the form ?xi /b, where b is a
constant and / is any of the symbols {>, <, =, , P, 6}. A constant
is either a data literal (e.g., a numerical value) or an individual defined in o.

~v, where To

~v [ Ro

Variables are divided into two categories; datatype and object
variables. A datatype variable refers to data values (e.g., integers)
and can be used only once in Ro
~v. On the other hand, an object variable refers to individuals (e.g., University_of_Aberdeen) and can
be used freely many times in Ro
~v. Equivalence and distinction between the values of object variables can be defined using OWL
properties sameAs and differentFrom, respectively, e.g., owl:sam-
eAs(?x,?y). In the rest of the paper, we use the symbols a, q, u,
and e as a short hand for semantic formulas. Given an ontology
o, a conditional policy is defined as a ! Nv:qa : u=e, where

(1) a, a conjunctive semantic formula, is the activation condi-

tion of the policy.

(2) N 2 {O, P, F} indicates if the policy is an obligation, permis-

sion or prohibition.

(3) v is the policy addressee and q describes v using only the role
concepts from the ontology (e.g., ?x:student(?x) ^ female(?x),
where student and female are defined as sub-concepts of the

role concept in the ontology). That is, q is of the form
i140riv, where ri v role. Note that v may directly refer to a
specific individual (e.g., John) in the ontology or a variable.

(4) a:u describes what is prohibited, permitted or obliged by the
policy. Specifically, a is a variable referring to the action to be
regulated by the policy and u describes a as an action
instance using the concepts and properties from the ontology
(e.g., ?a:SendFileAction(?a) ^ hasReceiver(?a, John) ^ hasFile
(?a, TechReport218.pdf), where SendFileAction is an action
concept). Each action concept has only a number of functional relations (aka. functional properties) [15] and these
relations are used while describing an instance of that action.

(5) e defines the expiration condition.

Table 1 illustrates how a conditional policy can be represented
using the proposed approach. This policy states that a person is obliged to leave a location when there is a fire risk.

Table 1
A person has to leave a location when there is a fire risk.

v:q
a:u

Place(?b) ^ hasFireRisk(?b,true) ^ in(?x,?b)

?x:Person(?x)
?a:LeavingAction(?a) ^ about(?a,?b) ^ hasActor(?a,?x)
hasFireRisk(?b,false)

M. Sensoy et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

Given a semantic representation for the state of the world, policies are used to reason about actions that are permitted, obliged or
prohibited. Let Do be a semantic representation for a state of the
world based on an ontology o. Each state of the world is partially
observable; hence Do is a partial representation of the world. Do itself is represented as an ontology composed of (TBoxo, ABoxD)
where ABoxD is an extension of ABoxo.

3. Reasoning with policies

When its activation conditions are satisfied, a conditional policy
leads to an activated policy. Definition 1 summarizes how a conditional policy is activated using ontological reasoning over a state of
the world. Here we use query answering to determine activated
policies and reason about actions. The query answering mechanism we use in this work is DL-safe; i.e., variables are bound only
to the named individuals, to guarantee decidability [16]. In this
section, we address some of the key issues in supporting decisions
governed by policies: activation and expiration, and reasoning
about interactions between policies and actions.

Definition 1. Let Do be a state of the world represented based on a
there is a substitution r such that
domain ontology o.
Do  (a ^ q) 	 r, but there is no substitution r0 such that Do  (e 	
r) 	 r0, then the policy (Nv(a:u)) 	 r becomes active. This policy
expires when there exists a substitution r0 such that Do  (e 	 r) 	 r0.

If

3.1. Policy activation

A policy is activated for a specific agent when the world state is
such that the activation condition holds for that agent and the
expiration condition does not hold, and expires when this latter
condition holds. The above definition is rather standard [19], but
we now describe how this is implemented efficiently through
query answering. A conjunctive semantic formula can be trivially
converted to a SPARQL query [26] and can be evaluated by OWLDL reasoners with SPARQL-DL [30] support such as Pellet [30] to
find a substitution for its variables satisfying a specific state of
the world. Therefore, we can test Do  (a ^ q) 	 r by writing a query
for (a ^ q) and testing whether it is entailed by Do or not. Consider
the conditional policy in Table 1 and assume that we have the partially represented state of the world in Fig. 1. We can write the
semantic query in Fig. 2 to find r for the conditional policy. When
we query the state of the world using SPARQL, each result in the

result set provides a substitution r; in our case, we have two r val-
ues: {?x/John,?b/Room245} and {?x/Jane,?b/Room245}, representing that there is a fire risk in the room 245 of the Central
Hospital and that John and Jane are in that room.

Now, using the computed r values, we should try to find a r0
such that Do  (e 	 r) 	 r0. In our case, for this purpose, we can
use the semantic query q():- hasFireRisk(Room245,false). When
the SPARQL representation of this query is executed over the state
of the world shown in Fig. 1, it returns false; that is the RDF graph
pattern represented by the query could not be found in the ontol-
ogy. This means that the policy in Table 1 should be activated using
the variable bindings in r. The result is activations of OJohn(?a:Leav-
ingAction(?a) ^ about(?a,Room245)) and OJane(?a:LeavingAction(?a)
^ about(?a,Room245)). These policies mean that John and Jane are
obliged to leave the room 245; the obligation expires when the fire
risk is removed.

3.2. Reasoning about actions

Let us assume that a specific action a0:u0 will be performed by x,
where a0 is a URI referring to the action instance and u0 is a conjunctive semantic formula describing a0 without using any vari-
ables. Let Do be the current state of the world. We can test if the
action a0 is permitted, forbidden or prohibited in Do. For this pur-
pose, based on Do, we create a sandbox (hypothetical) state of
the world D0
o shows what
happens if the action is performed. This is achieved by simply add-
o 14 Do [ u0. For examing the described action instance to Do, i.e., D0
ple, the state of the world in Fig. 1 is extended using action
instance LeaveAct_1: LeavingAction(LeaveAct_1) ^ hasActor(Leave-
Act_1,John) ^ about(LeaveAct_1,room245). The resulting state of
the world is shown in Fig. 3.

o to make what-if reasoning [34], i.e., D0

For each active policy Nx(y:uy), we test the expiration conditions on D0
o as explained before. If the policys expiration conditions
are satisfied, we can conclude that the action a0:u0 leads to the
expiration of the policy. Otherwise, a semantic query Q of the form
q~vuy : -uy is created, where ~vuy is the vector of variables in uy.
Then, D0
o is queried with Q. Let the query return a result set rs; each
result r 2 rs is a substitution such that D0
o  uy 	 r. If y 	 r = a0 for any
such r, then a0 is regulated by the policy. In this case, we can interpret the policy based on its modality as follows:

(1) Nx = O: in this case, the policy represents an obligation; that
is, x is obliged to perform a0. Performing a0 will remove this
obligation.

marriedTo

hasActor

in

Person

inChargeOf

Action

isA

isA

hasAge

isA

Doctor

LeavingAction

Patient

xsd:int

hasFireRisk

isA

xsd:boolean

Place

in

isA

isA

Room

Building
in

isA

Hospital

xsd:time

hasValue

CurrentTime

xsd:boolean

hasPatient

HospitalRoom

type

Jane

type

type

John

in

type

CentralHospital

inChargeOf

Room245

in

in

hasFireRisk

true

type

hasValue

13:00:00

Fig. 1. A partial state of the world represented based on a domain ontology.

Fig. 2. Query for the activation of a policy.

marriedTo

hasActor

in

Person

inChargeOf

Action

isA

LeavingAction

Patient

isA

hasAge

isA

Doctor

hasFireRisk

isA

xsd:boolean

Place

in

isA

isA

Room

Building
in

isA

xsd:time

hasValue

Hospital

CurrentTime

xsd:int

type

type

xsd:boolean

hasPatient

HospitalRoom

type

LeaveAct_1

Jane

John

in

type

type
in

CentralHospital

type

hasValue

13:00:00

hasActor

in

about

inChargeOf

Room245

hasFireRisk

true

Fig. 3. The sandbox (hypothetical) state of the world.

(2) Nx = P: performing a0 is explicitly permitted.
(3) Nx = F: performing a0 is prohibited.

After examining the active policies as described above, we can
identify a number of possible normative positions with respect to
the action instance a0: (i) doing a0 may be explicitly permitted if
there is a policy permitting it; (ii) doing a0 may be obligatory if there
exists a policy obliging it; (iii) doing a0 may be prohibited if there is
a policy prohibiting it; and (iv) there may be a conflict in the normative position with respect to a0 if it is either both prohibited and
explicitly permitted, or both prohibited and obliged.

4. Reasoning about policies

In this section, we demonstrate reasoning techniques to support
the analysis of policies in terms of their meaningfulness (Section
4.2) and possibility of conflict (Section 4.3), and hence address
our third desideratum. Prior to this, however, we propose methods
for reasoning about semantic formulas to underpin our mechanisms for policy analysis.

4.1. Reasoning about semantic formulas

Here, we introduce methods for reasoning about semantic
conjunctive formulas using query freezing and constraint
transformation.

4.1.1. Conjunctive Queries

There is an important relationship between conjunctive formulas and conjunctive queries that we exploit in this reasoning mod-

el. Conjunctive semantic formula can trivially be converted into a
conjunctive semantic query. For example, Ao
can be converted
~v1
into the query qA():- Ao
. In this way, we can use query reasoning
~v1
techniques to reason about semantic formulas. For instance, in order to reason about the subsumption between semantic formulas,
we can use query subsumption (containment).

In the conjunctive query literature, in order to test whether qA
subsumes qB, the standard technique of query freezing is used to reduce the query containment problem to query answering in
Description Logics [22,33]. For this purpose, we build a canonical
ABox UqB from the query qB():- Bo
in three steps. First, for each
~v2
variable in ~v 2, we put a fresh individual into UqB using the type
assertions about the variable. Note that this individual should
not exist in o. Second, we add each individual appearing in qB into
UqB . This is done using information about the individual from the
ABoxo (e.g., type assertions). Third, relationships between individuals and constants defined in qB are inserted into UqB . As a result
of this process, UqB contains a pattern that exists only in ontologies
that satisfy qB. We combine UqB and our TBoxo to create a new
canonical ontology, o0 14 TBoxo; UqB. Example 1 demonstrates a
simple case. Based on [33,22], we conclude that o  qB v qA if and
only if o0 entails qA. In order to test whether o0 entails qA or not,
we query o0. That is, o0 entails qA if there exists at least one match
for qA in o0. This can easily be achieved by converting qA to SPARQL
syntax and use Pellets SPARQL-DL query engine to answer qA on o0
[30].

1. Let

Person(?p) ^ married-
Example
To(?p,?x) ^ Patient(?x) and query qB be q Doctor(?x) ^ married-
To(?x,Jane) ^ hasChild(?x,?c). Then, UqB contains an individual x,

query

qA

be

q():-

M. Sensoy et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

Algorithm 1

Constraint transformation.
1: Input: Formula Fo

~v  To

~v [ Ro

~v [ Co
~v,

Ontology o  (ABoxo,TBoxo)

Ontology /  (ABoxo,TBox/)
~v ; TBox/ 14 TBoxo

2: Output: Formula F/
~u ,
~u 14 To
~v do

if (isDatatypeVariable(b)) then
cd 14 getConstraintsb; Co
~v
c = createConcept(r,cd,TBox/)
s = createTypeAssertion(a,c)
~u 14 F/
F/
~u 14 F/
F/
~v
cb 14 getConstraintsb; Co
~u then
if cb

3: Initialization: F/
4: for all ra; b 2 Ro
5:
6:
7:
8:
9:
10: else
11:
12:
13:
14:
15:
16:
17: end for

;&:cb  F/
~u [ cb
~u 14 F/
F/

~u [ s
~u [ ra; b

end if

end if

~v [ Ro

~v, and Co

~v is of the form To

The algorithm takes a conjunctive semantic formula Fo
~v [ Co

~v and the
ontology o as input (line 1). Fo
~v, where
To
~v ; Ro
~v are sets of type, relation and constraint assertions,
respectively. The output of the algorithm is the transformed
semantic formula F/
~u (containing no datatype variables) and the
updated ontology / (line 2). Initially, F/
~v and
/ is the same as o (line 3). For each relation assertion r(a,b) in Ro
~v,
we do the following (line 4). First, we check if b is a datatype variable (line 5). If so, this means that r is a datatype property with a
variable in its range. In this case, we extract the set of constraints
related to b from Co
~v, which is referred by cd (line 6). Based on r and
cd, we create a concept c in TBox/ using the createConcept function
(line 7). This function works as follows:

~u is set as equal to To

(1) If cd

 ;, then b implies some restrictions on the range of r. In
this case, c should refer to objects that have the property r
with the restrictions defined in cd on its range. While creating c in TBox/, we use data-ranges1 introduced in OWL 2.0 to
restrict the range of r accordingly. For example, if r(a,b) corresponds to hasAge(?c,?a) and cd = {?a P 10,?a 6 20}, then a
concept named AgeConst1 can be described as shown in
Fig. 5. For more sophisticated constraints, we create more
complex class expressions using the OWL constructors
owl:unionOf, owl:intersectionOf, and owl:complementOf.

(2) If cd = ;, then b has no constraints, which means that the
data-range of b is equivalent to the range of its datatype
(i.e., for xsd:int, the range is min inclusive 2147483648
and max inclusive 2147483647).

After creating the concept c in TBox/, we create a type assertion
s to declare a as an instance of c (e.g., AgeConst1(?c)) (line 8). This
type assertion is added to F/
~u in order to substitute r(a,b) and cd in
Fo
~v (line 9). On the other hand, if b is not a datatype variable (line
10), there are two possibilities: (1) r is a datatype property but b
is not a variable, or (2) r is an object property. In both cases, we di-
~u (line 11). If b has constraints defined in Co
rectly add r(a,b) to F/
~v,

1 http://www.w3.org/TR/2008/WD-owl2-syntax-20081008/#Data_Ranges.

Fig. 4. The ontology created for qB in Example 1.

which is created for the variable ?x. The individual x is defined as of
type Doctor. In UqB , we also have another individual Jane, which is
defined in the original ABoxo as an instance of the Patient class; we
get all of its type assertions from the ABoxo. Then, we insert the
object property marriedTo between the individuals x and Jane.
Lastly, we create another individual c for the variable ?c in UqB and
insert the hasChild object property between x and c. The resulting
ontology is shown in Fig. 4.

The query freezing method described above enables us to create
a canonical ABox for a semantic conjunctive formula; this ABox
represents a pattern which only exists in ontologies satisfying
the semantic formula. On the other hand, this method assumes
that variables in queries can be assigned fresh individuals in a
canonical ABox. However, in OWL-DL, individuals can refer to ob-
jects, but not data values [32]. Therefore, the proposed query freezing method can be used to test for subsumption between qA and qB
only if the variables in qA and qB refer to objects. A variable can refer to an object if it is used as the domain of an object or datatype
property (e.g., hasAge(?x,10)) or if it is used as the range of an object property (e.g., marriedTo(John,?x)). Unfortunately, in many
real-life settings, queries may have variables referring to data values with various constraints, which we refer to here as datatype
variables. In these settings, the query freezing described above cannot be used to test subsumption. Example 2 illustrates a simple
scenario.

Example 2. Let query qA be q():- Person(?p) ^ hasChild(?p,?c) ^
hasAge(?c,?y) ^ ?y P 12 ^ ?y 6 16 and query qB be q():- Doc-
tor(?x) ^ marriedTo(?x,Jane) ^ hasChild(?x,?c)
^ hasAge(?c,?a) ^
?a P 10 ^ ?a 6 20. In this example, the query freezing method
cannot be used directly to test subsumption between qA and qB,
because the variables ?y and ?a refer to data values, which cannot
be represented by individuals in an OWL-DL ontology.

4.1.2. Constraint Transformation

Here, we propose constraint transformation. It is a preprocessing step which enables us to create a canonical ABox for semantic
formulas with datatype variables. Note that a datatype variable is
used in a semantic formula to constrain one datatype property,
e.g., ?y is used to constrain the hasAge datatype property in qA
of Example 2. Constraint transformation in contrast uses dataranges introduced in OWL 2.0 [15] to transform each constrained
datatype property to a named OWL class. As a result, datatype
variables and related datatype properties and constraints are
replaced with type assertions. This procedure is detailed in
Algorithm 1.

Definition 2. A policy a ! Nv:qa : u=e is an idle policy if it does
not activate for any state of the world Do or there is a substitution
r0 such that Do  (e 	 r) 	 r0, whenever there is a substitution r such
that Do  (a ^ q) 	 r.

Let us demonstrate idle policies with a simple example. Assume
that object property hasParent is an inverse property of hasChild.
Also, let us assume in the domain ontology, we have a SWRL rule
such as hasSponsor(?c,true) hasParent(?c,?p) ^hasAge(?c, ?age)
^ ?age < 18, which means that children under 18 have a sponsor if
they have a parent. Now, consider the policy in Table 2. This policy
is activated when a person ?p has a child ?c, which is a student under
18. The activated policy expires when ?c has a sponsor. Interest-
ingly, whenever the policy is activated, the domain knowledge implies that ?c has a sponsor. That is, whenever the policy is
activated, it expires.

In order to detect idle policies, we reason about the activation
and expiration conditions of policies. Specifically, a policy
a ! Nv:qa : u=e is an idle policy if (a ^ q) is unrealistic or implies
e using the knowledge in the domain ontology. More formally, we
can show that the policy is idle if we show (a ^ q) never holds or
(a ^ q)?e. This can be achieved as follows. First, we freeze
(a ^ q) and create a canonical ontology o0. If the resulting o0 is
not a consistent ontology, then we can conclude that the policy
is an idle policy, because (a ^ q) never holds. Let o0 be consistent
and r be a substitution denoting the mapping of variables in
(a ^ q) to the fresh individuals in o0. If there exists a substitution
r0 such that o0  (e 	 r) 	 r0, we conclude that (a ^ q)?e. We can test
o0  (e 	 r) 	 r0 by querying o0 with q():- (e 	 r).

4.3. Anticipating conflicts between policies

In many settings, policies may conflict. In the simplest case, one
policy may prohibit an action while another requires it. There are,
however, many less obvious interactions between policies that
may lead to logical conflicts [17,27,20,12]. Further developing
our earlier example, consider the policy presented in Table 3 that
states that a doctor cannot leave a room with patients if he is in
charge of the room. This policy conflicts with the policy in Table
1 under some specific conditions. For example, in the scenario described Fig. 1, room 245 of Central Hospital has a fire risk and Dr.
John is in charge of the room, in which there are some patients.
In this setting, the policy in Table 1 obligates Dr. John to leave
the room while the policy in Table 3 prohibits this action until
the room has no patient.

If we can determine possible logical conflicts while designing
policies, we can create better policies that are less likely to raise
conflicts at run time. Furthermore, we can use various conflict

Table 2
A simple idle policy example.

v:q
a:u

hasChild(?p,?c) ^ Student(?c) ^ hasAge(?c,?age) ^ ?age < 18

?p:Person(?p)
?a:PayTuitionsOfStudent(?a) ^ about(?a,?c) ^ hasActor(?a,?p)
hasSponsor(?c,true)

Table 3
A doctor cannot leave a room containing patients if he is in charge of the room.
Room(?r) ^ hasPatient(?r,true) ^ inChargeOf(?d,?r)

?d:Doctor(?d)
?x:LeavingAction(?x) ^ about(?x,?r) ^ hasActor(?x,?d)
hasPatient(?r,false)

v:q
a:u

Fig. 5. A concept named AgeConst1 is created for hasAge(?c,?a) ^ ?a P 10 ^ ?a 6
20.

we extract these constraints and add them to F/
ready added (lines 1215).

~u if they are not al-

In order to test subsumption between qA and qB in Example 2, we
should transform the bodies of these queries and update the ontology they are based on. For this purpose, we use constraint transformation twice. That is, we first update the ontology by adding the
concept AgeConst1 to handle hasAge(?c,?y) ^ ?y P 10 ^ ?y 6 20
and transform qB to q():- Doctor(?x) ^ marriedTo(?x,Jane) ^ has-
Child(?x,?c) ^ AgeConst1(?c). Then, we add concept AgeConst2 to
the ontology to handle hasAge(?c,?y) ^ ?y P 12 ^ ?y 6 16 and
transform qA to q():- Person(?p) ^ hasChild(?p,?c) ^ AgeConst2(?c).
After this preprocessing step, we use query freezing to test qB v qA;
the ontology with a canonical ABox created during query freezing is
shown in Fig. 6.

With these techniques in place, we are now in a position to address the issue of policy analysis supported by OWL-POLAR. It is
descried in the following sections.

4.2. Idle policies

A policy is idle if it is never activated or the policys expiration
condition is satisfied whenever the policy is activated. This condition is formally described in Definition 2. If a policy is idle, it cannot
be used to regulate any action, because either it never activates or
whenever it activates an obligation, permission, or prohibition
about an action, the activated policy expires. While designing pol-
icies, we may take domain knowledge into account to avoid idle
policies.

Fig. 6. The Ontology created for qB in Example 2.

M. Sensoy et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

marriedTo

hasActor

in

Person

inChargeOf

Action

isA

LeavingAction

Patient

isA

hasAge

isA

Doctor

hasFireRisk

isA

xsd:boolean

Place

in

isA

isA

Room

Building
in

HospitalRoom

isA

xsd:time

hasValue

Hospital

CurrentTime

type

xsd:boolean

hasPatient

xsd:int

type

in

inChargeOf

hasPatient

hasFireRisk

true

true

Fig. 7. The canonical state of the world where the policies of Tables 1 and 3 conflict.

resolution strategies such as setting a priority ordering between
the policies to solve conflicts [19,34,35], once we determine that
two policies may conflict.

In this section, we propose techniques to anticipate possible conflicts between policies at design time. Suppose we have two non-idle
policies Pi 14 ai ! Avi:qiai : ui=ei and Pj 14 aj ! Bvj:qjaj : uj=ej.
These policies are active for the same policy addressee in the same
state of the world D if the following requirements are satisfied:

(1) D  (ai ^ qi) 	 ri, but no r0

i such that D  ei 	 ri 	 r0

(2) D  (aj ^ qj) 	 rj, but no r0
(3) vi 	 ri = vj 	 rj

j such that D  ej 	 rj 	 r0

The policies Pi and Pj conflict if the following requirements
are also satisfied:

(4) (ui 	 ri) v (uj 	 rj) or (uj 	 rj) v (ui 	 ri)
(5) A conflicts with B. That is, A 2 {P,O} while B 2 {F} or vice

versa.

Algorithm 2

Anticipate if Pi may conflict with Pj.
1: Input: Policy Pi 14 ai ! Avi :qiai : ui=ei,

Policy Pj 14 aj ! Bvj :qjaj : uj=ej

hD,rji = update(D,(aj ^ qj) 	 rk)

hD,rii = freeze(ai ^ qi)
ho0,_i = freeze(ui 	 ri)
rs = query(o0,uj)
for all (rk 2 rs) do

2: if ((A 2 {O,P} and B 2 {F}) or (A 2 {F} and B 2 {O,P})) then
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end if
15 return false

if (isConsistent(D)) then

return true

end for

end if

end if

if (query(D,ei 	 ri) = ; and query(D,(ej 	 rk) 	 rj) = ;) then

We can use Algorithm 2 to test if it is possible to have such a state
of the world where Pi conflicts with Pj. The first step of the algorithm
is to test if A conflicts with B (line 2). If they are conflicting, we continue with testing the other requirements. We create a canonical
state of the world D in which Pi is active by freezing (ai ^ qi) with
a substitution ri mapping the variables in (ai ^ qi) to the fresh individuals in D. Given that (uj 	 r) v uj for any substitution r mapping
variables into individuals, the requirement (iv)
implies that

(ui 	 ri) v uj. We test this as follows. First, we create a canonical
ontology o0 by freezing (ui 	 ri) (line 4) and then query o0 with uj
(line 5). Each answer to this query defines a substitution rk mapping
variables in uj into the terms in (ui 	 ri), so that (ui 	 ri) v (uj 	 rk). If
uj does not have any variable but it repeats in o0 as a pattern, the result set contains only one empty substitution. If the query fails, the
result set is an empty set (;), which means that it is not possible to
have a rk such that (ui 	 ri) v (uj 	 rk).
For each rk satisfying (ui 	 ri) v (uj 	 rk), we test the other
requirements as follows. First, we update D by freezing (aj ^ qj) 	 rk
without removing any individual from its existing ABox (line 7). Note
that as a result of this process, rj is the substitution mapping the
variables in (aj ^ qj) 	 rk to the new fresh individuals in the updated
D, so that vi 	 ri = (vj 	 rk) 	 rj. We test the consistency of the resulting state of the world D (line 8). If this is not consistent, we can conclude that it is not possible to have a state of the world satisfying the
requirements. If the resulting D is consistent, we check the expiration conditions of the policies. If both are active in the resulting state
of the world (line 9), the algorithm returns true (line 10). If any of
these requirements do not hold, the algorithm returns false (line 15).
As described above, the algorithm transforms the problem of
anticipating conflict between two policies into an ontology consistency checking problem. To check the consistency of the constructed
canonical state of the world D, we have used the Pellet [30] reasoner.
This reasoner adopts the open world assumption and does not have
Unique Name Assumption (UNA). Hence, it searches for a model2
of D, also considering the possible overlapping between the individuals (i.e., individuals referring the same object). If there is no model of D,
it is not possible to have a state of the world satisfying the requirements stated above. We should also note that, while anticipating the
conflict, Algorithm 2 tests only the case (ui 	 ri) v (uj 	 rj). However,
we also need to test (uj 	 rj) v (ui 	 ri) to capture the possibility of
conflict. Therefore, if the algorithm returns false, we should swap
the policies and run the algorithm again. If it returns true with the
swapped policies, we can conclude that there is a state of the world
where these policies may conflict.

To demonstrate the algorithm, let us use the policies presented
in Tables 1 and 3 and refer to them as Pi and Pj, respectively. In
this example, Pj is a prohibition while Pi is an obligation, so the
algorithm proceeds as follows (line 2). We create a canonical
state of the world D by freezing Person(?x) ^ Place(?b) ^ hasFire-
Risk(?b,true) ^ in (?x,?b) with a substitution ri = {?x/x,?b/b} (line
3). Now we create a canonical ontology a0 by freezing ui 	 ri with
substitution {?a/a} (line 4). This ontology has the following ABox
assertions: LeavingAction(a), about(a,b), hasActor(a,x). We query o0

2 A model of an ontology o is an interpretation of o satisfying all of its axioms [1].

with LeavingAction(?x) ^ about(?x,?r) ^ hasActor(?x,?d)
(line 5).
The result set is composed of only one substitution: rk = {?x/
a,?r/b,?d/x}. The next step is to update D by freezing Doc-
tor(x) ^ Room(b) ^ hasPatient(b,true) ^ inChargeOf(x,b) without removing the current ABox of D (line 7). The resulting canonical
state of the world is shown in Fig. 7. Lastly, we check whether
both policies remain in effect by checking their expiration conditions (line 9). In this example, we query D with hasFireRisk(b,false)
and hasPatient(b,false). Both of these queries return ;, hence we
conclude that there is a state of the world where these policies
conflict (line 10).

5. Conflict resolution

Legal theory and practice provide some doctrines to resolve
conflict between norms [36,4]. These doctrines can also be used
when a conflict arises between policies (aka norms). These doctrines are shortly described as follows:

 Lex superior: the norm issued by a more important legal entity

prevails when in conflict with another norm.

 Lex posterior: the newer norm is preferred over the older one.
 Lex specialis: the norm governing a specific subject matter over-

rides the norm which governs general matters.

Lex superior uses the hierarchy between the authorities issuing
norms while resolving conflicts between these norms. Hence, it
cannot be used to resolve conflicts between norms issued by one
same authority or norms issued by authorities without hierarchical
relationships between them. For instance, let us assume that the
policy of Table 1 is issued by the government and the policy of Table 3 is issued by the hospital management. In this case, the policy
of Table 1 would override the policy of Table 3, based on the lex
superior principle.

Lex posterior assumes newer norms are preferred over the older
ones. This assumption may hold only in certain conditions. In most
of the cases, the lex posterior principle may not be useful. Especially if the conflicting norms are issues by different authorities,
temporal relationships between these norms would be misleading.
However, if these norms belong to the same authority, this principle may be applicable under some circumstances. For instance, let
us assume that the policy of Table 3 is issued after the policy of Table 3 by the hospital management. Then, the policy of Table 3
would override the policy of Table 1.

Lex specialis considers a more specific norm as an exception
of a more general one. This principle may work especially if
these norms belong to the same organization. Unlike the other
two doctrines, Lex specialis requires some non-trivial reasoning
process that allow us to reason about the subsumption relationships between policies. In the next Section 5.1, we propose a
subsumption reasoning mechanism for OWL-POLAR policies.

5.1. Policy subsumption

Suppose we have two non-idle policies Pi 14 ai ! Avi:qiai : ui=ei
and Pj 14 aj ! Bvj:qjaj : uj=ej. The policy Pi subsumes Pj if whenever Pj is active for a policy addressee regarding an action, Pi is also
active for the same policy addressee and action. This can be described formally as follows. For each state D such that D  (aj ^
qj) 	 rj, there is a substitution ri such that

(1) D  (ai ^ qi) 	 ri
(2) vi 	 ri = vj 	 rj
(3) uj 	 rj v ui 	 ri
(4) there is no substitution r0

i such that D  ei 	 ri 	 r0

Based on this definition, in this section, we propose Algorithm 3
to check if Pi subsumes Pj. The algorithm first creates a canonical
state of the world characterizing all the states where Pj is active
(line 2). Then, it checks if Pj is also active in this canonical state
by finding all substitutions satisfying activation and role constraints of Pj (line 3). Lastly, it checks if any of these substitutions
satisfies the requirements listed above (lines 48). The algorithm
returns true if such a substitution exists (lines 57); otherwise it
returns false (line 9).

Algorithm 3

Check if Pi subsumes Pj.
1: Input: Policy Pi 14 ai ! Avi:qiai : ui=ei,
Policy Pj 14 aj ! Bvj:qjaj : uj=ej

2: hD,rji = freeze(aj ^ qj)
3: rs = query(D,ai ^ qi)
4: for all (ri 2 rs) do
5:

if vi 	 ri = vj 	 rj and uj 	 rj v ui 	

ri and query(D,(ei 	 ri)) fails then

return true

end if

6:
7:
8: end for
9; return false

5.2. On conflict avoidance and resolution

If one of two conflicting policies subsumes the other, Lex specialis can be used to resolve the conflict. However, in many scenarios,
there is no subsumption relationship between conflicting policies.
These policies conflict only in certain cases, in which the activation
conditions of both policies hold and their expiration conditions do
not hold. To anticipate such policy conflicts, we have proposed the
reasoning mechanisms presented in Section 4.3. These mechanisms try to construct a consistent state of the world satisfying
the condition necessary for the conflict.

A policy addressee may wish to avoid a conflict between two
specific policies by simply avoiding states of the world in which
their activation conditions hold at the same time. This requires
him to examine available courses of action, to avoid those actions
that would trigger the activation conditions of conflicting norms.
If the activation conditions of the policies concerned are in full control of the policy addressee, this may be feasible. In many cases,
however, the agent may have limited control over the satisfaction
of policy activation conditions. For instance, a doctor has limited
ability to control the fire risk in a hospital. That is, he cannot control
the activation of the policy in Table 1. On the other hand, he may
control some of his responsibilities in the hospital. To avoid the possibility of conflict, he could avoid being in charge of any room in the
hospital. This is a rather radical approach to conflict avoidance, and,
of course, will lead to other conflicts with, for example, a duty of
care. The doctor behaves as if there will indeed be a fire risk in
the hospital, regardless of its actual probability. A more reasonable
approach may be to consider the probability of specific world states
occurring. That is, the doctor may avoid being in charge of any room
in the hospital if the probability of fire risk in the hospital becomes
greater than a threshold. However, this solution involves significant
overhead of estimating likelihoods of policy activations.

Unless there is a subsumption relation between two conflicting
policies, a conflict between them may occur only under certain cir-
cumstances. Hence, it may be more feasible to resolve a policy conflict once it occurs rather than trying to avoid it in the first place.
This is especially the case when there are some predefined conflict
resolution strategies available to resolve the policy conflict. For in-

M. Sensoy et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

stance, if the policy of Table 1 is enforced by an authority with
more power (or hierarchically superior) than that enforcing the
policy of Table 3, then a conflict between these policies can be resolved using the lex superior strategy. However, this would be disregarding the rationale behind the policy of Table 3, i.e., always
having patients under the care of medical staff. A better approach
would be to refine the expiration conditions of the policies con-
cerned. If one of the two conflicting policies expires, the conflict
would be automatically resolved without making one policy override the other. Hence, the problem of conflict resolution turns into
the problem of satisfying policy expiration conditions once a conflict occurs between two policies.

A policy addressee may have certain actions to change the state
of the world and achieve his goals (e.g., desired states of the world).
Existing automated planning systems [23] can be used to find a plan
(i.e., sequence of actions) to achieve a certain goal. These systems
are given an initial state, a list of available actions (i.e., operators),
and a goal state. Actions typically have preconditions and postcon-
ditions. If the preconditions of an action hold in a specific state of
the world, the action specification states that if it were performed
in this state, the state is expected to change according to the post-
conditions. Given a state where two conflicting policies are activated at the same time for the same policy addressee, the policy
addressee may use an automated planner to find a plan whose actions will cause the state of the world to change in a way that the
expiration condition of one of the policies holds. The planner considers only actions available to the policy addressee while composing plans. Consider the previous example where a doctor is in
charge of a room in a hospital H with fire risk, i.e., hasFire-
Risk(H,true). In this situation, the policies of Tables 1 and 3, which
are conflicting, will be active at the same time. The first policy expires if the goal state hasFireRisk(H,false) is achieved. However, a
planner cannot produce a plan to achieve this state, since a doctor
does not have actions available. On the other hand, the expiration
conditions for the second policy is hasPatient(R,false). This state of
affairs can be achieved by the doctor if the doctor follows a plan
for evacuating the patients in the room. Such a plan can easily be
computed by an off-the-shelf planner [23]. Hence, in this example,
it is possible to come up with a plan for the policy addressee to
expire one of the conflicting active policies and resolve the
conflict.

Algorithm 4

xu1=e1 and

xu1=e1, N2

xu2=e2, state, normstate, A, policies

plans = plans [ getPlansForGoal(ei,state,A)

Obtain plans to expire active policies N1
xu2=e2.
N2
1: Input: N1
2: Output: R
3: R = ;
4: plans = ;
5: for all (i = 1;i 6 2;i++) do
6:
7: end for
8: for all (p 2 plans) do
9:
10:
11:
12:
13:
14:
15:
16: end for
17: return R

s = clone(state)
ns = clone(normstate)
for all (a 2 p) do

endfor
R = R [ {hp, nsi}

applyActionToState(a,s)
updateNormativeState(ns, s, policies)

Note that, there can be more than one plan to resolve a conflict.
However, each plan may have a different cost for the policy addres-
see, so plans can be ranked based on their cost. Moreover, plans to
be used to resolve a conflict between two policies may contain actions whose execution violates some other policies. Therefore, before choosing and executing a plan among possibilities,
it is
important to estimate the outcomes of plans in terms of policy vio-
lations. Algorithm 4 formalises this procedure. The input of this
algorithm is the conflicting active policies of the policy addressee
x, as well as xs current state and normative state, actions available
to x (A) and policies applicable to x. The normative state of x contains xs existing active policies (i.e., prohibitions, permissions,
obligations) and policy violations (e.g., violated prohibitions). The
algorithm firstly computes plans to satisfy expiration conditions
of the active policies (lines 57). To estimate how the normative
state of x would change if a specific plan p is followed, the algorithm creates copies of the current state of the world (i.e., sandbox
state) and xs normative state (lines 910). Then, it updates the
sandbox state by applying the actions in p and updates the normative state based on the sandbox state and policies of x (lines 11
14). As a result of the action, new active policies may be added
to the normative state, some active policies in the normative state
may become violated or expired, and some obligations may become satisfied. The algorithm associates each plan with the normative state it leads to (line 15) and lastly it returns all such
associations as the output (line 17). Using Algorithm 4, a policy addressee not only finds plans to resolve conflicts but also gets informed about the outcomes of these plans in terms of policy
activations, violations and fulfilments. Hence, he can review each
plan based on this information and pick the one most suitable
for himself if there is any.

Fig. 8 illustrates the interactions between policy reasoner, plan-
ner, and an agent. Let us explain how these interactions take place
through an example. Consider the state of the world in Fig. 1 and
policies of Tables 1 and 3. Then, John is obliged to leave the room
245 of Central hospital by the policy of Table 1 and he is prohibited
to do so by the policy of Table 3 since the patient Jane is in the
room. Let us assume that the planning domain has a simple operator sendPatientTo as described in Table 4. Operators in AI planning
correspond to atomic actions that can be performed to change
state of the world. During planning, the planner tries to achieve
the goal state by finding a sequence of actions with suitable
parameters.

Operators are described using their precondition and post conditions (i.e., addition and deletions). Hence, we need a language
more complex than Description Logics to properly describe complex operators [23]. To combine planning with ontologies,
researchers usually keep descriptions of operators in a planning
domain file, but they map these operators to action classes within
the ontology [29]. Table 5 describes sendPatientTo class added to
TBox to represent the operator described in Table 4. This class in
TBox is externally mapped to the sendPatientTo operator in the
planning domain. Whenever the agent execute an instance of this
operator, an instance of sendPatientTo class is added to the current
state of the world, which is represented as an ontology.

To resolve the conflict between the policies of Tables 1 and 3, John
interacts with the planner by requesting the plans to achieve the
goal hasPatient(room245,false). Given the additional ABox axioms
in Table 5, the planners returns two plans: hsendPatientTo(Jane,
room245,room246)i and hsendPatientTo(Jane, room245,backyard)i.
Both of these plans expire the policy of Table 3 for John. When John
executes the first plan, an instance i of sendPatientTo concept is created within the state of the world and described further using the
triples
and

hi,hasActor, Johni,hi, about, Janei,hi, from, room245i,

with this violation. However, the second plan does not lead to any
violation, since backyard is defined as an instance of SafePlace in Table 5. Hence, for John, it would be more beneficial to follow the second plan to resolve the conflict between the policies of Tables 1 and
3. To quantitatively compare normative states, we may need to add
penalties to policies [3]. The penalties determine the cost of policy
violations and allows policy addresses to prefer violating one policy
to another based on utility [35]. Penalties are not in the scope of this
paper and this issue is set as a future work.

6. Complexity of reasoning mechanisms

The computational complexity of the methods and algorithms
proposed in this paper can be summarised as follows. Policy activation described in Section 3.2 is based on testing activation and
expiration conditions of the policy through conjunctive query
answering in OWL-DL, which has been shown to be decidable under DL-safety restrictions [16]. Reasoning about actions is based on
creating a sandbox state of the world from the description of an ac-
tion, which is O(n) in size of terms in action description, and testing
activation and expiration conditions of policies. Hence, the complexity of this reasoning is equivalent to that of conjunctive query
answering. Constraint transformation introduced in Section 4.1 has
a complexity O(n2) in the size of terms in policy description.

Testing idle policies, policy subsumption, and anticipating
conflicts require constraint transformation, query freezing, consistency checking, and query answering. Query freezing has complexity O(n) in the size of terms in the policys activation and role
descriptions. Reasoning about idle policies, policy subsumption,
and conflicts are decidable but not tractable, because of the complexities of consistency checking and query answering in OWLDL [14].

We may note that although worst-case complexity of consistency checking and conjunctive query answering in OWL-DL is
NEXPTIME-complete [1], there are sub-languages of OWL-DL such
as DL-lite with a better complexity. For instance, reasoning services
such as consistency and instance checking and conjunctive query
answering in DL-lite have PTIME-complexity [14]. Furthermore, it
has been shown that reasoning performance in most of the existing
ontologies is much better than their worst-case complexities [30].
In this paper, planning is proposed as a tool to resolve conflicts
under specific conditions. Complexity of planning significantly depends on the restrictions put on the planning domain representation [23]. Worst-case complexity of planning is PSPACE-complete
in the set-theoretic representation [5]. Implementations such as
GraphPlan [23] achieves significant speed-up and contributes the
scalability of planning by backward constraint-directed search.

In summary, the reasoning mechanisms proposed in this paper
are decidable, but if we use OWL-DL they are not tractable, in the
worst case. If, however, we limit our language to DL-lite, we have a
better reasoning complexity (although our expressiveness is
curtailed).

7. Related work and discussion

We have proposed OWL-POLAR in [10] as an OWL-DL based
policy language that supports decidable policy analysis. One key
feature of OWL-POLAR is the reasoning mechanisms that allow
anticipation of possible conflicts between policies. In this paper,
we have extended [10] as follows. First, we have described how
existing conflict resolution strategies can be used for OWL-POLAR
policies. For this purpose, we have proposed a subsumption reasoning algorithm for policies, which allows us to determine if
one policy is a specialization of another one. Then, we have discussed in which situations existing conflict resolution strategies

Fig. 8. Interaction between planner, policy reasoner, and the agent.

Table 4
Description of sendPatientTo operator in a planning domain.

Operator
Precondition
Deletions
Additions

sendPatientTo(?p,?from,?to)
in(?p,?from) ^ patient(?p) ^ differentFrom(?from,?to)
in(?p,?from)
in(?p,?to)

Table 5
Additional TBox and ABox axioms.

TBox
sendPatientTo v Action
sendPatientTo v $about.Patient
sendPatientTo v $from.Place
sendPatientTo v $to.Place
SafePlace v Place
UnsafePlace v :SafePlace
$hasFireRisk.{true} v UnsafePlace

ABox

in(room246,CentralHopital)
SafePlace(backyard)

hi, to, room246i. In this example, the addition of this action instance
does not lead to any policy violation for John. However, it would
not be case if Central hospital has the policy of Table 6.

Let us describe how changes in normative state of John is deter-
mined, given Central hospital has the policy of Table 6. While
selecting operations to add to the current plan, the planner creates
a sandbox state of the world as described in [29]. That is, triples are
removed from and added to the current state of the world based on
operation descriptions. For instance, the planner starts with an
empty plan and the state of the world in Fig. 1. Then, send-
PatientTo(Jane, room245,room246) is added to the plan and the state
of the world is changed by removing the triple hJane, in, room245i
and adding the triple hJane, in, room246i. Lastly, the planner sends
the resulting sandbox state of the world to the policy reasoner for
violation detection based on the methods described in Section
3.2. The policy reasoner reveals that the policy of Table 6 would
be violated if this plan is executed. Therefore, the plan is annotated

Table 6
A doctor is prohibited to send a patient to an unsafe place.

Patient(?p) ^ in(?p,?from) ^ UnsafePlace(?to)

v:q ?x: Doctor(?x)
a:u ?a:SendPatientTo(?a) ^ about(?a,?p) ^

hasActor(?a,?x) ^ from(?a,?from) ^ to(?a,?to)
SafePlace(?to)

M. Sensoy et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 148160

may not be useful and proposed an automated planning-based approach to resolve policy conflicts under specific circumstances. In
this work, we have also discussed the computational complexity
of the proposed reasoning mechanisms and showed that these
mechanisms are decidable.

There have been several policy languages proposed that are
built upon Semantic Web technologies. Rei [18] is a policy language based on OWL-Lite and Prolog. It allows logic-like variables
to be used while describing policies. This gives it the flexibility to
specify relations like role value maps that are not directly possible
in OWL. The use of these variables, however, makes DL reasoning
services (e.g., static conflict detection between policies) unavailable for Rei policies. KAoS [34] is, probably, the most developed
language for describing policies that are built upon OWL. KAoS
was originally designed to use OWL-DL to define actions and poli-
cies. This, however, restricts the expressive power to DL and prevents KAoS from defining policies in which one element of an
actions context depends on the value of another part of the current
context. For example, KAoS cannot be used to represent a policy
like two soldiers are allowed to communicate only if they are in the
same team. To handle such situations, KAoS has been enhanced
with role-value maps using Stanford JTP, a general purpose theorem
prover [34]. Unfortunately, subsumption reasoning is undecidable
in the presence of arbitrary role-value-maps [1].

KAoS distinguishes between (positive and negative) obligation
policies and (positive and negative) authorization policies. Authorization policies permit (positive) or forbid (negative) actions,
whereas obligation policies require (positive) or do not require
(negative) action. Thus the general types of policies that can be described are similar to those that we have discussed in this paper.
Actions are also the object of a KAoS policy, and conditions on
the application of policies can be described (context), although
the subject (individual/role) of the policy is not explicit (it is, how-
ever, in Rei). In common with OWL-POLAR in its present form,
KAoS does not capture the notion of the authority from which/
whom a policy comes, but there is a notion of the priority of a policy which partially (although far from adequately) addresses this
issue. Unlike OWL-POLAR, Rei and KAoS do not provide means to
explicitly define expiration conditions of the policies.

Policy analysis within both KAoS and Rei is restricted to sub-
sumption. A policy in KAoS is expressed as an OWL-DL class regulating an action, which is expressed as an OWL-DL class expression
(e.g., using restrictions on properties such as performedBy and has-
Destination). Two policies are regarded in conflict if their actions
overlap (one subsumes another) while the modality of these policies conflict (e.g., negative vs. positive authorization). Similarly, if
there exist two policies within Rei that overlap with respect to
the agent and action concerned and they are obligued and prohib-
ited, then a conflict is recognised. In such a situation, meta-policies
are used to resolve the conflict. Policy conflicts can also be detected
within the Ponder2 framework [31,38], where analysis is far more
sophisticated than that developed for either KAoS or Rei, but analysis is restricted to design time. In general, different methods can
be used to resolve conflicts between policies. This issue has been
explored in detail elsewhere [19].

The expressiveness of OWL-POLAR is not restricted to DL. Using
semantic conjunctive formulas, it allows variables to be used while
defining policies. However, in semantic formulas, OWL-POLAR allows only object variables to be compared using owl:sameAs and
owl:differentFrom properties. On the other hand, datatype variables
can be used to define constraints on the datatype properties. In
other words, semantic formulas are restricted to describe states
of the world, each of which can be represented as an OWL-DL
ontology. Therefore, when a semantic formula is frozen, the result
is a canonical OWL-DL ontology. OWL-POLAR converts problems of
reasoning with and about policies into query answering and ontol-

ogy consistency checking problems. Then, it uses an off-the-shelf
reasoner such as Pellet [30] to solve these problems. It is known
that consistency checking in OWL-DL is decidable [30], and query
answering in OWL-DL has also been shown to be decidable under
DL-safety restrictions [16].

Ontology languages like KAoS are built on OWL 1.0, which does
not support data-ranges. Therefore, while defining policies, they
either do not allow complex constraints to be defined on datatype
properties or use non-standard representations for these con-
straints, which prevents them from using the off-the-shelf reasoning technologies. The clear distinctions between OWL-POLAR and
KAoS, however, are manifest in the fact that data ranges are
exploited in OWL-POLAR to enable the expression of more complex
constraints on policies, and the sophistication of the reasoning
mechanisms described in this paper.

To the best of our knowledge, OWL-POLAR is the first policy
framework that formally defines and detects idle policies. Existing
approaches like KAoS and Rei analyse policies only to detect some
type of conflict, considering only subsumption between policies.
On the other hand, OWL-POLAR provides advanced policy analysis
support that is not limited to subsumption checking. Consider the
following policies: (i) Dogs are prohibited from entering to a restau-
rant, and (ii) A member of CSI team is permitted to enter a crime
scene. There is no subsumption relationship between these poli-
cies, and so KAoS and Rei could not detect a conflict. However,
OWL-POLAR anticipates a conflict by composing a state of the
world where these policies are in conflict, e.g., the crime scene is
a restaurant and there is a dog in the CSI team.

Deontic logics study representation and relationships among
formal constructs asserting that certain actions or states of affairs
are obligatory, permitted, or forbidden [21]. Standard Deontic Logic
(SDL) builds upon propositional logic and is the most studied system of deontic logic. Especially, SDL is traditionally used to analyse
and identify ambiguities in sets of legal rules. For instance, Sergot
et al. represented aspects of the British nationality act using SDL
[28]. Cholvy and Cuppens used SDL to represent security policies
to detect conflicts in policy specification [8]. Their approach is
based on translating SDL into first order predicate logic to perform
the necessary conflict detection and analysis. SDL has been criticized as having some inherent paradoxes [21]. For instance, one axiom of SDL implies that an obligation can imply a permission while
another axiom indicates that a permission implies no obligation.
Extensions of deontic logic have been proposed to handle these
paradoxes [25]. However, even with these extensions, deontic log-
ics, as a formalism, pose challenges on humans who experience difficulties creating and understanding their (and others) policies
[11]. On the other hand, in this paper, we have proposed an expressive policy specification language based on Semantic Web, which
builds upon W3C standards and clear semantics both for humans
and machines.

In order to achieve decidability and improve reasoning perfor-
mance, we have imposed some restrictions in OWL-POLAR. First,
we have limited representation of constraints in conjunctive formulas so that two datatype variables cannot be compared. This allows us to convert these constraints into DL class expressions using
data-ranges. Without this restriction, a policy language would be
undecidable [1], since such constraints could be used to implement
role-value maps. Therefore, policies requiring comparisons of datatype variables cannot be expressed using OWL-POLAR, for the sake
of decidability. Second, we have only considered conjunctive
semantic formulas while representing activation conditions of pol-
icies, because inclusion of disjunctions in these formulas may lead
to more than one canonical states of the world during policy anal-
ysis. Disjunctions in activation conditions do not affect the decidability of OWL-POLAR, since a policy containing disjunctions in
its activation conditions can be converted into a set of policies with

only conjunctions in their activation conditions. However, we stick
to conjunctive semantic formulas in this paper for clarity and sim-
plicity. Another limitation of OWL-POLAR is its monotonicity during
conflict analysis. To anticipate conflicts between two policies,
OWL-POLAR creates a canonical state of the world, where these
two policies are active at the same time. Then, standard OWL-DL
consistency checking is used to test the possibility of such state.
However, standard reasoning in OWL-DL is monotonic [1]. That
is, if we know that x is an instance of A, then adding more information to the model cannot cause this to become false. Therefore, currently conflict detection between policies cannot be done using
OWL-POLAR when it requires non-monotonic reasoning. However,
this limitation would be relieved when non-monotonic reasoning
mechanisms becomes available in standard OWL-DL reasoners [6].

8. Conclusions

Policies provide useful abstractions to constrain and control the
behaviour of components in loosely coupled distributed systems.
Policies, also called norms, help designers of large-scale, open,
and heterogenous distributed systems (including multi-agent sys-
tems) to specify, in a concise fashion, acceptable (or policy-compli-
ant) global and individual computational behaviours,
thus
providing guarantees for the system as a whole. In this paper, we
have presented a semantically-rich representation for policies as
well as efficient mechanisms to reason with/about them. OWL-PO-
LAR meets all the essential requirements of policies, as well as
achieving an effective balance between expressiveness (realistic
policies can be adequately represented) and computational complexity of associated reasoning for decision-making and analysis
(reasoning with and about policies operate in feasible time).

The mechanisms proposed in this paper allow policy authors to
detect inconsistent or unfounded policies. These mechanisms provide means to determine the context in which different policies
may conflict. This kind of reasoning with context allows policy
authors and agents to resolve the conflict before they occur. We
have describe how conflict resolution doctrines from legal theory
and practice could be used to resolve policy conflicts. We have proposed a policy subsumption algorithm to allow lex specialis to be
used as a conflict resolution strategy for OWL-POLAR policies. Fur-
thermore, first time in the literature, we have used AI planning for
policy conflict resolution. Lastly, we have showed that all these
mechanisms are decidable.

Building upon this research, we plan to explore various extensions to OWL-POLAR. We will explore extending the representation
of policies to include deadlines and penalties associated with their
violation, along the lines of [3]. Also, we would like to investigate
how policing mechanisms [24] could make use of our representation and associated mechanisms to foster welfare in societies of
self-interested components/agents. Two further extensions should
address policies over many actions (as in, for instance, n is obliged
to perform u1 and u2) and disjunctions (as in, for instance, n is
obliged to perform u1 or u2). Finally, we are exploring the use
of OWL-POLAR in support of human decision-making, including
joint planning activities in hybrid human-software agent teams.

Acknowledgements

This research was sponsored by the U.S. Army Research
Laboratory and the U.K. Ministry of Defence and was accomplished
under Agreement Number W911NF-06-3-0001. The views and
conclusions contained in this document are those of the author(s)
and should not be interpreted as representing the official policies,
either expressed or implied, of the U.S. Army Research Laboratory,
the U.S. Government, the U.K. Ministry of Defence or the U.K.

Government. The U.S. and U.K. Governments are authorized to
reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.
