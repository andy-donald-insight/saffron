Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Searching web data: An entity retrieval and high-performance indexing model
Renaud Delbru a,

, Stephane Campinas a, Giovanni Tummarello a,b

a Digital Enterprise Research Institute, National University of Ireland, Galway, Ireland
b Fondazione Bruno Kessler, Trento, Italy

a r t i c l e

i n f o

a b s t r a c t

Article history:
Available online 13 May 2011

Keywords:
Sindice
Entity search and retrieval
Compression
Semantic Web
Semi-structured data
Inverted index

More and more (semi) structured information is becoming available on the web in the form of documents
embedding metadata (e.g., RDF, RDFa, Microformats and others). There are already hundreds of millions
of such documents accessible and their number is growing rapidly. This calls for large scale systems providing effective means of searching and retrieving this semi-structured information with the ultimate
goal of making it exploitable by humans and machines alike.

This article examines the shift from the traditional web document model to a web data object (entity)
model and studies the challenges faced in implementing a scalable and high performance system for
searching semi-structured data objects over a large heterogeneous and decentralised infrastructure.
Towards this goal, we define an entity retrieval model, develop novel methodologies for supporting this
model and show how to achieve a high-performance entity retrieval system. We introduce an indexing
methodology for semi-structured data which offers a good compromise between query expressiveness,
query processing and index maintenance compared to other approaches. We address high-performance
by optimisation of the index data structure using appropriate compression techniques. Finally, we demonstrate that the resulting system can index billions of data objects and provides keyword-based as well
as more advanced search interfaces for retrieving relevant data objects in sub-second time.

This work has been part of the Sindice search engine project at the Digital Enterprise Research Institute
(DERI), NUI Galway. The Sindice system currently maintains more than 200 million pages downloaded
from the web and is being used actively by many researchers within and outside of DERI.

O 2011 Elsevier B.V. All rights reserved.

1. Introduction

More and more structured and semi-structured data sources are
becoming available. With the current availability of data publishing standards and tools, publishing semi-structured data on the
web, which from here on we will simply call web data, is becoming
a mass activity. Indeed, nowadays it is not limited to a few trained
specialists any more. Instead, it is open to industries, governments
and individuals. For example, the Linked Open Data1 community
has made available hundreds of data sources, driven by the idea of
open access to data. There are also many prominent fields in which
examples of semi-structured data publishing efforts exist: e-govern-
ment, e.g., the UK government which publicly shares open government data; editorial world, e.g., the New York Times or Reuters
which publish rich metadata about their news articles; e-commerce,
e.g., BestBuy which publishes product descriptions in a machinereadable format; social networking, e.g., Facebooks Open Graph

 Corresponding author.

E-mail addresses: renaud.delbru@deri.org (R. Delbru), stephane.campinas@

deri.org (S. Campinas), giovanni.tummarello@deri.org (G. Tummarello).

1 Linked Data: http://linkeddata.org/

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.04.004

Protocol which enables any web page to become a rich object in a
social graph; etc.

However, the usefulness of web data publishing is clearly
dependent of the ease by which data can be discovered and consumed by others. Taking the e-commerce example, how can a user
find products matching a certain description pattern over thousands of e-commerce data sources? By entering simple keywords
into a web search system, the results are likely to be irrelevant
since the system will return pages mentioning the keywords and
not the matching products themselves. Current search systems
are inadequate for this task since they have been developed for a
totally different model, i.e., a web of documents. The shift from
documents to data objects poses new challenges for web search sys-
tems. One of the main challenge is to develop efficient and effective
methods for searching and retrieving data objects among decentralised data sources.

1.1. Information retrieval for web documents

Information retrieval systems for the web, i.e., web search
engines, are mainly devoted to finding relevant web documents
in response to a users query. Such a retrieval system is following

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

a document-centric model, since the whole system is organised
around the concept of document. The representation of a document which is commonly adopted is the full-text logical view [1]
where a document is seen as a set or sequence of words.

With the scale and nature of the web, information retrieval systems had to face new problems. Web search engines have to
provide access to billions of documents. Indexing and query processing at this scale requires advanced techniques to provide good
quality results in sub-second response time [2]. While information
retrieval systems for web data have to face similar problems, they
have to deal with an additional factor: the information and search
requests are semi-structured instead of being unstructured. Moving from unstructured to semi-structured information raises new
challenges but also provides new opportunities for developing
more advanced search techniques.

sations are needed to incorporate semi-structured information in
the system while sustaining a fast query computation and an efficient index maintenance. As a first step towards that goal, we propose in Section 4 a node-based inverted index that supports our
entity retrieval model. Then, in Section 5, we introduce a high-per-
formance compression technique which has tremendous benefits
with respect to the node-based indexing scheme. In Section 6,
we show that the combination of the compression technique with
the node-indexing scheme results in a high-performance retrieval
system that can index billions of entities while sustaining sub-sec-
ond response time.

In Section 2, we review existing works from domains such as
indexing and searching semi-structured data. Finally, Section 7 recalls the main findings of the research and discusses the tasks that
remain.

1.2. Information retrieval for web data

2. Related work

Compared to web documents, web data is providing some kind
of structure. However, since each of the web data sources has its
own defined schema, ranging from loosely to strictly defined, the
data structure does not follow strict rules as in a database. Even
in one data source, the schema might not be fixed and may change
as the information grows. The information structure evolves over
time, and new records can require new attributes. We therefore
consider web data as being semi-structured [3].

The semi-structured information items that are found on the
web are very diverse. These items can describe a person, a product,
a location, a document, etc. In fact, these items can describe any
kind of entities. We consider these items, which from here on we
will simply call entity descriptions, as the principal units of information to be searched and retrieved. Compared to a document, an entity description is a more complex data object which is composed
of a set of attribute and value pairs and possibly a set of relations to
other entities. We envision that the task of an information retrieval
system for web data is to provide a list of relevant entity descriptions in response to some information request which can be semistructured itself. This task is indeed a valuable one. A great fraction
of the users queries (more than half of them) are entity-centric [4],
and the potential of web data for this task is immense compared to
that of web documents. For these reasons, we organise our retrieval model around the concept of an entity as opposed to the concept of a document found in traditional web search engines, and
design the query language so to be able to support the needs of entity search (e.g., restrictions on attributes). We say that the retrieval system is following an entity-centric model.

1.3. Challenges in information retrieval for web data: contributions

In this article, we study the challenges in building a large scale
and high performance information retrieval system for web data.
By large scale, we mean a system that can be deployed on a very
large number of machines and that grows gracefully as the data
and user volume increases. By high-performance, we mean a system that handles a continuous flow of update requests and that answers many queries in sub-second time.

A first problem in building such a retrieval system is to cope
with the shift from the traditional document-centric model to
the entity-centric model. There is a need to reassess the retrieval
model as well as the boolean search model which does not provide
foundations for querying semi-structured data. Section 3 which
introduces our entity retrieval model undertakes precisely that
task.

Also, the retrieval system has to offer advanced search interfaces and provide an access to billions of entities in sub-second
response times. New index structures as well as new index optimi-

In this section, we first give a retrospective of retrieval systems
for structured documents, before presenting an overview of the
current approaches for indexing and querying RDF data. Finally,
we review existing search models for semi-structured and structured data.

2.1. Retrieval of structured documents

The problem with a traditional document retrieval system is its
inability to capture the structure of a document, since a document
is seen as a bag of words. In order to query the content as well as
the structure of the document, many models have been developed
to support queries integrating content (words, phrases, etc.) and
structure (for example, the table of contents). Amongst them, we
can cite the hybrid model, PAT expressions, overlapped lists and
proximal nodes. We refer the reader to [5] for a comparison of
these models. By mixing content and structure, more expressive
queries become possible such as relations between content and
structural elements or between structural elements themselves.
For example, it becomes possible to restrict the search of a word
within a chapter or a section, or to retrieve all the citations from
a chapter.

With the increasing number of XML documents published on
the web, new retrieval systems for the XML data model have been
investigated. Certain approaches [68] have investigated the use of
keyword-based search for XML data. Given a keyword query, these
systems retrieve document fragments and use a ranking mechanism to increase the search result quality. However, such systems
are restricted to keyword search and do not support complex
structural constraints. Therefore, other approaches [9,10] have
investigated the use of both keyword-based and structural con-
straints. In the meantime, various indexing techniques have been
developed for optimising the processing of XPath queries, the
standardised query language for XML [11]. The three major techniques are the node index scheme [1214], the graph index scheme
[1517], and the sequence index scheme [1822]. The node index
scheme relies on node labelling techniques [23] to encode the tree
structure of an XML document in a database or in an inverted in-
dex. Graph index schemes are based on secondary indexes that
contain structural path summaries in order to avoid join operations
during query processing. Sequence index schemes encode the
structure into string sequences and use string matching techniques
over these sequences to answer queries.

More recently, the Database community has investigated the
problem of search capabilities over semi-structured data in Dataspaces [24]. [25] proposes a sequence index scheme to support
search over loosely structured datasets using conditions on

attributes and values. However, the sequence indexing scheme is
inappropriate for large heterogeneous data collections as discussed
in Section 4.4.

2.2. Retrieval of RDF data

With the growth of RDF data published on the Semantic Web,
applications demand scalable and efficient RDF data management
systems. Different RDF storage strategies have been proposed
based on RDBMS [26], using native index [2730] or lately using
column-oriented DBMS using vertical partitioning over predicates
[31].

RDF data management systems excel in storing large amounts
of RDF data and are able of answering complex conjunctive SPARQL
queries involving large joins. Typical SPARQL queries are graph
patterns combining SQL-like logical conditions over RDF statements with regular-expression patterns. The result of a query is a
set of subgraphs matching precisely the graph pattern defined in
the query. This search and retrieval paradigm is well adapted for
retrieving data from large RDF datasets with a globally known
schema such as a social network graph or a product catalog data-
base. However, such approaches are of very limited value when
it comes to searching over highly heterogeneous data collections.
The variance in the data structure and in the vocabularies makes
it difficult to write precise SPARQL queries. In addition, none of
the above approaches uses structural and content similarity
between the query and the data graph for ordering results with
respect to their estimated relevance with the query.

Other approaches [3235] carried over keyword-based search
to Semantic Web and RDF data in order to provide ranked retrieval
using content-based relevance estimation. In addition, such systems are more easy to scale due to the simplicity of their indexing
scheme. However, such systems are restricted to keyword search
and do not support structural constraints. These systems do not
consider the rich structure provided by RDF data. A first step
towards a more powerful search interface combining imprecise
keyword search with precise structural constraints has been investigated by K-Search [36] and Semplore [37]. Semplore [37] has extended inverted index to encode RDF graph approximations and to
support keyword-based tree-shaped queries over RDF graphs.
However, we will see in Section 4.4 that the increase of query capabilities comes at a cost, and the scalability of the system becomes
limited.

2.3. Search models for semi-structured data

In addition to standardised query languages such as SPARQL,
XQuery or XPath, a large number of search models for semi-struc-
tured data can be found in the literature. Some of them focus on
searching structured databases [3841], XML documents [7,6,8]
or graph-based data [4244] using simple keyword search. Simple
keyword-based search has the advantages of (1) being easy to use
by users since it hides from the user any structural information of
the underlying data collection, and (2) of being applicable on any
scenarios. On the other hand, the keyword-based approach suffers
from limited capability of expressing various degrees of structure
when users have a partial knowledge about the data structure.

Other works [45,46,37,25,47] have extended simple keywordbased search with structured queries capabilities. In [25,47], they
propose a partial solution to the lack of expressiveness of the key-
word-based approach by allowing search using conditions on attributes and values. In [45,46,37], they present more powerful query
language by adopting a graph-based model. However, the increase
of query expressiveness is tied with the processing complexity, and
the graph-based models [45,46,37] are not applicable on a very
large scale.

The search model introduced in Section 3 is similar to [25,47],
i.e., it is defined around the concept of attributevalue pairs. How-
ever, our model is more expressive since it differentiates between
single and multi-valued attributes and it considers the provenance
of the information.

3. An entity retrieval model for web data2

In this section, we introduce an entity retrieval model for semistructured information found in distributed and heterogeneous
data sources. This model is used as a common framework to develop various methodologies for the entity retrieval system. For
example in this article, we use it to design our indexing system.
However, such a model has also been used for (1) designing a link
analysis technique [49] for measuring the importance of an entity
by exploiting the peculiar properties of links between datasets and
entities, and (2) distributed reasoning mechanism [48] which is
tolerant to low data quality. We start by examining the web data
scenario before introducing a data model that encompasses its core
concepts. We finally discuss some requirements for searching
semi-structured information and introduce our boolean search
model and query algebra.

3.1. An abstract model for web data

For the purpose of this work, we use the following model for
web data. We define web data as part of the Hypertext Transfer
Protocol (HTTP) [50] accessible web that returns semi-structured
information using standard interchange formats and practices.
The standard data interchange formats include HTML pages which
embed RDFa or Microformats as well as RDF models using different
syntaxes such as RDF/XML [51].

3.1.1. Practical use case

To support the web data scenario, we take as an example the
case of a personal web dataset. A personal web dataset is a set of
semi-structured information published on one personal web site.
The web site is composed of multiple documents, either using
HTML pages embedding RDFa or using plain RDF files. Each of these
documents contains a set of statements describing one or more
entities. By aggregating the semi-structured content of these documents altogether, we can recreate a dataset or RDF graph. Fig. 1
depicts such a case. The content, or database, of the web site
http://renaud.delbru.fr/ is exposed through various accessible online documents, among them http://renaud.delbru.fr/rdf/foaf and
http://renaud.delbru.fr/publications.html. The former document
provides information about the owner of the personal web site
and about its social relationships. The second document provides
information about the publications authored by the owner of the
web site. Each of them is providing complementary pieces of information which when aggregated provide a more complete view
about the owner of the web site.

3.1.2. Model abstraction

In the previous scenario, it is possible to abstract the following
core concepts in semi-structured data web publishing: dataset, entity and view:

A dataset is a collection of entity descriptions. One dataset is
usually the content of a database which powers a web application exposing metadata, be this a dynamic web site with just
partial metadata markups or a RDF database which exposes
its content such as the Linked Open Data datasets. Datasets

2 This section is partially based on [48].

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Fig. 1. A sample of a personal web dataset. The dataset http://renaud.delbru.fr/ makes available partial view of its content in the form of documents containing semistructured data. The aggregation of all the views enables the reconstruction of the dataset in the form of a data graph.

These abstract concepts are used to define a three layer model,
which is graphically represented in Fig. 2. This model forms a common base for various methodologies developed towards the creation of an entity retrieval system. As mentioned before, the
model is also used in other works [49,48].

3.2. Formal model

For the purpose of this work, we need a generic graph model
that supports the previous scenario and covers the three layer
model discussed previously. First, we define a labelled directed

however can also come in the form of a single RDF document,
e.g., an individual FOAF file posted on a persons homepage. A
dataset is uniquely identified by a URI, e.g., http://renaud.del-
bru.fr/ as depicted in Fig. 1.
An entity description is a set of assertions about an entity and
belongs to a dataset. The assertions provide information regarding the entity such as attributes, for instance, the firstname and
surname for a person, or relationships with other entities, for
instance, the family members for a person. An entity description has a unique identifier, e.g., a URI, with respect to a dataset.
A view represents a single accessible piece of information
which provides a full or partial view over the dataset content.
In the case of Linked Open Data, a typical view is the RDF model
returned when one dereferences the URI of an entity. The
Linked Open Data views are 1 to 1 mapping from the URI to
the complete entity description.3 This however is more the
exception than the rule for other kind of web data publishing
where most often only partial entity descriptions are provided
in the views. For example in the case of Microformats or RDFa,
views are pages that talk about different aspects of the entity,
e.g., a page listing social contacts for a person, a separate page
listing personal data as well as a page containing all the posts
by a user.

As shown in Fig. 1, the union of all the views provided within a
context, e.g., a web site, might enable an agent, e.g., a crawler, to
reconstruct the entire dataset. There is no guarantee on this since
it is impossible to know if we are in possession of every piece of
information about an entity. The problem of retrieving the views
and reconstructing a dataset is more related to data acquisition
which is out of scope of this article.

3 In Database terminology, this would be considered as a Record. However, we think
that the concept of View is more appropriate for the web data scenario given that
entity descriptions are in fact the result of a join query returning a set of triples.

Fig. 2. The three-layer model of web data.

graph model that covers the various type of web data sources, i.e.,
Microformats, RDFa, RDF databases, etc. This graph model represents datasets, entities and their relationships. With respect to
the graph model, we define an Entity AttributeValue model that
will shape the indexing system described in Section 4.

3.2.1. Data graph

We assume that a literal node is always internal to a dataset D. A
literal is always defined with respect to an entity and within the
context of a dataset. Therefore, if two literal nodes have identical
labels, we consider them as two different nodes in the graph G. This
assumption does not have consequences in this paper. In fact, this
can be seen as a denormalisation of the data graph which increases
the number of nodes in order to simplify data processing.

Let V be a set of nodes and A a set of labelled edges. The set of
nodes V is composed of two non-overlapping sets: a set of entity
nodes V E and a set of literal nodes V L. Let L be a set of labels composed of a set of node labels LV and a set of edge labels LA.

Web data is defined as a graph G over L, and is a tuple
G 14 hV; A; ki where k : V ! LV is a node labelling function. The set
labelled edges is defined as A # e; a; v j e 2 V E; a 2 LA;
of
v 2 Vg. The components of an edge a 2 A will be denoted by sour-
ce(a), label(a) and target(a) respectively.

3.2.2. Dataset

A dataset is defined as a subgraph of the web data graph:

Definition 1 (Dataset). A dataset D over a graph G 14 hV; A; ki is a
tuple D 14 hV D; AD;LV

D; ki with V D # V and AD # A.

We identify a subset LV

D #LV of node labels to be internal to a
dataset D, i.e., the set of entity identifiers and literals that originates
from this dataset. For example, such a set might include the URIs
and literals defined by the naming authority of the dataset [52].

Definition 2 (Internal node). A node v 2 V is said to be internal to
a dataset D if kv 2 LV
D.

Analogously, we identify as external a node with a label that

does not belong to LV
D.
Definition 3 (External node). A node v 2 V is said to be external to
a dataset D if kv R LV
D.

Two datasets are not mutually exclusive and their entity nodes
;, since (1) two datasets can
V E
D # V D may overlap, i.e., V E
contain identical external entity nodes, and (2) the internal entity
nodes in one dataset can be external entity nodes in another
dataset.

D1 \ V E

D2

3.2.3. Data links

While the notion of links is mainly used in link analysis scenario
[49], we describe it for the completeness of the model. In a dataset,
we identify two types of edges: intra-dataset and inter-dataset
edges. An intra-dataset edge is connecting two internal nodes, while
an inter-dataset edge is connecting one internal node with one
external node.

Definition 4 ( Intra-dataset edge). An edge a 2 AD is said to be
intra-dataset if ksourcea 2 LV

D; ktargeta 2 LV
D.

Definition 5 ( Inter-dataset edge). An edge a 2 AD is said to be
inter-dataset if ksourcea 2 LV
D or if ksource
a R LV

D; ktargeta R LV

D; ktargeta 2 LV
D.

We group inter-dataset links into linkset. For example, in Fig. 2
the inter-dataset links l1;3 between D1 and D3 are aggregated to
form the linkset L1;3 on the dataset layer.

Definition 6 (Linkset). Given two datasets Di and Dj, we denote a
linkset between Di and Dj with La;i;j 14 a j labela 14 a; sourcea 2
Di; targeta 2 Djg the set of edges having the same label a and
connecting the dataset Di to the dataset Dj.

Fig. 3. A visual representation of the RDF graph from Fig. 1 divided into three entities identified by the nodes me, _:b1 and paper/5.

Fig. 4. An example of EAV model derived from the three subgraphs from Fig. 3.

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

3.2.4. Entity AttributeValue model

A dataset provides information about an entity including its
relationships with other entities and its attributes. Consequently,
a subgraph describing an entity can be extracted from a dataset.

The simplest form of description for an (internal or external) entity node e is a star graph, i.e., a subgraph of a dataset D with one
central node e and one or more (inter-dataset or intra-dataset)
edges. Fig. 3 shows how the RDF graph from Fig. 1 can be split into
three entities me,
: b1 and paper/5. Each entity description forms a
sub-graph containing the incoming and outgoing edges of the entity node. More complex graphs might be extracted, but their discussion is out of scope of this paper.

D the entity node, Ae # e; a; v j a 2 LA

An entity description is defined as a tuple he; Ae; V ei where
e 2 V E
the set of labelled edges representing the attributes and V e # V D the set of
nodes representing values.

D; v 2 V e

Conceptually, an entity is represented by an identifier, a set of
attributes and a set of values for these attributes. Such a model is
similar to the Entity AttributeValue model (EAV) [53] which is
typically used for representing highly sparse data with many attri-
butes. Under this model, we can depict the dataset example from
Fig. 1 as a set of EAV tables in Fig. 4.

In its most simple form, an attribute is composed by one edge
connecting two nodes, the central entity node e and a value node
v. However, this is not always the case. More complex attributes
such as multi-valued attributes are composed of more than one
edge. We present and define in the following the possible types
of attributes we aim to support.

Attribute: An attribute represents an atomic characteristic of an
entity, for example the name of a person or the title of a publication.

Definition 7 (Attribute). Given an entity e, an attribute is defined
as being an edge e; a; v j a 2 LA; v 2 V e

Single-valued attribute: A single-valued attribute is an attribute
that holds exactly one value. For example, the birthdate is a sin-
gle-valued property since a person has only one birthdate.

Definition 8 (Single-valued attribute). Given an entity e, a singlevalued attribute is an attribute with exactly one value e; a;
v j 9!v targeta 14 v.

Multi-valued attribute: An attribute is multi-valued when it has
more than one value. For example, the email address of a person
can be a multi-valued attribute since a person has possibly more
than one email address.

Definition 9 (Multi-valued attribute). Given an entity e, a multivalued attribute is a set, with at least two members, of attributes
having the same label a 2 LA but with different value nodes.

Based on this Entity AttributeValue data model, we introduce a
boolean search model which enables the retrieval of entities. We
define formally this search model with a query algebra.

3.3. Search model

The search model presented here adopts a semi-structural logical view as opposed to the full-text logical view of traditional web
search engines. We do not represent an entity by a bag of words,
but we instead consider its set of attributevalue pairs. The search
model is therefore restricted to search entities using a boolean
combination of attributevalue pairs. We aim to support three
types of queries:

Fig. 5. A star-shaped query matching the description graph of the entity me from
Fig. 3. ? stands for the bound variable and q for a wildcard.

Full-text query: the typical keyword based queries, useful
when the data structure is unknown.
Structural query: complex queries specified in a star-shaped
structure, useful when the data schema is known.
Semi-structural query: a combination of the two where fulltext search can be used on any part of the star-shaped query,
useful when the data structure is partially known.

This gradual increase of query structure enables to accommodate various kind of information requests, from vague using fulltext queries to precise using structural queries. The type of request
depends on the awareness of the data structure by the user and on
the user expertise.

The main use case for which this search model is developed is entity search: given a description pattern of an entity, i.e., a star-shaped
queries such as the one in Fig. 5, locate the most suitable entities and
datasets. This means that, in terms of granularity, the search needs
to move from a document centric point of view (as per normal
web search) to a dataset-entity centric point of view.

3.3.1. Search query algebra

In this section, we introduce a formal model of our search query
language. For clarity reasons, we adopt a model which is similar to
the relational query algebra [54], where the inputs and outputs of
each operator are relations. All operators in the algebra accept
one or two relations as arguments and return one relation as a re-
sult. We first describe the relations that are used in the algebra before introducing the basic operators of the algebra.

Relations. For the purpose of the query algebra, we define an Entity AttributeValue table as a relation hdataset; entity; attribute;
valuei where the fields

Dataset: holds the label of a dataset D.
Entity: holds the label of the entity node e 2 V E
D.
Label: holds the attribute label a 2 LA.
Value: holds the label of the value node.

Table 1
An example of two Entity AttributeValue relations, R1 and R2.

(a) Entity AttributeValue relation R1
Dataset

Entity

delbru.fr
delbru.fr
delbru.fr

me
me
paper/5

(b) Entity AttributeValue relation R2
Dataset

Entity

delbru.fr
delbru.fr
delbru.fr

paper/5
paper/5
me

Attribute

name
knows
creator

Attribute

title
creator
name

Value

Renaud Delbru
_:b1
me

Value

ActiveRDF
me
Renaud Delbru

2a
2b
2c
2d

Table 2
An example showing the selection and projection operations.

(a) rv:renaudR1 or rv:renaud delbruR1
Dataset

Entity

delbru.fr
(b) pd;erv:renaudR1
Dataset

delbru.fr

me

Entity

me

Attribute

name

Value

Renaud Delbru

Attribute

Value

Table 1a and b depict two relations that are derived from the EAV
model of Fig. 4. These two relations are used as inputs in the following examples.

In the next algebra formulas, we will use d to denote the fields

dataset, e for entity, at for attribute and v for value.
Set operations. Since relations are considered as sets, boolean
operations such as union ([), intersection (\) and set-difference
(n) are applicable on relations.

Keyword selection. The search unit is a keyword k 2 K where K
is the lexicon of the web data graph, i.e., the set of distinct words
occurring in L.

Let W : L ! K a function that maps a label l 2 L to a set of
words K  K. Given a keyword k, if k 2 Wl, it means that the
word denoted by k appears at least one time in the label l. We
say that k matches l.

The Keyword selection operator r is a unary operator. The selection operator allows to specify the relation instances to retain
through a keyword selection condition.

Definition 10 (Keyword selection). Given a keyword selection
condition c and a relation R, the keyword selection operator
rcR is defined as a set of relation instances r j r 2 R
g for which
the condition c is true.

The most basic form of a keyword selection condition is to test if
a given keyword k occurs in one of the field f of a relation R, which
is denoted by f:k. For example, one can test if the keyword k occurs
in the dataset label of a relation instance R (denoted by r.d):
rd:kR : r j r 2 R; k 2 Wr:d
or in the value node label of a relation instance R (denoted by r.v):
rv:kR : r j r 2 R; k 2 Wr:v
as shown in Table 2a

The selection operator has the following properties. The proofs

of these properties can be found in [54].

Idempotent: multiple applications of the same selection operator have no additional effect beyond the first one as show in
Eq. (1a).
Commutative: the order of the selections has no effect on the
result as show in Eq. (1b).
Distributive: the selection is distributive over the set-differ-
ence, intersection and union operators as show in Eq. (1c),
where c 14 \;[ or n.
rf :krf :kR 14 rf :kR
1a
rf :k1rf :k2R 14 rf :k2rf :k1R
1b
rf :kRcS 14 rf :kRcrf :kS
1c
In general, the keyword selection condition is defined by a boolean combination of keywords using the logical operators ^, _ or :.
A keyword selection using a boolean combination of keywords is
identical to a boolean combination of keyword selections as shown
in Eqs. (2a)(2c). Also, two nested selections are equivalent to an
intersection of two selections as shown in Eq. (2d).

rf :k1R \ rf :k2R 14 rf :k1^f :k2R
rf :k1R [ rf :k2R 14 rf :k1_f :k2R
rf :k1R n rf :k2R 14 rf :k1:f :k2R
rf :k1rf :k2R 14 rf :k1^f :k2R

Dataset and entity projection. The projection operator p allows to extract specific columns, such as dataset or entity, from a relation. For
example, the expression:
pd;eR
returns a relation with only two columns, dataset and entity, as
shown in Table 2b.

The projection is idempotent, a series of projections is equivalent to the outermost projection, and is distributive over set union
but not over intersection and set-difference [54]:
perf :k1R1 \ rf :k2R2  perf :k1R1 \ perf :k2R2
perf :k1R1 [ rf :k2R2 14 perf :k1R1 [ perf :k2R2
perf :k1R1 n rf :k2R2  perf :k1R1 n perf :k2R2

4. Node-based indexing for web data4

In this section, we present the Semantic Information Retrieval
Engine, SIREn, a system based on information retrieval (IR) techniques and conceived to index web data and search entities in large
scale scenarios. The requirements have therefore been:

1. Support for the multiple formats which are used on the web of

data.

2. Support for entity centric search.
3. Support

for context

(provenance) of

information: entity

descriptions are given in the context of a website or dataset.

4. Support for semi-structural full text search, top-k query, incremental index maintenance and scalability via shard over clusters of commodity machines.

With respect to point 1, 2 and 3, we have developed SIREn to support the Entity AttributeValue model from Section 3.2.4 and the
boolean search model from Section 3.3, since these models cover
RDF, Microformats and likely other forms of semi-structured data
that can be found of the web. Finally, we will see in Section 4.3 that
the entity centric indexing enables SIREn to leverage well known
information retrieval techniques to address the point 4.

The section is organised as follows. We present the nodelabelled data model in Section 4.1 and the associated query model
in Section 4.2. We describe in Section 4.3 how to extend inverted
lists as well as update and query processing algorithms to support
the node labelled data model. An analysis of the differences and
theoretical performances between SIREn and other entity retrieval
systems is given in Section 4.4.

4.1. Node-labelled tree model

SIREn adopts a node-labelled tree model to capture the relation
between datasets, entities, attributes and values. The tree model is
pictured in Fig. 6a. The tree has four different kind of nodes: data-
set, entity, attribute and value. The organisation of the nodes in the
tree is based on the containment order of the concepts (i.e., dataset,
entity, attribute, value). Other tree layout would create unnecessary node repetitions. In fact, the tree encodes a Entity Attribute
Value table, where each branch of the tree represents one row of
the Entity AttributeValue table. Each node can refer to one or

4 This section is partially based on [55].

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Fig. 6. The node-labelled tree model. (a) Conceptual representation of the node-labelled tree model. (b) Node-labelled tree model using Deweys encoding of the example
dataset from Section 3.1.1.

more terms. In the case of RDF, a term is not necessarily a word
from a literal, but can be an URI or a local blank node identifier.
The incoming relations of an entity, i.e., all edges where the entity
node is the target, are symbolised by a attribute node with a1 tag
in Fig. 6b.

A node-labelled tree model enables one to encode and efficiently establish relationships between the nodes of a tree. The
two main types of relations are ParentChild and Ancestor
Descendant which are also core operations in XML query languages
such as XPath. To support these relations, the requirement is to
assign unique identifiers, called node labels, that encode the relationships between the nodes. Several node labelling schemes have
been developed [23] but in the rest of the paper we use a simple
prefix scheme, the Dewey Order encoding [56].

In Dewey Order encoding, each node is assigned a vector that
represents the path from the trees root to the node and each component of the path represents the local order of an ancestor node.
Using this labelling scheme, structural relationships between elements can be determined efficiently. An element u is an ancestor
of an element v if label(u) is a prefix of label(v). Fig. 6b depicts a
data tree where nodes have been labelled using Deweys encoding.
Given the label h1:1:1:1i for the term Renaud, we can find that its
parent is the attribute name, labelled with h1:1:1i.

The node labelled tree is embedded into an inverted index. The
inverted index stores for each term occurrence its node label, i.e.,
the path of elements from the root node (dataset) to the node
(attribute or value) that contains the term. A detailed description
of how this tree model is encoded into an inverted index is given
in Section 4.3.

4.2. Query model

In this section, we present a set of query operators over the content and the structure of the node-labelled tree which covers the
boolean search model presented in Section 3.3. We will present
the query operators of SIREn and whenever possible compare them
with the search query algebra from Section 3.3.1.

4.2.1. Content operators

The content operators are the only ones that access the content
of a node and are orthogonal to the structure operators. The atomic
search element is a keyword. Multiple keywords can be combined
with traditional keyword search operations. Such operations include boolean operators (intersection, union, difference), proximity
operators (phrase, near, before, after, etc.), fuzzy or wildcard oper-
ators, etc.

These operators give the ability to express complex keyword
queries. A keyword query is used to retrieve a particular set of
nodes. Interestingly, it is possible to apply these operators not only

on literals, but also on URIs, if URIs are tokenised and normalised.
For example one could just use an RDF local name, e.g., name, to
match foaf:name ignoring the namespace.

With respect to the search query algebra of Section 3.3.1, the content operators are mapped to the keyword selection operators. The
query algebra only defines the boolean operators, but it is easy to
see how to extend the algebra for including proximity or other oper-
ators. Also, the content operators allow to restrict keyword search to
a particular type of nodes, being either dataset, entity, attribute or
value. However, in the following we assume that this restriction is
implicit and thus is not shown in the following examples.

4.2.2. Structure operators

The structure operators are accessing the structure of the data
tree. The atomic search element is a node. Multiple nodes can be
combined using tree and set operators. The tree operators, i.e.,
the AncestorDescendant and ParentChild operators, allow to
query node relationships and to retrieve the paths matching a given pattern. The combination of paths are possible using set oper-
ators, enabling the computation of star-shaped queries such as the
one pictured in Fig. 5.

Ancestordescendant  A//D. A node A is the ancestor of a node D if
there exists a path between A and D. The operator checks the node
labels and retains only relations where the label of A is a prefix of
the label of D. For example, in Fig. 6b, the operation renaud.del-
bru.fr // paper/5 will retain the relations [1] // [1.1.3.1]
and [1] // [1.2]. With respect to the query algebra from Section 3.3.1, we interpret an AncestorDescendant operator as a keyword selection applied on a second operation. For example, Query
(Q4) in Appendix A can be interpreted as an AncestorDescendant
operator where rd:biblio is the ancestor and R1 \ R2 is the descendant.
ParentChild  P/C. A node P is the parent of a node C if P is an
ancestor of C and C is exactly one level above P. The operator
checks the node labels and retains only relations where the label
of P is the longest prefix matching the label of C. For example, in
Fig. 6b, the operation creator1 / paper/5 will retain the relation
[1.1.3] / [1.1.3.1]. With respect to the query algebra from Section 3.3.1, we also interpret a ParentChild operator as a keyword
selection applied on a second operation. Query (Q2) in Appendix A
can be interpreted as an ParentChild operator where rat:author is
the parent and rv:john^v:smith is the child.

Set manipulation operators. These operators allow the manipulation of nodes (dataset, entity, attribute and value) as sets, implementing union ([), difference (n) and intersection (\). These
operators are mapped one to the set operators found in the query
algebra from Section 3.3.1. For example, Query (Q3) in Appendix A
can be interpreted as an intersection R1 \ R2 between two Parent
Child operators, rat:authorrv:john^v:smith and rat:titlerv:search^v:engine.

Projection. The dataset and entity projection defined in Section 3.3.1 are simply performed by applying a filter over the node
labels in order to keep the dataset and entity identifiers and filter
out unnecessary identifiers such as the attribute or value identifier.

The strategy is to associate each term to a set of different inverted
files depending on which node the term appears as described next.
This depicted in Fig. 7.

4.3. Implementing the model

We describe in this section how the tree model is implemented
on top of an inverted index. We first describe how we extend the
inverted index data structure before explaining the incremental index updating and query processing algorithms. We refer the reader
to [55] which discusses how query results are ranked during query
processing.

4.3.1. Node-based inverted index

An inverted index is composed of (1) a lexicon, i.e., a dictionary
of terms that allows fast term lookup; and (2) of a set of inverted
lists, one inverted list per term. In a node-based inverted index,
the node labels, or Deweys vectors, that are associated with each
term are stored within the inverted lists. Compared to the traditional document-based inverted index, the difference is situated
in the structure of the inverted lists. Originally, an inverted list is
composed of a list of document identifiers, a list of term frequencies and a list of term positions. In our implementation, an inverted
list is composed of five different streams of integers: a list of entity
identifiers, of term frequencies, of attribute identifiers, of value
identifiers and of term positions. The term frequency corresponds
to the number of times the term has been mentioned in the entity
description. The term position corresponds to the relative position
of the term within the node.

However, it is unnecessary to associate each term to the five
streams of integers. For example, a term appearing in an attribute
node does not have a value identifier. Also, the probability to have
more than one occurrence of the same term in the entity, attribute
and dataset nodes is very low. Therefore, we assume that terms
from entity, attribute and dataset nodes always appear a single
time in the node. In that case, their term frequency is always equal
to one, and it becomes unnecessary to store it. By carefully selecting what information is stored for each term, the index size is reduced and the overall performance improves since less data has
to be written during indexing and read during query processing.

Dataset and entity: Terms from a dataset or entity node are
associated to a list of entity identifiers and to their relative position within the node as shown in Fig. 7a. The position of a term
within a node is necessary to support phrase and proximity
queries.
Attribute: Terms from an attribute node are associated to a list
of entity identifiers, a list of attribute identifiers and to their relative position within the node as shown in Fig. 7b.
Value: Terms from a value node are associated to a list of entity
identifiers, a list of term frequencies, a list of attribute identifi-
ers, a list of value identifiers and to their relative positions
within the node as shown in Fig. 7c. We consider that a term
can appear more than once in one or more value nodes. There-
fore, each entity identifier is associated to a variable number
(specified by the term frequency) of attribute identifiers, value
identifiers and positions.

Instead of storing the dataset identifier of the Deweys vector,
we are encoding the relation between dataset terms and entity
identifiers. This can be considered as a simplification of the data
model from Section 4.1. This approach only enables a partial support of dataset queries since queries such as Query (Q5) in Appendix A can not be answered efficiently. However, this reduces the
update complexity and enables more efficient incremental up-
dates. Such a choice is discussed more in details in Section 4.3.4.

4.3.2. Incremental update of the inverted lists

The proposed model supports incremental updates of entities as
it is performed for documents in traditional inverted indexes [57].
Adding an entity corresponds to adding a set of statements to the
inverted index. The statements are first transformed into a nodelabelled tree data model as in Fig. 6b. Then, for each term of the
tree, the associated inverted lists are accessed and updated with
respect to the Deweys vectors and positions of the term.

For example, to add a new occurrence of a term t from a value

node, the following operations are performed:

(a) Inverted files for dataset and
entity nodes

(b) Inverted files for attribute
nodes

(c) Inverted files for value nodes

Fig. 7. Diagram showing the set of inverted lists and their inter-connection for each type of terms.

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

1. The inverted files of t is accessed by performing a lookup in the

lexicon.

2. A new entry is appended to the list of entity identifiers, term
frequencies, attribute identifiers, value identifiers and positions.

If the same term t appears in a entity or dataset node, then the operations are similar than the previous ones with the difference that
only the list of entity identifiers and term positions is accessed
and updated.

The complexity of

insertion of one term occurrence is
Ologn  k, where the term log(n) denotes the cost of looking
up a term in a lexicon of n terms and the term k denotes the cost
of appending an integer to k inverted lists. The lexicon lookup is
the predominant cost during the insertion of one term occurrence.
However, updates are usually performed by batches of multiple
entities. In this case, the update complexity becomes linear with
the number of term occurrences since a lexicon lookup is only performed once per term.

Compared to common RDF databases, we do not support deletion on a statement granularity, but we support the deletion of a
dataset or entity. When an entity is removed, its identifier is inserted into a deletion table. When a dataset is deleted, the associated entity identifiers are inserted into the deletion table. During
query processing, each entity identifiers is checked against the
deletion table in O(1) to ensure that it has not been deleted. The
deletion table is integrated back to the inverted index only when
a certain amount of deletion is sufficient to amortize the cost of
a such maintenance operation.

4.3.3. Query processing

The evaluation of a query works in a bottom-up fashion. First,
matching on the content (terms) of a node is performed, then node
information is used during list intersection for filtering the result
candidates that do not belong to a same node or branch of the tree.
The intersection of two inverted lists is the most common operation during query processing. For example, a content operator
such as the boolean intersection and phrase proximity operators
relies on a list intersection. The structure operators such as the
AncestorDescendant, ParentChild and boolean intersection operators also rely on list intersection. The methodology is identical for
all of them and is described by the following merge algorithm:

1. The inverted list of each term is retrieved.
2. We position the pointers to the first element of each inverted

list.

3. We then walk through the inverted lists simultaneously. At

each step, we perform the following operations:

(a) We first compare their node information.
(b) If the comparison is positive,

(i) we add the entity identifier to the result list;
(ii) we advance the list pointers to their next position.

(c) If the comparison is negative, we move forward the list poin-

check a AncestorDescendant relation, then only the entity identifier is compared.

Given the query creator1/ paper/5, the query evaluation is
performed as follows. In the following examples, we display the
node information as a Deweys vector and omit the position information for simplicity.

1. Postings list fetching.

(a) Retrieve the inverted list

for

the term creator-1:

[1.0],[1.3].

(b) Retrieve the inverted list for the term paper/5: [1.3.1].

2. Inverted list merging.

(a) Position the pointers to the first element of the two lists.
(b) Compare the entity identifiers. The two entity identifiers are

equal to 1.

(c) Compare the attribute identifiers. The first pointer has an
attribute identifier <0> inferior to the second pointer <3>.
We move the first pointer to the next occurrence.

(d) Compare the attribute identifiers. This time, the two attribute identifiers are equal to 3. We have a match and add
the entity identifier in the result list.

The worst-case complexity of a query evaluation is in time linear to the total number of term occurrences in the inverted list
[58]. In the average case, the complexity of an intersection is reduced to sub-linear time with the use of self-indexing [59] over
the entity identifiers in order to skip and avoid unnecessary record
comparisons.

Each query operator delivers output in sorted order. Multiple
operators can be nested without losing the sorted order of the out-
put, therefore enjoying the concept of interesting orderings [60]
enabling the use of effective merge-joins without intermediate result sorting.

4.3.4. Handling dataset query

As we explained previously, the current implementation partially supports dataset queries. Dataset queries such as Query (Q4)
from Appendix A that restrict entity matching to a certain dataset
are possible. However, it is not possible to support efficiently dataset queries such as Query (Q5) that retrieve all datasets involving
two or more entity queries. The reason is that it is not possible at
query time to know the dataset identifier associated to each entity,
thus making impossible the intersection of the two entity relations
based on an equality condition over their dataset identifier.

In order to support such an intersection, we would have to encode the relation between the dataset node with all the other
nodes by storing a list of dataset identifiers in every inverted lists.
However, this will considerably increase the update complexity
since it will be necessary to keep a global order in the dataset identifier lists and a local order in the entity identifier lists within each
dataset. With such requirements, it is difficult to implement an
efficient incremental update procedure.

ter with the smallest identifier to its next position.

4.4. Comparison among entity retrieval systems

However, the comparison between node information that is
performed at step 3.a is slightly different depending on the query
operator employed. In the case of a boolean intersection between
two words, the algorithm compares first their entity identifiers,
then their attribute identifiers and finally compare their value
identifiers. In the case of a proximity operator, the position information is additionally compared. Concerning AncestorDescendant and ParentChild operators, the comparison is restricted to
the elements of the ancestor node to mimic the node label prefix
matching as explained in Section 4.1. For example, if a word from
a dataset node and a word from a value node are intersected to

In this section, we evaluate four entity retrieval systems: SIREn
based on a node-labelled index, field-based indexes [25], RDF databases [2730] based on quad tables and Semplore [37]. These
techniques are representative of the current approaches for entity
retrieval. Comparing under fair conditions these techniques with
an experimental benchmark is extremely difficult due to the differences in the implementations, programming languages, code
optimisations, and features for which these systems are designed
and built. In this work we therefore concentrate on providing a
theoretical comparison which is summarised in Table 3. However,
we refer the interested reader to [55] where we perform an

Table 3
Summary of comparison among the four entity retrieval systems.

Criteria

Dictionary lookup

Quad lookup

Field index

Node
index
Ologn Ologn  m
Ologn Ologn  m
Yes

Join in quad lookup
Star query evaluation Sub-linear Sub-linear

No

Update cost

Ologn Ologn  m
Multiple indices
No
Query expressiveness Star
Full-text
Yes
Multi-valued support Yes
Context
Precision (false

No
Star
Yes (on literals) No
Yes
No
Yes
Partial
Yes
No

Partial
No

Quad table

Semplore

Ologn
Ologn
Ologn  logk Ologn
No
On
Ologn  logk Ologn  logl
Yes
Graph

No
On  logn

Yes
Tree
Yes (on literals)
No
Partial
Yes

positive)

experimental benchmark to compare the performance of SIREn
against RDF databases with respect to incremental indexing and
structured query processing. As we can see in this paper, perhaps
one of the most striking difference is in the index maintenance
where inverted index shows a nearly-constant update time.

A quad table is a fairly conventional data structure in RDF database management systems that is composed of four columns s, p, o,
c called respectively subject, predicate, object and context.
In each table, quads are sorted and indexed by a particular set of
columns. This allows for the quick retrieval of quads that conform
to access patterns where any of these columns are specified. These
access patterns are the basic building blocks from which complex
SPARQL queries are constructed. In order to cover all access pat-
terns, multiple quad tables are necessary [27].

Field-based indexing schemes are generally used in standard
document retrieval systems such as Apache Lucene5 to support basic
semi-structured information like documents fields, e.g., the title. A
field index is a type of sequence index scheme (see Section 2.1) that
constructs lexicon terms by concatenating the field or attribute name,
e.g., the predicate URI, with the terms from the content of this field. For
example, in the graph depicted in Fig. 3, the index terms for the entity
giovanni and its predicate name will be represented as name:giovan-
ni and name:tummarello. In fact, the field index encodes the relation
between an attribute and a value directly within the lexicon.

Semplore is an information retrieval engine for querying
Semantic Web data which supports hybrid queries, i.e., a subset
of SPARQL mixed with full text search. Semplore is also built on inverted lists and relies on three inverted indexes: (1) an ontology index that stores the ontology graph (concepts and properties), (2) an
individual path index that contains information for evaluating path
queries, and (3) an individual content index that contains the content of the textual properties.

In the following, we assume that term dictionaries as well as
quad tables are implemented on top of a b+-tree data structure
for fast record lookups. The comparison is performed according
to the following criteria: Processing Complexity, Update Complexity,
Query Expressiveness and Precision. Processing Complexity evaluates
the theoretical complexity for processing a query (lookups, joins,
etc.). Update Complexity evaluates the theoretical complexity of
maintenance operations. Query Expressiveness indicates the type
of queries supported. Precision evaluates if the system returns
any false answers in the query result set.

4.4.1. Processing complexity

Since the field-based index encodes the relation between an
attribute and a value term in the dictionary, its dictionary may

5 Apache Lucene: http://lucene.apache.org/.

quickly become large when dealing with heterogeneous data. A
dictionary lookup has a complexity of Ologn  m where n is
the number of terms and m the number of attributes. This overhead can have a significant impact on the query processing time.
In contrast, the other systems has a term dictionary of size n and
thus a dictionary lookup complexity of Ologn.

To lookup a quad or triple pattern, the complexity of the node and
field index is equal to the complexity of looking up a few terms in the
dictionary. In contrast, RDF databases have to perform an additional
lookup on the quad table. The complexity is Ologn  logk with
logn the complexity to lookup a term in the dictionary and logk
the complexity to lookup a quad in a quad table, with k being the
number of quads in the database. In general, it is expected to have
considerably more quads than terms, with k generally much larger
than n. Therefore, the quad table lookup has a substantial impact
on the query processing time for very large data collection.

For quad patterns containing two or more terms, for example
(?c,?s,p, o), the node index has to perform a merge-join between
the posting lists of the two terms in order to check their relation-
ships. However, this kind of join can be performed on average in
sub-linear time. On the contrary, the other indexes do not have
to perform such a join, since the field index encodes the relationship between predicate and object in the dictionary, the quad table
in the b+-tree and Semplore in the inverted list for each term
occurrence (but only for URI terms and not literal terms). Further-
more, in Semplore, access patterns where the predicate is not specified trigger a full index scan which is highly inefficient.

For evaluating a star-shaped query (joining multiples quad pat-
terns), each index has to perform a join between the results of all
the patterns. Such a join is linear with the number of results in
the case of the quad table, and sub-linear in average for the node
and field index with the use of the self-indexing method [59]. In
contrast, Semplore has often to resort to possibly expensive external sort before merge-join operations.

4.4.2. Update complexity

In a b+-tree system the cost of insertion of one quad represents
the cost of searching the related leaf node, i.e., Ologn  logk,
the cost of adding a leaf node if there is no available leaf node
and the cost of rebalancing the tree. These costs become problematic with large indices and requires advanced optimisations [61]
that in return cause a degradation in query performance. In con-
trast, the cost of insertion for a node and field index is equal to
the cost of a dictionary lookup as discussed in Section 4.3.2, which
is Ologn and Ologn  m for the node index and the field index
respectively. Furthermore, quad tables are specific to access pat-
terns. Hence multiple b+-tree indexes, one for each access pattern,
have to be maintained which limits effective caching. Concerning
the size of the indexes, all of them are linear with the data.

Concerning Semplore, the original system could not perform
updates or deletions of triples without full re-indexing. The
authors have recently [37] proposed an extension for incremental
maintenance operations based on the landmark [62] technique
but the update complexity remains sustained. The update cost is
Ologn  logl with l the number of landmarks in the inverted
list. The fact that Semplore uses multiple indexes and landmarks
considerably increase the update complexity. For example, index
size and creation time reported in [37] are higher than for the
state-of-the-art RDF database RDF-3X [30].

4.4.3. Query expressiveness

In terms of query expressiveness, RDF databases have been designed to answer complex graph-shaped queries which are a superset of the queries supported by the other systems. On the other
hand, the other systems are especially designed to support natively
full-text search which is not the case for quad table indexes.

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Compared to field index and Semplore, the node index provides
more flexibility since it enables to keyword search on every parts
of a quad. In addition, node indexes support set operations on
every nodes, giving the ability to express queries over multi-valued
attributes. For example, a field-based index and Semplore cannot
process Query (Q1) from Appendix A without potentially returning
false-positive results.

While Semplore supports relational, tree shaped, queries, it
does not index relations between a resource and a literal. Hence,
it is not possible to restrict full-text search of a literal using a pred-
icate, e.g., queries such as (?s, <foaf:name>, "renaud").

The node indexing scheme, field-based indexing scheme and
Semplore only support dataset queries partially. The three systems
are using an identical technique that consists of encoding the relation between dataset terms with the entity identifiers as explained
in Section 4.3. This approach makes difficult the processing of the
dataset query (Q5).

4.4.4. Precision

The field indexing scheme encodes the relation between an
attribute and a value term in the index dictionary, but loses an
important structural information: the distinction between multiple values. As a consequence, the field index may return false-po-
sitive results for Query (Q1). Semplore suffers from a similar
problem: it aggregates all the values of an entity, disregarding
the attribute, into a single bag of words. On the contrary, the node
index and the quad table are able to distinguish distinct values and
do not produce wrong answers.

4.5. Conclusion on node-based indexing

We presented SIREn, a node-based indexing scheme for semistructured data. SIREn is designed for indexing very large datasets
and handling the requirements of indexing and querying web data:
constant time incremental updates and very efficient entity lookup
using semi-structural queries with full text search capabilities.
With respect to DBMS and IR systems, SIREn positions itself somewhere in the middle as it allows semi-structural queries while
retaining many desirable IR features: single inverted index, effective caching, top-k queries and efficient distribution of processing
across index shards.6

We have described its model and implementation and have
shown how it supports the entity retrieval model from Section 3.
We have also compared its theoretical performance with other entity retrieval systems, and shown that the node-based indexing
scheme offers a good compromise between query expressiveness,
query processing and index maintenance compared to other ap-
proaches. In the next section, we show how to considerably improve the overall performance of the system by developing a
high-performance compression technique which is particularly
effective with respect to the node-based inverted index.

5. High-performance compression for node indexing scheme

Inverted lists represent an important proportion of the total index size and are generally kept on disk. It is desirable to use compression techniques in order to reduce disk space usage and
transfer overhead. On the one hand, efficient decompression can
provide faster transfer of data from disk to memory, and therefore
faster processing, since the time of fetching and decompressing a
data segment is less than fetching an uncompressed form [63].
On the other hand, efficient compression can provide faster indexing since the time of compressing and writing a data segment is
less than the time to write an uncompressed form. To summarise,

6 An index shard is a particular subset of the entire index.

compression is useful for saving disk space, but also to maximise IO
throughput and therefore to increase the update and query
throughput.

In the past years, compression techniques have focussed on CPU
optimised compression algorithms [6467]. It has been shown in
[6567] that the decompression performance depends on the complexity of the execution flow of the algorithm. Algorithms that require branching conditions tend to be slower than algorithms
optimised to avoid branching conditions. In fact, simplicity over
complexity in compression algorithms is a key for achieving high
performance. The challenge is however to obtain a high compression rate while keeping the execution flow simple.

Previous works [65,68,66,69,70,67] have focussed solely on two
factors, the compression ratio and the decompression perfor-
mance, disregarding the compression performance. While decompression performance
throughput,
compression performance is crucial for update throughput. We
therefore propose to study compression techniques with an additional third factor, the compression performance. We show that
compression performance is also dependent on an optimised execution flow.

for query

is

essential

In this section, we introduce a high-performance compression
technique, the Adaptive Frame Of Reference (AFOR), which provides considerable benefits with respect to the node-indexing
scheme presented in the previous section. We compare our approach against a number of state-of-the-art compression techniques for
inverted indexes. We perform an experimental
evaluation based on three factors: indexing time, compression ratio and query processing time. We show that AFOR can achieves
significant performance improvements on the indexing time and
compression ratio while maintaining one of the fastest query processing time.

5.1. Background

In this section, we first introduce the block-based data structure
of an inverted list which is the backbone of the compression mech-
anism. We then recall the delta encoding technique for inverted
lists which is commonly employed before compression. Finally,
we describe five compression algorithms selected for the experiments and discuss their implementation.

5.1.1. Block-based inverted list

We now describe the implementation of an inverted file on disk.
For performance and compression efficiency, it is best to store separately each data stream of an inverted list [71]. In a non-inter-
leaved index organisation, the inverted index is composed of five
inverted files, one for each inverted lists. Each inverted file stores
contiguously one type of list, and five pointers are associated to
each term in the lexicon, one pointer to the beginning of the inverted list in each inverted file.

An inverted file is partitioned into blocks, each block containing
a fixed number of integers as shown in Fig. 8. Blocks are the basic
units for writing data to and fetching data from disk, but also the
basic data unit that will be compressed and decompressed. A block
starts with a block header. The block header is composed of the
length of the block in bytes and additional metadata information
that is specific to the compression technique used. Long inverted
lists are often stored across multiple blocks, starting somewhere
in one block and ending somewhere in another block, while multiple small lists are often stored into a single block. For example, 16
inverted lists of 64 integers can be stored in a block of 1024 inte-
gers. We use blocks of 1024 integers in our experiments, since this
was providing the best performance with respect to the CPU cache.
The performance of all the compression techniques decreased with
smaller block sizes.

Fig. 8. Inverted index structure. Each lexicon entry (term) contains a pointer to the beginning of its inverted list in the compressed inverted file. An inverted file is divided into
blocks of equal size, each block containing the same number of values.

5.1.2. Delta encoding of inverted lists

A more compact representation for node-based index structure
is to represent node labels as delta values, a technique first introduced in [72]. The key idea of the delta compression is to store
the difference between consecutive values instead of the values
themselves. This allows to encode an ordered list of integers using
much smaller integers, which theoretically can be encoded in less
bits. In a node-based indexing scheme, the delta values are much
smaller than those obtained in a document-based indexing. This
is due (1) to the usually more verbose and repetitive nature of
structured data, e.g., the same URI used multiple times, and (2)
to the locality of the attribute identifiers, the value identifiers
and the term positions. Compression techniques are then used to
encode the delta values with the smallest number of bits possible.

5.1.3. Algorithms for compressing inverted lists

Binary Interpolative Coding [73] has been shown to provide a
very good compression rate and it could have been a reference
for comparing compression rates. However, it is very inefficient
in decoding, and we found that Rice is competitive enough in terms
of compression rate to use it as a reference.

Rice coding. In Rice [74], an integer n is encoded in two parts: a
quotient q 14 b n
2bc and a remainder r 14 n mod 2b. The quotient is
stored in unary format using q  1 bits while the remainder is
stored in binary format using b bits. In our implementation, the
parameter b is chosen per block such that 2b is close to the average
value of the block.

The main advantage of Rice is its very good compression ratio.
However, it is in general the slowest method in terms of compression and decompression. The main reason is that Rice needs to
manipulate the unary word one bit at a time during both compression and decompression, which is costly in CPU cycles.

Variable byte coding (VByte). Variable byte compression encodes
an integer with a variable number of bytes. VByte is byte-aligned,
i.e., coding and decoding is done one byte at a time. Each byte consists of 7 bits to encode the partial binary representation of the
integer, and one bit used as status flag to indicate if the following
byte is part of the current number.

The advantages of VByte are: (1) it is simple to implement; and
(2) its overall compression and decompression performance are
good. Compared to bitwise techniques like Rice, VByte requires a
single branching condition for each byte which is more CPU cost-
effective. However, the branching condition leads to branch mispredictions which makes it slower than CPU optimised techniques
such as the one presented next. Moreover, VByte has a poor compression ratio since it requires one full byte to encode one small
integer (i.e., 8n j n < 27).

Simple coding family. The idea behind the simple coding is to
pack as many integers as possible into one machine word (being
32 or 64 bits). We describe one simple coding method (referred
to as S-64 in our experiments) based on 64-bit machine words, recently introduced in [67]. In our experiments, we report only S-64
results since its performance was always superior to Simple9 [65].
In S-64, each word consists of 4 status bits and 60 data bits. The 4

status bits are used to encode one of the 16 possible configurations
for the data bits. A description of the 16 configurations can be
found in [67]. S-64 wastes generally less bits than Simple9 and
therefore provides a better compression ratio.

In addition to providing good compression ratio, decompression
is done efficiently by reading one machine word at a time and by
using a precomputed lookup table over the status bits in order to
select an optimised routine (one routine per configuration) to decode the data bits using shift and mask operations only. However,
one disadvantage is that compression cannot be done efficiently.
The typical implementation is to use a sliding window over the
stream of integers and to find the best configuration, i.e., the one
providing the best compression ratio, for the current window. This
generally requires repetitive try and error iterations over the possible configurations for each new window. In addition, Simple coding has to perform one table lookup per machine word and
consumes more CPU cycles than the techniques presented next.

Frame Of Reference (FOR). FOR determines the range of possible
values in a block, called a frame, and maps each value into this
range by storing just enough bits to distinguish between the values
[64]. Given a frame [min, max], FOR needs dlog2max  min  1e
bits, that we call bit frame in the rest of the paper, to encode each
integer in a block. In the case of the delta-encoded list of values,
since the probability distribution generated by taking the delta
tends to be naturally monotonically decreasing, one common practice [66,67] is to choose as frame the range [0, max] where max is
the largest number in the group of delta values.7

The main disadvantage of FOR is that it is sensitive to outliers in
the group of values. For example, if a block of 1024 integers contains
1023 integers inferior to 16, and one value superior to 128, then the
bit frame will be dlog2128  1e 14 8, wasting 4 bits for each other
values. However, compression and decompression is done very efficiently using highly-optimised routines [66] which avoid branching
conditions. Each routine is loop-unrolled to encode or decode m values using shift and mask operations only. Listings 1 and 2 show the
routines to encode or decode 8 integers with a bit frame of 3. There is
a compression and decompression routine for each bit frame.

Given a block of n integers, FOR determines a frame of reference
for the block and encodes the block by small iterations of m integers using the same compression routine at each iteration. Usually,
and for questions of performance, m is chosen to be a multiple of 8
so that the routines match byte boundaries. In our implementation,
FOR relies on routines to encode and decode 32 values at a time.
The selection of the appropriate routine for a given bit frame is
done using a precomputed lookup table. The compression step performs one pass only over the block to determine the bit frame.
Then, it selects the routine associated to the bit frame using the
lookup table. Finally, the bit frame is stored using one byte in the
block header and the compression routine is executed to encode
the block. During decompression, FOR reads the bit
frame,

7 This assumes that a group of values will always contain 0, which is not always the
case. However, we found that taking the real range [min, max] was only reducing the
index size by 0.007% while increasing the complexity of the algorithm.

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Listing 1
Loop unrolled compression routine that encodes 8 integers using 3 bits each.

Listing 2
Loop unrolled decompression routine that decodes 8 integers represented by 3 bits
each.

performs one table lookup to select the decompression routine and
executes iteratively the routine over the compressed block.

Patched Frame Of Reference (PFOR). PFOR [66] is an extension of
FOR that is less vulnerable to outliers in the value distribution. PFOR
stores outliers as exceptions such that the frame of reference [0, max]
is greatly reduced. PFOR first determines the smallest max value such
that the best compression ratio is achieved based on an estimated
size of the frame and of the exceptions. Compressed blocks are divided in two: one section where the values are stored using FOR, a
second section where the exceptions, i.e., all values superior to
max, are encoded using 8, 16 or 32 bits. The unused slots of the exceptions in the first section are used to store the offset of the next exceptions in order to keep a linked list of exception offsets. In the case
where the unused slot is not large enough to store the offset of the
next exceptions, a compulsive exception [66] is created. Instead, we
use the non-compulsive approach proposed in [70], where the
exceptions are stored along with their offset in the second block sec-
tion, since it has been shown to provide better performance. During
our experimentations, we tried PFOR with frames of 1024, 128 and
32 values. With smaller frames, the compression rate was slightly
better. However, the query time performance was decreasing. We
therefore decided to use PFOR with frames of 1024 values as it was
providing a good reference for query time.

The decompression is performed efficiently in two phases.
First, the list of values are decoded using the FOR routines. Then,

the list of values is patched by: (1) decompressing the exceptions
and their offsets and (2) replacing in the list the exception val-
ues. However,
the compression phase cannot be efficiently
implemented. The main reason is that PFOR requires a complex
heuristic that require multiple passes over the values of a block
in order to find the frame and the set of exceptions providing
the highest compression.

Vector of Split Encoding. At the time of the writing, we discovered the Vector of Split Encoding (VSE) [75] which is similar to
AFOR. The two methods can be considered as an extension of
FOR which are less sensitive to outliers by adapting their encoding
to the value distribution. To achieve this, the two methods are
encoding a list of values by partitioning it into frames of variable
lengths and rely on algorithms to automatically find the list parti-
tioning. AFOR relies on a local optimisation algorithm for partitioning a list, while VSE adds a Dynamic Programming method to find
the optimal partitioning of a list.

For the purpose of our comparison, we use AFOR to show how
adaptive techniques can be peculiarly effective with respect to
node-based indexes. Given its almost identical nature, we can expect the results to be very close to those of a VSE implementation.

5.2. Adaptive Frame Of Reference

Adaptive Frame Of Reference (AFOR) attempts to retain the best
of FOR, i.e., a very efficient compression and decompression using
highly-optimised routines, while providing a better tolerance
against outliers and therefore achieving a higher compression ra-
tio. Compared to PFOR, AFOR does not rely on the encoding of
exceptions in the presence of outliers. Instead, AFOR partitions a
block into multiple frames of variable length, the partition and
the length of the frames being chosen appropriately in order to
adapt the encoding to the value distribution.

To elaborate, AFOR works as follow. Given a block B of n inte-
gers, AFOR partitions it into m distinct frames and encodes each
frame using highly-optimised routines. Each frame is independent
from each other, i.e., each one has its own bit frame, and each one
encodes a variable number of values. This is depicted in Fig. 9 by
AFOR-2. Along with each frame, AFOR encodes the associated bit
frame with respect to a given encoder, e.g., a binary encoder. In
fact, AFOR encodes (resp., decodes) a block of values by:

1. Encoding (resp., decoding) the bit frame.
2. Selecting the compression (resp., decompression) routine asso-

ciated to the bit frame.

3. Encoding (resp., decoding)

the frame using the selected

routine.

Finding the right partitioning, i.e., the optimal configuration of
frames and frame lengths per block, is essential for achieving high
compression ratio [75]. If a frame is too large, the encoding becomes
more sensitive to outliers and wastes bits by using an inappropriate
bit frame for all the other integers. On the contrary, if the frames are
too small, the encoding wastes too much space due to the overhead

Fig. 9. Comparison of block compression between FOR and AFOR. We alternate colours to differentiate frames. AFOR-1 denotes a first implementation of AFOR using a fixed
frame length. AFOR-2 denotes a second implementation of AFOR using variable frame lengths. AFOR-3 denotes a third implementation using variable frame lengths and the
frame stripping technique. BFS denotes the byte storing the bit frame selector associated to the next frame.

of storing a larger number of bit frames. Also, alternating between
large and small frames is not only important for achieving high compression ratio but also for achieving high performance. If frames are
too small, the system has to perform more table lookups to select the
appropriate routine associated to each frame, and as a consequence
the compression and decompression performance decrease. The
appropriate strategy is to rely on large frames in the presence of a
dense sequence of values, and on small frames in the presence of
sparse sequence of values. To find a block partitioning, our solution
uses a local optimisation algorithm which is explained next.

5.2.1. Partitioning blocks into variable frames

Finding the optimal configuration of frames and frame lengths
for a block of values is a combinatorial problem. For example, with
three different frame lengths (32, 16 and 8) and a block of size
1024, there are 1:18 
 1030 possible combinations. While such a
combinatorial problem can be solved via Dynamic Programming
algorithms [75],
the complexity of such algorithms is still
On 
 k, with the term n being the number of integers and the
term k the size of the larger frame, and therefore greatly impacts
the compression performance. We remind the reader that we are
interested not only by fast decompression speed and high compression ratio, but also by fast compression speed. Therefore, in
our experiments, we do not rely on the optimal configuration. In-
stead, we use a local optimisation algorithm that provides a satisfactory compression rate and that is efficient to compute.

[32],

[16,16],

[16,8,8],

[8,16,8],

AFOR computes the block partitioning by using a sliding window over a block and determines the optimal configuration of
frames and frame lengths for the current window. Given a window
of size w and a list of possible frame lengths, we compute beforehand the possible configurations. For example, for a window size of
32 and three different frame lengths, 32, 16 and 8, there are six
configurations:
[8,8,16],
[8,8,8,8]. The size of the window as well as the number of possible
frame lengths are generally chosen to be small in order to reduce
the number of possible configurations. Then, we first compute
the bit frames of the smallest frames by doing one pass over the
values of the window as shown in the Algorithm 1 (lines 15).
On the previous example, this means that we compute the bit
frames for the configuration [8,8,8,8]. The bitFrames array stores
the bit frame for each of frame of this configuration. Given these bit
frames, we are able to compute the bit frames of all the other
frames. The second step, lines 612 in Algorithm 1, iterates over
the possible configurations and estimates the size of each configuration in order to find the optimal one for the current window. Gi-
ven
the
EstimateSize function computes the cost of encoding the window for a given configuration, accounting also the overhead of
storing the bit
the configuration
[8,8,8,8] with four frames of size 8 each, and with four associated

bit frames, b1 to b4, the size of the encoding is computed as follow:
4 
 8  8 

i141...4bi is the size (in bits) of the
four encoded frames and 4 
 8 is the overhead (in bits) to store the
four bit frames.

i141...4bi, where 8 


previously

frames. For example,

for

the

computed

bitFrames

array,

This simple algorithm is efficient to compute, in particular if the
window size is small and if there is a few number of possible frame
lengths. However, it is easy to see that such a method does not provide the optimal configuration for a complete block. There is a
trade-off between optimal partitioning and complexity of the algo-
rithm. One can possibly use a more complex method for achieving
a higher compression if the compression speed is not critical. How-
ever, this is not the case for a web search engine where high update
throughput is crucial. We decided to use this method since in our
experiments we found that a small window size of 32 values and
three frame lengths, 32, 16 and 8, were providing satisfactory re-

sults in terms of compression speed and compression ratio. More
details about our implementations of AFOR are given next.

5.2.2. Frame stripping

In an inverted list, it is common to encounter a long sequence of
1 to encode. For example, this occurs with terms that appear frequently in many entities. With RDF data, such a very common term
might be a predicate URI or a ubiquitous class URI. As a conse-
quence, the list of entity identifiers is composed of many consecutive identifiers, which is encoded as a list of 1 using the delta
representation. Also, the schema used across the entity descriptions coming from a same dataset is generally similar. When indexing batch of entities coming from a same dataset, we benefit from a
term clustering effect: all the schema terms are associated with
long runs of consecutive entity identifiers in the inverted index.
There is also other cases where a long run of 1 is common, for
example in:

 The list of term frequencies for terms that appear frequently a

single time in the entity description, e.g., class URIs.

 The list of value identifiers for terms that appear frequently in

single-valued attributes.

 The list of term positions for nodes holding a single term, e.g.,

URIs.

In presence of such long runs of 1, AFOR still needs to encode
each value using 1 bit. For example, a frame of 32 values will encode a sequence of 1 using 32 bits. The goal of the Frame Stripping
method is to avoid the encoding of such frames. Our solution is to
strip the content of a frame if and only if the frame is exclusively
composed of 1. We encode such a case using a special bit frame.

5.2.3. Implementation

We present three different implementations of the AFOR encoder class. We can obtain many variations of AFOR by using various
sets of frame lengths and different parameters for the partitioning
algorithm. We tried many of them during our experimentation and
report here only the ones that are promising and interesting to
compare.

AFOR-1. The first implementation of AFOR, referred to as AFOR-1
and depicted in Fig. 9, is using a single frame length of 32 values. To

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

clarify, this approach is identical to FOR applied on small blocks of
32 integers. This first implementation shows the benefits of using
short frames instead of long frames of 1024 values as in our original FOR implementation. In addition, AFOR-1 is used to compare
and judge the benefits provided by AFOR-2, the second implementation using variable frame lengths. Considering that, with a fixed
frame length, a block is always partitioned in the same manner,
AFOR-1 does not rely on the partitioning algorithm presented
previously.

AFOR-2. The second implementation, referred to as AFOR-2 and
depicted in Fig. 9, relies on three frame lengths: 32, 16 and 8. We
found that these three frame lengths give the best balance between
performance and compression ratio. Additional frame lengths were
rarely selected and the performance decreased due to the larger
number of partitioning configurations to compute. Reducing the
number of possible frame lengths was providing slightly better
performance but slightly worse compression ratio. There is a
trade-off between performance and compression effectiveness
when choosing the right set of frame lengths. Our implementation
relies on the partitioning algorithm presented earlier, using a window size of 32 values and six partitioning configurations [32],
[16,16], [16,8,8], [8,16,8], [8,8,16], [8,8,8,8].

AFOR-3. The third implementation, referred to as AFOR-3 and
depicted in Fig. 9, is identical to AFOR-2 but employs the frame
stripping technique. Compared to AFOR-2, the compressed block
can contain frames encoded by a single bit frame as depicted in
Fig. 9. AFOR-3 implementation relies on the same partitioning
algorithm as AFOR-2 with an additional step to find and strip
frames composed of a sequence of 1 in the partitions.

Compression and decompression routines. Our implementations
rely on highly-optimised routines such as the ones presented in
Listings 1 and 2, where each routine is loop-unrolled to encode
or decode a fixed number of values using shift and mask operations
only. There is one routine per bit frame and per frame length. For
example, for a frame length of 8 values, the routine encodes 8 values using 3 bits each as shown in Listing 1, while for a frame length
of 32, the routine encodes 32 values using 3 bits each.

Since AFOR-1 uses a single frame length, it only needs 32 routines for compression and 32 routines for decompression, i.e.,
one routine per bit frame (132). With respect to AFOR-2, since
it relies on three different frame lengths, it needs 96 routines for
compression and 96 routines for decompression. With respect to
AFOR-3, one additional routine for handling a sequence of 1 is
added per frame length. The associated compression routine is
empty and does nothing since the content of the frame is not en-
coded. Therefore the cost is reduced to a single function call. The
decompression routine consists of returning an array of 1. Such
routines are very fast to execute since there are no shift or mask
operations.

Bit frame encoding. Recall that the bit frame is encoded along
with the frame, so that, at decompression time, the decoder can
read the bit frame and select the appropriate routine to decode
the frame. In the case of AFOR-1, the bit frame varies between 1
and 32. For AFOR-2, there are 96 cases to be encoded, where cases
132 refer to the bit frames for a frame length of 8, cases 3363 for
a frame length of 16, and cases 6496 for a frame length of 32. In
AFOR-3, we encode one additional case per frame length with respect to the frame stripping method. Therefore, there is a total of
99 cases to be encoded. The cases 97 to 99 refer to a sequence of
1 for a frame length of 8, 16 and 32 respectively.

In our implementation, the bit frame is encoded using one byte.
While this approach wastes some bits each time a bit frame is
stored, more precisely 3 bits for AFOR-1 and 1 bits for AFOR-2
and AFOR-3, the choice is again for a question of efficiency. Since
bit frames and frames are interleaved in the block, storing the bit
frame using one full byte enables the frame to be aligned with

the start and end of a byte boundary. Another implementation to
avoid wasting bits is to pack all the bit frames at the end of the
block. We tried this approach and report that it provides slightly
better compression ratio, but slightly worse performance. Since
the interleaved approach was providing better performance, we
decided to use it in our experiment.

Routine selection. A precomputed lookup table is used by the encoder and decoder to quickly select the appropriate routine given a
bit frame. Compared to AFOR-1, AFOR-2 and AFOR-3 have to perform more table lookups for selecting routines since they are likely
to rely on small frames of 8 or 16 values when the value distribution is sparse. While these lookups cost additional CPU cycles, we
will see in the experiments that the overhead is minimal.

5.3. Experiments

This section describes the benchmark experiments which aim
to compare the techniques introduced by AFOR with the compression methods described in Section 5.1.3. The first experiment measures the indexing performance based on two aspects: (1) the
indexing time; and (2) the index size. The second experiment compares the query execution performance.

Experimental settings. The hardware system we use in our experiments is a 2 x Opteron 250 @ 2.4 GHz (2 cores, 1024 KB of cache
size each) with 4GB memory and a local 7200 RPM SATA disk.
The operating system is a 64-bit Linux 2.6.31-20-server. The version of the Java Virtual Machine (JVM) used during our benchmarks
is 1.6.0_20. The compression algorithms and the benchmark platform are written in Java and based on the open-source project
Apache Lucene.8

Experimental design. Each measurement was made by (1) flushing the OS cache; (2) initialising a new JVM and (3) warming the
JVM by executing a certain number of times the benchmark. The
JVM warmup is necessary in order to be sure that the OS and the
JVM have reached a steady state of performance, e.g., that the critical portion of code is JIT compiled by the JVM. The implementation
of our benchmark platform is based on the technical advice from
[76], where more details about the technical aspects can be found.
Data collection. We use three real web datasets for our

comparison:

Geonames: a geographical database and contains 13.8 million
of entities.9 The size is 1.8 GB compressed.
DBPedia: a semi-structured version of Wikipedia and contains
17.7 million of entities.10 The size is 1.5 GB compressed.
Sindice: a sample of the data collection currently indexed by
Sindice. There is a total of 130,540,675 entities. The size is
6.9 GB compressed.

We extracted the entity descriptions from each dataset as pic-

tured in Fig. 3.

5.3.1. Indexing performance

The performance of indexing is compared based on the index
size (compression ratio), commit time (compression speed) and
optimise time (compression and decompression speed). The indexing is performed by adding incrementally 10,000 documents at a
time (which is similar to what others have done [77]) and finally
by optimising the index. For each batch of documents, the commit
operation creates a small inverted index (called an index segment).
The optimisation merges all the index segments into a single

8 Apache Lucene: http://lucene.apache.org/.
9 Geonames: http://www.geonames.org/.
10 DBpedia: http://dbpedia.org/.

Fig. 10. The total time spent to commit batches of 10,000 document.

Fig. 11. The total time spent to optimise the complete index.

segment. We believe this to be a common operation for incremental inverted index.

We report the results of the indexing experiments in Table C.5.
The table comprises two columns with respect to the indexing
time: the total commit time (Total) to add all the documents and
the optimisation time (Opt). The time collected is the CPU time
used by the current thread and comprises the user time and the
system time. The index size in Table C.5 is studied based on the
size of the individual inverted file (entity, frequency, attribute, value and position) and on the total index size (by summing the size
of the five inverted files). We also provide bar plots to visualise better the differences between the techniques.

Commit time. Fig. 10 shows the total time spent by each method.
As might be expected, Rice is the slowest method due to its execution flow complexity. It is followed by FOR and PFOR. We can notice the inefficiency of PFOR in terms of compression speed. On a
large dataset (Sindice), VByte is the best-performing method while
AFOR-1, AFOR-2, AFOR-3 and S-64 provide a similar commit time.
On DBpedia, AFOR-1 and AFOR-2 are the best performing methods.
On Geonames, AFOR-1, AFOR-2 and AFOR-3 are the best performing methods, while S-64 and VByte perform similarly. On smaller
datasets (DBpedia and Geonames), VByte, FOR and PFOR perform
similarly.

Optimisation time. Fig. 11 shows the optimise time for each
methods. The time to perform the optimisation step is quite different due to the nature of the operation. The optimisation operation
has to read and decompress all the index segments and compress
them back into a single segment. Therefore, decompression performance is also an important factor, and algorithms having good
decompression speed becomes more competitive. For example,
while FOR and PFOR was performing similarly to Rice in terms of
indexing time on the Sindice dataset, it is ahead of Rice in terms
of optimisation time. Rice is penalised by its low decompression
speed. Similarly, S-64 provides close or even better optimisation

performance than VByte due to its faster decompression. On smaller datasets (DBpedia and Geonames), VByte is performing well
due to its good compression and decompression speed. However,
we can notice on a large dataset (Sindice) the compression ratio
and the decompression speed of VByte incur a large overhead.
The best-performing methods are AFOR-1, AFOR-2 and AFOR-3,
with AFOR-3 performing better on large datasets. The AFOR techniques take the advantage due their optimised compression and
decompression routines and their good compression rate. AFOR-3
is even twice as fast as Rice on the Sindice dataset.

Compression ratio. Fig. 12 shows the total index size achieved by
each method. We can clearly see the inefficiency of the VByte ap-
proach. While VByte performs generally better than FOR on traditional document-centric inverted indexes, this is not true for
inverted indexes based on a node indexing scheme. VByte is not
adapted to such an index due to the properties of the delta-en-
coded lists of values. Apart from the entity file, the values are generally very small and the outliers are rare. In that case, VByte is
penalised by its inability of encoding a small integer in less than
a byte. On the contrary, FOR is able to encode many small integers
in one byte. Also, while PFOR is less sensitive to outliers than FOR,
the gain of compression rate provided by PFOR is minimal since
outliers are more rare than in traditional inverted indexes. In con-
trast, AFOR and S-64 are able to better adapt the encoding to the
value distribution and therefore provide a better compression rate.
AFOR is even able to provide better compression ratio than Rice on
the Geonames and Sindice dataset. Compared to AFOR-2, we can
observe in Table C.5 that AFOR-3 provides better compression rate
on the frequency, value and position files, and slightly better on the
entity file. This result corroborates the existence of long runs of 1 in
these files, as explained in Section 5.2.2.

Conclusion on indexing performance. The indexing experiment
shows that the compression speed is also an important factor to
take into consideration when designing a compression algorithm

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Fig. 12. The index size achieved by each compression technique.

for an inverted index. Without a good compression speed, the update throughput of the index is limited. Also the experiment shows
that the optimisation operation is dependent of the decompression
performance, and its execution time can double without a good
compression and decompression speed. With respect to index opti-
misation, the compression ratio must also be taken into consider-
ation. While VByte provides in general correct commit times, we
can observe on a large dataset (Sindice) that its performance during optimisation is limited by its poor compression ratio. Overall,
the method providing the best balance between indexing time,
optimise time and compression ratio is AFOR. AFOR-1 provides fast
compression speed and better compression ratio than FOR and
PFOR. AFOR-2 provides a notable additional gain in compression
ratio and optimise time but undergoes a slight increase of indexing
time. AFOR-3 provides another additional gain in compression ratio while providing better compression speed than AFOR-2.

5.3.2. Query processing performance

We now compare the decompression performance in real set-
tings, where inverted indexes are answering queries of various
complexities. We focus on two main classes of queries, the value
and attribute queries, which are the core elements of a star-shaped
query. Among these two classes, we identify types of keyword queries which represent the common queries received by a web search
engine: conjunction, disjunction and phrase. Under our model,
these two classes of queries are the most complex ones and therefore cover the other cases (e.g., selection of a dataset or entity
name and of a value).

Query generation. The queries are generated based on the
selectivity of the words composing them. The word selectivity
determines how many entities match a given keyword. The
words are grouped into three selectivity ranges: high, medium
and low. We differentiate also two groups of words based on
their position in the data graph: attribute and value. We follow
the technique described in [78] to obtained the ranges of each
word group. We first order the words by their descending fre-
quency, and then take the first k words whose cumulative frequency is 90% of all word occurrences as high range. The
medium range accounts for the next 10%, and the low range is
composed of all the remaining words. For the phrase queries,
we follow a similar technique. We first extract all the 2-gram
and 3-gram11 from the data collection. We then compute their
frequency and sort them by descending frequency. We finally create the three ranges as explained above.

Value queries. Value queries are divided into three types of keyword queries: conjunction, disjunction and phrase queries. These

11 A n-gram is n words that appear contiguously.

queries are restricted to match within one single value, similar to
Query (Q1). Therefore, the processing of conjunction and disjunction queries relies on the entity, frequency, attribute and value inverted files. Phrase queries rely on one additional inverted file, the
position inverted file.

Conjunction and disjunction queries are generated by taking
random keywords from the high range group of words. 2-AND
and 2-OR (resp. 4-AND and 4-OR) denotes conjunction and disjunction queries with 2 random keywords (resp. 4 random key-
words). Similarly, a phrase query is generated by taking random
n-grams from the high range group. 2-Phrase (resp. 3-Phrase)
denotes phrase queries with 2-gram (resp. 3-gram). Benchmarks
involving queries with words from low and medium ranges are
not reported here for questions of space, but the performance results are comparable with the ones presented here.

Attribute queries. An attribute query is generated by associating
one attribute keyword with one value query. An attribute keyword
is randomly chosen from the high range groups of attribute words.
The associated value query is obtained as explained previously. An
attribute query intersects the result of a value query with an attribute keyword.

Query benchmark design. For each type of query, we (1) generate
a set of 200 random queries which is reused for all the compression
methods, and (2) perform 100 measurements. Each measurement
is made by performing n times the query execution of the 200 random queries, with n chosen so that the runtime is long enough to
minimise the time precision error of the OS and machine (which
can be 110 ms) to a maximum of 1%. All measurements are made
using warm cache, i.e., the part of the index read during query processing is fully loaded in memory. The measurement time is the
CPU time, i.e., user time and system time, used by the current
thread to process the 200 random queries.

Query execution time is sensitive to external events which can affect the final execution time recorded. For instance, background system maintenance or interruptions as well as cache misses or system
exceptions can occur and perturb the measurements. All these
events are unpredictable and must be treated as noise. Therefore,
we need to quantify the accuracy of our measurements. As recommended in [79], we report the arithmetic mean and the standard
deviation of the 100 measurements. The design of the value and
attribute query benchmarks includes three factors:

Algorithm having height levels: AFOR-1, AFOR-2, AFOR-3, FOR,
PFOR, Rice, S-64, and VByte;
Query having six levels: 2-AND, 2-OR, 4-AND, 4-OR, 2-Phrase,
and 3-Phrase; and
Dataset having three levels: DBpedia, Geonames and Sindice.

Each condition of the design, e.g., AFOR-1 / 2-AND / WIKIPEDIA,

contains 100 separate measurements.

Query benchmark results. We report the results of the query
benchmarks in Table B.4a and b for the value and attribute queries
respectively. Based on these results, we derive multiple graphical
charts to better visualise the differences between each algorithm.
These charts are then used to compare and discuss the performances of each algorithm.

Figs. 13 and 14 report the sum of the average processing time of
the boolean and phrase query levels for the value and attribute
queries respectively. Fig. 13a and Fig. 14a depict the sum of the
average processing time of the boolean query levels (2-AND, 2-
OR, 4-AND, 4-OR). Fig. 13b and Fig. 14b depict the sum of the average processing time of the phrase query level (2-Phrase, 3-Phrase).
The query processing time are obtained by summing up the average time of each query from Table B.4a for the value queries and
Table B.4b for the attribute queries. For example, the processing
time of AFOR-1 on the DBpedia dataset in Fig. 13 is obtained by
summing up the processing times of
the queries 2-Phrase
(43.2 ms) and 3-Phrase (32.6 ms) reported in Table B.4a.

Value query. In Fig. 13, and in particular on the Sindice dataset
(large dataset), we can distinguish three classes of algorithms: the
techniques based on FOR, a group composed of S-64 and VByte,
and finally Rice. The FOR group achieves relatively similar results,
with AFOR-2 slightly behind the others.

Rice has the worst performance for every query and dataset, followed by VByte. However, Rice performs in many cases twice as
slow as VByte. In Fig. 13a provides similar performance to VByte
on boolean queries but we can see in Fig. 13b that it is faster than
VByte on phrase queries. However, S-64 stays behind FOR, PFOR
and AFOR in all the cases.

FOR, PFOR and AFOR have relatively similar performances on all
the boolean queries and all the datasets. PFOR seems to provide
generally slightly better performance on the phrase queries but
seems to be slower on boolean queries.

Attribute query. In Fig. 14, and in particular on the Sindice dataset (large dataset), we can again distinguish the same three classes
of algorithms. However, the performance gap between S-64 and
VByte becomes larger.

Rice has again the worst performance for every query and data-
set. Compared to the performance on value queries, we can see in
Fig. 14a that S-64 provides similar performance to PFOR and AFOR-
2 on boolean queries. FOR and AFOR-3 seem to be the best performing methods on boolean queries. With respect to the phrase
queries in Fig. 14b has better performance than VByte. However,
PFOR does not achieve any more the best performance on phrase
queries. Instead, it seems that AFOR-2 and FOR achieve a slightly
better processing time.

FOR, PFOR and AFOR have again relatively similar performances
on all the queries and all the datasets. AFOR-2 appears to be slower
to some degree, while the gap between AFOR-3 and PFOR becomes
less perceptible.

5.3.3. Performance trade-off

We report in Fig. 15 the trade-off between the total query processing time and the compression ratio among all the techniques
on the Sindice dataset. The total query time has been obtained
by summing up the average time of all the queries. The compression ratio is based on the number of bytes read during query processing which are reported in Table B.4a and b.

Fig. 13. The sum of the average processing time of the boolean query levels (2-AND, 2-OR, 4-AND, 4-OR) for the value queries that is achieved by each compression technique.

Fig. 14. The sum of the average processing time of the phrase query levels (2-Phrase, 4-Phrase) for the attribute queries that is achieved by each compression technique.

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Fig. 15. A graphical comparison showing the trade-off between querying time and compression ratio on the Sindice dataset. The compression ratio is represented by the
number of bytes read during the query processing.

We can distinctively see that the AFOR techniques are close to
Rice in terms of compression ratio, while being relatively close to
FOR and PFOR in terms of query processing time. Compared to
AFOR-1, AFOR-2 achieves a better compression rate in exchange
of a slightly slower processing time. However, AFOR-3 accomplishes a better compression rate with a very close processing time
to AFOR-1.

We report in Fig. 16 the trade-off between the total query processing time and the indexing time among all the techniques on
the Sindice dataset. The indexing time has been obtained by summing up the commit and optimise time from Table C.5. We can distinctively see that the AFOR techniques achieve the best trade-off
between indexing and querying time. AFOR-3 produce very similar
indexing and querying times to AFOR-1, while providing a much
better compression rate. It is interesting to notice that PFOR provides a slightly better querying time than FOR but at the price of
a much slower compression. Also, S-64 and VByte provide a relatively close performance trade-off. To conclude, AFOR-3 seems to
offer the best compromise between querying time, indexing time,
and compression rate.

5.4. Discussion

In general, even if FOR has more data to read and decompress, it
still provides one of the best query execution time. The reason is
that our experiments are performed using warm cache. We therefore ignore the cost of disk IO accesses and measure exclusively the
decompression efficiency of the methods. With a cold cache, i.e.,
when IO disk accesses have to be performed, we expect a drop of
performance for algorithms with a low compression ratio such as

FOR and PFOR compared to AFOR-2 and AFOR-3. Future work will
investigate this aspect.

Compression and decompression performance do not only depend on the compression ratio, but also on the execution flow of
the algorithm and on the number of cycles needed to compress or
decompress an integer. Therefore, CPU-optimised algorithms which
provides at the same time a good compression ratio are more likely
to increase the update and query throughputs of web search engines.
In that context, AFOR seems to be a good candidate since it is well
balanced in all aspects: it provides very good indexing and querying
performance and one of the best compression ratio.

The Simple encoding family is somehow similar to AFOR. At
each iteration, S-64 encodes or decodes a variable number of integers using CPU optimised routines. AFOR is however not tied to the
size of a machine word, is simpler to implement and provides better compression ratio, compression speed and decompression
speed.

Another interesting property of AFOR which is not discussed
in this paper is its ability to skip quickly over chunks of data
without having to decode them. This is not possible with techniques such as Rice, VByte or PFOR. AFOR has to decode the
bit frame to know the length of the following frame, and is
therefore able to deduce the position of the next bit frame. Such
a characteristic could be leveraged to simplify the self-indexing
of inverted files [59].

5.5. Conclusion on high-performance compression

We presented AFOR, a novel class of compression techniques
for inverted lists which provide tremendous benefit in the case

Fig. 16. A graphical comparison of the compression techniques showing the trade-off between querying time and indexing time on the Sindice dataset.

of a node-based inverted index. AFOR is specifically designed to
increase update and query throughput of web search engines.
We have described three different implementations of the AFOR
encoder class. We have compared AFOR to alternative ap-
proaches, and have shown experimental evidences that AFOR
provides a well balanced trade-off between three factors: indexing time, querying time and compression ratio. In particular,
AFOR-3 achieves similar query processing times than FOR and
PFOR, a better compression rate than Rice and the best indexing
times.

The results of the experiment lead to interesting conclusions.
In particular and with respect
to the node-based indexing
scheme, we have shown (1) that VByte is inadequate due to
its incapacity of efficiently encoding small values; and (2) that
PFOR provides only limited benefits compared to FOR. On the
contrary, techniques such as S-64 and AFOR which are able to
adapt their encoding based on the value distribution yield better
results.

6. Overall scalability of the retrieval system

In the previous experiments, we have seen that AFOR-3 is the
most suitable compression technique for the node-based inverted
index. Based on these results, we decide to combine AFOR-3 and
the node-based index in a large scale experiment which simulates
the conditions of a real web data search engine such as Sindice. We
use the full Sindice data collection to create three indexes of
increasing size and we generate a set of star queries of increasing
complexity. We compare the query rate (queries per second) that
the system can answer with respect to the size of the index and
the complexity of the query.

ment records the query rate, i.e., the number of query the system
can process per second, using a single thread. As recommended
in [79], we report the harmonic mean and the standard deviation
of the 100 measurements.

The design of the scalability benchmark includes three factors:

Dataset having three levels: Small, Medium and Large.
Query size having five levels: 1, 2, 4, 8, and 16.
Term selectivity having two levels: Low-Medium-High (LMH)
and Medium-High (MH).

Each condition of the design, e.g., Small/4/LMH, contains 100 separate measurements. The term selectivity denotes the selectivity
ranges that has been used to generate the query terms. For
example, the MH selectivity level means that all the query terms
have been generated from either the medium or high range.

6.1. Indexing performance

We report that during the indexing of the data collection per
batch of 100,000 entities, the commit time stayed constant, with
an average commit time of 2062 ms. The optimisation of the full
index were performed in 119 minutes. The size of the five inverted
files is 19.279 GB, with 10.912 GB for the entity file, 0.684 GB for
the frequency file, 3.484 GB for the attribute file, 1.810 GB for the
value file and 2.389 GB for the position file. The size of the dictionary is 8.808 GB and the size of the skip lists, i.e., the data structure
for self-indexing,
is 7.644 GB. The total size of the index is
35.731 GB which represents an average of 8 bytes per RDF
statement.

Experimental settings and design. The experimental settings and

6.2. Querying performance

design are identical as the ones found in Section 5.3.

Data collection. We use the full Sindice data collection which is
currently composed of more than 120 millions of documents
among 90.000 datasets. For each dataset, we extracted the entities
as pictured in Fig. 3. We filtered out all the entity descriptions containing less than two facts. After filtering, there is a total of
907,542,436 entities for 4,689,599,183 RDF statements. We create
three datasets:
for
1,240,674,545 RDF statements; Medium containing 447,305,647
entities for 2,535,658,099 RDF statements; and Large containing
the complete collection of entities.

containing 226,129,319 entities

Small

Query benchmark design. We generate star queries of increasing complexity, starting with 1 attribute query up to 16. Each
attribute query is generated by selecting at random (following
a uniform distribution) an attribute term from the high, medium
or low selectivity ranges. The associated value query is generated
by selecting at random (following a uniform distribution) a conjunction (2-AND or 4-AND) or a disjunction (2-OR or 4-OR). Each
term of the value query is selected from the high, medium or low
selectivity ranges at random (following a uniform distribution).
Such a query generation scheme provides star queries of average
complexity, i.e., queries composed of terms from any selectivity
range.

With respect to the creation of the three selectivity ranges for
the value terms, we observed the presence of a longer tail in the
term frequency distribution compared to the previous experiment.
Consequently, we modified the way the ranges are computed. The
high range represents the first k words whose cumulative frequency is 50% of all word occurrences. The medium range accounts
for the next 30%, and the low range is composed of all the remaining words.

For each type of star query, we (1) generate a set of 400 random
queries, and (2) perform 100 measurements. Each measurement is
made as explained in Section 5.3.2 using warm cache. A measure-

We report the results of the scalability benchmark in Table
C.6. Based on these results, we derive two graphical charts in
Fig. 17 to better visualise the evolution of the query rate with
respect to the size of the dataset and the complexity of the
queries.

With respect to the size of the queries, we can observe that
the query rate increases with the number of attribute value pairs
until a certain point (up to 2 or 4 pairs), and then starts to de-
crease. The lowest query rate is obtained when the star query
is composed of only one attribute query. Such a query produces
a higher number of hits compared to other queries, and as a consequence the system has to read more data. For example, in Table C.6, we can see that the amount of data read is at least three
times higher than in any other queries. On the other hand, the
precision of the query increases with the number of attribute
queries, and the chance of having a large number of hits decreases consequently. In that case, the self-indexing technique
provides considerable benefits since it enables the system to
avoid a large amount of unnecessary record comparisons and
to answer complex queries in sub-linear time.

Concerning the term selectivity, we can note a drop of query
rate between Fig. 17a where query terms of low selectivity are
employed and Fig. 17b where query terms of low selectivity is
not employed. In the later case, the system has to perform more
record comparisons. Whenever a term with a low selectivity is
used in the query, the system is able to take advantage of the
self-indexing and to skip a larger number of records during query
processing.

The size of the data collection has only a limited impact on
the query rate. The reason is that the query processing complexity is bound by the size of the inverted lists which is itself
dependent of the term distribution. Therefore, the size of the
data collection has a weak influence on the size of the inverted

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Fig. 17. The evolution of the average query rate with respect to the size of the star queries over different dataset size.

lists, apart for very frequent terms. A term with a low or medium
selectivity will have a short inverted lists even if the data collection is very large.

To conclude, the results show that the query rate of the system
scales gracefully with the size of the data and the complexity of the
query. A single-threaded system is able to sustain a query rate of
17 queries per second up to 292 queries per second depending of
the kind of queries. At this rate, the system is able to support many
requests, or users, at the same time.

7. Conclusions and future work

In this article, we have introduced an entity retrieval model for
decentralised infrastructures providing semi-structured data such
as the Semantic Web. The Entity AttributeValue model is generic
enough to be compatible with various semi-structured data models such as RDF or Microformats. This retrieval model provide a
common framework for the development of techniques which
are applicable not only for web data search but on a wider variety
of scenarios.

We have introduced a node-based indexing scheme that fulfils
the requirements of our entity retrieval model. Compared to other
entity retrieval systems, such a retrieval system is able to provide
semi-structured search while sustaining fast query processing and
efficient index maintenance. Since the performance of the retrieval
system is a key issue in large web search engines, we have developed a high-performance compression technique which is particularly efficient with respect to the node-based inverted index
described in this article. Finally, we have shown that the resulting
retrieval system can index billions of data objects and can answer a
large number of complex requests per second on hardware less
powerful than the average laptop available today.

Future work will concentrate on increasing the query expressiveness of the system. We have to investigate the feasibility of
path-based queries which will enable to query relations between
entities. Supporting such queries while keeping a system in the
same class of scalability is still an open problem.

Appendix A. Examples of algebra queries

Value query
Q1: Find all entities with a value matching keywords renaud and

delbru.
Q1 14 perv:renaudrv:delbruR

14 perv:renaud^v:delbruR

Q1

Attribute query
Q2: Find all entities with a value matching keywords john and

smith associated to an attribute matching the keyword author.
Q2 14 perat:authorrv:john^v:smithR

14 perat:author^v:john^v:smithR
Star query
Q3: Find all entities with a value matching keywords john and
smith associated to an attribute matching the keyword author,
and a value matching keywords search and engine associated to
an attribute matching the keyword title.
R1 14 perat:authorrv:john^v:smithR
R2 14 perat:titlerv:search^v:engineR
Q3 14 R1 \ R2

Q2

Q3

Table C.5
Total indexing time, optimise time and index size.

Method

Time (s)
Total

Opt

Sizes (GB)
Ent

Frq

Att

Val

Pos

Total

(a) DBpedia
AFOR-1
AFOR-2
AFOR-3

Rice
S-64
VByte

(b) Geonames

AFOR-1

AFOR-2

AFOR-3

Rice

S-64
VByte

(c) Sindice
AFOR-1
AFOR-2
AFOR-3

Rice
S-64
VByte

Table B.4
Query time execution for per query type, algorithm and dataset. We report for each query type the arithmetic mean (l in millisecond), the confidence interval with 95%
confidence level for the mean (c in millisecond), the standard deviation (r in millisecond) and the total amount of data read during query processing (MB in megabyte).

(a) Value Query

(b) Attribute Query

R. Delbru et al. / Web Semantics: Science, Services and Agents on the World Wide Web 10 (2012) 3358

Table C.6
Query rate per dataset, term selectivity and query size. We report for each query size the arithmetic mean (l in queries per second), the confidence interval with 95% confidence
level for the mean (c in queries per second), the standard deviation (r in queries per second) and the total amount of data read during query processing (MB in megabyte).

Dataset query
Q4: Find all entities from the dataset biblio with a value
matching keywords john and smith associated to an attribute
matching the keyword author, and a value matching keywords
search and engine associated to an attribute matching the keyword title.
R1 14 pd;erat:authorrv:john^v:smithR
R2 14 pd;erat:titlerv:search^v:engineR
Q4 14 perd:biblioR1 \ R2

Q4

Q5: Find all datasets with two entities, the first one with a value
matching keywords john and smith associated to an attribute
matching the keyword name, and the second one with a value
matching keywords search and engine associated to an attribute
matching the keyword title.
R1 14 pdrat:namerv:john^v:smithR
R2 14 pdrat:titlerv:search^v:engineR
Q5 14 R1 \ R2

Q5

Appendix B. Results of the compression benchmark

This appendix provides tables containing the results of the
benchmarks that have been performed for comparing the indexing
and querying performance of the node-based index with various
compression algorithms. Table C.5 have been used for generating
the charts from Section 5.3.1. Tables B.4 have been used for generating the charts from Section 5.3.2.

Appendix C. Results of the scalability benchmark

This appendix provides the table containing the results of the
scalability benchmark. Table C.6 has been used for generating the
charts from Section 6.2.

Appendix D. Source code

The source code of SIREn is publicly available at http://siren.sin-
dice.com/. The source code of the benchmark platform and of the
compression algorithms as well as the datasets and raw experimental results are available on request.
