Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Interactive ontology debugging: Two query strategies for efficient
fault localization q

Kostyantyn Shchekotykhin


, Gerhard Friedrich, Philipp Fleiss 1, Patrick Rodler 1

Alpen-Adria Universitat, Universitatsstrasse 65-67, 9020 Klagenfurt, Austria

a r t i c l e

i n f o

a b s t r a c t

Article history:
Available online 30 December 2011

Keywords:
Ontology debugging
Query selection
Model-based diagnosis
Description logic

Effective debugging of ontologies is an important prerequisite for their broad application, especially in
areas that rely on everyday users to create and maintain knowledge bases, such as the Semantic Web.
In such systems ontologies capture formalized vocabularies of terms shared by its users. However in
many cases users have different local views of the domain, i.e. of the context in which a given term is
used. Inappropriate usage of terms together with natural complications when formulating and understanding logical descriptions may result in faulty ontologies. Recent ontology debugging approaches
use diagnosis methods to identify causes of the faults. In most debugging scenarios these methods return
many alternative diagnoses, thus placing the burden of fault localization on the user. This paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by
querying an oracle about entailments of the target ontology. To identify the best query we propose
two query selection strategies: a simple split-in-half strategy and an entropy-based strategy. The latter
allows knowledge about typical user errors to be exploited to minimize the number of queries. Our evaluation showed that the entropy-based method significantly reduces the number of required queries compared to the split-in-half approach. We experimented with different probability distributions of user
errors and different qualities of the a priori probabilities. Our measurements demonstrated the superiority of entropy-based query selection even in cases where all fault probabilities are equal, i.e. where no
information about typical user errors is available.

 2011 Elsevier B.V. Open access under CC BY-NC-ND license.

1. Introduction

Ontology acquisition and maintenance are important prerequisites for the successful application of semantic systems in areas
such as the Semantic Web. However, as state of the art ontology
extraction methods cannot automatically acquire ontologies in a
complete and error-free fashion, users of such systems must formulate and correct logical descriptions on their own. In most of the
cases these users are domain experts who have little or no experience in expressing knowledge in representation languages like
OWL 2 DL [2]. Studies in cognitive psychology, e.g. [3,4], indicate
that humans make systematic errors while formulating or
interpreting logical descriptions, with the results presented in
[5,6] confirming that these observations also apply to ontology

q This article is a substantial extension of the preliminary results published in

Proceedings of the 9th International Semantic Web Conference (ISWC 2010) [1].

 Corresponding author. Tel.: +43 463 2700 3768; fax: +43 463 2700 993768.

E-mail addresses: kostya@ifit.uni-klu.ac.at (K. Shchekotykhin), gerhard@ifit.
(P. Fleiss), prodler@ifit.

(G. Friedrich), pfleiss@ifit.uni-klu.ac.at

uni-klu.ac.at
uni-klu.ac.at (P. Rodler).

development. Moreover, the problem gets even more if an ontology
is developed by a group of users, such as OBO Foundry2 or NCI
Thesaurus,3 is based on a set of imported third-party ontologies,
etc. In this case inconsistencies might appear if some user does not
understand or accept the context in which shared ontological descriptions are used. Therefore, identification of erroneous ontological
definitions is a difficult and time-consuming task.

Several ontology debugging methods [710] were proposed to
simplify ontology development and maintenance. Usually the main
aim of debugging is to obtain a consistent and, optionally, coherent
ontology. These basic requirements can be extended with additional ones, such as test cases [9], which must be fulfilled by the
target ontology Ot. Any ontology that does not fulfill the requirements is faulty regardless of how it was created. For instance, an
ontology might be created by an expert specializing descriptions
of the imported ontologies (top-down) or by an inductive learning
algorithm from a set of examples (bottom-up).

Note that even if all requirements are completely specified,
many logically equivalent target ontologies might exist. They may

1 The research project is funded by grants of the Austrian Science Fund (Project

V-Know, Contract 19996).

2 http://www.obofoundry.org
3 http://ncit.nci.nih.gov

1570-8268 O 2011 Elsevier B.V.
doi:10.1016/j.websem.2011.12.006

Open access under CC BY-NC-ND license.

differ in aspects such as the complexity of consistency checks, size
or readability. However, selecting between logically equivalent theories based on such measures is out of the scope of this paper. Fur-
thermore, although target ontologies may evolve as requirements
change over time, we assume that the target ontology remains stable throughout a debugging session.

Given an set of requirements (e.g. formulated by a user) and a
faulty ontology, the task of an ontology debugger is to identify the
set of alternative diagnoses, where each diagnosis corresponds to
a set of possibly faulty axioms. More concretely, a diagnosis D is a
subset of an ontology O such that one should remove (change) all
the axioms of a diagnosis from the ontology (i.e. O n D) in order to
formulate an ontology O0 that fulfills all the given requirements.
Only if the set of requirements is complete the only possible ontology O0 corresponds to the target ontology Ot. In the following we
refer to the removal of a diagnosis from the ontology as a trivial
application of a diagnosis. Moreover, in practical applications it
might be inefficient to consider all possible diagnoses. Therefore,
modern ontology debugging approaches focus on the computation
of minimal diagnoses. A set of axioms Di is a minimal diagnosis iff
there is no proper subset D0
i  Di which is a diagnosis. Thus, minimal
diagnoses constitute minimal required changes to the ontology.

Application of diagnosis methods can be problematic in the
cases for which many alternative minimal diagnoses exist for a given set of test cases and requirements. A sample study of realworld incoherent ontologies, which were used in [8], showed that
hundreds or even thousands of minimal diagnoses may exist. In
the case of the Transportation ontology the diagnosis method
was able to identify 1782 minimal diagnoses.4 In such situations
a simple visualization of all alternative sets of modifications to the
ontology is ineffective. Thus an efficient debugging method should
be able to discriminate between the diagnoses in order to select
the target diagnosis Dt. Trivial application of Dt to the ontology O allows a user to extend O n Dt with a set of additional axioms EX and,
thus, to formulate the target ontology Ot, i.e. Ot 14 O n Dt [ EX.

One possible solution to the diagnosis discrimination problem
would be to order the set of diagnoses by various preference crite-
ria. For instance, Kalyanpur et al. [11] suggest a measure to rank
the axioms of a diagnosis depending on their structure, usage in
test cases, provenance, and impact in terms of entailments. Only
the top ranking diagnoses are then presented to the user. Of course
this set of diagnoses will contain the target diagnosis only in cases
where the faulty ontology, the given requirements and test cases
provide sufficient data to the appropriate heuristic. However, it is
difficult to identify which information, e.g. test cases, is really required to identify the target diagnosis. That is, a user does not
know a priori which and how many tests should be provided to
the debugger to ensure that it will return the target diagnosis.

In this paper we present an approach for the acquisition of additional information by generating a sequence of queries, the answers
of which can be used to reduce the set of diagnoses and ultimately
identify the target diagnosis. These queries should be answered by
an oracle such as a user or an information extraction system. In order to construct queries we exploit the property that different
ontologies resulting from trivial applications of different diagnoses
entail unequal sets of axioms. Consequently, we can differentiate
between diagnoses by asking the oracle if the target ontology
should entail a set of logical sentences or not. These entailed logical
sentences can be generated by the classification and realization
services provided in description logic reasoning systems [1214].
In particular, the classification process computes a subsumption
hierarchy (sometimes also called inheritance hierarchy of parents and children) for each concept description mentioned in a

4 In Section 5, we will give a detailed characterization of these ontologies.

TBox. For each individual mentioned in an ABox, the realization
computes all the concept names of which the individual is an instance [12].

We propose two methods for selecting the next query of the set
of possible queries: The first method employs a greedy approach
that selects queries which try to cut the number of diagnoses in
half. The second method exploits the fact that some diagnoses
are more likely than others because of typical user errors [5,6]. Beliefs for an error to occur in a given part of a knowledge base, represented as a probability, can be used to estimate the change in
entropy of the set of diagnoses if a particular query is answered.
In our evaluation the fault probabilities of axioms are estimated
by the type and number of the logical operators employed. For
example, roughly speaking, the greater the number of logical operators and the more complex these operators are, the greater the
fault probability of an axiom. For assigning prior fault probabilities
to diagnoses we employ the fault probabilities of axioms. Of course
other methods for guessing prior fault probabilities, e.g. based on
context of concept descriptions, measures suggested in the previous work [11], etc., can be easily integrated in our framework. Given a set of diagnoses and their probabilities the method selects a
query which minimizes the expected entropy of a set of diagnoses
after an oracle answers a query, i.e. maximizes the information
gain. An oracle should answer such queries until a diagnosis is
identified whose probability is significantly higher than those of
all other diagnoses. This diagnosis is most likely to be the target
diagnosis.

In the first evaluation scenario we compare the performance of
both methods in terms of the number of queries needed to identify
the target diagnosis. The evaluation is performed using generated
examples as well as real-world ontologies presented in Tables 8
and 12. In the first case we alter a consistent and coherent ontology
with additional axioms to generate conflicts that result in a predefined number of diagnoses of a required length. Each faulty ontology is then analyzed by the debugging algorithm using entropy,
greedy and random strategies, where the latter selects queries
at random. The evaluation results show that in some cases the en-
tropy-based approach is almost 60% better than the greedy one
whereas both approaches clearly outperformed the random
strategy.

In the second evaluation scenario we investigate the robustness
of the entropy-based strategy with respect to variations in the
prior fault probabilities. We analyze the performance of entropybased and greedy strategies on real-world ontologies by simulating
different types of prior fault probability distributions as well as the
quality of these probabilities that might occur in practice. In par-
ticular, we identify the cases where all prior fault probabilities are
(1) equal, (2) moderately varied or (3) extremely varied.
Regarding the quality of the probabilities we investigate cases
where the guesses based on the prior diagnosis probabilities are
good, average or bad. The results show that the entropy method
outperforms split-in-half in almost all of the cases, namely when
the target diagnosis is located in the more likely two thirds of the
minimal diagnoses. In some situations the entropy-based approach
achieves even twice the performance of the greedy one. Only in
cases where the initial guess of the prior probabilities is very vague
(the bad case), and the number of queries needed to identify the
target diagnosis is low, split-in-half may save on average one
query. However, if the number of queries increases, the performance of the entropy-based query selection increases compared
to the split-in-half strategy. We observed that if the number of
queries is greater than 10, the entropy-based method is preferable
even if the initial guess of the prior probabilities is bad. This is due
to the effect that the initial bad guesses are improved by the Bayesupdate of the diagnoses probabilities as well as an ability of the
entropy-based method to stop in the cases when a probability of

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

some diagnosis is above an acceptance threshold predefined by the
user. Consequently, entropy-based query selection is robust enough to handle different prior fault probability distributions.

Additional experiments performed on big real-world ontologies
demonstrate the scalability of the suggested approach. In our
experiments we were able to identify the target diagnosis in an
ontology with over 33,000 axioms using entropy-based query
selection in only 190 s using an average of five queries.

The remainder of the paper is organized as follows: Section 2
presents two introductory examples as well as the basic concepts.
The details of the entropy-based query selection method are given
in Section 3. Section 4 describes the implementation of the approach and is followed by evaluation results in Section 5. The paper
concludes with an overview of related work.

2. Motivating examples and basic concepts

We begin by presenting the fundamentals of ontology diagnosis
and then show how queries and answers can be generated and employed to differentiate between sets of diagnoses.

2.1. Description logics

Since the underlying knowledge representation method of
ontologies in the Semantic Web is based on description logics,
we start by briefly introducing the main concepts, employing the
usual definitions as in [15,16]. A knowledge base is comprised of
two components, namely a TBox (denoted by T ) and a ABox A.
The TBox defines the terminology whereas the ABox contains
assertions about named individuals in terms of the vocabulary defined in the TBox. The vocabulary consists of concepts, denoting
sets of individuals, and roles, denoting binary relationships between individuals. These concepts and roles may be either atomic
or complex, the latter being obtained by employing description
operators. The language of descriptions is defined recursively by
starting from a schema S 14 CN ; RN ;IN of disjoint sets of names
for concepts, roles, and individuals. Typical operators for the construction of complex descriptions are C t D (disjunction), C u D
(conjunction), :C (negation), 8R:C (concept value restriction), and
9R:C (concept exists restriction), where C and D are elements of
CN and R 2 RN .

Knowledge bases are defined by a finite set of logical sentences.
Sentences regarding the TBox are called terminological axioms
whereas sentences regarding the ABox are called assertional axi-
oms. Terminological axioms are expressed by C v D (Generalized
Concept Inclusion) which corresponds to the logical implication.
Let a; b 2 IN be individual names. Ca and Ra; b are thus assertional axioms.

Concepts (rsp. roles) can be regarded as unary (rsp. binary)
predicates. Roughly speaking description logics can be seen as fragments of first-order predicate logic (without considering transitive
closure or special fixpoint semantics). These fragments are specifically designed to ensure decidability or favorable computational
costs.

The semantics of description terms are usually given using an
interpretation I 14 hDI ;	Ii, where DI is a domain (non-empty uni-
verse) of values, and 	I is a function that maps every concept
description to a subset of DI , and every role name to a subset of
DI 
 DI . The mapping also associates a value in DI with every individual name in IN . An interpretation I is a model of a knowledge
base iff it satisfies all terminological axioms and assertional axi-
oms. A knowledge base is satisfiable iff a model exists. A concept
description C is coherent (satisfiable) w.r.t. a TBox T , if a model
I of T exists such that CI  ;. A TBox is incoherent iff an incoherent concept description exists.

2.2. Diagnosis of ontologies

Example 1. Consider a simple ontology O with the terminology T :
ax1 : A v B
ax2 : B v C
ax3 : C v D ax4 : D v R
and assertions A : fAw;:Rw; Avg.

Assume that the user explicitly states that the three assertional
axioms should be considered as correct, i.e. these axioms are added
to a background theory B. The introduction of a background theory
ensures that the diagnosis method focuses purely on the potentially faulty axioms.

12ax1 D2 : 12ax2 D3 : 12ax3 D4 : 12ax4

Furthermore, assume that the user requires the currently inconsistent ontology O to be consistent. The only irreducible set of axioms (minimal conflict set) that preserves the inconsistency is
CS : hax1; ax2; ax3; ax4i. That is, one has to modify or remove the
axioms of at least one of the following diagnoses
D1 :
to restore the consistency of the ontology. However, it is unclear
which of the ontologies Oi 14 O n Di obtained by application of diagnoses from the set D : fD1; . . . ; D4g is the target one.
Definition 1. A target ontology Ot is a set of logical sentences
characterized by a set of background axioms B, a set of sets of
logical sentences P that must be entailed by Ot and the set of sets of
logical sentences N that must not be entailed by Ot.
A target ontology Ot must fulfill the following necessary

requirements:

 Ot must be satisfiable (optionally coherent)
 B #Ot
 Ot  p 8p 2 P
 Ot 2 n 8n 2 N

Given B; P, and N, an ontology O is faulty iff O does not fulfill all

the necessary requirements of the target ontology.

Note that the approach presented in this paper can be used with
any knowledge representation language for which there exists a
sound and complete procedure to decide whether O  ax and the
entailment operator  is extensive, monotone and idempotent.
For instance, these requirements are fulfilled by all subsets of
OWL 2 which are interpreted under OWL Direct Semantics.

since O2 [ f:Cwg

Definition 1 allows a user to identify the target diagnosis Dt by
providing sufficient information about the target ontology in the
sets B; P and N. For instance, if in Example 1 the user provides the
information that Ot  fBwg and Ot 2 fCwg, the debugger will return only one diagnosis, namely D2. Application of this diagnosis results in a consistent ontology O2 14 O n D2 that entails fBwg
because of ax1 and the assertion Aw. In addition, O2 does not entail
fCwg
and, moreover,
f:Rw; ax4; ax3g  f:Cwg. All other ontologies Oi 14 O n Di obtained by the application of the diagnoses D1;D3 and D4 do not fulfill
the given requirements, since O1 [ fBwg is inconsistent and therefore any consistent extension of O1 cannot entail fBwg. As both O3
and O4 entail fCwg; O2 corresponds to the target diagnosis Ot.
Definition 2. Let hO;B; P; Ni be a diagnosis problem instance, where
O is an ontology, B a background theory, P a set of sets of logical
sentences which must be entailed by the target ontology Ot, and N a
set of sets of logical sentences which must not be entailed by Ot.
A set of axioms D #O is a diagnosis iff the set of axioms O n D

is

consistent

can be extended by a logical description EX such that:

1. O n D [ B [ EX is consistent (and coherent if required)
2. O n D [ B [ EX  p 8p 2 P
3. O n D [ B [ EX 2 n 8n 2 N

A diagnosis Di defines a partition of the ontology O where each
axiom axj 2 Di is a candidate for changes by the user and each axiom axk 2 O n Di is correct. If Dt is the set of axioms of O to be changed (i.e. Dt is the target diagnosis) then the target ontology Ot is
O n Dt [ B [ EX for some EX defined by the user.
In the following we assume the background theory B together
with the sets of logical sentences in the sets P and N always allow
formulation of the target ontology. Moreover, a diagnosis exists iff
a target ontology exists.

Proposition 1. A diagnosis D for a diagnosis problem instance
hO;B; P; Ni exists iff
B [

p2P

is consistent (coherent) and
8n 2 N : B [

p 2 n

p2P

The set of all diagnoses is complete in the sense that at least one
diagnosis exists where the ontology resulting from the trivial
application of a diagnosis is a subset of the target ontology.

Proposition 2. Let D  ; be the set of all diagnoses for a diagnosis
problem instance hO;B; P; Ni and Ot the target ontology. Then a
diagnosis Dt 2 D exists s.t. O n Dt #Ot.

The set of all diagnoses can be characterized by the set of min-

imal diagnoses.

Definition 3. A diagnosis D for a diagnosis problem instance
hO;B; P; Ni is a minimal diagnosis iff there is no D0  D such that D0
is a diagnosis.

Proposition 3. Let hO;B; P; Ni be a diagnosis problem instance. For
every diagnosis D there is a minimal diagnosis D0 s.t. D0 #D.

Definition 4. A diagnosis D for a diagnosis problem instance
hO;B; P; Ni is a minimum cardinality diagnosis iff there is no diagnosis D0 such that jD0j < jDj.

To summarize, a diagnosis describes which axioms are candidates for modification. Despite the fact that multiple diagnoses
may exist, some are more preferable than others. E.g. minimal diagnoses require minimal changes, i.e. axioms are not considered for
modification unless there is a reason. Minimal cardinality diagnoses
require changing a minimal number of axioms. The actual type of
error contained in an axiom is irrelevant as the concept of diagnosis
defined here does not make any assumptions about errors them-
selves. There can, however, be instances where an ontology is faulty
and the empty diagnosis is the only minimal diagnosis, e.g. if some
axioms are missing and nothing must be changed.

The extension EX plays an important role in the ontology repair
process, suggesting axioms that should be added to the ontology.
For instance, in Example 1 the user requires that the target ontology must not entail fBwg but has to entail fBvg, that is
N 14 ffBwgg and P 14 ffBvgg. Because, the example ontology O
is inconsistent some sentences must be changed. The consistent
ontology O1 14 O n D1, neither entails fBvg nor fBwg (in particular O1  f:Bwg). Consequently, O1 has to be extended with a set
EX of logical sentences in order to entail fBvg. This set of logical
sentences can be approximated with EX 14 fBvg. O1 [ EX is

consistent but

satisfiable, entails fBvg but does not entail fBwg. All other
ontologies Oi 14 O n Di; i 14 2; 3; 4 are
entail
fBw; Bvg and must be rejected because of the monotonic
semantics of description logic. That is, there is no such extension
EX that Oi [ EX 2 fBwg. Therefore, the diagnosis D1 is the minimum cardinality diagnosis which allows the formulation of the
target ontology. Note that formulation of the complete extension
is impossible, since our diagnosis approach deals with changes to
existing axioms and does not learn new axioms.

The following corollary characterizes diagnoses without
employing the true extension EX to formulate the target ontology.
The idea is to use the sentences which must be entailed by the target ontology to approximate EX as shown above.

Corollary 1. Given a diagnosis problem instance hO;B; P; Ni, a set of
axioms D #O is a diagnosis iff
O n D [ B [

Condition 1

p2P

is satisfiable (coherent) and
8n 2 N : O n D [ B [

p 2 n

p2P

Proof sketch:

Condition 2

for

is
for

p 2 P
follows

for
all
n 2 N it

satisfiable
p 2 P,

(coherent)
follows
it

O n D [ B [ EX  p
all

) Let D #O be a diagnosis for hO;B; P; Ni. Since there is an EX
O n D [ B [ EX
and
s.t.

O n D [ B [ EX  p
that

O n D [ B [ EX [
p2P p is satisfiable (coherent) and therefore O n D [ B [
p2P p is satisfiable (coherent). Conse-
quently, the first condition of the corollary is fulfilled.
and
Since

O n D [ B [ EX 2 n
that
O n D [ B [ EX [
p2Pp 2 n for all n 2 N. Consequently,
O n D [ B [
p2Pp 2 n for all n 2 N and the second condition
of the corollary is fulfilled.
( Let D #O and hO;B; P; Ni be a diagnosis problem instance.

Without limiting generality let EX 14 P. By Condition 1 of
the corollary O n D [ B [
p2Pp is satisfiable (coherent).
Therefore, for EX 14 P the sentences O n D [ B [ EX are satisfiable (coherent), i.e. the first condition for a diagnosis is
fulfilled and these sentences entail p for all p 2 P which corresponds to the second condition a diagnosis must fulfill.
Furthermore,
corollary
O n D [ B [ EX 2 n for all n 2 N holds and therefore the
third condition for a diagnosis is fulfilled. Consequently,
D #O is a diagnosis for hO;B; P; Ni. 

Condition

the

of

all

by

Conflict sets, which are the parts of the ontology that preserve
the inconsistency/incoherency, are usually employed to constrain
the search space during computation of diagnoses.

Definition 5. Given a diagnosis problem instance hO;B; P; Ni, a set
of axioms CS # O is a conflict set iff CS [ B [
p2P p is inconsistent
(incoherent) or n 2 N exists s.t. CS [ B [

p2P p  n.

Definition 6. A conflict set CS for an instance hO;B; P; Ni is minimal
iff there is no CS0  CS such that CS0 is a conflict set.

A set of minimal conflict sets can be used to compute the set of
minimal diagnoses as shown in [17]. The idea is that each diagnosis
must include at least one element of each minimal conflict set.

Proposition 4. D is a minimal diagnosis for the diagnosis problem
instance hO;B; P; Ni iff D is a minimal hitting set for the set of all
minimal conflict sets of hO;B; P; Ni.

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

p [ Q is consistent coherent

O n Di [ B [
8n 2 N : O n Di [ B [

p2P

p [ Q 2 n

p2P

If the oracle answers no then every diagnosis Di 2 D is a diagnosis for
N [ fQg iff both conditions hold:

O n Di [ B [

p is consistent coherent

p2P

8n 2 N [ fQg : O n Di [ B [

p 2 n

p2P

In particular, a query partitions the set of diagnoses D into three

disjoint subsets.

Definition 7. For a query Q, each diagnosis Di 2 D of a diagnosis
problem instance hO;B; P; Ni can be assigned to one of the three
sets DP; DN or D; where

 Di 2 DP iff it holds that

O n Di [ B [

p  Q

p2P

 Di 2 DN iff it holds that

O n Di [ B [

p [ Q
p2P

is inconsistent (incoherent).
 Di 2 D; iff Di 2 D n DP [ DN

Given a diagnosis problem instance we say that the diagnoses in
DP predict a positive answer (yes) as a result of the query Q, diagnoses in DN predict a negative answer (no), and diagnoses in D; do
not make any predictions.

Property 2. Given a diagnosis problem instance hO;B; P; Ni, a set of
diagnoses D, a query Q and an oracle:

If the oracle answers yes then the set of rejected diagnoses is DN

and the set of remaining diagnoses is DP [ D;.

If the oracle answers no then the set of rejected diagnoses is DP and

the set of remaining diagnoses is DN [ D;.

Consequently, given a query Q either DP or DN is eliminated but
D; always remains after the query is answered. For generating queries we have to investigate for which subsets DP; DN # D a query
exists that can differentiate between these sets. A straight forward
approach is to investigate all possible subsets of D. In our evaluation we show that this is feasible if we limit the number n of minimal diagnoses to be considered during query generation and
selection. E.g. for n 14 9, the algorithm has to verify 512 possible
partitions in the worst case.

Given a set of diagnoses D for the ontology O, a set P of sets of
sentences that must be entailed by the target ontology Ot and a set
of background axioms B, the set of partitions PR for which a query
exists can be computed as follows:

1. Generate the power set PD; PR   ;
2. Assign an element of PD to the set DP

i and generate a set of
p2P p,

common entailments Ei of all ontologies O n Dj [ B [
where Dj 2 DP
PD   PD n fDP

i.e.
i g and goto Step 2. Otherwise set Q i   Ei.

the current element DP
i ,

then reject

3. If Ei 14 ;,

4. Use Definition 7 and the query Q i to classify the diagnoses
i . The generated partition
i ig and

Dk 2 D n DP
is added to the set of partitions PR   PR [ fhQ i; DP
set PD   PD n fDP

i ; DN
i g. If PD  ; then go to Step 2.

i into the sets DP

i and D;

i ; DN

i ; D;

set

Given a set of sets S, a set H is a hitting set of S iff H \ Si

 ; for
all Si 2 S and H #
Si2SSi. Most modern ontology diagnosis methods
[710] are implemented according to Proposition 4 and differ only
in details, such as how and when (minimal) conflict sets are computed and the order in which hitting sets are generated.

2.3. Differentiating between diagnoses

The diagnosis method usually generates a set of diagnoses for a
given diagnosis problem instance. Thus, in Example 1 an ontology
debugger returns a set of four minimal diagnoses fD1; . . . ;D4g. As
explained in the previous section, additional information, i.e. sets
of logical sentences P and N, can be used by the debugger to reduce
the set of diagnoses. However, in the general case the user does not
know which sets P and N to provide to the debugger such that the
target diagnosis will be identified. Therefore, the debugger should
be able to identify sets of logical sentences on its own and only ask
the user or some other oracle, whether these sentences must or
must not be entailed by the target ontology. To generate these
sentences the debugger can apply each of the diagnoses in
D 14 fD1; . . . ;Dng and obtain a set of ontologies Oi 14 O n Di;
i 14 1; . . . ; n that fulfill the user requirements. For each ontology
Oi a description logic reasoner can generate a set of entailments
such as entailed subsumptions provided by the classification
service and sets of class assertions provided by the realization
service. These entailments can be used to discriminate between
the diagnoses, as different ontologies entail different sets of
sentences due to extensivity of the entailment relation. Note that
in the examples provided in this section we consider only two
types of entailments, namely subsumption and class assertion. In
general, the approach presented in this paper is not limited to
these types and can use all of the entailment types supported by
a reasoner.

For instance, in Example 1 for each ontology Oi 14 O n Di;
i 14 1 . . . 4, the realization service of a reasoner returns the set of
class assertions presented in Table 1. Without any additional information the debugger cannot decide which of these sentences must
be entailed by the target ontology. To obtain this information the
diagnosis method must query an oracle that can specify whether
the target ontology entails some set of sentences or not. E.g. the
debugger could ask an oracle if fDwg is entailed by the target
ontology Ot  fDwg. If the answer is yes, then fDwg is added
to P and D4 is considered as the target diagnosis. All other diagnoses are rejected because O n Di [ B [ fDwg for i 14 1; 2; 3 is
inconsistent. If the answer is no, then fDwg is added to N and
D4 is rejected as O n D4 [ B  fDwg and we have to ask the oracle another question. In the following we consider a query Q as a
set of logical sentences such that Ot  Q holds iff Ot  qi for all
qi 2 Q.
Property 1. Given a diagnosis problem instance hO;B; P; Ni, a set of
diagnoses D, a set of logical sentences Q representing the query
Ot  Q and an oracle able to evaluate the query:
for P [ fQg iff both conditions hold:

If the oracle answers yes then every diagnosis Di 2 D is a diagnosis

Table 1
Entailments of ontologies Oi 14 O n Di ; i 14 1; . . . ; 4 in Example 1
returned by realization.

Ontology
O1
O2
O3
O4

Entailments

fBwg
fBw; Cwg
fBw; Cw; Dwg

In Example 1 the set of diagnoses D of the ontology O contains 4
elements. Therefore, the power set PD includes 15 elements
ffD1g;fD2g; . . . ; fD1;D2;D3;D4gg, assuming we omit the element
corresponding to ; as it does not contain any diagnoses to be eval-
uated. Moreover, assume that P and N are empty. In each iteration
an element of PD is assigned to the set DP
i . For instance, the algo-
1 14 fD1;D2g. In this case the set of common entailrithm assigns DP
ments is empty as O n D1 [ B has no entailed sentences (see Table
1). Therefore, the set fD1;D2g is rejected and removed from PD.
in the next
Assume that
iteration the algorithm selects
2 14 fD2;D3g.

In this case the set of common entailments
E2 14 fBwg is not empty and so Q 2 14 fBwg. The remaining diagnoses D1 and D4 are classified according to Definition 7. That is, the
algorithm selects the first diagnosis D1 and verifies whether
O n D1 [ B  fBwg. Given the negative answer of the reasoner,
the algorithm checks if O n D1 [ B [ fBwg is inconsistent. Since
the condition is satisfied the diagnosis D1 is added to the set DN
2 .
The second diagnosis D4 is added to the set DP
2 as it satisfies the
first requirement O n D4 [ B  fBwg. The resulting partition
hfBwg;fD2;D3;D4g;fD1g;;i is added to the set PR.

However, a query need not include all of the entailed sentences.
If a query Q partitions the set of diagnoses into DP; DN and D; and
an (irreducible) subset Q0  Q exists which preserves the partition
then it is sufficient to query Q0. In our example, Q 2 : fBw; Cwg
2 : fCwg. If there are multiple irrecan be reduced to its subset Q0
ducible subsets that preserve the partition then we select one of
them.

All of the queries and their corresponding partitions generated
in Example 1 are presented in Table 2. Given these queries the
debugger has to decide which one should be asked first in order
to minimize the number of queries to be answered. A popular
query selection heuristic (called split-in-half) prefers queries
which allow half of the diagnoses to be removed from the set D
regardless of the answer of an oracle.

Using the data presented in Table 2, the split-in-half heuristic
determines that asking the oracle if Ot  fCwg is the best query
(i.e. the reduced query Q 2), as two diagnoses from the set D are removed regardless of the answer. Assuming that D1 is the target
diagnosis, then an oracle will answer no to our question (i.e.
Ot 2 fCwg). Based on this feedback, the diagnoses D3 and D4
are removed according to Property 2. Given the updated set of
diagnoses D and P 14 ffCwgg the partitioning algorithm returns
the only partition hfBwg;fD2g;fD1g;;i. The heuristic then selects
the query fBwg, which is also answered with no by the oracle.
Consequently, D1 is identified as the only remaining minimal
diagnosis.

In general, if n is the number of diagnoses and we can split the
set of diagnoses in half with each query, then the minimum number of queries is log2n. Note that this minimum number of queries
can only be achieved when all minimal diagnoses are considered at
once, which is intractable even for relatively small values of n.

However, in case probabilities of diagnoses are known we can

reduce the number of queries by utilizing two effects:

1. We can exploit diagnoses probabilities to assess the likelihood
of each answer and the expected value of the information
contained in the set of diagnoses after an answer is given.

Table 2
Possible queries in Example 1.

Query
Q 1 : fBwg
Q 2 : fBw; Cwg
Q 3 : fBw; Cw; Qwg

fD2;D3; D4g
fD3;D4g
fD4g

fD1g
fD1;D2g
fD1;D2; D3g

D;

2. Even if multiple diagnoses remain, further query generation
may not be required if one diagnosis is highly probable and
all other remaining diagnoses are highly improbable.

Example 2. Consider an ontology O with the terminology T :
ax1 : A1 v A2 u M1 u M2
ax2 : A2 v :9s:M3 u 9s:M2 ax5 : M3  B t C
ax3 : M1 v :A u B

ax4 : M2 v 8s:A u D

the

and
background
A : fA1w; A1u; su; wg.

theory

containing

the

assertions

The ontology is inconsistent and the set of minimal conflict sets
CS 14 fhax1; ax3; ax4i; hax1; ax2; ax3; ax5ig. To restore consistency,
the user should modify all axioms of at least one minimal
diagnosis:
D1 : 12ax1
D2 : 12ax3

D3 : 12ax4; ax5
D4 : 12ax4; ax2

Following the same approach as in the first example, we compute a set of possible queries and corresponding partitions using
the algorithm presented above. A set of possible irreducible queries
for Example 2 and their partitions are presented in Table 3. These
queries partition the set of diagnoses D in a way that makes the
application of myopic strategies, such as split-in-half, inefficient.
A greedy algorithm based on such a heuristic would first select the
first query Q 1, since there is no query that cuts the set of diagnoses
in half. If D4 is the target diagnosis then Q 1 will be answered with
yes by an oracle (see Fig. 1). In the next iteration the algorithm
would also choose a suboptimal query, the first untried query Q 2,
since there is no partition that divides the diagnoses D1; D2, and
D4 into two groups of equal size. Once again, the oracle answers
yes, and the algorithm identifies query Q 4 to differentiate between
D1 and D4.

However, in real-world settings the assumption that all axioms
fail with the same probability is rarely the case. For example, Roussey et al. [6] present a list of anti-patterns where an anti-pattern
is a set of axioms, such as fC1 v 8R:C2; C1 v 8R:C3; C2  :C3g
that corresponds to a minimal conflict set. The study performed
by [6] shows that such conflict sets often occur in practice due to
frequent misuse of certain language constructs like quantification
or disjointness. Such studies are ideal sources for estimating prior
fault probabilities. However, this is beyond the scope of this paper.
Our approach for computing the prior fault probabilities of axioms is inspired by Rector et al. [5] and considers the syntax of a
knowledge representation language, such as restrictions, conjunc-
tion, and negation. For instance, if a user frequently changes the
universal to the existential quantifier and vice versa in order to
restore coherency, then we can assume that axioms including such
restrictions are more likely to fail than the other ones. In [5] the
authors report that in most cases inconsistent ontologies are
created because users (a) mix up 8r:S and 9r:S, (b) mix up :9r:S
and 9r::S, (c) mix up t and u, (d) wrongly assume that classes
are disjoint by default or overuse disjointness, or (e) wrongly apply

Table 3
Possible queries in Example 2.

Query
Q 1 : fB v M3g
Q 2 : fBwg
Q 3 : fM1 v Bg
Q 4 : fM1w; M2ug
Q 5 : fAwg
Q 6 : fM2 v Dg
Q 7 : fM3ug

fD1; D2; D4g
fD3; D4g
fD1; D3; D4g
fD2; D3; D4g
fD2g
fD1; D2g
fD4g

fD3g
fD2g
fD2g
fD1g
fD3; D4g

D;

fD1g

fD1g
fD3;D4g
fD1;D2;D3g

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

in axi

is faulty. E.g.

paxi 14 pFse1 [ Fse2 [ . . . [ Fsen
where Fse1 . . . Fsen represent the events that the occurrence of a syntax element sej
for ax2 of Example 2
pax2 14 pFv [ F: [ F9 [ Fu [ F9. Assuming that each occurrence
of a syntax element fails independently, i.e. an erroneous usage of
a syntax element sek makes it neither more nor less probable that
an occurrence of syntax element sej is faulty, the failure probability
of an axiom is computed as:
paxi 14 1 
1  Fsecse

se2S

where csej returns number of occurrences of the syntax element
sej in an axiom axi. If among other failure probabilities the user
states that pFv 14 0:001; pF: 14 0:01; pF9 14 0:05 and pFu 14
0:001 then pax2 14 pFv [ F: [ F9 [ Fu [ F9 14 0:108.
Given the failure probabilities paxi of axioms, the diagnosis
algorithm first calculates the a priori probability pDj that Dj is
the target diagnosis. Since all axioms fail independently, this probability can be computed as [18]:
pDj 14

1  paxm

paxn

axn2Dj

axm2OnDj

The prior probabilities for diagnoses are then used to initialize
an iterative algorithm that includes two main steps: (a) the selection of the best query and (b) updating the diagnoses probabilities
given query feedback.

According to information theory the best query is the one that,
given the answer of an oracle, minimizes the expected entropy of
the set of diagnoses [18]. Let pQ i 14 yes be the probability that
query Q i is answered with yes and pQ i 14 no be the probability
for the answer no. Furthermore, let pDjjQ i 14 yes be the probability of diagnosis Dj after the oracle answers yes and pDjjQ i 14 no be
the probability after the oracle answers no. The expected entropy
after querying Q i is:
HeQ i 14

pDjjQ i 14 vlog2 pDjjQ i 14 v

pQ i 14 vx 

v2fyes;nog

Dj2D

Based on a one-step-look-ahead information theoretic measure,
the query which minimizes the expected entropy is considered
best. This formula can be simplified to the following score function
[18] which we use to evaluate all available queries and select the
one with the minimum score to maximize information gain:
scQ i 14

pQ i 14 vlog2 pQ i 14 v

  pD;

i  1

v2fyes;nog

where v 2 fyes; nog is a feedback of an oracle and D;
i is the set of
diagnoses which do not make any predictions for the query Q i.
The probability of the set of diagnoses pD;
i  as well as of any other
set of diagnoses Di like DP
pDi 14

is computed as:

i and DN

pDj

Dj2Di

because by Definition 2, each diagnosis uniquely partitions all of the
axioms of an ontology O into two sets, correct and faulty, and thus
all diagnoses are mutually exclusive events.

i ; DN

i and D;

Since, for a query Q i, the set of diagnoses D can be partitioned
i , the probability that an oracle will an-

into the sets DP
swer a query Q i with either yes or no can be computed as:
pQ i 14 yes 14 pDP
pQ i 14 no 14 pDN

i   pD;
i   pD;

i =2
i =2

Clearly this assumes that for each diagnosis of D;
i both outcomes
are equally likely and thus the probability that the set of diagnoses
i predicts either Q i 14 yes or Q i 14 no is pD;
D;

i =2.

Fig. 1. The search tree of the greedy algorithm.

negation. Observing that misuses of quantifiers are more likely
than other failure patterns one might find that the axioms ax2
and ax4 are more likely to be faulty than ax3 (because of the use
of quantifiers), whereas ax3 is more likely to be faulty than ax5
and ax1 (because of the use of negation).

Detailed justifications of diagnoses probabilities are given in the
next section. However, let us assume some probability distribution
of the faults according to the observations presented above such
that: (a) the diagnosis D2 is the most probable one, i.e. single fault
diagnosis of an axiom containing a negation; (b) although D4 is a
double fault diagnosis, it follows D2 closely as its axioms contain
quantifiers; (c) D1 and D3 are significantly less probable than D4
because conjunction/disjunction in ax1 and ax5 have a significantly
lower fault probability than negation in ax3. Taking this information into account asking query Q 1 is essentially useless because it
is highly probable that the target diagnosis is either D2 or D4
and, therefore, it is highly probable that the oracle will respond
with yes. Instead, asking Q 3 is more informative because regardless
of the answer we can exclude one of the highly probable diagnoses,
i.e. either D2 or D4. If the oracle responds to Q 3 with no then D2 is
the only remaining diagnosis. However, if the oracle responds with
yes, diagnoses D4; D3, and D1 remain, where D4 is significantly
more probable compared to diagnoses D3 and D1. If the difference
between the probabilities of the diagnoses is high enough such
that D4 can be accepted as the target diagnosis, no additional questions are required. Obviously this strategy can lead to a substantial
reduction in the number of queries compared to myopic approaches as we demonstrate in our evaluation.

Note that in real-world application scenarios failure patterns
and their probabilities can be discovered by analyzing the debugging actions of a user in an ontology editor, like Protege. Learning
of fault probabilities can be used to personalize the query selection algorithm to prefer user-specific faults. However, as our evaluation shows, even a rough estimate of the probabilities is capable
of outperforming the split-in-half heuristic.

3. Entropy-based query selection

To select the best query we exploit a priori failure probabilities
of each axiom derived from the syntax of description logics or
some other knowledge representation language, such as OWL. That
is, the user is able to specify own beliefs in terms of the probability
of syntax element such as 8; 9; andu, being erroneous; alterna-
tively, the debugger can compute these probabilities by analyzing
the frequency of various syntax elements in the target diagnoses of
different debugging sessions. If no failure information is available
then the debugger can initialize all of the probabilities with some
small value. Compared to statistically well-founded probabilities,
the latter approach provides a suboptimal but useful diagnosis
discrimination process, as discussed in the evaluation.

Given the failure probabilities of all syntax elements se 2 S of a
knowledge representation language used in O, we can compute the
failure probability of an axiom taxi 2 O

of the diagnoses must be updated to take the new information into
account. The update is made using Bayes rule for each Dj 2 D:
pDjjQ s 14 v 14 pQ s 14 vjDjpDj
where the denominator pQ s 14 v is known from the query selection step (Eq. (4)) and pDj is either a prior probability (Eq. (2))
or is a probability calculated using Eq. (5) after a previous iteration
of the debugging algorithm. We assign pQ s 14 vjDj as follows:

pQ s 14 v

1;
0;

2 ;

if Dj predicted Q s 14 v;
if Dj is rejected by Q s 14 v;
if Dj 2 D;

8><
>:

pQ s 14 vjDj 14

Example 1 (continued). Suppose that the debugger is not provided
with any information about possible failures and therefore
assumes that all syntax elements fail with the same probability
0.01 and therefore paxi 14 0:01 for all axi 2 O. Using Eq. (2) we can
calculate probabilities for each diagnosis. For instance, D1 suggests
that only one axiom ax1 should be modified by the user. Hence, we
can calculate the probability of diagnosis D1 as pD1 14 pax1
1  pax21  pax31  pax4 14 0:0097. All other minimal
diagnoses have the same probability, since every other minimal
diagnosis suggests the modification of one axiom. To simplify the
discussion we only consider minimal diagnoses for query selection.
Therefore, the prior probabilities of the diagnoses can be normalized to pDj 14 pDj=

Dj2DpDj and are equal to 0.25.

Given the prior probabilities of the diagnoses and a set of queries (see Table 2) we evaluate the score function (Eq. (3)) for each
query. E.g. for the first query Q 1 : fBwg the probability pD; 14 0
and the probabilities of both the positive and negative outcomes
are: pQ 1 14 1 14 pD2  pD3  pD4 14 0:75 and pQ 1 14 0 14
pD1 14 0:25. Therefore the query score is scQ 1 14 0:1887.
The scores computed during the initial stage (see Table 4) suggest that Q 2 is the best query. Taking into account that D1 is the
target diagnosis the oracle answers no to the query. The additional
information obtained from the answer is then used to update the
probabilities of diagnoses using the Eq. (5). Since D1 and D2
predicted this answer, their probabilities are updated, pD1 14
pD2 14 1=pQ 2 14 1 14 0:5. The probabilities of diagnoses D3 and
D4 which are rejected by the oracles answer are also updated,
pD3 14 pD4 14 0.

In the next iteration the algorithm recomputes the scores using
the updated probabilities. The results show that Q 1 is the best
query. The other two queries Q 2 and Q 3 are irrelevant since no
information will be gained if they are asked. Given the oracles negative feedback to Q 1, we update the probabilities pD1 14 1 and
pD2 14 0. In this case the target diagnosis D1 was identified using
the same number of steps as the split-in-half heuristic.

However, if the user specifies that the first axiom is more likely
to fail, e.g. pax1 14 0:025, then Q 1 : fBwg will be selected first
(see Table 5). The recalculation of the probabilities given the negative outcome Q 1 14 0 sets pD1 14 1 and pD2 14 pD3 14
pD4 14 0. Therefore the debugger identifies the target diagnosis
in only one step.

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

Example 2 (continued). Suppose that in ax4 the user specified 8s:A
instead of 9s:A and :9s:M3 instead of 9s::M3 in ax2. Therefore D4 is
the target diagnosis. Moreover, assume that the debugger is
provided with observations of three types of faults: (1) conjunc-
tion/disjunction occurs with probability p1 14 0:001, (2) negation
p2 14 0:01, and (3) restrictions p3 14 0:05. Using Eq. (1) we can
calculate the probability of the axioms containing an error:
pax1 14 0:0019; pax2 14 0:1074; pax3 14 0:012; pax4 14 0:051,
and pax5 14 0:001. These probabilities are exploited to calculate
the prior probabilities of the diagnoses (see Table 6) and to
initialize the query selection process. To simplify matters we focus
on the set of minimal diagnoses.

In the first iteration the algorithm determines that Q 3 is the best
query and asks the oracle whether Ot  fM1 v Bg is true or not
(see Table 7). The obtained information is then used to recalculate
the probabilities of the diagnoses and to compute the next best
subsequent query, i.e. Q 4, and so on. The query process stops after
the third query, since D4 is the only diagnosis that has the probability pD4 > 0.
Given the feedback of the oracle Q 4 14 yes for the second query,
the updated probabilities of the diagnoses show that the target diagnosis has a probability of pD4 14 0:9918 whereas pD3 is only
0.0082. In order to reduce the number of queries a user can specify
a threshold, e.g. r 14 0:95. If the absolute difference in probabilities
of two most probable diagnoses is greater than this threshold, the
query process stops and returns the most probable diagnosis. There-
fore, in this example the debugger based on the entropy query selection requires less queries than the split-in-half heuristic. Note that
already after the first answer Q 3 14 yes the most probable diagnosis
D4 is three times more likely than the second most probable diagnosis D1. Given such a great difference we could suggest to stop the
query process after the first answer if the user would set r 14 0:65.

4. Implementation details

The iterative ontology debugger (Algorithm 1) takes a faulty
ontology O as input. Optionally, a user can provide a set of axioms
B that are known to be correct as well as a set P of axioms that must
be entailed by the target ontology and a set N of axioms that must
not. If these sets are not given, the corresponding input arguments
are initialized with ;. Moreover, the algorithm takes a set FP of fault
probabilities for axioms axi 2 O, which can be computed as
described in Section 3 by exploiting knowledge about typical user
errors. Alternatively, if no estimates of such probabilities are avail-
able, all probability values can be initialized using a small constant.
We show the results of such a strategy in our evaluation section.
The two other arguments r and n are used to improve the performance of the algorithm. r specifies the diagnosis acceptance
threshold, i.e. the minimum difference in probabilities between
the most likely and second-most likely diagnoses. The parameter
n defines the maximum number of most probable diagnoses that
should be considered by the algorithm during each iteration. A
further performance gain in Algorithm 1 can be achieved if we
approximate the set of the n most probable diagnoses with the
set of the n most probable minimal diagnoses, i.e. we neglect
non-minimal diagnoses. We call this set of at most n most probable
minimal diagnoses the leading diagnoses. Note, under the reason-

Table 4
Expected scores for minimized queries (paxi 14 0:01).

Query
Q 1 : fBwg
Q 2 : fCwg
Q 3 : fQwg

Initial score

Table 5
Expected
pax1 14 0:025; pax2 14 pax3 14 pax4 14 0:01.

scores

for

Q 2 14 yes

Query
Q 1 : fBwg
Q 2 : fCwg
Q 3 : fQwg

minimized

queries

Initial score

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

fault probabilities FP, the algorithm expands a leaf node in a
search-tree if it is an element of the path corresponding to the maximum probability hitting set of minimal conflict sets computed so
far. The probability of each minimal hitting set can be computed
using Eq. (2). Consequently, the algorithm computes a set of diagnoses ordered by their probability starting from the most probable
one. HS-TREE terminates if either the n most probable minimal diagnoses are identified or no further minimal diagnoses can be found.
Thus the algorithm computes at most n minimal diagnoses regardless of the number of all minimal diagnoses.

HS-TREE uses QUICKXPLAIN to compute required minimal conflicts.
This algorithm, given a set of axioms AX and a set of correct axioms
B returns a minimal conflict set CS # AX, or ; if axioms AX [ B are
consistent. In the worst case, to compute a minimal conflict QUICKXPLAIN performs 2klogs=k  1 consistency checks, where k is the
size of the generated minimal conflict set and s is the number of
axioms in the ontology. In the best case only logs=k  2k are performed [19]. Importantly, the size of the ontology is contained in
the log function. Therefore, the time needed for consistency checks
in our test ontologies remained below 0.2 s, even for real world
knowledge bases with thousands of axioms. The maximum time
to compute a minimal conflict was observed in the Sweet-JPL
ontology and took approx. 5 s (see Table 9).

In order to take past answers into account the HS-TREE updates
the prior probabilities of the diagnoses by evaluating Eq. (5). All required data is stored in the query history QH as well as in the sets P
and N. When complete, HS-TREE returns a set of tuples of the form
hDi; pDii where Di is contained in the set of the n most probable
minimal diagnoses (leading diagnoses) and pDi is its probability
calculated using Eqs. (2) and (5).

Algorithm 1. ONTODEBUGGINGO;B; P; N; FP; n; r

Answers

Table 6
Probabilities of diagnoses after answers.
D1

Prior
Q 3 14 yes
Q 3 14 yes; Q 4 14 yes
Q 3 14 yes; Q 4 14 yes; Q 1 14 yes

D2

D3

D4

Table 7
Expected scores for queries.

Queries
Q 1 : fB v M3g
Q 2 : fBwg
Q 3 : fM1 v Bg
Q 4 : fM1w; M2ug
Q 5 : fAwg
Q 6 : fM2 v Dg
Q 7 : fM3ug

Initial

Q 3 14 yes

Q 3 14 yes; Q 4 14 yes

able assumption that the fault probability of each axiom paxi is
less than 0.5, for every non-minimal diagnosis ND a minimal diagnosis D  ND exists which from Eq. (2) is more probable than ND.
Consequently the query selection algorithm presented here operates on the set of minimal diagnoses instead of all diagnoses (i.e.
non-minimal diagnoses are excluded). However, the algorithm
can be adapted with moderate effort to also consider non-minimal
diagnoses.

We use the approach proposed by Friedrich et al. [9] to compute
diagnoses and employ the combination of two algorithms, QUICKXPLAIN
[19] and HS-TREE [17]. In a standard implementation the latter is a
breadth-first search algorithm that takes an ontology O, sets P and
N, and the maximum number of most probable minimal diagnoses
n as an input. The algorithm generates minimal hitting sets using
minimal conflict sets, which are computed on-demand. This is motivated by the fact that in some circumstances a subset of all minimal
conflict sets is sufficient for generating a subset of all required minimal diagnoses. For instance, in Example 2 the user wants to compute only n 14 2 leading minimal diagnoses and a minimal conflict
search algorithm returns CS1. In this case HS-TREE identifies two required minimal diagnoses D1 and D2 and avoiding the computation
of the minimal conflict set CS2. Of course, in the worst case, when all
minimal diagnoses have to be computed the algorithm should compute all minimal conflict sets. In addition, the HS-TREE generation
reuses minimal conflict sets in order to avoid unnecessary computa-
tions. Thus, in the real-world scenarios we evaluated (see Table 8),
less than 10 minimal conflict sets were contained in the faulty ontologies having at most 13 elements while the maximal cardinality of
minimal diagnoses was observed to be at most 9. Therefore, space
limitations were not a problem for the breadth-first generation.
However, for scenarios involving diagnoses of greater cardinalities
iterative-deepening strategies could be applied.

In our implementation of HS-TREE we use the uniform-cost
search strategy. Given additional information in terms of axiom

Table 8
Diagnosis results for several of the real-world ontologies presented in [8]. #C/#P/#I are the number of concepts, properties and individuals in each ontology. #CS/min/max are the
number of conflict sets, and their minimum and maximum cardinality. The same notation is used for diagnoses #D/min/max. The ontologies are available upon request.

Ontology

Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

1.
2.
3.
4.
5.
6.
7.

ALCHF D
ALCON D
ALCHOF D

SOIN D


Axioms

#C/#P/#I

#CS/min/max

#D/min/max

Domain

48/20/0
21/5/6
1537/121/50
183/44/0
30/12/4
339/53/482
445/93/183

6/5/6
3/4/4
1/13/13
3/2/6
4/3/5
8/3/4
9/2/6

6/1/3
10/1/3
13/1/1
48/3/3
90/3/4
864/4/8
1782/6/9

Chemical elements
Training
Earthscience
Biological science
Training
Mid-level
Mid-level

Table 9
Min/avg/max time and calls required to compute the nine leading most probable diagnoses as well as all diagnoses for the real-world ontologies. Values are given for each stage,
i.e. consistency checking, computation of minimal conflicts and minimal diagnoses, together with the total runtime needed to compute the diagnoses. All time values are 15 trial
averages and are given in milliseconds.

Ontology

Chemical

Koala

Sweet-JPL

miniTambis

University

Economy

Transportation

Leading diagnoses

All diagnoses

Consistency

Conflicts

Diagnoses

Consistency

Conflicts

Diagnoses

0/3/8

90/107/128

1/97/326

0/3/18

105/130/179

2/126/402

Overall runtime: 723

Overall runtime: 892

0/1/3

19/25/30

Overall runtime: 120

0/11/70

0/2/4

24/30/37

0/12/105

Overall runtime: 148

1/31/112

5185/5185/5185

0/586/5332

31/106/195

5192/5192/5192

1/438/5319

Overall runtime: 5991

Overall runtime: 6312

0/5/14

84/157/210

0/57/504

1/5/15

88/167/225

3/19/537

Overall runtime: 586

Overall runtime: 1027

0/2/3

31/41/54

Overall runtime: 205

0/20/157

0/2/5

37/46/60

2/5/200

Overall runtime: 536

1/12/26

410/460/569

0/282/2085

1/9/80

418/510/681

16/25/1929

Overall runtime: 2857

Overall runtime: 25,369

0/11/58

237/438/683

0/352/3176

1/9/130

222/429/636

16/29/6394

Overall runtime: 3671

Overall runtime: 65,010

Time
Calls

Time
Calls

Time
Calls

Time
Calls

Time
Calls

Time
Calls

Time
Calls

Algorithm 2. SELECTQUERYDP;O;B; P

In the query-selection phase Algorithm 1 calls SELECTQUERY
function (Algorithm 2) to generate a tuple T 14 hQ ; DP; DN; D;i,
where Q is the minimum score query (Eq. (3)) and DP; DN and D;
the sets of diagnoses constituting the partition. The generation

algorithm carries out a depth-first search, removing the top
element of the set D and calling itself recursively to generate all
possible subsets of the leading diagnoses. The set of leading
diagnoses D is extracted from the set of tuples DP by the GETDIAGNOSES function. In each leaf node of the search tree the GENERATE
function calls CREATEQUERY creates a query given a set of diagnoses
DP by computing common entailments and partitioning the set
of diagnoses D n DP, as described in Section 2.3. If a query for the
set DP does not exist (i.e. there are no common entailments) or
DP 14 ; then CREATEQUERY returns an empty tuple T 14 h;; ;;;;;i. In
all inner nodes of the tree the algorithm selects a tuple that
corresponds to a query with the minimum score as found using
the GETSCORE function. This function may implement the entropybased measure (Eq. (3)),
split-in-half or any other preference
criteria. Given an empty tuple T 14 h;;;;;;;i the function returns
the highest possible score of a used measure. In general, CREATEQUERY
is called 2n times, where we set n 14 9 in our evaluation. Further-
more, for each leading diagnosis not in DP, CREATEQUERY has to check
if the associated query is entailed. If a query is not entailed, a consistency check has to be performed. Entailments are determined by
classification/realization and a subset check of the generated sen-
tences. Common entailments are computed by exploiting the intersection of entailments for each diagnosis contained in DP. Note that
the entailments for each leading diagnosis are computed just once
and reused in for subsequent calls of CREATEQUERY.

In the function MINIMIZEQUERY, the query Q of the resulting tuple
hQ ; DP; DN; D;i is iteratively reduced by applying QUICKXPLAIN such
that sets DP; DN and D; are preserved. This is implemented by

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

replacing the consistency checks performed by QUICKXPLAIN with
checks that ensure that the reduction of the query preserves the
partition. In order to check if a partition is preserved, a consis-
tency/entailment check is performed for each element in DN and
D;. Elements of DP need not be checked because these elements
entail the query and therefore any reduction. In the worst case
n2k logs=k  2k consistency checks have to be performed in
MINIMIZEQUERY where k is the length of the minimized query. Entailments of leading diagnoses are reused.

Algorithm 1 invokes the function GETQUERY to obtain the query
from the tuple stored in T and calls GETANSWER to query the oracle.
Depending on the answer, Algorithm 1 extends either the set P
or the set N and thus excludes diagnoses not compliant with the
query answer from the results of HS-TREE in further iterations. Note,
the algorithm can be easily adapted to allow the oracle to reject a
query if the answer is unknown. In this case the algorithm proceeds with the next best query (w.r.t. the GETSCORE function) until
no further queries are available.

Algorithm 1 stops if the difference in the probabilities of the top
two diagnoses is greater than the acceptance threshold r or if no
query can be used to differentiate between the remaining diagnoses (i.e. the score of the minimum score query equals to the maximum score of the used measure). The most probable diagnosis is
then returned to the user. If it is impossible to differentiate between a number of highly probable minimal diagnoses, the algorithm returns a set that includes all of them. Moreover, in the
first case (termination due to r), the algorithm can continue if
the user is not satisfied with the returned diagnosis and at least
one further query exists.

Additional performance improvements can be achieved by
using greedy strategies in Algorithm 2. The idea is to guide the
search such that a leaf node of the left-most branch of a search tree
contains a set of diagnoses DP that might result in a tuple
hQ ; DP; DN; D;i with a low-score query. This method is based on

the property of Eq. (3) that scQ 14 0 if

pDi 14

pDj 14 0:5 and pD; 14 0

Di2DP

Dj2DN

Consequently, the query selection problem can be presented as
a two-way number partitioning problem: given a set of numbers,
divide them into two sets such that the difference between the
sums of the numbers in each set is as small as possible. The Complete KarmarkarKarp (CKK) algorithm [20], which is one of the
best algorithms developed for the two-way partitioning problem,
corresponds to an extension of the Algorithm 2 with a set differencing heuristic [21]. The algorithm stops if the optimal solution
to the two-way partitioning problem is found or if there are no further subsets to be investigated. In the latter case the best found
solution is returned.

The main drawback of applying CKK to the query selection process is that none of the pruning techniques can be used. Also even
if the algorithm finds an optimal solution to the two-way partitioning problem there just might be no query for a found set of diagnoses DP. Moreover, since the algorithm is complete it still has to
investigate all subsets of the set of diagnoses in order to find the
minimum score query. To avoid this exhaustive search we extended CKK with an additional termination criterion: the search
stops if a query is found with a score below some predefined
threshold c. In our evaluation section we demonstrate substantial
savings by applying the CKK partitioning algorithm.

To sum up, the proposed method depends on the efficiency of
the classification/realization system and consistency/coherency
checks given a particular ontology. The number of calls to a reasoning system can be reduced by decreasing the number of leading
diagnoses n. However, the more leading diagnoses provide the
more data for generating the next best query. Consequently, by

varying the number of leading diagnoses it is possible to balance
runtime with the number of queries needed to isolate the target
diagnosis.5

5. Evaluation

We evaluated our approach using the real-world ontologies
presented in Table 8 with the aim of demonstrating its applicability real-world settings. In addition, we employed generated examples to perform controlled experiments where the number of
minimal diagnoses and their cardinality could be varied to make
the identification of the target diagnosis more difficult. Finally,
we carried out a set of tests using randomly modified large realworld ontologies to provide some insights on the scalability of
the suggested debugging method.

For the first test we created a generator which takes a consistent and coherent ontology, a set of fault patterns together with
their probabilities, the minimum number of minimum cardinality
diagnoses m, and the required cardinality jDtj of these minimum
cardinality diagnoses as inputs. We also assumed that the target
diagnosis has cardinality jDtj. The output of the generator is an
alteration of the input ontology for which at least the given
number of minimum cardinality diagnoses with the required cardinality exist. Furthermore, to introduce inconsistencies (incoheren-
cies), the generator applies fault patterns randomly to the input
ontology depending on their probabilities.

In this experiment we took five fault patterns from a case study
reported by Rector et al. [5] and assigned fault probabilities according to their observations of typical user errors. Thus we assumed
that in cases (a) and (b) (see Section 2.3), where an axiom includes
some roles (i.e. property assertions), axiom descriptions are faulty
with a probability of 0.025, in cases (c) and (d) 0.01 and in case (e)
0.001. In each iteration, the generator randomly selected an axiom
to be altered and applied a fault pattern. Following this, another
axiom was selected using the concept taxonomy and altered correspondingly to introduce an inconsistency (incoherency). The fault
patterns were randomly selected in each step using the probabilities provided above.

For instance, given the description of a randomly selected concept A and the fault pattern misuse of negation, we added the
construct u:X to the description of A, where X is a new concept
name. Next, we randomly selected concepts B and S such that
S v A and S v B and added uX to the description of B. During the
generation process, we applied the HS-TREE algorithm after each
introduction of an incoherency/inconsistency to control
two
parameters: the minimum number of minimal cardinality diagnoses in the ontology and their cardinality. The generator continues
to introduce incoherences/inconsistencies until
the specified
parameter values are reached. For instance, if the minimum number of minimum cardinality diagnoses is equal to m 14 6 and their
cardinality is jDtj 14 4, then the generated ontology will include at
least 6 diagnoses of cardinality 4 and possibly some additional
number of minimal diagnoses of higher cardinalities.

The resulting faulty ontology as well as the fault patterns and
their probabilities were inputs for the ontology debugger. The
acceptance threshold r was set to 0.95 and the number of most
probable minimal diagnoses n was set to 9. In addition, one of
the minimal diagnoses with the required cardinality was randomly
selected as the target diagnosis. Note, the target ontology is not
equal to the original ontology, but rather a corrected version of
the altered one in which the faulty axioms were repaired by replac-

5 The source code as well as precompiled binaries can be downloaded from http://
rmbd.googlecode.com. The package also includes a Protege-plugin implementing the
methods as described.

Fig. 2. Average number of queries required to select the target diagnosis Dt with
threshold r 14 0:95. Random and split-in-half are shown for the cardinality of
minimal diagnoses jDtj 14 2.

ing them with their original (correct) versions according to the target diagnosis. The tests were performed using the ontologies bike2
to bike9, bcs3, galen and galen2 from Racers benchmark suite.6

The average results of the evaluation performed on each test
ontology (presented in Fig. 2) show that the entropy-based
approach outperforms the split-in-half heuristic as well as the
random query selection strategy by more than 50% for the
jDtj 14 2 case due to its ability to estimate the probabilities of diagnoses and to stop once the target diagnosis crossed the acceptance
threshold. On average the algorithm required 8 s to generate a
query. In addition, Fig. 2 shows that the number of queries required increases as the cardinality of the target diagnosis increases,
regardless of the method. Despite this, the entropy-based approach
remains better than the split-in-half method for diagnoses with
increasing cardinality. The approach did however require more
queries to discriminate between high cardinality diagnoses because in such cases more minimal conflicts were generated. Conse-
quently, the debugger should consider more minimal diagnoses in
order to identify the target one.

For the next test we selected seven real-world ontologies described in Tables 8 and 9.7 Performance of both the entropy-based
and split-in-half selection strategies was evaluated using a variety
of different prior fault probabilities to investigate under which conditions the entropy-based method should be preferred.

In our experiments we distinguished between three different
distributions of prior fault probabilities: extreme, moderate and
uniform (see Fig. 3 for an example). The extreme distribution
simulates a situation in which very high failure probabilities are
assigned to a small number of syntax elements. That is, the provider of the estimates is quite sure that exactly these elements
are causing a fault. For instance, it may be well known that a user
has problems formulating restrictions in OWL whereas all other
elements, such as subsumption and conjunction, are well under-
stood. In the case of a moderate distribution the estimates provide
a slight bias towards some syntax elements. This distribution has
the same motivation as the extreme one, however, in this case
the probability estimator is less sure about the sources of possible
errors in axioms. Both extreme and moderate distributions correspond to the exponential distribution with k 14 1:75 and k 14 0:5
respectively. The uniform distribution models the situation where
no prior fault probabilities are provided and the system assigns
equal probabilities to all syntax elements found in a faulty ontol-
ogy. Of course the prior probabilities of diagnoses may not reflect

6 Available at http://w ww.racer-systems.com/products/download/

benchmark.phtml.

7 All experiments were performed on a PC with Core2 Duo (E8400), 3 Ghz with

8 Gb RAM, running Windows 7 and Java 6.

Fig. 3. Example of prior fault probabilities of syntax elements sampled from
extreme, moderate and uniform distributions.

the actual situation. Therefore, for each of the three distributions
we differentiate between good, average and bad cases. In the good
case the estimates of the prior fault probabilities are correct and
the target diagnosis is assigned a high probability. The average case
corresponds to the situation when the target diagnosis is neither
favored nor penalized by the priors. In the bad case the prior distribution is unreasonable and disfavors the target diagnosis by
assigning it a low probability.

We executed 30 tests for each of the combinations of the distributions and cases with an acceptance threshold r 14 0:85 and a required number of most probable minimal diagnoses n 14 9. Each
iteration started with the generation of a set of prior fault probabilities of syntax elements by sampling from a selected distribution
(extreme, moderate or uniform). Given the priors we computed
the set of all minimal diagnoses D of a given ontology and selected
the target one according to the chosen case (good, average or bad).
In the good case the prior probabilities favor the target diagnosis
and, therefore, it should be selected from the diagnoses with high
probability. The set of diagnoses was ordered according to their
probabilities and the algorithm iterated through the set starting
from the most probable element. In the first iteration the most
probable minimal diagnosis D1 is added to the set G. In next iteration j a diagnosis Dj was added to the set G if
3 and to
the set A if
3. The obtained set G contained all most
probable diagnoses which we considered as good. All diagnoses
in the set A n G were classified as average and the remaining diagnoses D n A as bad. Depending on the selected case we randomly
selected one of the diagnoses as the target from the appropriate
set.

i6jpDi 6 2

i6jpDi 6 1

The results of the evaluation presented in Table 10 show that
the entropy-based query selection approach clearly outperforms
split-in-half in good and average cases for the three probability
distributions. The average time required by the debugger to perform such basic operations as consistency checking, computation
of minimal conflicts and diagnoses is presented in Table 11. The results indicate that on average at most 17 s required to compute up
to 9 minimal diagnoses and a query. Moreover, the number of
axioms in a query remains reasonable in most of the cases i.e.
between 1 and 4 axioms per query.

In the uniform case better results were observed since the diagnoses have different cardinality and structure, i.e. they include different syntax elements. Consequently, even if equal probabilities
for all syntax elements (uniform distribution) are given, the probabilities of diagnoses are different. Axioms with a greater number
of syntax elements receive a higher fault probability. Also, diagnoses with a smaller cardinality in many cases receive a higher
probability. This information provides enough bias to favor the
entropy-based method.

In the bad case, where the target diagnosis received a low probability and no information regarding the prior fault probabilities

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

Table 10
Minimum, average and maximum number of queries required by the entropy-based and split-in-half query selection methods to identify the target diagnosis in real-world
ontologies. Ontologies are ordered by the number of diagnoses.

Ontology

Case

Distribution

Extreme

Min

Avg.

Max

Min

Avg.

Max

Moderate

Uniform

Min

Avg.

Max

Entropy-based query selection

Chemical

Koala

Sweet-JPL

miniTambis

University

Economy

Transportation

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Split-in-half query selection

Chemical

Koala

Sweet-JPL

miniTambis

University

Economy

Transportation

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Good
Avg.
Bad

Table 11
Average time required to compute at most nine minimal diagnoses (DT) and a query (QT) in each iteration, as well as the average number of axioms in a query after minimization
(QL). The averages are shown for extreme, moderate and uniform distributions using the entropy-based query selection method. Time is measured in milliseconds.

Ontology

Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

Good

Average

Bad

was given, we observed that the performance of the entropymethod improved as more queries were posed. In particular, in
the University ontology the performance is essentially similar

(7.27 vs. 7.37) whereas in the Economy and Transportation ontology the entropy-based method can save and average of two
queries.

diagnoses,

such as

Economy

of minimal

Split-in-half appears to be particularly inefficient in all good,
average and bad cases when applied to ontologies with a large
number
and
Transportation. The main problem is that no stop criteria can be
used with the greedy method as it is unable to provide any
ordering on the set of diagnoses. Instead, the method continues
until no further queries can be generated, i.e. only one minimal
diagnosis exists or there are no discriminating queries. Conversely,
the entropy-based method is able to improve its probability estimates using Bayes-updates as more queries are answered and to
exploit the differences in the probabilities in order to decide when
to stop.

Fig. 4. Average time/query gain resulting from the application of the extended CKK
partitioning algorithm. The whiskers indicate the maximum and minimum possible
average gain of queries/time using extended CKK.

The most significant gains are achieved for ontologies with
many minimal diagnoses and for the average and good cases, e.g.
the target diagnosis is within the first or second third of the minimal diagnoses ranked by their prior probability. In these cases
the entropy-based method can save up to 60% of the queries.

Therefore, we can conclude that even rough estimates of the
prior fault probabilities are sufficient, provided that the target
diagnosis is not significantly penalized. Even if no fault probabilities are available and there are many minimal diagnoses, the
entropy-based method is advantageous. The differences between
probabilities of individual syntax elements appears not to influence the results of the query selection process and affect only
the number of outliers, i.e. cases in which the diagnosis approach
required either few or many queries compared to the average.

Another interesting observation is that often both methods
eliminated more than n diagnoses in one iteration. For instance,
in the case of the Transportation ontology both methods were able
to remove hundreds of minimal diagnoses with a small number of
queries. This behavior appears to stem from relations between the
diagnoses. That is, the addition of a query to either P or N allows
the method to remove not only the diagnoses in sets DP or DN,
but also some unobserved diagnoses that were not in any of the
sets of n leading diagnoses computed by HS-TREE. Given the sets
P and N, HS-TREE automatically invalidates all diagnoses which do
not fulfill the requirements (see Definition 2).

The extended CKK method presented in Section 4 was evaluated
in the same settings as the complete Algorithm 2 with acceptance
threshold c 14 0:1. The obtained results presented in Fig. 4 show
that the extended CKK method decreases the length of a debugging
session by at least 60% while requiring on average 0.1 queries more
than Algorithm 2. In some cases (mostly for the uniform distribu-
tion) the debugger using CKK search required even fewer queries
than Algorithm 2 because of the inherent uncertainty of the do-
main. The plot of the average time required by Algorithm 2 and
CKK to identify the target diagnosis presented in Fig. 5 shows that
the application of the latter can reduce runtime significantly.

In the last experiment we tried to simulate an expert developing large real-world ontologies8 as described in Table 12. Often in
such settings an expert makes small changes to the ontology and
then runs the reasoner to verify that the changes are valid, i.e. the
ontology is consistent and its entailments are correct. To simulate
this scenario we used the generator described in the first experiment
to introduce from 1 to 3 random changes that would make the ontology incoherent. Then, for each modified ontology, we performed 15
tests using the fault distributions as in the second test. The results
obtained by the entropy-based query selection method using CKK
for query computation are presented in Table 13. These results show
that the method can be used for analysis of large ontologies with
over 33,000 axioms while requiring a user to wait for only a minute
to compute the next query.

8 The ontologies taken from TONES repository http://owl.cs.manchester.ac.uk/

repository.

Fig. 5. Average time required to identify the target diagnosis using CKK and brute
force query selection algorithms.

K. Shchekotykhin et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1213 (2012) 88103

Table 12
Statistics for the real-world ontologies used in the stress-tests measured for a single
random alteration. #CS/min/max are the number of minimal conflict sets, and their
minimum and maximum cardinality. The same notation is used for diagnoses #D/
min/max. The minimum/average/maximum time required to make a consistency
check (Consistency), compute a minimal conflict set (QuickXplain) and a minimal
diagnosis are measured in milliseconds. Overall runtime indicates the time required
to compute all minimal diagnoses in milliseconds.

Ontology

Cton

Opengalen-no-propchains

Axioms

#CS/min/max
#D/min/max
Consistency
QuickXplain
Diagnosis
Overall runtime

6/3/7
15/1/5
5/209/1078
17,565/20,312/38,594
1/5285/38,594

ALEHIF D
9/5/8
110/2/6
1/98/471
7634/10,175/12,622
10/1043/19,543

Table 13
Average values measured for extreme, moderate and uniform distributions in each of
the good, average and bad cases. #Query is the number of queries required to find the
target diagnosis. Overall runtime as well as the time required to compute a query (QT)
and at least nine minimal diagnoses (DT) are given in milliseconds. Query length (QL)
shows the average number of axioms in a query.

Ontology

#Query

Overall

Good
Cton
Opengalen-no-propchains

Average
Cton
Opengalen-no-propchains

Bad
Cton
Opengalen-no-propchains

6. Related work

Despite the range of ontology diagnosis methods available (see
[79]), to the best of our knowledge no interactive ontology debugging methods, such as our split-in-half or entropy-based meth-
ods, have been proposed so far. The idea of ranking of diagnoses
and proposing a target diagnosis is presented in [11]. This method
uses a number of measures such as: (a) the frequency with which
an axiom appears in conflict sets, (b) impact on an ontology in
terms of its lost entailments when an axiom is modified or re-
moved, (c) ranking of test cases, (d) provenance information about
axioms, and (e) syntactic relevance. For each axiom in a conflict set,
these measures are evaluated and combined to produce a rank va-
lue. These ranks are then used by a modified HS-TREE algorithm to
identify diagnoses with a minimal rank. However, the method fails
when a target diagnosis cannot be determined reliably with the
given a priori knowledge. In our work required information is
acquired until the target diagnosis can be identified with confi-
dence. In general, the work of [11] can be combined with the ideas
presented in this paper as axiom ranks can be taken into account
together with other observations for calculating the prior probabilities of the diagnoses.

The idea of selecting the next best query based on the expected
entropy was exploited in the generation of decisions trees in [22]
and further refined for selecting measurements in the model-based
diagnosis of circuits in [18]. We extend these methods to query
selection in the domain of ontology debugging.

In the area of debugging logic programs, Shapiro [23] developed
debugging methods based on query answering. Roughly speaking,
Shapiros method aims to detect one fault at a time by querying an
oracle about the intended behavior of a Prolog program at hand. In

our terminology, for each answer that must not be entailed this
diagnosis approach generates one conflict at a time by exploiting
the proof tree of a Prolog program. The method then identifies a
query that splits the conflict in half. Our approach can deal with
multiple diagnoses and conflicts simultaneously which can be
exploited by query generation strategies such as split-in-half
and entropy-based methods. Whereas the split-in-half strategy
splits the set of diagnoses in half, Shapiross method focuses on
one conflict. Furthermore, the exploitation of failure probabilities
is not considered in [23]. However, Shapiros method includes
the learning of new clauses in order to cover not entailed answers.
Interleaving discrimination of diagnoses and learning of descriptions is currently not considered in our approach because of their
additional computational costs.

From a general point of view Shapiros method can be seen as a
prominent example of inductive logic programming (ILP) including
systems such as [24,25]. In particular, [25] proposes inverse entailments combined with general to specific search through a refinement graph with the goal of generating a theory (hypothesis)
which covers the examples and fulfills additional properties. Compared to ILP, the focus of our work lies on the theory revision. How-
ever, our knowledge representation languages are variants of
description logics and not logic programs. Moreover, our method
aims to discover axioms which must be changed while minimizing
user interaction. Preferences of theory changes are expressed by
probabilities which are updated through Bayes rule. Other preferences based on plausible extensions of the theory were not consid-
ered, again because of their computational costs.

Although model-based diagnosis has also been applied to logic
programs [26], constraint knowledge-bases [27] and hardware
descriptions [28], none of these approaches propose a query generation method to discriminate between diagnoses.

7. Conclusions

In this paper we presented an approach to the interactive diagnosis of ontologies. This approach is applicable to any ontology language with monotonic semantics. We showed that the axioms
generated by classification and realization reasoning services can
be exploited to generate queries which differentiate between diag-
noses. For selecting the best next query we proposed two strate-
gies: The split-in-half strategy prefers queries which allow
eliminating a half of leading diagnoses. The entropy-based strategy
employs information theoretic concepts to exploit knowledge
about the likelihood of axioms needing to be changed because
the ontology at hand is faulty. Based on the probability of an axiom
containing an error we predict the information gain produced by a
query result, enabling us to select the best subsequent query
according to a one-step-lookahead entropy-based scoring function.
We described the implementation of a interactive debugging algorithm and compared the entropy-based method with the split-in-
half strategy. Our experiments showed a significant reduction in
the number of queries required to identify the target diagnosis
when the entropy-based method is applied. Depending on the
quality of the prior probabilities the number of queries required
may be reduced by up to 60%.

In order to evaluate the robustness of the entropy-based method we experimented with different prior fault probability distributions as well as different qualities of the prior probabilities.
Furthermore, we investigated cases where knowledge about failure
probabilities is missing or inaccurate. Where such knowledge is
unavailable, the entropy-based methods ranks the diagnoses based
on the number of syntax elements contained in an axiom and the
number of axioms in a diagnosis. If we assume that this is a reasonable guess (i.e. the target diagnosis is not at the lower end of the

diagnoses ranked by their prior probabilities) then the entropybased method outperforms split-in-half. Moreover, even if the
initial guess is not reasonable, the entropy-based method improves
the accuracy of the probabilities as more questions are asked. Fur-
thermore, the applicability of the approach to real-world ontologies containing thousand of axioms was demonstrated by
extensive set of evaluations which are publicly available.
