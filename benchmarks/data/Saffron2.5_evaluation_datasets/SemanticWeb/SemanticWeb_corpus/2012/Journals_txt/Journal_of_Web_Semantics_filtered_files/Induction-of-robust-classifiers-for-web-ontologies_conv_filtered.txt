Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Induction of robust classifiers for web ontologies through kernel machines


, Claudia dAmato


Nicola Fanizzi

, Floriana Esposito

Dipartimento di Informatica, Universita degli studi di Bari, Campus Universitario, Via Orabona 4, 70125 Bari, Italy

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 3 January 2011
Received in revised form 1 November 2011
Accepted 13 November 2011
Available online 22 November 2011

Keywords:
Kernel methods
Support vector machines
Individual classification
Inductive reasoning
Similarity
Ontology mining

The paper focuses on the task of approximate classification of semantically annotated individual
resources in ontological knowledge bases. The method is based on classification models built through
kernel methods, a well-known class of effective statistical learning algorithms. Kernel functions encode
a notion of similarity among elements of some input space. The definition of a family of parametric lan-
guage-independent kernel functions for individuals occurring in an ontology allows the application of
these statistical learning methods on Semantic Web knowledge bases. The classification models induced
by kernel methods offer an alternative way to classify individuals with respect to the typical exact and
approximate deductive reasoning procedures. The proposed statistical setting enables further inductive
approaches to a variety of other tasks that can better cope with the inherent incompleteness of the
knowledge bases in the Semantic Web and with their potential incoherence due to their distributed nat-
ure. The effectiveness of the proposed method is empirically proved through experiments on the task of
approximate classification with real ontologies collected from standard repositories.

O 2011 Elsevier B.V. All rights reserved.

1. Introduction and motivation

In the context of the Semantic Web (henceforth SW) many complex applications require the accomplishment of data-intensive
tasks that can be effectively carried out by means of methods borrowed from machine learning [1]. However, while a growing
amount of metadata is being produced, most of the research effort
addresses the problem of learning for the SW (knowledge acquisition from structured or unstructured data, like text [2]). As pointed
out in [3], less attention was devoted to the problems of learning
from data and metadata in the SW [4,5] which are expressed in rich
formal representations typically supported by Description Logics
(DLs) [6].

Although, in principle, machine learning methods are generalpurpose and could be employed in various scenarios and for different tasks (such as ontology learning and evolution, clustering, anomaly detection, etc.) we focus on methods for inducing efficient
classification models from examples and demonstrate their usage
in data-driven forms of approximate concept retrieval. Indeed,
while the classification of individuals is crucial for many applica-
tions, carrying out this task through logic reasoning may be both
computationally demanding, because of its complexity, and also
error-prone because of inconsistency or (inherent) incompleteness
in the knowledge bases, which is not infrequent with heteroge-

 Principal corresponding author. Tel.: +39 080 544 2246; fax: +39 080 544 3196.
 Corresponding author.

E-mail addresses: fanizzi@di.uniba.it (N. Fanizzi), claudia.damato@di.uniba.it (C.

dAmato), esposito@di.uniba.it (F. Esposito).

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.11.003

neous and distributed data sources in the SW context. These problems are related also to dynamic aspects of the ontologies:1 e.g.,
they may be triggered by a change in the domain to be modeled,
or by previously unknown pieces of knowledge that may become
available along the time.

To deal with inconsistency, incoherency, invalidity in evolving
ontologies alternative logical reasoning procedures have been proposed (e.g., see [7,8]). Other approaches tackle the problem of
inconsistency through commonsense, default reasoning [9] or by
repairing defective knowledge bases [10]. More recently, further
alternative approximate reasoning methods based on notions of
semantic similarity have been proposed [11].

Regarding these problems from another perspective, efficient
machine learning methods, originally developed for simple data
representations, can be effectively upgraded to work with richer
structured representations, where the individuals provide a source
of instances (learning data) through their related assertions. These
methods have been shown to effectively solve supervised and unsupervised learning problems in DLs [12,13], particularly those based
on classification, clustering and ranking of individuals.

Among the other statistical

learning approaches [1], kernel
methods (also known as kernel-based learning machines) [14] represent a family of very efficient algorithms, that ultimately aim at
building classifiers by solving linear separation problems in highdimensional feature spaces whereto a kernel function implicitly
maps the original (even non numerical) features of the considered

1 See http://www.ontologydynamics.org for concrete cases.

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

instances (kernel trick). Essentially, the underlying classification
procedures are based on the density of pre-classified instances
(examples) w.r.t. a target concept, i.e., the one the classifier is induced for. Kernel functions may be thought to encode some notion
of similarity between instances which can be exploited for density
estimation.

Although kernel methods are generally devised to work on
numerical represented instances (tuples of real numbers), suitable
kernel functions allow for applying these algorithms also when the
instances are represented in expressive languages allowing structured descriptions [15]. Specifically, in this work we demonstrate
the exploitation of kernel methods for inducing classifiers for individuals in OWL ontologies. Indeed, kernel functions have been recently proposed for languages of average expressiveness, such as
the family of kernels for ALC concept descriptions [16]. However,
the scope of their applicability was limited because of two factors:
the definition in terms of a normal form for concept descriptions
and the employment of the notion of (approximations of) most specific concepts [6] in order to lift instances to the concept-level,
where the proposed kernels actually worked.

In order to overcome such limitations, we have proposed a novel parametric family of kernel functions for DL representations
[17], which is inspired to semantic pseudo-metrics for DLs [12].
These functions encode a notion of similarity between individuals
in a knowledge base by exploiting only semantic aspects of the reference representation. A related approach was adopted also for
other simple kernels that have been recently proposed [3], with
kernels that act separately on different aspects of similarity [18],
based on the concepts and properties occurring in the ontology.
Ours integrate these aspects being parametrized on a set of features
(concept descriptions) constructed by utilizing such concepts and
properties as building blocks.

Furthermore, these features need not be fixed but may be selected (constructed), in a pre-processing phase, by enforcing the
discernibility of different instances. Similarly to metric-learning
procedures based on stochastic search [13], a method for optimizing the choice of the feature sets may also be considered. This pro-
cedure, based on genetic programming, can be exploited in case
the concepts in the ontologies, that may have been designed for
knowledge representation (descriptive purposes) would turn out
to be weak for discriminative purposes, depending on the applica-
tion. This has been observed to happen very frequently in practice:
it has been observed that often disjointness axioms are overlooked
and not explicitly included thus affecting the intended semantics
of the knowledge bases.
The effectiveness of

the proposed kernel-based inductive
approach is empirically tested through experiments on the task
of (approximate) individual classification performed with real
ontologies collected from standard repositories. The cases of
kernel-based algorithms for both hard and soft margin classifiers
are considered [19] and also the preliminary optimization of the
parameters of the kernel function.

As discussed also in previous works, alternative individual classification methods, while remaining often as effective as deductive
reasoning, are able to suggest new knowledge (membership
assertions) that is not logically derivable which may be useful for
ontology population purposes [12]. Moreover, both the induction
and the usage of such classifiers can be carried out very efficiently,
providing a further advantage on the standard instance-checking
procedures, especially in time-critical applications.

Another exemplar application scenario is knowledge integration,
which is enabled by ontology matching [20]
tasks. Major
approaches, such as the extensional or the semantic ones, are based
on techniques that aim at detecting the possible relationships
between classes or properties on the ground of their approximate
extensions, as determined by inductive forms of
retrieval.

Similarly, also tasks such as entity resolution [21] (or instance
matching)2 in the context of Semantic Web knowledge bases, can
be pursued by resorting to approximate instance-checking procedures based on statistical models.

The paper, which surveys and extends previous works (e.g., see
[16,17]), is organized as follows. The basics of kernel methods are
presented in the next section, recalling in particular hard and soft
margin support vector machines. In Section 3, after an excursus on
related works about kernels for complex representations, the family of kernels is defined together with a discussion on the optimization of
the inductive classification
problem and its solution through an inductive method are formally
defined in Section 4 and experimentally evaluated in Section 5.
Conclusions and further applications of ontology mining methods
are finally outlined in Section 6.

its parameters. Then,

2. Inducing statistical classifiers with kernel methods

Statistical learning methods aim at eliciting patterns that may
be hidden in the data with the purpose of performing specific tasks,
such as clustering, visualization, classification, ranking, etc. [1]. For
example, classification requires fitting a model to the available
data, e.g., a function h (hypothesis), which is adopted by a decision
procedure to predict the correct categorization of other individuals
efficiently (evaluating a simple model, such as a linear function)
and effectively (close approximation). Kernel methods are particularly well suited for discovering patterns from an engineering point
of view because the learning algorithm (inductive bias) and the
choice of the kernel function (language bias) are almost completely
independent [19].

Essentially these methods aim at detecting linear relations. A
computational shortcut (the kernel function) makes it possible to
efficiently map linear models to high-dimensional spaces to ensure
adequate representational power. Different kernel functions may
be related to different hypothesis spaces. Hence, the same kernel
method can be applied to different (even very rich) representa-
tions, provided that suitable kernel functions are available. Two
distinct components will perform the required steps. The mapping
component, implicitly defined by the kernel function, from the
space of the instances to the space of features that serve to categorize them. It will depend on the specific data type and domain
knowledge expected from the particular data source. The learning
component is general-purpose, robust and also efficient: it requires
an amount of computational resources that is polynomial in the
size and number of data items even when the dimension of the feature space grows exponentially. To summarize, the kernel methods
are characterized by the following points:

 instances are embedded into a vector space, the feature (or

embedding) space;

 linear models are sought among the images of the instances in

the feature space;

 the algorithms are implemented in such a way that only the
inner products of the embedded points are needed, rather than
their coordinates;

 the products can be computed efficiently directly from the ori-

ginal instances using a kernel function.

An efficient algorithm working on instances represented, say, as
tuples, may be adapted to work on structured spaces [15] (e.g.,
trees, graphs) by merely replacing the kernel function with a suitable one. In a classification setting, positive and negative examples
of the intended categorization are to be provided to the learning

2 See http://www.instancematching.org.

algorithm to produce a classification model (i.e., a definition for a
given target class, e.g., a query) in the form of a linear decision
function based on a vector of coefficients (weights). Therefore,
we start off from the problem of learning linear classifiers and
show how to solve the general problem in case of complex logical
spaces and nonlinear features.

2.1. From distance-based classifiers and density estimation to kernelbased algorithms

In a binary classification problem, let us consider an input space
X of training instances represented by tuples of real numbers
X 14 Rn (in categorical cases even simpler boolean input features
are taken into account X 14 Bn 14 f0; 1gn) extended with an additional categorical feature y 2 Y (e.g., Y 14 f1;1g) indicating the
intended classification (i.e., the membership w.r.t. a target con-
cept): ~x; y 2 X 
 Y. We will consider the problem of learning from
a training sample S #X 
 Y of these augmented vectors that will
be named examples (positive or negative depending on the sign).
Among the other non-parametric learning approaches [1], the
key idea of distance-based methods is that the properties featured
by any particular instance are likely to be correspond to those of
instances in its vicinity. To perform some density estimation, one
can simply measure the density with which instances are scattered
in the neighborhood of the instance to be classified. If the neighborhood is too small, it can hardly contain any instance; con-
versely, if it is too large, it may include all the data, resulting in a
density estimate that is the same everywhere. Then a limited
neighborhood is considered, large enough to include a number
(k) of instances, as in the nearest-neighbors methods [1]: Given a
query instance ~x 2 X, the prediction of y for ~x, is obtained combining the classification (Y-value) of the k nearest training neighbors
of ~x by means of a (weighted) majority vote.

Each training instance ~xi is thought of as generating a sort of local density function Di of its own (which corresponds to the kernel
function j~xi;	, discussed later) that assigns a probability to each
instance in the space within a limited distance (often called a win-
dow). This function normally depends only on the distance d~xi;~x

from ~xi to ~x according to a metric d. The density estimate as a whole
is just the normalized sum of all the values of the local densities:
P~x 14
~xi2SDi~x=jSj. Classification amounts to taking a weighted
combination of all the predictions w.r.t. the neighboring training
examples. The weight of the i-th example for a query instance ~x
is determined by the value of its local density function Di~x. The
problem with this idea is that (dimensionally) complex input
spaces are likely too sparse, hence neighbors may be very distant
making the method ineffective and inefficient.

A related approach is based on finding simple (linear) local classification models. In this setting the aim is finding a vector of coefficients ~w 2 Rn1 for a linear function (i.e., a hyperplane equation3)
to make a decision on the correct y label to be assigned to a query
instance ~x;	. Given this vector, a linear separation model is defined,
and the classification procedure is straightforward:

if ~w 	 ~x then predict ~x to be positive (y   +1)
else it is classified as negative (y   1)

which can be written compactly as

y   h~x 14 sign~w 	 ~x
As an example of such an approach, the PERCEPTRON is a well-known
simple algorithm for learning a majority function, i.e., the coefficients of a linear classifier [1]. It amounts to a single-layer neural net-

3 Note that ~w 2 Rn1 because of the bias b = w0. Hence, w.l.o.g. for each instance ~x, a
further component is considered x0 = 1, in order to simplify the notation.

work, where the output unit is linked from each input one through a
weighted connection. In the training phase, for each incoming training example ~xi; yi, the algorithm predicts classifications ^yi according to the decision function determined by the current choice of ~w

and compares the outcomes with the correct labels yi which are

known for the training examples. The weights ~w are revised depending on the set M of examples that provoked a mistake (erroneous pre-
diction): ~w 14
~x2Myi~xi. Then, the resulting decision function can be
written ~w 	 ~x 14
~xi2Myi~xi 	 ~x. Note that it depends on an inner prod-
uct, which is a common characteristic of these methods. Unfortunately these simple and efficient methods apply only to linearly
separable problems. Learning classifiers for complex concepts generally requires more complex models and algorithms [1].

In this perspective, neural networks represent a composition of
units that can learn simple classifiers which aim at finding linear
functions for the separation of the instances in the input space.
Multilayer networks would be able to learn general nonlinear functions as well, but they are hard to train because of the abundance
of local minima and the high dimensionality of the weight space.
This issue may often be circumvented by resorting to the kernel
trick. Essentially it consists in exploiting a (nonlinear) transformation / : X ! F mapping the examples onto a suitable feature
space F (likely one with many more, or even infinite, dimensions),

that allows for the linear separation between positive and negative
examples (see Fig. 1). For example, the decision function for
~xi2Myi/~xi 	 /~x, where / denotes
the PERCEPTRON becomes:
the embedding transformation. Often the embedding space is
high-dimensional, i.e., / : Rn ! RN, with n  N. However, the
mapping need not be explicitly performed because the target
model requires the values of the inner products only.

Definition 2.1 (Kernel). Given an input space X, a valid kernel
function j : X 
 X ! R such that
8x; z 2 X : jx; z 14 /x 	 /z

Then a kernel corresponds to the inner product of the transformed inputs in the new feature space, ensures that an embedding exists [14]. It is worthwhile to note that the spaces involved
are not bound to be vectorial: kernels can be constructed from
different types of input spaces to reflect their structure. This will
be exploited in the next section.

The kernel or Gram matrix K is the n 
 n matrix containing all
the values of the inner products between the examples. It can be
shown that K is positive semi-definite (i.e., its eigenvalues are all
non-negative) when the corresponding kernel is valid [19]. A symmetric function for which the matrices formed by restriction to any
finite subset of the input space are positive semi-definite is called
finitely positive semi-definite. This fact enables using kernels without necessarily considering the corresponding feature space focusing on the induced similarity4 measure [19].

Resorting to kernels allows for efficiently solving problems in
the feature space. For instance, the decision function for the model
~x2Myij~xi;~x (kernel perceptron). In
discussed before becomes:
general, finding a good separating hyperplane in the embedding
space would require solving a N 
 N linear problem (primal form).
However it can be shown that, the problem can be converted into
its dual form, solved by
h~x 14

XjSj

i141

aij~xi;~x

4 Kernels can be regarded as defining similarity measures between two data points.
Considering their normalized versions they can be thought of as computing the a
priori probability of the inputs being in the same class minus the a priori probability
of their being in disjoint classes.

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

Fig. 1. Mapping original 2D examples into a 3D feature space, where they are linearly separable.

which depends on the sample size. This allows the algorithm to efficiently work with spaces of many dimensions (even an exponential
or infinite number).

In principle, any algorithm for learning linear classifiers which
is ultimately based on a decision function involving an inner product could be adapted to work on non-linearly separable problems
by resorting to kernel functions which implicitly encode the transformation into the embedding space. Even more so, an infinite
number of hyperplanes is likely to be able to separate the exam-
ples, then a further criterion has to be introduced for the choice.

2.2. Sparse kernel machines

Among the other kernel methods, the support vector machines
(SVMs) [19] aim at finding a hyperplane that maximizes the mar-
gin, that is the distance from the areas containing positive and negative training examples. The classifier is computed according to the
closest instances w.r.t. the boundary (support vectors).

These algorithms are very efficient (polynomial) since they
solve the following optimization problem through quadratic programming techniques once the kernel matrix is produced [14]:

given
find

subject to

S 14 f~x1; y1; . . . ;~xjSj; yjSjg

~a that maximizes W~a, where:
jaiajyiyjj~xi;~xj
W~a 14 
iai 14 1 and
iaiyi 14 0 with

ai P 0, "i 2 {1, . . .,jSj}.

Given ~a, the resulting separator is: h~x 14 sign

iaiyij~x;~xi

The expression to be optimized5 has a number of important
properties: (1) it has a single global maximum that can be found effi-
ciently; (2) data are involved only in the form of inner products of
pairs of instances, and this holds also for h; (3) the weights ai associated with each instance are null except for the support vectors, i.e.,
instances that are the closest to the separator; since there are usually
much fewer such vectors than examples, the effective number of
parameters defining the optimal separator is much less than jSj
(sparseness); (4) the usage of the kernel instead of the product of
the instances requires only the computation of the kernel matrix
rather than the product of the full list of features. The above problem
is solved, in its dual form, by a hard-margin SVM which can be implemented as in Fig. 2.

In real-world problems data are frequently noisy. Ensuring linear separation in the feature space in such cases would require

Fig. 2. (1-norm) Hard-margin SVM algorithm.

very complex kernels that may risk to overfit the data. Since the
hard-margin SVM always produces a consistent hypothesis w.r.t.
the training data, it is extremely sensitive to noise. The dependence
on the margin exposes the method to the danger of being very sensitive to a few instances, resulting in a poor estimator.

This problem motivates more robust versions that can tolerate
some noise and outliers in the training set without drastically
altering the solution. Relaxing the assumptions it is possible to tolerate some misclassification cases in the training data. One must
optimize a combination of the margin c and the 1-norm of the vector ~n, where ~ni 14 c  h~xi, with functor (	)+ standing for the
identity function for positive arguments and null otherwise. The
typical setting is depicted in Fig. 3. Introducing this vector into

y = 0

y = 1

y = +1


i < 1

i = 0

i > 1

i = 0

5 For its derivation the reader is referred to specific manuals, e.g., [19].

Fig. 3. Soft-margin SVM and slack variables.

Fig. 4. (1-norm) Soft-margin SVM algorithm.

the optimization criterion results in a problem with slack variables
that allows the margin constraints to be violated.

The new optimization problem can be formulated as follows:

given
find
subject to

S 14 f~x1; y1; . . . ;~xjSj; yjSjg
~a that maximize c  C
k~ak 14 1 and yi~a 	 /~xi P c  ni,
ni P 0"i 2 {1, . . .,jSj}.

ini

The parameter C > 1/jSj controls the trade-off between the margin
and the size of the slack variables. The soft-margin SVM algorithm,
solving the problem in its dual form, is shown in Fig. 4. The
algorithm is equivalent to the maximal margin hyperplane, with
the additional constraint that all the ai are upper bounded by
C (box constraint). The trade-off parameter between accuracy and
regularization directly controls the size of the ai. Intuitively this
makes sense as the box constraints limit the influence of outliers.
In practice the parameter C is assessed using a separate validation
set or a cross-validation technique using only the training set.

As the parameter C runs through a range of values, the margin c
varies smoothly through a corresponding range. Hence, for a given
problem, choosing a particular value for C corresponds to choosing
a value for c, and then minimizing k~nk1 for that size of margin. The
parameter C has hardly an intuitive meaning. However, the restriction C = 1/(mjSj) applies here. Again this suggests using it with
m 2 (0,1] as this leads to a control on the number of outliers. This
form of the SVM is known as the new support vector machine
(m-SVM) [19]:

Theorem 2.1 (Robustness). Given d > 0 and m 2 (0,1], suppose that a
sample S is drawn according to some unknown distribution D. Let c, ~a
and h be the output returned by the m-SVM based on a kernel j. Then,
with probability 1  d, the generalization error of h is bounded as
follows

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi

j~xi;~xi

ffiffiffiffiffiffiffiffi

ln 2

2jSj

Pryh~x 6 m  m

W~a

jSjc

Moreover, there are at most mjSj training instances that fail to achieve a
margin c.

Hence the parameter m corresponds to the noise level inherent
in the data, a value that imposes a lower bound on the generalization error achievable by any learning algorithm. This can guide the
choice of m: one should apply the bound for a range of values of m,
in order to work with the bound with non-fixed values of m.

The choice of kernel functions is very important as their computation should be efficient enough for controlling the complexity of
the overall learning process.

3. A family of kernels for individuals in DL knowledge bases

When examples and background knowledge are expressed
through structured (even logical) representations, a further level

of complexity is to be faced with, that may include structure
manipulation up to reasoning. One way to solve the problem
may involve the transformation of statistical classifiers into logical
ones. However, while learning approaches based on mapping DL
knowledge bases onto artificial neural networks have been proposed (e.g., see [22]), direct solutions based on specific statistical
relational learning approaches [23] are still to be investigated in
this context, especially for non-trivial DL languages.

In order to build classification models for instances expressed
through complex representations, the kernel-based algorithms require suitable functions, that are able to implicitly operate the
transformation into embedding spaces where the learning problem
can be solved. In the following, the preliminaries of such transformations are recalled, to serve as the basis for the definition of a
family of kernels that can be applied to data represented through
Web ontology languages.

3.1. Kernels for structured representations

Learning algorithms make use of inner products in the feature
space. The complexity of evaluating inner products is proportional
to the dimension of the space. Kernels compute such values directly on the input features, without explicitly computing the mapping /. In other words the feature-vector representation step can
be by-passed using a kernel function.

An appealing quality of the class of valid kernel functions is its
closure w.r.t. many operations. In particular this class is closed
w.r.t. the convolution [24]:

kconvx; y 14

kixi; yi

i141

x 2 R1x
y 2 R1y

where R is a composition relationship building a single compound
out of D simpler objects, each from a space that is already endowed
with a valid kernel. Note that the choice of R (and, hence, of its inverse R-1 used to decompose complex objects) is generally non-triv-
ial and likely depends on the specific application.

Interesting kernels are based on graph structures [25,26]. Then
new kernels can be defined for complex structures based on simpler kernels defined for their parts using the closure property
w.r.t. this operation. Many definitions have exploited this property,
introducing kernels for strings, trees, graphs and other discrete
structures. In particular, Gartner et al. [15] provide a principled
framework for defining new kernels based on type construction
where types are defined in a declarative way. Other kernels for logic representations have been proposed in [27,28], where the
structure of logic proofs (concept membership for the individuals)
is exploited in the kernel definition.

While these kernels were defined as depending on specific
structures, a more flexible method is building kernels as parametrized on concepts described with another representation. Such
kernel functions allow for the employment of algorithms, such as
the SVMs, that can simulate feature generation. These functions
transform the initial representation of the instances into the related active features, thus allowing for learning the classifier directly from structured data. As an example, Cumby & Roth
propose kernels based on a simple DL representation, the Feature
Description Language [29].

Kernels for richer DL representations have been proposed in
[16,30]. Such functions are actually defined for comparing ALC
concepts based on the structural similarity of the AND-OR trees
corresponding to a normal form of the input concept descriptions.
However these kernels are not only structural since they ultimately rely on the semantic similarity of the primitive concepts

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

(on the leaves) assessed by comparing their extensions through a
set kernel. Although this proposal was criticized for possible counterintuitive outcomes, as it might seem that semantic similarity
between two input concepts were not fully coped with, the kernels
are actually applied to couples of individuals, after having lifted
them to the concept level by means of (approximations of) their
most specific concept [6]. Since these concepts are constructed on
the grounds of the same ABox and TBox, it is likely that structural
and semantic similarity tend to coincide.

A more recent definition of kernel functions for individuals in
the context of the standard SW representations is reported in [3].
The authors define a set of kernels for individuals and for the various types of assertions in the ABox (on concepts, datatype proper-
ties, object properties). However, the integration of such functions
which cope with different aspects of the individuals is left to generic combination operators; the preliminary evaluation on specific
classification problems regarded single kernels or simple additive
combinations. The family of kernels we propose can subsume more
fine-grained versions of such kernels (with more features to con-
sider) and offer a natural way to combine them.

3.2. Preliminaries on representation and logic inference

In the following we report the basic DL terminology utilized in
this paper and that represent the foundations of the Web ontology
languages (see [6] for a thorough and precise reference).

Concept descriptions are built using a vocabulary hNC,NR, NIi
made up by a set of concept names NC, a set of role names NR
and a set of individual names NI, respectively. An interpretation
I 14 DI
;	I assigns the meaning to these formul via 	I mapping
the names to the corresponding element subsets, binary relations,
and objects of the domain DI . A DL language provides specific constructors and rules for building complex concept descriptions
based on these building blocks and for deriving their interpreta-
tion. The Open World Assumption (OWA) is made in the underlying
semantics, which is convenient for the SW context (unless an explicit closed-world assumption is specified).

A knowledge base K 14 hT ;Ai is composed by a TBox T and an
ABox A. T is a set of intensional axioms (definitions) C  D, where
C is the defined (concept) name and D is its DL description constructed on primitive concept and role names by means of the
operators allowed by the particular DL language6. More general
definitions through inclusions axioms (C v D) are also admitted.
We will also assume that these definitions are not recursive (as they
would require more complex semantics [6]). The ABox A contains
assertions (ground facts) on the world state, e.g., C(a) and R(a,b)
meaning that aI 2 CI and aI ; bI 2 RI . IndA will denote the set
of all individuals occurring in the ABox. Normally, the interpretations of interest are limited to those satisfying the knowledge base:
an interpretation I is a model for K if every axiom contained therein
is satisfied by I.

Subsumption, that is deciding the sub-concept relationship
between concepts w.r.t. the models of the knowledge base (i.e.,
1  CI
2 for any such model I), is the most important inference ser-

vice. This notion may be defined over roles also. For our purposes,
equally important is another inference service, instance checking,
that amounts to deciding whether an individual a is an instance
of a concept C in the context of K (i.e., w.r.t. its models) [6]:
K  Ca. The inherent incompleteness of the knowledge base
under open-world semantics may cause reasoners not to be able
to assess the target class-membership. Moreover this can be com-

6 We can assume that the DL language has the expressiveness of OWL-DL [31].

putationally expensive reasoning service. Hence we aim at learning
efficient alternative classifiers that can help solving these problems
effectively.

3.3. Kernel definition

The main limitations of the kernels proposed in [16] for the
space of ALC descriptions are represented by the dependency on
the DL language and by the approximation of the most specific
concept which may be computationally expensive. The use of a
normal form has been also criticized since this is more a structural
(syntactic) criterion that contrasts notion of semantic similarity.

In order to overcome these limitations, we propose a different
set of kernels, based on ideas that inspired a family of inductive
distance measures [13,12], which can be applied directly to
individuals:

Definition 3.1 (L kernel functions). Let K 14 hT ;Ai be a knowledge
base. Given a set of concept descriptions F = {F1,F2, . . ., Fm}, the
family of kernel functions
p : IndA 
 IndA ! 120; 1
kF

is defined as follows: 8a; b 2 IndA

1=p

wijia; b


pa; b :14
kF
where p > 0 and "i 2 {1, . . . ,m} the simple kernel function for Fi is de-
fined: 8a; b 2 IndA

jia; b 14

Fia 2 A ^ Fib 2 A _ :Fia 2 A ^ :Fib 2 A
Fia 2 A ^ :Fib 2 A _ :Fia 2 A ^ Fib 2 A

1=2 otherwise

i141

8><>:
8><>:

or, model-theoretically:

jia; b 14

K  Fia ^K  Fib_ K  :Fia ^K  :Fib
K  :Fia ^ K  Fib _K  Fia ^K  :Fib

1=2 otherwise

The rationale for these kernels is that the similarity among individuals is determined by their similarity w.r.t. each concept in a given committee of features. In turn, two individuals are maximally
similar w.r.t. a given concept Fi if they exhibit the same behavior,
i.e., both are instances of the same concept (resp. of their comple-
ments). Conversely, the minimal similarity holds when they belong
to opposite concepts. Because of the OWA, sometimes a reasoner
may not be able to ascertain the concept-membership of some indi-
viduals, hence, since both possibilities are open, a default value (1/2)
is assigned to reflect a case of uncertainty. However, this value could
be better chosen if prior knowledge is available about the extension
of a feature w.r.t. the overall, e.g., a probability PrK  :Fia for
some a 2 IndA, or its likelihood given the available information.
To this purpose, the retrieval of the given feature concept may be
used to estimate these values. Of course statistical approximations
would fall short in case a few individuals were available.

The kernel functions may be thought of as being a sort of combination of numerical features computed on the ground of logical
ones. They are meant to capture statistically some regularity which
would be hard to express in terms of logics. As mentioned, in-
stance-checking is to be employed for assessing the value of the
simple similarity functions. Yet this is known to be computationally expensive (also depending on the specific DL language of
choice). Alternatively, especially for ontologies that are rich of

8>>>>>>>>>>>><>>>>>>>>>>>>:

T 14

A 14

9>>=>>;

Female  :Male;
Father  Male u 9hasChild:>;
Mother  Female u 9hasChild:>;
Parent  Mother t Father;
FemaleJOCASTA; FemalePOLYNEIKES;
MaleOEDIPUS; MaleTHERSANDROS;
hasChildJOCASTA; OEDIPUS;
hasChildJOCASTA; POLYNEIKES;
hasChildOEDIPUS; POLYNEIKES;
hasChildPOLYNEIKES; THERSANDROS;
ParricideOEDIPUS;
:ParricideTHERSANDROS

9>>>>>>>>>>>>=>>>>>>>>>>>>;

explicit class-membership information (assertions), a simple lookup may be sufficient, as suggested by the first definition of the ji
functions. The parameter p was borrowed from the form of the
Minkowskis measures [32]. Once the feature set is fixed, the possible values for the kernel function are determined, hence p has an
impact on the granularity of the measure.

The computation of the kernel functions is shown in the follow-

ing example:

Example 3.1. (Computing the kernel) Suppose we have the
following knowledge base K 14 hT ;Ai:

Now j1OEDIPUS; JOCASTA 14 1=2,

and the feature committee F = {F1,F2,F3} with: F1 14 :Parricide,
F2 = Female and F3 = $hasChild.>. Let us consider the case of uniform weights.
since K  Parricide
Analogously:

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi

OEDIPUS,
j2OEDIPUS; JOCASTA 14 0 and j3OEDIPUS; JOCASTA 14 1.
3 	 1
2  1
3 	 1

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi

K 2 ParricideJOCASTA.

ffiffiffiffiffiffiffiffiffiffiffiffiffi

3 	 0

2OEDIPUS; JOCASTA 14
kF
Then,
:027  0  :1

 0:373.

2  1

0:138

yet

3.4. Discussion

The most important property of a kernel function is its validity
(i.e., it must correspond to an inner product in a certain embedding
space). This result can be assessed by proving the function kF
p positive semi-definite [19]. Alternatively, it is easier to prove the validity by showing that the function can be obtained by composing
simpler valid kernels through operations that guarantee the closure w.r.t. this property [24].

Proposition 3.1 (Validity). Given an integer p > 0 and a committee
of features F, the function kF

p is a valid kernel.

Proof. A simple kernel function ji (for i = 1, . . .,n), can be obtained
using the embedding:
/i : IndA ! 120; 1 
 120; 1
where /ix 14 lFix; l:Fix
of the membership of an individual to some concept C, i.e.:

 with l(C(x)) is a rough measure


8><>:

lCx 14

K  Cx

K  :Cx

1=2 otherwise

then "a,b: ji(a,b) = /i (a) 	 /i(b). h

Analogously for the alternative form of ji based on the tests

Fia 2 A.

The validity of kF

p follows from the closure of the class of the valid kernels w.r.t. the operations of sum, multiplication by positive
constants and kernel product [24].

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

Coming back to Example 3.1 above, considering the individuals
a = OEDIPUS and b = JOCASTA, we have that j1OEDIPUS;
JOCASTA 14 1

2 since /1a 	 /1b 14 1; 0 	 1

2  0 	 1

Note that these kernels can be considered as simplified forms of
probability kernels [19]. Indeed the simple kernels ki may be redefined to better reflect the membership probability in case it does
not logically follow from the knowledge base. We may redefine
l(C(x)) as a measure of the probability of membership of an individual to some concept C, i.e.: l(C(x)) 2 [0,1] when K 2 Cx and
K 2 :Cx.

14 1 	 1

2 14 1
2.

2 ; 1

It can be noted that the proposed a family of kernels extends
(and integrate) those defined in [3]. For instance, the common class
kernels may constitute a simplified version of our DL-kernels were
only named concepts in the ontology are allowed while, in princi-
ple, any complex concept definition is allowed in our case. More-
over, they are essentially based on intersection of the common
classes while our kernels also consider the case of class-member-
ship w.r.t. the negated concepts and the uncertain membership
case. As regards the data-property and object-property kernels,
again the similarity is assessed by comparing (restrictions of) domains and ranges of defined relations related to the assertions on
the input individuals. Thus again this may be encoded by further
concept definitions to be included in the committee, especially if
they allow for the separation of different individuals.

Furthermore, the uniform choice of the weights assigned to the
various features in the sum (1/mp) may be replaced by assigning
different weights reflecting the importance of a certain feature in
discerning the various instances. A good choice may be based on
the amount of entropy related to each feature (then the weight vector has only to be normalized) [12].

It is worthwhile to note that this is indeed a family of kernels
parametrized on the choice of features. Experiments on instancebased classification task demonstrated the effectiveness of the kernel using the very set of both primitive and defined concepts found
in the knowledge bases. However, the choice of the concepts to be
included in the committee F is crucial and may be the object of a
preliminary learning problem to be solved (feature selection).

3.5. Choosing discriminating features for determining good kernel
functions

While, as previously mentioned, kernel methods can determine
an implicit space, a highly-dimensional space determined by the
distribution of the training instances may expose the classifier to
the well-known curse of the dimensionality [1,14,19]. therefore it
may be crucial to employ kernel functions that could implicitly
transpose the learning problem into an embedding space where
a solution can be easily found. This may be facilitated if one can
use part of the available data to determine beforehand a good
choice of parameters for the proposed kernels, i.e., discriminating
features.

As for the pseudo-metric that inspired the kernel definition
[33], a preliminary phase may concern finding an optimal choice
of features. This may be carried out by means of randomized optimization procedures, similar to those developed for the pseudo-
distance. However, the integration of the algorithm with suitable
kernel methods guarantees that the feature construction job is performed automatically by the learning algorithm (the features correspond to the dimensions of the embedding space).

3.5.1. Entropic criterion

The underlying idea in the kernel definition is that similar individuals should exhibit the same behavior w.r.t. the concepts in F.
Here, one may make the assumption that the feature-set F represents a sufficient number of (possibly redundant) features that

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

are able to discriminate different individuals (in terms of a discernibility measure).

The definition should also take into account the discriminating
power of each feature through the vector of weights ~w. A possible
setting for these weights may reflect the amount of information
conveyed by each feature [12,34]. This quantity is estimated by
the entropy of the features:
HFi 14

Pi
k log

k141;0;1

Pi

computed from the prior class-membership probability as elicited
v 14 Prchecka 2 Fi 14 v, where the
from the knowledge base: Pi
values v stand, resp., for non-membership (-1), uncertainty (0)
and membership (+1). Then, the weights can be set as follows:
wi :14 HFi=

jHFj, for i = 1, . . .,m.

3.5.2. Variance criterion

Another alternative is based on an estimate of the feature variance [1]. Given a dissimilarity measure di for individuals w.r.t. feature Fi (e.g., di(a,b) = jpi(a)  pi(b)j = 1  ji(a,b), with the projection
functions defined in [12]), the estimate is determined by:
varFi 14

12dia; b2

2 	 jIndAj2

a 2 IndA
b 2 IndA

varFi, for

This estimate induces the choice of weights: wi 14 1=2 	
i = 1, . . . ,m.

This choice may be also employed to encode asymmetric cost
functions [1] that assign a different impact to the values that the
similarity on a given feature (ji(a,b)p) may assume.

3.5.3. Stochastic search

Finally, it is worthwhile to note that finding a good committee of
discriminating features may be the objective of specific algorithms
[13]. Namely, since the function is strictly dependent on the committee of features F, two immediate heuristics arise: the number
of concepts of the committee; their discriminating power in terms
of a discernibility factor, i.e., a measure of the amount of difference
between individuals. Finding optimal sets of discriminating
features, should profit also by their composition employing the specific constructors made available by the representation language.
These objectives can be accomplished by means of randomized
optimization techniques, especially when knowledge bases with
large sets of individuals are available. For instance in [33] we have
proposed a metric optimization procedure based on stochastic
search. Namely, part of the entire data can be drawn in order to
learn optimal feature sets, in advance with respect to the successive usage for all other purposes.

A specific optimization algorithm founded in genetic programming has been devised to find optimal choices of discriminating
concept committees. Essentially it searches the space of all possible feature committees starting from an initial guess based on
the concepts (both primitive and defined) currently referenced in
the knowledge base K, starting with a committee of a given
cardinality.

The heuristic guiding the search is this fitness of the feature
sets. The fitness can be determined as the discernibility factor
yielded by the feature set, as computed on the whole set of individuals or on a smaller sample. For instance, given the sample of individuals IS # IndA the fitness function may be:
discernibilityF 14 f 	

XjFj

jdia; bj

a;b2IS2

i141

where f is a normalizing factor that depends on the overall number
of couples involved.

The possible refinements of concept description are language-
specific. For example, for the case of ALC logic, refinement operators have been proposed in [35,36].

4. Inductive classification of individuals

The classification of instances by means of inductively constructed functions is one of the classic topics of Machine Learning.
This task can be considered as a basic activity which enables lots of
more complex tasks such as retrieval, query answering, etc..
Roughly, it amounts to determining the value for a special attribute
of a query instance, the class, given its tuple of working attribute
values. In a typical machine learning setting these classes are considered to be mutually disjoint. Normally the working attributes
are also numeric or, in the case of symbolic attributes, some transformations are preliminarily performed on them so that efficient
methods can be applied [1].

In the context of DL knowledge bases one may start from a basic
definition of the classification problem as a multiple instancechecking problem [6]:

Definition 4.1 (Individual classification problem). Let K 14 hT ;Ai be
a knowledge base. The individual classification problem can be
defined:given
 a query individual a 2 IndA and
 a set C 14 fC1; . . . ; Csg of concepts that can be formed in K
find C #C such that
8j 2 f1; . . . ;jCjg : K  Cja:

This definition is clearly underspecified and therefore has to be

further detailed to be tailored for the intended context.

The first thing to be specified is the inductive procedure to be
adopted for finding C. Some similarity-based methods, like the
nearest neighbor procedure [12,32], perform the classification following an implicit model exploiting a number of available training
pre-classified instances, playing the role of exemplars of the target
concepts. Other methods explicitly build classification models, typically functions hi : IndA ! B, which can approximate the
behavior of a logic (deductive) instance-checking procedure when
determining the classification of the query individual, i.e., whether
K  Cia or not.

In this paper we resort to kernel methods like the mentioned
SVMs [19], that can efficiently induce binary classifiers by implicitly mapping the instances into an embedding feature space, where
they can be discriminated by means of a linear boundary (see Section 2). Given the kernel functions for individuals within DL knowledge bases defined in the previous section, an SVM can determine a
classifier h (with a simple analytical form) which can be employed
to solve classification problems efficiently in the context of SW
ontologies.

To meet such purposes, we ought to further reformulate the
problem to be solved. While in the standard problem setting for
databases, there may be a number of target classes (the values in
the Y set mentioned in Section 2) that are normally considered
as disjoint, this generally does not hold in a multi-relational context like the one of the SW knowledge base s, where an individual
can be an instance of more than one class/concept. Then, in a new
different setting, the multi-class classification problem is decomposed into smaller binary classification problems (one per class).
Therefore, a simple binary value set Y, generally {1, +1} rather
than B, may be employed, where +1 indicates that an individual
a is instance of the considered concept Cj and 1 indicates that a
is not instance of Cj (w.r.t. the given knowledge base).

This setting suits the context of databases (and related machine
learning methods for knowledge discovery) where generally an implicit Closed World Assumption (CWA) is made. Conversely, for the
context of the SW knowledge bases, in which the OWA is adopted,
this is not sufficient because of the uncertainty brought by the different assumptions on the semantics. To deal with this peculiarity,
the absence of information on whether a certain query individual a
is an instance of the concept Cj should not be interpreted nega-
tively; rather, it should count as neutral information. Thus, an augmented set of values has to be considered for the examples, namely
Y0 14 Y [ f0g, where the added value 0 denotes an uncertain membership assignment. More complex settings, e.g., Y 14 Bn (n-tuples
of bits, one per target concept), will be considered to be out of
the scope of this paper.

Thus, given a query instance a, for every concept Cj 2 C, the classifier will return +1 if a is an instance of Cj, 1 if a is an instance of
:Cj, and 0 otherwise. The classification is performed on the
grounds of the linear models built from the set of available training
examples whose correct labels are provided by an external source,
an expert, or by a reasoner. With algorithms inducing binary classifiers an easy solution is, for each concept Cj, to build two classifiers resp. for membership and explicit non-membership (:Cj) and
build a ternary classification procedure on top of them.

It was mentioned that solving classification problems enables
many other related tasks. Especially applications in which retrieval
w.r.t. certain concepts is a crucial service, they could profitably exploit induced classifiers that may provide an approximate efficient
service which only requires the initial computational overhead of
the model induction. Considering a knowledge base K and a query
concept Q, a learning problem can be created providing a limited
(properly sampled) set of training individuals that are members
of Q (examples) and of its complement (counterexamples). Then
the learning algorithm will produce a classifier for deciding the
class-membership of individuals w.r.t. Q; this can be indeed extended to all other individuals in A, thus giving a way to provide
an inductive approximate concept retrieval service. In a similar
way, also a concept query answering service can be devised.

As mentioned before, the classifier is generally very efficient
since only simple mathematical computation is carried out in the
evaluation of the kernelized decision function (hyperplane). The
related cost depends on the complexity of the kernel adopted. In
our case, for example, j depends on the computation of the simple
kernel function values w.r.t. the feature concepts Fi. This may require a number of (direct) logical inferences (Definition 3.1), in
model theoretical form. However this cost can be limited by choosing the Fis among the primitive concepts only (which often contain
most of the information required to separate disjoint classes), so
that the necessary effort reduces to a mere look-up for a given
assertion in the knowledge base. In terms of the space, besides
the model coefficients, classification requires the kernel values
on the training data. Such computational costs must be compared
to those of the standard implementations of logical instancechecking services generally implemented through (optimized) tableau algorithms which are theoretically more demanding in terms
of space and in time (see [6], Ch. 3 and 9).

As regards the effectiveness, see also the empirical study in the
next Section, its performance on concept retrieval tasks may be
compared to that of a logic reasoner. Moreover, the classifier
may be able, in some cases, to provide a definite answer (member-
ship or non-membership) even when a reasoner could not; that is
to hint by induction logical assertions that are likely to hold but are
not logically derivable from the knowledge base. One may also
consider using binary classifiers only, in order to force the answer
to belong to {+1, 1}, or provide a measure of likelihood for this
answer [12]. This will not be further discussed here as this goes
beyond the scope of this work.

Table 1
Facts about the ontologies employed in the experiments.

Ontology DL language

#Concepts #Obj.
prop.

#Data
prop.

#Individuals

BIOPAX

FINANCIAL

ALCOFD
ALCIOD
ALCHOIND
ALCIFD

alch
ALCIOD
ALRHID

5. Experimental evaluation

In previous works [17], we have demonstrated the effectiveness
and efficiency of the induced classifiers exploiting the presented
kernels over other proposed functions [16]. The experimental evaluations presented in this section aimed at investigating the correctness of the inductive conclusions made by exploiting a
trained classifier when performing the retrieval of randomly generated concepts. In particular the robustness of the methods presented in Section 2, with varying choices of the parameters.

From a technical point of view, the kernel functions were implemented so that the matrices could be produced as inputs for the
support vector machines offered by the LIBSVM library.7 However,
in principle, they could be easily integrated also into other SVM
implementations such as SVMlight (or its extension8 proposed in [3]).
Comparable methods belonging to the same paradigms are
illustrate in previous papers, namely [12] (k-nearest neighbors),
[30] (structural kernels), and [37] (Reduced Coulomb Energy net-
works). Other learning algorithms have been devised to work with
Web ontologies, the aim is concept learning, thus remaining in the
context of logic models.

5.1. Setup

A number of different OWL knowledge bases for various domains were selected (most of them employed in the mentioned
and related experiments), namely: WINE, SURFACE-WATER-MODEL
(SWM), and NEW TESTAMENT NAMES (NTN) from the Protege library,9
the FINANCIAL ontology10 used in the SEMINTEC project, the BioPax glycolysis ontology11 (BIOPAX), the Semantic Web Service Discovery dataset12 (SWSD), one of the ontologies randomly generated by the
Lehigh University Benchmark13 (LUBM) and an exemplar of a business
process repository obtained exploiting the Business Process Modelling
Notation ontology14 (BPMN). Technical details about these knowledge bases are reported in Table 1.

For each ontology, 50 query concepts were randomly
generated15 using concepts and roles occurring in the ontology (with
the constraint that both positive and negative instances were available in the ABox). A ten-fold cross validation experimental design16

7 http://www.csie.ntu.edu.tw/cjlin/libsvm.
8 http://www.aifb.uni-karlsruhe.de/WBS/sbl/software/jnikernel/.
9 http://protege.stanford.edu/plugins/owl/owl-library.
10 http://www.cs.put.poznan.Pl/alawrynowicz/financial.owl.
11 http://www.biopax.org/Downloads/Level1v1.4/biopax-example-ecocyc-
glycolysis.owl.
12 https://www.uni-koblenz.de/FB4/Institutes/IFI/AGStaab/Projects/xmedia/dl-
tree.htm.
13 http://swat.cse.lehigh.edu/projects/lubm.
14 https://dkm.fbk.eu/index.php/BPMN_Ontology.
15 A snippet of the code used to generate the query concept descriptions is available
at: http://lacam.di.uniba.it:8000/nico/research/snippet1.html.
16 The set of examples is randomly divided into ten parts then, in each fold, one part
is used to validate the classifier induced using the instances in the other parts as
training examplesa [1].

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

was adopted in order to overcome the variability in the composition
of the training and test sets of examples. Examples were labeled
according to the reasoner response; the classifier was then induced
by the SVM exploiting the kernel matrix calculated for one of the
functions in the family,17 for the subset of training examples selected
in each run of the experiment. The classifier was then tested on the
remaining individuals assessing its performance w.r.t. the correct
theoretical classification provided by the reasoner.

Unlike the experiment reported in [3], where the ontology was
randomly populated and only seven selected concepts were considered while no roles were taken into account, in this case, we
considered only populated ontologies with their genuine composi-
tion. The population was randomly generated, not to bias the classifier resulting from the learning process with the influence of a
specific population algorithm.

The performance of the classifier was evaluated by comparing
its responses on test instances to those returned by a standard reasoner18 used as baseline. As mentioned, the experiment has been
performed by adopting a ten-fold cross validation procedure. Then
the results presented in the following tables are averaged over these
folds and over all the randomly generated concepts for each
ontology.

Specifically, for each query concept, the following indices19 have

been measured for the evaluation:

 match rate: percentage of cases of test individuals that got
exactly the same classification by both the classifier and the
reasoner with respect to the overall number of test individuals;
 omission error rate: percentage of test individuals that were
labeled as of uncertain classification while they actually were
to be classified as members of the concept or of its complement;
 commission error rate: percentage of test individuals classified
as instances of the concept, while the reasoner assigns them
to the complement or vice-versa;

 induction rate: percentage of test individuals that were found to
belong to a concept or its complement, while this information is
not logically derivable by the reasoner.

The choice of the same indices used in previous works
[12,30,17,37] allows for a comparison of the performance of the
new algorithms.

The experiment is aimed at showing that statistical classification is comparably effective w.r.t. the standard (deductive) classification obtained though logic reasoning. Meanwhile it is very
efficient (because of the simple form of the function it is based
on) and is also able to suggest (by induction) assertions that are
not logically derivable from the knowledge base.

5.2. Outcomes

The outcomes of the experiments regarding the classification of
all the query concepts generated for each ontology are reported in
Table 2.

By looking at the table, it is important to note that, for every
ontology, the commission error was (almost) null. This means that
the classifier did not make critical mistakes, i.e., cases when an
individual is deemed to be an instance of a concept while it really

17 The feature set for the DL-kernel was made by all concepts in the ontology and
parameter p was set to 1 for simplicity and efficiency purposes.
18 PELLET 2.0, that is publicly available at: http://clarkparsia.com/pellet.
19 Briefly, the true positive, negative and uncertain instances correspond to the match
case, the false positive and negative instances correspond to the commission case, the
false uncertain instances corresponds to the omission case, while the remaining ones
represent induced assertions for which no ground truth to compare with is available
(uncertain membership cases that the model deems as examples or counter-
examples).

Table 2
Results (average rates  standard deviation) of the experiments using the soft-margin
SVM with C = 0.5.

Ontology Match rate

Commission error
rate

Omission error
rate

Induction
rate

BIOPAX

FINANCIAL

95.1  05.5
95.7  04.5
95.7  05.6
97.4  03.2
93.0  10.4
98.9  02.0
98.7  02.6
99.0  01.2

00.0  00.0
00.3  01.0
01.0  02.6
02.4  03.1
00.0  00.0
00.0  00.0
00.6  01.4
00.0  00.0

04.1  05.2
03.5  04.6
02.2  04.6
00.0  00.0
03.6  05.9
01.1  02.0
00.2  01.0
01.0  01.2

00.8  03.2
00.5  01.2
01.0  03.6
00.1  00.5
03.4  09.9
00.0  00.0
00.5  01.3
00.0  00.0

is an instance of other disjoint concepts. At the same time it is
important to note that very high match rates were attained on
all ontologies (above the 93%). Particularly, it is interesting to observe that the match rate increases with the ontologies with larger
numbers of individuals. This is because the performance of statistical methods is likely to improve with the availability of large
numbers of training examples, which means that there is more
information for better separating the example space.

In general, a sort of conservative behavior was observed: the
SVM-trained classifiers, with the given parameters, tended to
reproduce largely the performance of deductive classification. Indeed the omission error rates were quite low, yet slightly higher
than the rates of commission errors and also the variance increases
(up to 5% in some cases), a sign that some of the generated queries
were harder. This was probably due to a high number of training
examples classified as unknown w.r.t. certain concepts. To decrease the tendency to a conservative behavior of the method, a
threshold could be introduced for the consideration of the training
examples with an unknown classification.

The variance of the outcomes is low for all cases but for the NTN
ontology: this is probably due to the particular set of query concepts generated. It seems that such concepts exhibited a larger variability of the resulting distributions of the examples. More
specifically, some of the queries generated with NTN presented
an abundance of (test) instances with unknown membership
which likely provoked this shift towards the induction as non-con-
servative classifiers were induced. This possibility is enforced also
by the similar results obtained with different parameter choices. A
possible solution may come from the employment of advanced
algorithms for unbalanced situation.

In almost all cases the classifier was able to induce a few classmembership assertions that were not logically derivable. However,
the assessment of the quality of the induced knowledge is not easy
because the correct answer to the inferred membership assertions
may be only known to the domain-experts that assisted the construction and population of the ontologies.

These outcomes, as well as the next ones, can be compared with
those attained applying the classifier adopting the structural/
semantic kernels for specific DL languages,20 namely ALC [16] and
ALCN [30]. It is possible to note that the classifiers trained adopting
the new DL-kernel generally improve both match rate and omission
rate with respect to the one induced adopting the older kernels (in
the cases where they do not improve the difference is not large). Con-
versely, the observed induction rates are generally in favor of the classifiers induced with the older kernels. This can be explained with the
higher precision of the classifiers induced adopting the DL-kernels,
which increased the match rate in many cases when the reasoner
was not able to give a certain classification. The commission rates

20 Since the languages of the ontologies are generally more complex than ALC and
ALCN , we considered the individuals to be represented by approximations of the
most specific concepts of such individuals w.r.t. the ABox [6].

Table 3
Results (average rates  standard deviation) of the experiments using the soft-margin
SVM with C = 3.0.

Table 4
Results (average rates  standard deviation) of the experiments using the soft-margin
SVM with m = 0.9.

Ontology

Match rate

BIOPAX

FINANCIAL

98.4  02.0
97.5  02.6
98.3  02.4
99.8  00.4
93.0  10.4
98.9  02.0
99.2  00.3
99.9  00.0

Commission
error rate

00.0  00.0
00.1  00.4
00.5  01.2
00.1  00.2
00.0  00.0
00.0  00.0
00.3  01.2
00.0  00.0

Omission
error rate

01.5  01.9
02.1  02.7
01.0  02.1
00.0  00.0
03.6  05.9
01.1  02.0
00.2  01.0
00.1  00.2

Induction rate

Ontology

Match rate

00.1  00.6
00.3  00.7
00.3  00.9
00.1  00.3
03.3  09.6
00.0  00.0
00.2  01.3
00.0  00.0

BIOPAX

FINANCIAL

86.8  17.2
87.1  10.9
77.6  10.3
99.1  02.9
93.0  10.4
93.0  10.4
98.7  02.6
91.7  12.1

Commission
error rate

00.0  00.0
00.0  00.0
01.4  03.6
00.0  00.0
00.0  00.0
05.2  09.1
00.6  01.4
00.0  00.0

Omission
error rate

04.6  06.8
12.9  10.9
02.6  06.4
00.9  02.9
03.6  05.9
00.0  00.0
00.2  01.0
04.4  06.4

Induction rate

08.6  16.3
00.0  00.0
18.3  11.9
00.0  00.0
03.4  09.9
01.8  05.2
00.5  01.3
03.9  11.8

are comparable. More interestingly, one may also observe that the
outcomes of the classifiers trained adopting the new kernel showed
a more stable behavior as testified by the limited deviations
observed.

Other experimental sessions have been carried out for evaluating the performance of the classifiers trained by the soft-margin
SVMs adopting the new kernels. In these sessions all the previous
settings were maintained but the choice of C varied allowing the
induced model to make some more mistakes on training instances,
thus avoiding overfitting the data. As expected, by increasing this
parameter more robust classifiers should be obtained. The outcomes of one of these session, where C is set to 3.0 are reported
in Table 3.

The first worthwhile difference is the reduced variance in the
results (more than halved) which is in a range of [0%;2.7%]. The table also shows that the classifiers induced with this new setting
tend to be even more precise than with the previous one, as shown
by the average match rate values which are now higher on all
ontologies (above 97.5% and up to 99.9% for the most populated
ontology). As for all statistical methods, the number of (positive
and negative) instances available during the training phase has
an impact on the resulting classifiers, as shown when their quality
is assessed against the test set. A noteworthy exception is represented by the NTN ontology for which the figures remain almost
exactly the same and again the most of the mass of cases is shared
between match and induction rates, implying that the cases in
which the classifier does not follow the same behavior of the reasoner are likely those of unknown membership (0) which is then
suggested by the inductive classifier (1). Likewise, the omission
and commission error rates decrease below a few percentage
points.

Analogous trends were observed with other values for the C
parameter. The best choice can be made selectively for each ontology by using a cross-validation procedure on a part of instances
which is held out before the learning phase [1].

Adopting the other type of kernel method presented in Section
2.2, the soft-margin m-SVM, one may better control the learning
phase when some qualitative information on the data is available.
Recall that, differently from C, this parameter has a more exact
interpretation: it corresponds to the noise level in the data. Further
experimental sessions have been then carried out using m-SVMs.
Table 4 presents the results obtained setting the parameter m to
.9. This roughly corresponds to choosing relatively small values21
for C and indeed the effect is that the outcomes are comparable (BIO
PAX, NTN, FINANCIAL) to from those shown in the tables above or, in the
other cases, a decay of match rate is determined in favor of the
induction (and sometimes also the omission) rate caused by queries
with many individuals of unknown membership. Indeed since m also
depends on the number of training instances available, it should be

21 C and m are inversely proportional, but this relation also depends on the sample
set cardinality: C = 1/(jSjm).

Table 5
Results (average rates  standard deviation) of the experiments using the soft-margin
SVM with m = 0.009.

Ontology Match rate

BIOPAX

FINANCIAL

99.2  05.3
98.4  04.3
48.0  33.2
96.0  17.8
95.6  18.8
62.0  40.7
53.7  45.8
43.6  33.9

Commission
error rate

00.0  00.0
00.1  00.3
11.0  23.4
00.0  00.0
00.0  00.0
00.0  00.0
24.7  37.8
00.0  00.0

Omission
error rate

00.2  00.7
00.7  01.1
01.8  10.2
00.0  00.1
00.0  00.1
00.5  01.2
19.0  37.0
06.7  17.8

Induction
rate

00.7  04.6
00.8  03.3
39.2  29.2
04.0  17.8
04.4  18.7
37.5  40.8
02.6  14.2
49.7  37.4

better selected with a different value per ontology. Generally, a preprocessing procedure would be required to assess the values of m or C
through cross-validation [19].

It is interesting to observe what happens when lower values of m
are considered, i.e., deciding that lower rates of noise are contained
in the data. For instance in Table 5 the outcomes of the experiments when the value of m was set to 0.009. It shows that the
match rates are much lower than with the previous settings. Inter-
estingly, for most of the ontologies this resulted in an increase of
the induction rates in many cases (see the entries for LUBM, BPMN,
and SWSD). This can be explained as a large number of instances
with an unknown (deductive) classification w.r.t. the given query
concepts that could receive a 1 classification assigned by the classifiers built by the learning algorithm. The choice of the parameters
had the effect of making the resulting classifiers less cautious and
conservative. Equally noteworthy is the fact that in most cases the
error rates remained low, except for cases (see the entries for
BPMN and FINANCIAL) when the examples and counter-examples of
the given query concepts were not sufficiently numerous to build
robust models. This is also testified by the high variance caused
by the mentioned cases. Summing up, one may conclude that with
this kind of algorithm, we gain a way to control the desired kind of
behavior of the resulting inductive classifiers, but a preprocessing
phase for fine-tuning the parameter would be required.

5.3. Using kernels with feature optimization

In Section 3.5 it was shown that the feature sets determining
the kernel function may be optimized so that discriminating ones
are chosen, thus yielding an improvement of the resulting classifi-
ers. We experimentally tested this claim by repeating the concept
retrieval experiment reported above, after a preliminary feature
selection phase had been carried out in order to produce optimal
feature sets F for each ontology.

In the experiment, the same random queries were proposed to
the system, after the feature sets for the kernel functions were induced by an implementation of the algorithm recalled in Section
3.5.3 (see also [33]). The results of this experiment are reported

N. Fanizzi et al. / Web Semantics: Science, Services and Agents on the World Wide Web 11 (2012) 113

Table 6
Results (average rates  standard deviation) of the experiments on random concept
retrieval with kernel functions with optimized feature sets.

Ontology Match rate

Commission error
rate

Omission error
rate

Induction
rate

BIOPAX

FINANCIAL

93.3  05.5
95.5  02.6
95.3  03.0
91.0  07.1
99.1  01.2
98.0  02.1
99.5  00.8
98.9  01.7

00.8  03.3
00.1  00.4
00.1  00.2
00.2  01.2
00.0  00.2
00.0  00.0
00.2  00.1
00.0  00.0

00.1  00.0
02.1  02.7
01.4  02.1
08.6  07.1
00.0  00.2
01.4  02.1
00.1  00.2
00.0  00.0

02.2  03.5
02.3  00.7
03.2  01.4
00.2  01.2
00.9  01.2
00.0  00.0
00.2  00.9
01.1  01.7

in Table 6. By comparing them to those reported in the tables
above, a general improvement is observed especially in terms of
matching cases. Even more so the standard deviation decreases
so that these figures are much more stable over the various
queries.

In conclusion, it was mentioned that the named concepts only
and uniform weights were considered in the previous experiments.
In terms of effectiveness, the outcomes show that adopting a preliminary optimization leads to comparable results w.r.t. using of
soft-margin SVMs (with a proper choice of the parameters). How-
ever, the selection of good committees is meant also to speed up
the kernel evaluation if a few concepts are able to capture (most
of) the discriminant information in the data. This suggests an efficiency criterion may further help to decide between the two ap-
proaches. Even more so, on further experiments, not reported in
this paper for brevity, it was observed that selecting the sole primitive concepts may often suffice to identify a good committee that
is able to retain enough information for the kernel.

5.4. A comparison with another learning procedure

ffiffiffiffi

For a comparison with analogous similarity-based method
based on a different lazy learning strategy, namely the k-Nearest
Neighbor classification procedure, we performed a run of the
experiments on the same ontologies with the same cross-valida-
tion design (k was set to
, where N was the number of training
instances considered). In Table 7, we report the outcomes of these
experiments. Such results clearly show the effectiveness of the
SVMs, as a mass of cases moves from the match case to omission
error rates, which is only partially compensated by the number
cases of induction. Like the SVMs, the k-Nearest Neighbor appears
robust w.r.t. commission error cases. Even more so, a higher variance is also observed, denouncing an inferior stability of the algo-
rithm. An advantage of the k-Nearest Neighbor procedure may be
that, differently from the SVMs, it does not require an actual training phase. However suitable data structures have to be managed to
efficiently retrieve the neighbors during the classification phase.
Conversely, once the SVMs has learned the classification model
in the training phase, this can efficiently answer to instance classification queries.

Table 7
Results with the k-NN learning procedure on the same datasets.

Ontology Match rate

Commission error
rate

Omission error
rate

Induction
rate

BIOPAX

FINANCIAL

67.4  10.3
74.1  07.4
57.5  03.2
73.7  08.1
63.8  05.0
65.2  14.2
63.1  02.5
63.5  04.6

00.0  00.0
00.2  00.4
00.0  00.0
00.0  00.0
00.0  00.0
00.0  00.0
01.5  00.4
00.0  00.0

18.4  06.5
16.7  03.8
22.2  23.1
09.6  02.6
20.6  03.0
18.2  07.3
26.0  01.9
16.7  02.8

14.2  08.7
08.9  05.6
00.3  01.2
16.6  08.3
15.6  03.9
16.5  11.0
09.4  01.6
19.9  03.5

6. Conclusions and outlook

This work investigated the possibility of extending efficient statistical learning methods to work on knowledge bases that are
going to constitute the next generation information systems. While
purely logical approaches may fall short to cope with the complexity of the inherent problems also due to the distributed nature of
these knowledge bases, approximate hybrid (numeric/symbolic)
methods may be integrated with the mentioned approaches for
the sake of efficiency and effectiveness. In particular we focused
on the exploitation of such statistical methods for performing
essential basic tasks such as approximate classification and concept retrieval on knowledge bases expressed in Description Logics
(ontologies).

More specifically, we proposed the application of kernel methods for inducing classifiers based on exemplar individuals in an
ontology. The choice was dictated by the favorable characteristic
of the kernel methods which is the separation between the learning algorithm and the representation (and transformation) of the
instances. Inspired from previous works on dissimilarity measures
in DLs, a family of semantic kernel functions for individuals has
been defined based on their behavior w.r.t. a number of features
(encoded as concepts). Unlike previous proposals, the kernels are
language-independent being based on instance-checking (or ABox
look-up) w.r.t. the features and can be easily integrated with a kernel method (a SVM in our case) for performing a broad spectrum of
activities related to ontologies.

The resulting analytical models (classifiers) can be used to perform the task of individual classification, and then also retrieval, in
a more efficient yet still effective way, compared with the standard
deductive procedures. Another advantage is represented by the
possibility of such classifiers to work even in the presence of noise
in the data, i.e., contradicting assertions which may undermine the
possibility of the standard reasoning. It has been experimentally
shown that its performance is not only comparable to the one of
a standard reasoner, but the classifier is also able to suggest asser-
tions, which are not logically derivable (e.g., by using a DL rea-
soner). Particularly, an increase in predictive accuracy was
observed when the instances are homogeneously spread, as expected from statistical methods. Hence the induced classifiers
can be exploited for predicting / suggesting missing information
about individuals which emerges from the available data, thus
completing large ontologies. Specifically, it can be used to semiautomate the population of an ABox. Indeed, the new assertions
can be suggested to the knowledge engineer that has only to validate their acquisition. This constitutes a new approach in the SW
context, since the efficiency of the statistical-numerical approaches
and the effectiveness of a symbolic representation have been combined [22].

A related supervised task is ranking. Sometimes information implicit information about an underlying ordering of the individuals is
available but can be naturally expressed through extra-logic means
only. Ranking functions built through ad hoc kernel methods may
offer valid solutions to these problems. Further ontology mining
methods can be based on kernel methods such as conceptual clustering which allows the discovery of interesting subgroups of individuals which may require the definition of a new concept or to
track the drift of existing concepts over time (with the acquisition
of new individuals) or even to detect new emerging concepts [33].
