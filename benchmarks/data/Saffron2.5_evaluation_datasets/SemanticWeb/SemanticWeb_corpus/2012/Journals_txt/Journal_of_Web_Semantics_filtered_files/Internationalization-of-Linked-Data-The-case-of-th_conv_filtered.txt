Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Internationalization of Linked Data: The case of the Greek DBpedia edition
Dimitris Kontokostas a, Charalampos Bratsas a,, Soren Auer b, Sebastian Hellmann b, Ioannis Antoniou a,
George Metakides a
a Web Science Program, Mathematics Department, Aristotle University of Thessaloniki, Greece
b University Leipzig, Institut fur Informatik, Leipzig, Germany

a r t i c l e

i n f o

a b s t r a c t

This paper describes the deployment of the Greek DBpedia and the contribution to the DBpedia
information extraction framework with regard to internationalization (I18n) and multilingual support.
I18n filters are proposed as pluggable components in order to address issues when extracting knowledge
from non-English Wikipedia editions. We report on our strategy for supporting the International Resource
Identifier (IRI) and introduce two new extractors to complement the I18n filters. Additionally, the paper
discusses the definition of Transparent Content Negotiation (TCN) rules for IRIs to address de-referencing
and IRI serialization problems. The aim of this research is to establish best practices (complemented by
software) to allow the DBpedia community to easily generate, maintain and properly interlink languagespecific DBpedia editions. Furthermore, these best practices can be applied for the publication of Linked
Data in non-Latin languages in general.

 2012 Elsevier B.V. All rights reserved.

Article history:
Received 11 June 2011
Received in revised form
9 January 2012
Accepted 9 January 2012
Available online 18 January 2012

Keywords:
DBpedia
Multilingual
Internationalization
Linked Data

1. Introduction

The Semantic Web [1] is the evolution of the World Wide
Web towards representing meaning of information in a way that
is processable by machines. Most recently the Semantic Web
vision was enriched by the concept of the Linked Data (LD) [2],
a movement which within a short time led to a vast amount of
Linked Data on the Web accessible in a simple yet standardized
way. DBpedia [3] is one of the most prominent LD examples. It is an
effort to extract knowledge represented as RDF from Wikipedia as
well as to publish and interlink the extracted knowledge according
to the Linked Data principles. DBpedia is currently the largest hub
on the Web of Linked Data [4].

The early versions of the DBpedia Information Extraction
Framework (DIEF) used only the English Wikipedia as sole source.
Since the beginning, the focus of DBpedia has been to build a
fused, integrated dataset by integrating information from many
different Wikipedia editions. The emphasis of this fused DBpedia

 Corresponding author. Tel.: +30 2310 997897.

was still on the English Wikipedia as it is the most abundant
language edition. During the fusion process, however, languagespecific information was lost or ignored. The aim of this research is
to establish best practices (complemented by software) that allow
the DBpedia community1 to easily generate, maintain and properly
interlink language-specific DBpedia editions. We realized this best
practice using the Greek Wikipedia as a basis and prototype and
contributed this work back to the original DIEF. We envisage the
Greek DBpedia to serve as a hub for an emerging Greek Linked Data
(GLD) Cloud [5].

The Greek Wikipedia is, when compared to other Wikipedia
language editions, still relatively small  66th in article count2
 with around 65,000 articles. Although the Greek Wikipedia is
currently not as well organized  regarding infobox usage and
other aspects  as the English one there is a strong support action
by the Greek government3 foreseeing Wikipedias educational
value to promote article authoring in schools, universities and by
everyday users. This action is thus quickly enriching the GLD cloud.
In addition, the Greek government, following the initiative of open
access of all public data, initiated the geodata project,4 which is

E-mail addresses: jimkont@math.auth.gr (D. Kontokostas),

cbratsas@math.auth.gr (C. Bratsas), auer@informatik.uni-leipzig.de (S. Auer),
hellmann@informatik.uni-leipzig.de (S. Hellmann), iantonio@math.auth.gr
(I. Antoniou), george@metakides.net (G. Metakides).

URLs: http://webscience.auth.gr (D. Kontokostas), http://webscience.auth.gr

(C. Bratsas), http://aksw.org (S. Auer), http://aksw.org (S. Hellmann),
http://webscience.auth.gr (I. Antoniou), http://webscience.auth.gr (G. Metakides).

1 The authors established the DBpedia Internationalization Committee to gather
other interested community members aiming to create a network of internationalized DBpedia editions (http://dbpedia.org/internationalization).
2 Accessed on 20/10/2011: http://stats.wikimedia.org/EN/Sitemap.htm.
3 http://advisory.ellak.gr/?p=12.
4 http://geodata.gov.gr.

1570-8268/$  see front matter  2012 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2012.01.001

D. Kontokostas et al. / Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

publishing data from the public sector. The Greek DBpedia will not
only become the core where all these datasets will be interlinked,
but also provides guidelines on how they could be published, how
non-Latin characters can be handled and how the Transparent
Content Negotiation (TCN) rules (RFC 2295) [6] for de-referencing
can be implemented.

After discussing the current status of DBpedia (in Section 2), we
present the results of the development and implementation of a
new internationalized DBpedia Information Extraction Framework
(I18n-DIEF), which consists in particular of the following novelties:

i. Implementation of the DBpedia I18n Framework by plugging
I18n filters into the original DBpedia framework (Section 3).
This is now part of the official DBpedia Framework and it is used
for the Greek and three additional localized DBpedia editions
(cf. Section 2.3).

ii. Use of DBpedia as a statistical diagnostic tool for Wikipedia

correctness (Sections 4.1 and 7).

iii. Development of the Template-Parameter Extractor to facilitate a
semi-automatic mapping add-on tool and provide the basis for
the infobox-to-ontology mapping statistics (Section 4.2).

iv. Justification of the need for language-specific namespaces

(Section 5).

v. The linking of language-specific DBpedia editions to existing
the English DBpedia in the LOD Cloud

link targets of
(Section 5.1).

vi. Definition of Transparent Content Negotiation rules for IRI

dereferencing (Section 6.1).

vii. Identification of IRI serialization problems and the proposal for

an effective solution (Section 6.2).

2. Background and problem description

In this section we introduce the state of DBpedia with regard
to the extraction framework, the data topology as well as the
internationalization support.

2.1. The DBpedia Information Extraction Framework

The semantic extraction of information from Wikipedia is accomplished using the DBpedia Information Extraction Framework
(DIEF).5 The DIEF is able to process input data from several sources
provided by Wikipedia:

The Wikipedia dumps6 are used to produce the static DBpedia
versions (version 3.5.1 in March 2010, 3.6 in October/November
2010 and 3.7 in August 2011). In total, 279 language-specific
Wikipedia editions exist.7

An article update stream is used by the DBpedia Live Extraction [7] to provide instant updates. As soon as an English Wikipedia
article is edited, the extraction is performed on the latest revision
and the DBpedia Live SPARQL endpoint is updated via SPARQL Update queries.

The DIEF can also process single articles using the MediaWiki

API.

The actual extraction is performed by a set of pluggable
Extractors (usually named after their intended output, e.g. Label
Extractor), which rely on certain Parsers for different datatypes (e.g.
the DateTime Parser).

5 http://wiki.dbpedia.org/Documentation.
6 http://en.wikipedia.org/wiki/Wikipedia:Download.
7 Accessed on 18/03/2011: http://s23.org/wikistats/wikipedias_html.

2.2. Data topology

The datasets extracted by the DIEF are made available either via
the DBpedia SPARQL endpoint or as downloadable dumps.8 Prior
to this work, the focus of these datasets was clearly on the English
part of Wikipedia. Although other language editions were used as
sources, the DIEF, the pluggable extractors and the parsers had not
been tailored to deal with specialties of other language editions.
With minor exceptions, the extractors which have been designed
for the English edition have been applied as-is on other sources and
lacked customizability. Currently, 98 Wikipedia language editions
are used and basic information such as abstracts, labels, titles,
links, page links and geographic information is provided for these.
In order to provide a single identifier for all languages, DBpedia
currently uses the Wikipedia interlanguage links9 for assigning
information extracted from non-English articles the corresponding
English resource identifiers. For instance, there exist articles about
the city of Thessaloniki in Greek and other languages, but all RDF
facts extracted from these various articles use the same resource
identifier http://dbpedia.org/resource/Thessaloniki as a subject.

2.3. The current state of the internationalization effort

The introduction of the Mapping-Based Infobox Extractor in
[3] alongside crowd-sourcing approaches in [7] allowed the
international DBpedia community to easily define infobox-to-
ontology mappings using a relatively simple syntax10 (cf. Listing
1). As a result of this development, there are currently mappings
for 15 languages11 defined in addition to English.

The DBpedia 3.7 release12 was the first DBpedia release to
use the new I18n-DIEF. The 15 languages that had infobox-to-
ontology mappings defined were extracted as localized datasets.
The extensions to the DIEF that are described in this article were
generic enough to create the other localized editions. This indicates
that the internationalized DIEF is sufficiently configurable to cope
with the extensive amount of languages provided by Wikipedia.

At the time of writing, five official DBpedia chapters, besides
the English one, exist: German, Greek, Korean, Portuguese and
Russian.13 The Portuguese and Russian DBpedia editions were
created right from the start using the new I18n-DIEF. Although
the German and Korean chapters existed before the Greek chapter,
they faced the challenges that are addressed in this article: both
chapters had developed their own approaches independently and
while the German chapter used percent-encoded URIs the Korean
chapter also made an effort to export their datasets with localized
IRIs [8]. In particular the German chapter had used the DIEF,
thus discarding all articles without an English interlanguage link,
which resulted in a fragmented graph structure (targets of links
were not contained in the knowledge base). Furthermore, both the
German and the Korean DBpedias did not incorporate the DBpedia
Ontology and the mapping rules (Section 4) which resulted in a
low data quality and a scheme which was not synchronized with
the fused DBpedia. Recently the German and Korean chapters have
adopted the best practices that we described in this article, i.e.
they configured the DIEF to their language, created mappings and
realized the TCN rules for IRI de-referencing as defined in Section 6.

8 Endpoint: http://dbpedia.org/sparql Downloads: http://wiki.dbpedia.org/
Downloads.
9 http://en.wikipedia.org/wiki/Help:Interlanguage_links.
10 http://mappings.dbpedia.org.
11 ca, de, el, es, fr, ga, hr, hu, it, nl, pl, pt, ru, sl, tr.
12 http://blog.dbpedia.org/2011/09/11/dbpedia-37-released-including-15-
localized-editions/.
13 Accessed
on
Chapters.

http://wiki.dbpedia.org/Internationalization/

25/10/2011:

{{ TemplateMapping
| mapToClass = Actor
| mappings =

}}

{{ PropertyMapping |

templateProperty

= name | ontologyProperty = foaf :name }}

{{ PropertyMapping |

templateProperty

= birth_place | ontologyProperty = birthPlace }}

Listing 1: Example infobox-to-ontology mapping.

3. A solution overview: the I18n extension of the DBpedia
Information Extraction Framework

Prior this work, the main focus of the DIEF was on the English
Wikipedia and non-English languages were generally limited
to information adhering to common patterns across different
Wikipedia language editions (cf. Section 2.2). In this section, we
describe our I18n extensions of the DIEF, which improve I18n
support of the software and introduce customizability features for
language-specific Wikipedia editions. The Greek language is well
suited for a I18n case study as a language with non-Latin characters
and an average amount of articles. The described steps can be
easily adjusted through configuration files for other language
editions14 and thus help to easily produce DBpedia datasets for
each Wikipedia language edition.

All described extensions have been merged into the main
DIEF trunk and are available as open source.15 Furthermore, an
internationalization committee16 was founded to coordinate the
data publishing process of the localized DBpedia editions.

The main rationale for facilitating better internationalization
is to: (a) provide better customization options in extractors, that
need to be customized for each language edition and (b) implement
I18n filters where necessary, that can be plugged into the DIEF and
filter the input and output of some components (cf. Fig. 1).

In order to increase the amount and quality of the produced
triples, the extractors had to be customized and tuned, where
appropriate, for language-specific aspects. As illustrated in Fig. 1
five extractors and four parser have been enhanced, namely:
Disambiguation Extractor extracts disambiguation pages and
can be enabled by defining the language-specific token
used by Wikipedia in the article title (e.g. (disambigua-
tion) for the English Wikipedia17).

Homepage Extractor extracts links to the homepage of a resource by searching for the term homepage as a link
title in the External Links section of an article. Other
languages must provide translations for the homepage and External links terms (e.g. I `o oo and
E  o`  `v o respectively in Greek).
Image Extractor generates links to Wikimedia Commons images.
The extractor has to evaluate copyright templates in
order to exclude non-free images for licensing issues.
Our extractor extension allows one to configure such
language-specific templates.

Generic Infobox Extractor generates triples from infobox templates using properties from the http://dbpedia.org/
property/ namespace. Every Wikipedia language edition
defines different infoboxes and properties and some of

14 http://wiki.dbpedia.org/Internationalization/Guide.
15 https://sourceforge.net/projects/dbpedia/.
16 http://dbpedia.org/Internationalization.
17 http://en.wikipedia.org/wiki/Wikipedia:Disambiguation#Naming_the_
disambiguation_page.

them do not provide any added value (e.g. width and
height of an image, highlight colors, layout information,
etc.), which thus can be configured to be excluded.

Mapping-Based Infobox Extractor extracts triples from infobox
templates that are mapped to the DBpedia ontology and
therefore uses properties from the http://dbpedia.org/
ontology/ namespace. Internationalization is achieved by
defining template mappings (cf. Section 4).

DateTime Parser parses a date from a given string. We extended
this parser in a way such that
customizability of
language-specific months, cardinality and era tokens can
be provided.

Duration Parser parses durations from a given string. Other
languages must provide translations for duration tokens
(e.g.   ` for minutes).

Flag-Template Parser parses custom templates named after a
countrys code (e.g. {{GRE}} for Greece) that display
the countrys flag followed by the country link. Other
languages must provide the corresponding country
article for every flag template (e.g.
for
{{GRE}}).

E`

Unit-Value Parser parses units of measurement from a given
string. Other languages must provide translations for different unit types (e.g. m, meter, metres, metre, , ` ,
` o for the metre unit type).

In addition two new extractors were implemented for the I18n-

DIEF:
Interlanguage-Link Extractor creates RDF triples with the
dbpedia-owl:interlanguageLink predicate between resources across different Wikipedia language edi-
tions. The Interlanguage-Link Extractor was an essential
addition, since DBpedias global naming namespace previously did not allow the representation of articles without an interlanguage link to an English article (cf. Section 5.1).

Template-Parameter Extractor extracts the named parameters18
that are declared by each Wikipedia template. The
Template-Parameter Extractor was envisaged as an addon tool that could facilitate a semi-automatic infobox-to-
ontology mapping (cf. Section 4.2).

To further increase the number of extracted triples, we had to
extract Greek articles without an English translation. By including
all Greek articles, the triple count was increased by 41.7% (see
Table 2 for details). This, however, was not compatible with
DBpedias resource naming convention, as the untranslated articles
could not be matched to the English namespace. This was the
reason for defining language-specific resource namespaces (cf.
Section 5). In this regard, four filters were created:
Page Filter enables the addition of all articles in the article queue,

18 http://en.wikipedia.org/wiki/Help:Template#Handling_parameters.

D. Kontokostas et al. / Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

Fig. 1. The internationalized DBpedia Information Extraction Framework including the I18n filters and the two new extractors.

Article Name Filter enables the use of the original article name

instead of the English translation,

Namespace Filter enables the extraction in a language-specific

namespace and

IRI Filter enables the extraction of resources in the IRI form.

The IRI form was adopted, replacing the commonly used URI,
for reasons discussed in Section 6. As a result the resource
names are differentiated because IRIs allow the use of UTF-8
characters, while URIs are limited to Latin characters and the
percent-encoding of all other characters. For example, the IRI
http://el.dbpedia.org/resource/X`o would be represented in the
URI form as http://el.dbpedia.org/page/%CE%A7%CE%AF%CE%BF%
CF%82.

Virtuoso Universal Server (VUS)19 was chosen as a triple store
for two reasons: firstly, VUS fully supports IRIs and UTF-8 according
to [9] and secondly, VUS allows one to use the same code-base as
the English DBpedia for Linked Data publishing. This code-base was
modified in order to handle IRIs (since the English DBpedia uses
URIs) and to parametrically accept language-specific namespaces
and graphs, apart from the default namespace and graph http://
dbpedia.org (IRI & Namespace Filter). A thorough representation
of the necessary steps is discussed in Section 6.1. Eventually, the
software and Linked Data interfaces were completely compatible
with the standard DBpedia as shown in Fig. 2.

4. Infobox mappings and properties

Among the richest sources of structured information in
Wikipedia articles tapped by DBpedia are infobox templates.20 For
this purpose two kinds of extractors exist, namely: the Generic
Infobox Extractor and the Mapping-Based Infobox Extractor. The
former is straightforward and creates one triple for every infobox
parameter in the form:
1. subject  article resource,
2. predicate  concatenation of the property namespace (i.e. http:
//dbpedia.org/property/) with the parameter name (in camel
case form, removing spaces and underscores) and

3. object  the attribute value.

On the basis of the detection of links in the attribute value
a heuristic decides whether to generate a resource or a literal
value in the object position. Although the Generic Infobox Extractor
extracts all the structured information from an article, it results
in relatively low quality knowledge [3, Section 2.2]. This is
due to the fact that properties do not refer to an ontology
and different infobox templates may use different parameters
referring to the same concept (e.g. birthplace and placeofbirth),
thus creating undesireable synonymy. The occurrence of unwanted
synonymy is potentially increased for languages which use nonLatin or an extended Latin script, where, for example, diacritics
induce additional variation (e.g. in Greek  o ` and
 o  or German umlauts a, u, etc.).

To address this synonymy problem, the manually created
DBpedia Ontology associates the most commonly used infobox
templates with OWL classes arranged in a class hierarchy
and associates infobox parameters with corresponding ontology
properties and expected datatypes.

The ontology was initially created by a small team of engineers
using a closed database. [7] describes the migration to an open
and collaborative Mappings Wiki to crowdsource the ontology
editing as well as the data curation processes. Using this Mappings
Wiki,21 different infobox templates and template parameters
can be collaboratively mapped by the DBpedia community to
their corresponding DBpedia Ontology classes and properties.
The Mapping-Based Infobox Extractor now uses this information
to properly parse template parameter values according to the
datatype specified in the DBpedia Ontology and to classify
resources obtained from the Wikipedia articles based on the
infobox template used and additional classification rules given
in the Mappings Wiki. Since infobox parameters from different
Wikipedia language editions can be aligned with the DBpedia
Ontology, the mapping rules in the wiki represent a common
framework in which to semantically annotate and integrate
infoboxes across different Wikipedia language editions (cf. Fig. 3).
In the context of the work described in this article almost all 140
Greek Wikipedia infobox template mappings have been created.

19 http://virtuoso.openlinksw.com/.
20 http://en.wikipedia.org/wiki/Help:Infobox.

21 http://mappings.dbpedia.org.

Fig. 2. HTML representation using TCN rules.

` ` (1)  o `  `, (163),  o 
`_ ` (1),  o ` ` (1) across six different
templates due to orthography, case and gender variations.
We also detected the presence of multiple templates for the
same topic, e.g. 50 articles using the template Infobox Band and
80 using the template Mo `o `o  which is the
Greek equivalent of the English band infobox. This indicates the
need for better coordination of the Greek Wikipedia community.
The results of our study were directly announced to the Greek
Wikipedia community.22 They were perceived as helpful and
initiated a collective effort to correct the parameters and the
templates. This feedback will eventually lead to better DBpedia
results and also shows DBpedias contribution to Wikipedia.

An effort was also initiated to rewrite Greek templates on
the Greek Wikipedia taking not only the corresponding English
template conventions into consideration but the DBpedia ontology
as well. In this context, the DBpedia Ontology is used as a common
vocabulary, which could well develop as a standard ontology for
data representation across languages within Wikipedia. Although
it is per se difficult to evaluate an ontology, the employment and
reuse of the DBpedia Ontology by a second community can be
evaluated as positive according to the criteria Adaptability and
Clarity of [10, p. 56] and therefore shows the usefulness and
adequacy of the DBpedia Ontology.

4.2. The Template-Parameter Extractor

Even though the extended mapping system is much more convenient than the previous one, the process still involves manual
operations. In order to facilitate the mapping effort further, tools
can be created in order to validate and (semi-)automatically link
parameters and ontology properties. A step in this direction is
the introduction of a new extractor that we implemented in the
I18n-DIEF, the Template-Parameter Extractor (TPE). TPE extracts
the parameters declared by each template by parsing them directly from the template pages and generates RDF triples using the

22 http://el.wikipedia.org/wiki/B  :DBpedia/M   `_15-12.

Fig. 3. Depiction of the mapping from the Greek and English Wikipedia templates
about books to the same DBpedia Ontology class.

4.1. A Greek Wikipedia case study

During the implementation process the following issues were
tackled: (a) the infobox templates may not contain all the desired
information, (b) some templates may be more abstract, and thus
cannot map to a specific class, (c) some templates are missing and
(d) some templates are not used inside the articles. In order to
statistically verify these issues, a slightly modified version of the
Generic Infobox Extractor was developed. This Extractor generates
a table with a row for every property, consisting of the following
columns: article name, template name, raw property name. Using
this helper table we obtained statistics about parameter counts,
template counts and property counts per template. The results
revealed, as was expected, many misspellings which led to errors
and variations in property naming of the same entity across
different templates.
For example, the entity record label was referred to by
the following parameter variations:  o `  `
(4), 	 o ` (99),  o ` (46), 	 o 

D. Kontokostas et al. / Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

Fig. 4.

Infobox-to-ontology mapping statistics using the Template-Parameter Extractor as a core dataset.

dbpedia-owl:template UsesParameter predicate (cf. Listing 2). Template pages contain the template definitions and declare
their parameters in wiki format using three consequent brackets
({{{parameter name[\][default value]}}}). Template parameters are
actually the allowed infobox properties to be used by articles.
The infobox-to-ontology mapping statistics23 is an existing
application of the Template-Parameter Extractor (cf. Fig. 4). The
offline output of this extractor and the statistics from Section 4.1
are used to demonstrate the mapping coverage of the infobox
templates and their parameters as well as how often parameters
occur in a language-specific Wikipedia edition. Prospectively,
this extractor can be exploited easily to develop semi-automatic
tools for data curation, when combined with the DBpedia Live
Extraction. All available infobox properties can for example be
curated directly in a graphical mappings editor and periodic checks
are possible, for whether previously defined mappings are using a
non-valid parameter (e.g. if a template is edited on Wikipedia and
a new unmapped parameter is introduced).

3. The extracted non-English articles cannot provide information
other than their abstract and label, as everything else either
conflicts with an English definition or creates multiple defini-
tions.

4. The English Wikipedia is treated as the authority, which may
not be the case for language-specific articles. For instance,
the article about the Eiffel Tower in the French Wikipedia24
contains more detailed information. Up to now, though, the
English version of DBpedia was the only available option.
It is more appropriate that resources in non-English languages
are published according to the Wikipedias naming strategy, i.e.
with the original article name, using a language-specific namespace (e.g. http://el.dbpedia.org/ for Greek). As new languages are
publishing their data, the English DBpedia might be transferred
into http://en.dbpedia.org/ and the default namespace could be
used solely for the Cross-language knowledge fusion [3, p. 164].

5. Language-specific design of DBpedia resource identifiers

5.1. Inter-DBpedia linking

Currently, the fused DBpedia extracts non-English Wikipedia
articles only when they provide an English interlanguage link
and the created resources use the default DBpedia namespace
(cf. Section 2.2). Although this approach minimizes the use of
non-Latin characters in resource identifiers, it has the following
drawbacks:
1. The merging is solely based on the link from the non-English
resource to the English article. It has been shown that such
links are more appropriate if the interlanguage links go in
both directions [11]. Because we introduce owl:sameAs (a
transitive property) to link between language editions, we
especially conducted measurements to test the integrity of our
design (cf. Section 5.1).

2. A large number of articles, without an English translation link,
are discarded. For instance, the DIEF produces 30% less triples
for the Greek DBpedia (cf. Table 2) than the I18n-DIEF.

Using the language-specific resource naming approach, an
interlanguage link (ILL) can be utilized to connect resources
across different DBpedia language editions and thus creates a
multilingual semantic space. To accomplish this, a new extractor
was developed for the I18n-DIEF, called Interlanguage-Link (ILL)
Extractor. It extracts ILLs and generates RDF triples using the
dbpedia-owl:interlanguageLink predicate. Using these
links as a raw dataset, we examine whether they can be used to
generate owl:sameAs links between resources extracted from
different Wikipedia language editions. The ILL correspondence
is not always reliable since on following ILLs across different
languages, conflicts may appear [12], as the following example
illustrates:
(valve)  it:Rubinetto  es:Grifo 

en:Tap
en:Griffin

23 http://mappings.dbpedia.org/index.php/Mapping_Statistics.

24 http://fr.wikipedia.org/wiki/Tour_Eiffel [accessed on 2011/11/07].

dbp-tpl:Infobox_book dbp-owl:templateUsesParameter name@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter title_orig@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter image@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter image_caption@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter author@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter illustrator@en
dbp-tpl:Infobox_book dbp-owl:templateUsesParameter cover_artist@en

Listing 2: Example dataset extracted with TPE for infobox book.

Table 1
The ILL Graph Properties for all edges and for the subgraph of two-way edges. The calculations were performed with
the open-source R Project for Statistical Computing (http://www.r-project.org/).

Property
Graph type
Graph order (number of nodes)
Graph size (number of links)
Connected components (weak)
Conflicts (paths between two English articles)
Different (English) articles in conflicts
Total English articles (as of August 2008)

Graph with all links
Directed

47,487,880

5400 (0.21%)
2.5M

Graph restricted to two-way links
Undirected

(2) 23,001,554

(2) 16,063
1900 (0.07%)

We performed an analysis on the ILLs, which form a directed
graph (V , E), where V is the set of Wikipedia pages as nodes
and E is the set of ILLs between two pages which define the
edges. Wikipedia mentions the following editor guideline: An
interlanguage link is mainly suitable for linking to the most closely
corresponding page in another language25. Thus, each concept,
represented as a set of Wikipedia pages, can be defined as a
subgraph consisting of the corresponding pages in each language.
When this subgraph contains at most one article from each
language, the correspondence is consistent; otherwise we consider
it a conflict situation. Using the simplified dataset provided by
[12],26 the graph properties were re-calculated and presented in
Table 1. From the results, we can estimate the extent of conflicts
and whether the conflicts are reduced if the ILL graph is restricted
to two-way links only. The conflict analysis was performed using
the English articles as the starting point for the measurements
since they constitute the largest dataset.

We observe that the relative error is very small: 0.21% of
the total number of English articles are participating in conflicts,
creating a total number of 380,902 conflicts. On restricting the
graph to two-way links, 52,569 nodes and 1484,772 edges are
discarded. However, the discarded nodes (5.9%) are responsible for
65% of the different English articles participating in conflicts, and
the discarded links (3.1%) are responsible for 91.6% of the conflicts
in English articles. The fact that the connected components are
reduced from 34,623 to 34,412, i.e. 0.61%, is an indication that
the graph structure does not change significantly if the graph is
restricted to two-way edges. Even though the relative error is
small, by removing the one-way edges from ILLs, the conflicts are
further reduced to 0.07% of the total number of English articles
and the conflicts are reduced to 8.4%. The reason for conducting
the measurements was the strong semantical implications of
owl:sameAs, as it produces equivalence classes in a multilingual
network of language-specific DBpedia. This is why it is necessary
to reduce any errors to a minimum. Our analysis indicates that the
created conflicts are not significant if the owl:sameAs triples are
considered only for two-way ILL edges.

In order to implement this analysis a new tool was created, that
utilizes the ILL Extractor output from two languages, and generates
owl:sameAs triples only for two-way edges. An example of a link
extracted in this way is:

Fig. 5. The Greek DBpedia in the I18n LOD Cloud.

dbp-el:  ov` owl:sameAs dbp:Thessaloniki
Additionally to the inter-DBpedia linking, another tool was
developed that transitively links a non-English DBpedia to all
the external LOD datasets that are linked to the English DBpedia
(cf. Listing 3). Even though this could be accomplished using a
SPARQL query [13], this procedure would consume substantial
server resources in querying and loading all the datasets. Our tool
does not have this problem because the triples are created offline,
directly in the N-Triples format.

In total, 33,148 owl:sameAs links to the English DBpedia were
established (2339 links were only one-way and have been removed
(6.59%)). As a result of our inter-DBpedia linking, a total of 101,976
additional owl:sameAs links were created, linking the Greek
DBpedia with 20 external LOD datasets27 (cf. Fig. 5).

6. International Resource Identifiers

Linked datasets are expected to provide machine processable
as well as user readable and interpretable content (e.g. an HTML

25 http://meta.wikimedia.org/wiki/Help:Interwiki_linking#Interlanguage_links.
26 http://wikitools.icm.edu.pl/m/dumps/.

27 http://el.dbpedia.org/en/datasets.

D. Kontokostas et al. / Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

For the English link:
dbp:Thessaloniki rdf:type geonames:734077

and the owl:sameAs link
dbp-el:  ov`i owl:sameAs dbp:Thessaloniki
the following link is produced:
dbp-el:  ov`i rdf:type geonames:734077

Listing 3: Example of the transitive linking to the LOD Cloud.

representation) [14]. However, the requirements for readability
lexvos presentation of the term door28 and the
(see e.g.
translation links), and for manual SPARQL query construction (cf.
Listing 4) in non-Latin languages such as Greek cannot be satisfied
using URIs. Therefore, the only option currently available is to
use International Resource Identifiers (IRIs) as defined in RFC
3987 [15].

IRIs are known to impose security issues, as certain characters
from different languages appear identical to users (e.g. the Greek
and Latin A, B, K characters correspond to different IRIs).
In the Linked Data case, however, IRIs are mainly processed by
machines and thus do not represent a security issue in most
cases. Also, security issues are mitigated, since we do not advocate
the use of non-Latin characters in the domain name part of
IRIs and the internationalized datasets are published just by
one authoritative source, i.e. the Greek DBpedia project under
the domain el.dbpedia.org. Another issue concerning the IRI
form is the lack of support by all triple serialization formats [9],
which introduces difficulties in defining TCN rules [6] for IRI dereferencing purposes.

6.1. Transparent Content Negotiation rules

The DBpedia Linked Data publication code is designed according to W3C guidelines [14] and is responsible for handling all the
URI requests. Depending on the client request, the client can get
either a human readable XHTML/RDFa representation of the re-
source, or a serialization in the desired format (e.g. RDF/XML, N-
Triples, n3, etc.). This is achieved by defining proper Transparent Content Negotiation (TCN) rules (RFC 2295) for 303 content
redirection. The default redirection for a DBpedia resource (http:
//dbpedia.org/resource/) is the XHTML/RDFa representation and
the client is redirected to http://dbpedia.org/page/. For a resource
serialization, the client may request the http://dbpedia.org/data/
 URL for an RDF/XML serialization, or append a custom extension (i.e. n3) for a specific serialization format (e.g. http://dbpedia.
org/data/Thessaloniki.n3). All the resource serialization requests
are served by redirecting to an on-the-fly generated SPARQL DESCRIBE query [13, Section 10.4], exported in the desired serialization format. However, in order to limit the page size, the XHTM-
L/RDFa presentation is programmatically created. This is achieved
by using multiple SPARQL SELECT queries, which limit the number
of objects for every predicate to a maximum (i.e. 200).

In the I18n-DIEF, the DBpedia Linked Data publication sourcecode was modified in order to optionally serve resources from a different domain and graph (i.e. http://el.dbpedia.org). Additionally,
changes were also applied in order to optionally serve IRIs instead
of URIs. However, IRI content negotiation cannot be implemented
as straightforwardly as URI content negotiation for two reasons: (1)
the triple-store resource storage implementation and (2) the definition of the HTTP protocol (RFC 2616) [16]. Current triple-store

implementations store the resource identifiers as strings; thus an
IRI (containing non-Latin characters) and a corresponding URI referring to the same resource will not be equal. In the frame of
the HTTP protocol, Section 3.2 of the RFC states that the HTTP accepts only URIs, thus when a resource is requested, the server can
only accept a URI request. Since the resources are stored as strings
and the server can only accept URIs, three scenarios can be distin-
guished:
3. Resources are stored in the URI form: both the HTTP request
and the data are in the same format and everything will work
as expected.

3. Resources are stored in the IRI form: The URI contained in
the HTTP request for a certain resource has to be decoded
into an IRI
in order to be used subsequently. Since the
XHTML/RDFa representation is programmatically created, the
request decoding can be easily handled. Other serializations,
however, must be handled with care because the decoding must
take place inside the TCN rules.

3. Resources are stored in mixed form: While this may be a rare
case, the XHTML/RDFa representation could be accomplished
using a SPARQL UNION of the normal and the decoded resource.
For other serializations, the SPARQL DESCRIBE function cannot
deliver all the triples and the only solution would be to
programmatically re-implement the DESCRIBE function.
The I18n DBpedia Linked Data publication code is parameteriz-
able, accepting both URIs and IRIs. Thus the same code is compatible for an international DBpedia (using either IRIs or URIs) and the
current fused DBpedia (using URIs). Furthermore, certain problems
which appeared in cases where the UTF-8 characters were not en-
coded/decoded correctly were resolved. These problems were not
addressed before because there was no need to do so, as the previous version of DBpedia was restricted to Latin characters.

By encoding/decoding the HTTP request, we managed to work
around the imposed RFC limitations and provide de-referenceable
IRIs that could not be accomplished otherwise (cf. Fig. 6).

6.2. IRI serialization

Among the most popular triple-serialization formats only
RDF/JSON (informal), RDFa [17] and Notation 3 [18] are fully
IRI compatible. N-Triples [19] and N-Quads [20] do not support
IRIs at all, since they use seven-bit US-ASCII character encoding.
Turtle [21] and RDF/XML [22] provide partial IRI support as their
grammar definition is not fully mapped to the IRI grammar [15,
page 7]. In particular:
 In the frame of the RDF/XML representation, subjects and objects
are defined as XML Attributes, and therefore can be serialized
as IRIs. Predicates, on the other hand, must be declared as XML
Elements. According to the XML specification, XML Elements
have many restrictions29 on the allowed characters.

28 http://www.lexvo.org/page/term/eng/door.

29 http://www.w3.org/TR/REC-xml/#NT-Name.

dbp: A   ?p ?o .

PREFIX dbp: <http : / / el . dbpedia . org / resource / >
SELECT ?p ?o

1 # SPARQL query with I R I s

4 WHERE {

6 # SPARQL query with URIs

9 WHERE
dbp:

Listing 4: Simple SPARQL query about Athens (Greek:A  v) using IRIs and URIs.

\%CE\%91\%CE\%B8\%CE\%AE\%CE\%BD\%CE\%B1 ?p ?o .

PREFIX dbp: <http : / / el . dbpedia . org / resource / >
SELECT ?p ?o

the serialization format (e.g. URIs for N-Triples). The same solution
applies for content negotiation rules as well, as no special cases
have to be implemented to handle resource-encoding/decoding for
the HTTP URI request.

7. Statistics and evaluation

In this section we will look at the attained improvements due to
the I18n revision of the DIEF. The results with regard to extracted
triples for all available extractors (presented in Table 2) allow
comparison between the Greek and the English DBpedia editions.
Extractions of the Greek Wikipedia with the I18n-DIEF refer to the
same Wikipedia dump as the Greek DBpedia v 3.5.1. The final result
of our efforts is presented in the column labeled I18n-aa (Greek
DBpedia I18n-DIEFall articles).

In the last row, the total numbers of extracted triples per
page (Triples/PageID) are listed. Despite the relatively small size of
the Greek Wikipedia, the results indicate the equivalence of the
Greek DBpedia to the English DBpedia as regards average extracted
triples per page (28.52 compared with 29.12). Furthermore,
there is an increase by 62.6% in total triples,31 when the I18nDIEF is compared to the standard DBpedia DIEF for Greek (EL-
3.5.1). The percentage increase is damped by the fact that many
potential triples were not extracted from raw infobox properties
(cf. Section 3), titles and links (DIEF restricted them only in article
pages) as they did not offer any added value. To compare our
extraction on a per extractor basis with the English version, we
calculated a %-Diff using the following formula:
TriplesEN3.5.1
PageIDsEN3.5.1
The resulting percentage signifies the increase in triples per page
per extractor when comparing the English and the Greek version.
Although many factors influence the calculated %-Diff (e.g. the
implementation of the extractors), the value can be considered
as an indicator for the differences in the structure of the English
and the Greek Wikipedia. For instance: (a) 57% in ontology
types means that infoboxes are used less frequently in the Greek
Wikipedia, (b) 65% in Raw Infobox Property usage means that
infoboxes in the Greek Wikipedia do not use as many properties
as in the English Wikipedia. The issues (a) and (b) may be verified
by manually examining translated articles directly.

 TriplesI18naa

PageIDsI18naa

The impact of the I18n-DIEF in the DBpedia 3.7 release is
depicted in Fig. 7. The percentage increase in triple count from the
initial DBpedia 3.7 release to DBpedia 3.7 with the I18n extensions
was 61.26%, just by enabling the Article Name Filter (cf. Section 3).
As Fig. 7 illustrates, this increase refers only to the 15 localized
languages and ranges from 13.05% in Gabon (ga) to 84.27% in
Russian (ru). This means that despite all the I18n extensions
that each language had already implemented, there exist many
articles that do not have an English translation. With the previous
framework, all this information would have been discarded.

 1.

I18n Content Negotiation for URI and IRI dereferencing (modification of [14,

Fig. 6.
Section 4.3]).

 Turtle can serialize IRIs but only when using absolute resources.
The Turtle abbreviated form has certain limitations30 that affect
both IRIs and URIs.
Despite the aforementioned limitations, triple stores are not
so strict with the file formats and invalid serialization can be
imported in some cases. However, depending on the format and
the implementation, triple stores do not accept all IRI characters.
For example, Virtuoso may accept IRIs in N/Triples and N/Quads
files but without the > character; similar cases apply for other
triple-store implementations (cf. [9, table 5]). These characters
were handled by encoding them using the URI %-encoding.

In order to solve IRI serialization issues, existing serialization
standards should be adapted or new ones may be created. Apart
from the serialization standards, triple stores could also provide a
global solution to IRI issues by treating a URI and its corresponding
IRI representation uniformly. Triple stores have full knowledge or
control [15, Section 5.1] of the resources, so it could be considered
safe to use one of the two as an internal storage representation and
transform input/output accordingly. This solution has, however,
one limitation. Allowing the accepting queries in both formats may
require disambiguation in cases when a SPARQL query contains
the URI escape character % in a resource. The advantage of this
approach is to choose the resource representation according to

30 http://www.w3.org/TeamSubmission/turtle/#resource.

31 1677,656 compared to 2679,214.

D. Kontokostas et al. / Web Semantics: Science, Services and Agents on the World Wide Web 15 (2012) 5161

Table 2
Statistics of the extracted triples. Columns: (EN-3.5.1) English DBpedia v 3.5.1, (EL-3.5.1) Greek
DBpedia v 3.5.1, (I18n-ota) Greek DBpedia I18n-DIEFonly translated articles, (I18n-aa) Greek
DBpedia I18n-DIEFall articles, (%-Diff) percentage difference per extractor (positive values lean
towards the Greek DBpedia).

EN-3.5.1

EL-3.5.1

I18n-ota

I18n-aa

%-Diff

Abstracts
Extended
Short
Category
Labels

Articles
Geo coordinates
Images
Links
Disambiguation
External
Homepages
Page
To article
Ontology infobox
Property usage
Specific properties
Types
Page IDs
Person data

Raw infobox
Property usage
Property definitions
Redirects
Revision IDs
Titles
Totals
Triples
Page IDs
Triples/page ID

10,925,705

119,077,682
21,997,875


1,297,366

1,197,756

1,562,772

11,135,755

9,042,227

38,119,014


257,869,814
9,042,227

1,677,656

1,890,436

2,679,214

Fig. 7. Percentage of increase in triples, achieved just by enabling the Article Name Filter for the 15 localized languages in the DBpedia 3.7 release.

8. Conclusions

Acknowledgments

With the maturing of Semantic Web technologies proper
support for internationalization is a crucial issue. This particularly
involves the internationalization of resource identifiers, RDF
serializations and corresponding tool support. The Greek DBpedia
is the first step towards Linked Data internationalization and the
first successful attempt to serve Linked Data with de-referenceable
IRIs that also serves as a guide for LOD publishing in non-Latin
languages. Apart from the de-referenceable IRI solution, this work
provides the tools for a truly international DBpedia, as Greek
is a comparatively complex language with non-Latin characters
and non-standard punctuation. Since the Greek DBpedia provides
qualitative information comparable to that from the English
DBpedia, our I18n-DIEF can be easily transferred to other nonLatin Wikipedia editions and can (with slight language-specific
adoptions) be expected to give similar qualitative results.

As a result of our findings, the main DBpedia edition can also
significantly contribute towards the IRI adoption. The switch of
the English DBpedia edition as one major Linked Data hub to use
IRIs will encourage other Linked Data providers to follow. Already
17.8% (i.e. 1679,124 out of 9485,630 in the 3.6 release) of all
resources contain the % escape character and can therefore be more
simply written as IRIs.

A follow-up of this work is the institutionalization of the
Internationalization Committee32 (IC). The IC could play a stronger
role in the coordination and management of the various language
editions of DBpedia as well as maintaining and revising the best
practices laid out in this article. The committee should establish
a common platform for sharing, hosting and collaboratively
integrating various language editions. It should also agree upon
the required technical specifications for I18n DBpedia editions as
well as providing the appropriate documentation (guidelines and
support documents) for the I18n operation.

Another area of research is the more efficient utilization of
the Wikipedia interlanguage links. The approach discussed in
Section 5.1 was safe and straightforward. A further analysis of the
conflict situations and how they could be resolved will be of great
importance both for Wikipedia and the internationalization of the
Semantic Web. The conflict situations analysis could also provide
new data and make us re-examine the use of owl:sameAs  as
too strong a semantic implication  with other vocabularies (i.e.
SKOS). We could also utilize the conflicts, which are now discarded,
by adding rdfs:seeAlso links.

Infobox Mappings will play a central role in the integration
and evolution of international DBpedia editions. Developing better
mapping tools is a crucial strategy for facilitating this process. The
new Template-Parameter Extractor (Section 4.2) can be utilized
in order to assist the ontology-to-infobox mappings definition
and organization. In the next few months we will obtain first
indications on the results of the Wikipedia infobox restructuring
and (re-)mapping (Section 4.1) in future DBpedia releases.

DBpedias goal is to make it easier for the amazing amount
of information in Wikipedia to be used in new and interesting
ways and to inspire new mechanisms for navigating, linking
and improving the encyclopedia itself.33 Our work provides new
tools for improving Wikipedia, because DBpedia may serve as an
important statistical diagnostic tool (cf. Sections 4.1 and 7) for
Wikipedia that helps to identify and resolve existing and emerging
issues.

This project would not have been completed without the
continuous support of the DBpedia team, the students and the
staff of the Webscience M.Sc., Mr. Konstantino Stampouli,34 Greek
Wikipedia administrator, and the Webscience M.Sc. program of
Aristotle University of Thessaloniki that facilitated this effort. The
administrative and financial support of the municipality of Veria is
gratefully acknowledged. This work was also partially supported
by a grant from the European Unions 7th Framework Programme
provided for the project LOD2 (GA No. 257943).
